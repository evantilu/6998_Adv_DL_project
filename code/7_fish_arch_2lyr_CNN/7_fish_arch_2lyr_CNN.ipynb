{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold CV with 2-layer CNN as the best performance model in the Fish Classification Paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and env settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.min_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables/parameters used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../../data/home_sale_data_324_features_5_class.csv'\n",
    "\n",
    "ckpt_path = \"./ckpt/5_cls_lr005/\"\n",
    "os.makedirs(ckpt_path, exist_ok=True)\n",
    "\n",
    "num_classes = 5\n",
    "lr = 0.005\n",
    "epochs = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>TotalSF</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>Total_Bathrooms</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>YrBltAndRemod</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>Foundation_PConc</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>hasfireplace</th>\n",
       "      <th>ExterQual_Gd</th>\n",
       "      <th>BsmtQual_Ex</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>HeatingQC_Ex</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>Total_porch_sf</th>\n",
       "      <th>BsmtFinType1_GLQ</th>\n",
       "      <th>KitchenQual_Ex</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>GarageFinish_Fin</th>\n",
       "      <th>...</th>\n",
       "      <th>BsmtExposure_No</th>\n",
       "      <th>Neighborhood_OldTown</th>\n",
       "      <th>Foundation_BrkTil</th>\n",
       "      <th>GarageFinish_None</th>\n",
       "      <th>GarageCond_None</th>\n",
       "      <th>GarageQual_None</th>\n",
       "      <th>GarageType_None</th>\n",
       "      <th>MSSubClass_30</th>\n",
       "      <th>LotShape_Reg</th>\n",
       "      <th>PavedDrive_N</th>\n",
       "      <th>Foundation_CBlock</th>\n",
       "      <th>MSZoning_RM</th>\n",
       "      <th>HeatingQC_TA</th>\n",
       "      <th>CentralAir_N</th>\n",
       "      <th>GarageType_Detchd</th>\n",
       "      <th>MasVnrType_None</th>\n",
       "      <th>GarageFinish_Unf</th>\n",
       "      <th>BsmtQual_TA</th>\n",
       "      <th>FireplaceQu_None</th>\n",
       "      <th>KitchenQual_TA</th>\n",
       "      <th>ExterQual_TA</th>\n",
       "      <th>dummy_1</th>\n",
       "      <th>dummy_2</th>\n",
       "      <th>dummy_3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.761086</td>\n",
       "      <td>0.553543</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.252530</td>\n",
       "      <td>0.431452</td>\n",
       "      <td>0.952632</td>\n",
       "      <td>0.341235</td>\n",
       "      <td>0.500568</td>\n",
       "      <td>0.963768</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.740833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.428916</td>\n",
       "      <td>0.290505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.496274</td>\n",
       "      <td>0.277790</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.415789</td>\n",
       "      <td>0.210855</td>\n",
       "      <td>0.288718</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.350917</td>\n",
       "      <td>0.174730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>0.269495</td>\n",
       "      <td>0.389345</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.638216</td>\n",
       "      <td>0.405415</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.252530</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.283843</td>\n",
       "      <td>0.410625</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.674300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mid-hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.425781</td>\n",
       "      <td>0.212748</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.251426</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.557895</td>\n",
       "      <td>0.328135</td>\n",
       "      <td>0.470124</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.359354</td>\n",
       "      <td>0.411166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low-mid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OverallQual  GrLivArea   TotalSF  GarageCars  Total_Bathrooms  \\\n",
       "1388     0.777778   0.761086  0.553543         0.6         0.252530   \n",
       "2193     0.555556   0.496274  0.277790         0.4         0.001104   \n",
       "616      0.222222   0.350917  0.174730         0.0         0.163872   \n",
       "2216     0.666667   0.638216  0.405415         0.6         0.252530   \n",
       "1290     0.444444   0.425781  0.212748         0.2         0.251426   \n",
       "\n",
       "      GarageArea  YrBltAndRemod  TotalBsmtSF  1stFlrSF  YearBuilt  FullBath  \\\n",
       "1388    0.431452       0.952632     0.341235  0.500568   0.963768      0.50   \n",
       "2193    0.268817       0.415789     0.210855  0.288718   0.304348      0.25   \n",
       "616     0.000000       0.178947     0.269495  0.389345   0.304348      0.25   \n",
       "2216    0.548387       0.857895     0.283843  0.410625   0.898551      0.50   \n",
       "1290    0.193548       0.557895     0.328135  0.470124   0.695652      0.25   \n",
       "\n",
       "      YearRemodAdd  Foundation_PConc  TotRmsAbvGrd  hasfireplace  \\\n",
       "1388      0.933333               1.0      0.740833           1.0   \n",
       "2193      0.750000               0.0      0.516936           0.0   \n",
       "616       0.000000               0.0      0.421336           0.0   \n",
       "2216      0.783333               1.0      0.674300           1.0   \n",
       "1290      0.300000               0.0      0.421336           0.0   \n",
       "\n",
       "      ExterQual_Gd  BsmtQual_Ex  Fireplaces  HeatingQC_Ex  MasVnrArea  \\\n",
       "1388           1.0          0.0    0.293793           1.0    0.428916   \n",
       "2193           0.0          0.0    0.000000           1.0    0.000000   \n",
       "616            0.0          0.0    0.000000           0.0    0.000000   \n",
       "2216           0.0          0.0    0.293793           1.0    0.000000   \n",
       "1290           0.0          0.0    0.000000           0.0    0.359354   \n",
       "\n",
       "      Total_porch_sf  BsmtFinType1_GLQ  KitchenQual_Ex  OpenPorchSF  \\\n",
       "1388        0.290505               0.0             0.0     0.226456   \n",
       "2193        0.460231               0.0             0.0     0.000000   \n",
       "616         0.069808               0.0             0.0     0.000000   \n",
       "2216        0.438068               0.0             0.0     0.203123   \n",
       "1290        0.411166               0.0             0.0     0.000000   \n",
       "\n",
       "      GarageFinish_Fin  ...  BsmtExposure_No  Neighborhood_OldTown  \\\n",
       "1388               1.0  ...              0.0                   0.0   \n",
       "2193               0.0  ...              1.0                   0.0   \n",
       "616                0.0  ...              1.0                   0.0   \n",
       "2216               0.0  ...              0.0                   0.0   \n",
       "1290               0.0  ...              0.0                   0.0   \n",
       "\n",
       "      Foundation_BrkTil  GarageFinish_None  GarageCond_None  GarageQual_None  \\\n",
       "1388                0.0                0.0              0.0              0.0   \n",
       "2193                1.0                0.0              0.0              0.0   \n",
       "616                 1.0                1.0              1.0              1.0   \n",
       "2216                0.0                0.0              0.0              0.0   \n",
       "1290                0.0                0.0              0.0              0.0   \n",
       "\n",
       "      GarageType_None  MSSubClass_30  LotShape_Reg  PavedDrive_N  \\\n",
       "1388              0.0            0.0           0.0           0.0   \n",
       "2193              0.0            0.0           1.0           0.0   \n",
       "616               1.0            1.0           1.0           1.0   \n",
       "2216              0.0            0.0           1.0           0.0   \n",
       "1290              0.0            0.0           1.0           0.0   \n",
       "\n",
       "      Foundation_CBlock  MSZoning_RM  HeatingQC_TA  CentralAir_N  \\\n",
       "1388                0.0          0.0           0.0           0.0   \n",
       "2193                0.0          0.0           0.0           0.0   \n",
       "616                 0.0          0.0           1.0           1.0   \n",
       "2216                0.0          0.0           0.0           0.0   \n",
       "1290                1.0          0.0           0.0           0.0   \n",
       "\n",
       "      GarageType_Detchd  MasVnrType_None  GarageFinish_Unf  BsmtQual_TA  \\\n",
       "1388                0.0              0.0               0.0          0.0   \n",
       "2193                1.0              1.0               1.0          0.0   \n",
       "616                 0.0              1.0               0.0          1.0   \n",
       "2216                0.0              1.0               1.0          0.0   \n",
       "1290                0.0              0.0               0.0          1.0   \n",
       "\n",
       "      FireplaceQu_None  KitchenQual_TA  ExterQual_TA  dummy_1  dummy_2  \\\n",
       "1388               0.0             0.0           0.0      0.0      0.0   \n",
       "2193               1.0             1.0           1.0      0.0      0.0   \n",
       "616                1.0             1.0           1.0      0.0      0.0   \n",
       "2216               0.0             0.0           1.0      0.0      0.0   \n",
       "1290               1.0             1.0           1.0      0.0      0.0   \n",
       "\n",
       "      dummy_3    label  \n",
       "1388      0.0       hi  \n",
       "2193      0.0      mid  \n",
       "616       0.0      low  \n",
       "2216      0.0   mid-hi  \n",
       "1290      0.0  low-mid  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(file)\n",
    "\n",
    "'''suffle rows randomly'''\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "labels = data['label']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>TotalSF</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>Total_Bathrooms</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>YrBltAndRemod</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>Foundation_PConc</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>hasfireplace</th>\n",
       "      <th>ExterQual_Gd</th>\n",
       "      <th>BsmtQual_Ex</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>HeatingQC_Ex</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>Total_porch_sf</th>\n",
       "      <th>BsmtFinType1_GLQ</th>\n",
       "      <th>KitchenQual_Ex</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>GarageFinish_Fin</th>\n",
       "      <th>...</th>\n",
       "      <th>Neighborhood_IDOTRR</th>\n",
       "      <th>BsmtExposure_No</th>\n",
       "      <th>Neighborhood_OldTown</th>\n",
       "      <th>Foundation_BrkTil</th>\n",
       "      <th>GarageFinish_None</th>\n",
       "      <th>GarageCond_None</th>\n",
       "      <th>GarageQual_None</th>\n",
       "      <th>GarageType_None</th>\n",
       "      <th>MSSubClass_30</th>\n",
       "      <th>LotShape_Reg</th>\n",
       "      <th>PavedDrive_N</th>\n",
       "      <th>Foundation_CBlock</th>\n",
       "      <th>MSZoning_RM</th>\n",
       "      <th>HeatingQC_TA</th>\n",
       "      <th>CentralAir_N</th>\n",
       "      <th>GarageType_Detchd</th>\n",
       "      <th>MasVnrType_None</th>\n",
       "      <th>GarageFinish_Unf</th>\n",
       "      <th>BsmtQual_TA</th>\n",
       "      <th>FireplaceQu_None</th>\n",
       "      <th>KitchenQual_TA</th>\n",
       "      <th>ExterQual_TA</th>\n",
       "      <th>dummy_1</th>\n",
       "      <th>dummy_2</th>\n",
       "      <th>dummy_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.0</td>\n",
       "      <td>2911.0</td>\n",
       "      <td>2911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.565136</td>\n",
       "      <td>0.542763</td>\n",
       "      <td>0.293431</td>\n",
       "      <td>0.353143</td>\n",
       "      <td>0.200863</td>\n",
       "      <td>0.317285</td>\n",
       "      <td>0.660811</td>\n",
       "      <td>0.326706</td>\n",
       "      <td>0.487996</td>\n",
       "      <td>0.719569</td>\n",
       "      <td>0.391876</td>\n",
       "      <td>0.570892</td>\n",
       "      <td>0.447956</td>\n",
       "      <td>0.542786</td>\n",
       "      <td>0.512882</td>\n",
       "      <td>0.335967</td>\n",
       "      <td>0.087255</td>\n",
       "      <td>0.171718</td>\n",
       "      <td>0.511508</td>\n",
       "      <td>0.159954</td>\n",
       "      <td>0.203170</td>\n",
       "      <td>0.290622</td>\n",
       "      <td>0.069392</td>\n",
       "      <td>0.176542</td>\n",
       "      <td>0.245620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031261</td>\n",
       "      <td>0.652697</td>\n",
       "      <td>0.082102</td>\n",
       "      <td>0.106493</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.053590</td>\n",
       "      <td>0.047750</td>\n",
       "      <td>0.637582</td>\n",
       "      <td>0.073514</td>\n",
       "      <td>0.423222</td>\n",
       "      <td>0.158708</td>\n",
       "      <td>0.293370</td>\n",
       "      <td>0.066644</td>\n",
       "      <td>0.266919</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.421848</td>\n",
       "      <td>0.439368</td>\n",
       "      <td>0.487118</td>\n",
       "      <td>0.511852</td>\n",
       "      <td>0.616627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.155988</td>\n",
       "      <td>0.125193</td>\n",
       "      <td>0.119772</td>\n",
       "      <td>0.152244</td>\n",
       "      <td>0.133043</td>\n",
       "      <td>0.143976</td>\n",
       "      <td>0.242686</td>\n",
       "      <td>0.131930</td>\n",
       "      <td>0.131369</td>\n",
       "      <td>0.219351</td>\n",
       "      <td>0.138140</td>\n",
       "      <td>0.348189</td>\n",
       "      <td>0.497369</td>\n",
       "      <td>0.128554</td>\n",
       "      <td>0.499920</td>\n",
       "      <td>0.472409</td>\n",
       "      <td>0.282257</td>\n",
       "      <td>0.181727</td>\n",
       "      <td>0.499953</td>\n",
       "      <td>0.218350</td>\n",
       "      <td>0.163524</td>\n",
       "      <td>0.454127</td>\n",
       "      <td>0.254163</td>\n",
       "      <td>0.183129</td>\n",
       "      <td>0.430528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174051</td>\n",
       "      <td>0.476195</td>\n",
       "      <td>0.274568</td>\n",
       "      <td>0.308520</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.225245</td>\n",
       "      <td>0.213273</td>\n",
       "      <td>0.480781</td>\n",
       "      <td>0.261024</td>\n",
       "      <td>0.494155</td>\n",
       "      <td>0.365467</td>\n",
       "      <td>0.455385</td>\n",
       "      <td>0.249447</td>\n",
       "      <td>0.442425</td>\n",
       "      <td>0.488798</td>\n",
       "      <td>0.493939</td>\n",
       "      <td>0.496395</td>\n",
       "      <td>0.499920</td>\n",
       "      <td>0.499945</td>\n",
       "      <td>0.486292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.451636</td>\n",
       "      <td>0.213557</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.088659</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.247193</td>\n",
       "      <td>0.395003</td>\n",
       "      <td>0.590580</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.547807</td>\n",
       "      <td>0.282326</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.164976</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.652632</td>\n",
       "      <td>0.308172</td>\n",
       "      <td>0.481664</td>\n",
       "      <td>0.731884</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516936</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.621247</td>\n",
       "      <td>0.356447</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.252530</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.405490</td>\n",
       "      <td>0.582574</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.353218</td>\n",
       "      <td>0.327089</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 324 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       OverallQual    GrLivArea      TotalSF   GarageCars  Total_Bathrooms  \\\n",
       "count  2911.000000  2911.000000  2911.000000  2911.000000      2911.000000   \n",
       "mean      0.565136     0.542763     0.293431     0.353143         0.200863   \n",
       "std       0.155988     0.125193     0.119772     0.152244         0.133043   \n",
       "min       0.000000     0.000000     0.000000     0.000000         0.000000   \n",
       "25%       0.444444     0.451636     0.213557     0.200000         0.088659   \n",
       "50%       0.555556     0.547807     0.282326     0.400000         0.164976   \n",
       "75%       0.666667     0.621247     0.356447     0.400000         0.252530   \n",
       "max       1.000000     1.000000     1.000000     1.000000         1.000000   \n",
       "\n",
       "        GarageArea  YrBltAndRemod  TotalBsmtSF     1stFlrSF    YearBuilt  \\\n",
       "count  2911.000000    2911.000000  2911.000000  2911.000000  2911.000000   \n",
       "mean      0.317285       0.660811     0.326706     0.487996     0.719569   \n",
       "std       0.143976       0.242686     0.131930     0.131369     0.219351   \n",
       "min       0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.215054       0.473684     0.247193     0.395003     0.590580   \n",
       "50%       0.322581       0.652632     0.308172     0.481664     0.731884   \n",
       "75%       0.387097       0.905263     0.405490     0.582574     0.934783   \n",
       "max       1.000000       1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          FullBath  YearRemodAdd  Foundation_PConc  TotRmsAbvGrd  \\\n",
       "count  2911.000000   2911.000000       2911.000000   2911.000000   \n",
       "mean      0.391876      0.570892          0.447956      0.542786   \n",
       "std       0.138140      0.348189          0.497369      0.128554   \n",
       "min       0.000000      0.000000          0.000000      0.000000   \n",
       "25%       0.250000      0.250000          0.000000      0.421336   \n",
       "50%       0.500000      0.716667          0.000000      0.516936   \n",
       "75%       0.500000      0.900000          1.000000      0.600315   \n",
       "max       1.000000      1.000000          1.000000      1.000000   \n",
       "\n",
       "       hasfireplace  ExterQual_Gd  BsmtQual_Ex   Fireplaces  HeatingQC_Ex  \\\n",
       "count   2911.000000   2911.000000  2911.000000  2911.000000   2911.000000   \n",
       "mean       0.512882      0.335967     0.087255     0.171718      0.511508   \n",
       "std        0.499920      0.472409     0.282257     0.181727      0.499953   \n",
       "min        0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "25%        0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "50%        1.000000      0.000000     0.000000     0.293793      1.000000   \n",
       "75%        1.000000      1.000000     0.000000     0.293793      1.000000   \n",
       "max        1.000000      1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "        MasVnrArea  Total_porch_sf  BsmtFinType1_GLQ  KitchenQual_Ex  \\\n",
       "count  2911.000000     2911.000000       2911.000000     2911.000000   \n",
       "mean      0.159954        0.203170          0.290622        0.069392   \n",
       "std       0.218350        0.163524          0.454127        0.254163   \n",
       "min       0.000000        0.000000          0.000000        0.000000   \n",
       "25%       0.000000        0.066964          0.000000        0.000000   \n",
       "50%       0.000000        0.179849          0.000000        0.000000   \n",
       "75%       0.353218        0.327089          1.000000        0.000000   \n",
       "max       1.000000        1.000000          1.000000        1.000000   \n",
       "\n",
       "       OpenPorchSF  GarageFinish_Fin  ...  Neighborhood_IDOTRR  \\\n",
       "count  2911.000000       2911.000000  ...          2911.000000   \n",
       "mean      0.176542          0.245620  ...             0.031261   \n",
       "std       0.183129          0.430528  ...             0.174051   \n",
       "min       0.000000          0.000000  ...             0.000000   \n",
       "25%       0.000000          0.000000  ...             0.000000   \n",
       "50%       0.180865          0.000000  ...             0.000000   \n",
       "75%       0.309510          0.000000  ...             0.000000   \n",
       "max       1.000000          1.000000  ...             1.000000   \n",
       "\n",
       "       BsmtExposure_No  Neighborhood_OldTown  Foundation_BrkTil  \\\n",
       "count      2911.000000           2911.000000        2911.000000   \n",
       "mean          0.652697              0.082102           0.106493   \n",
       "std           0.476195              0.274568           0.308520   \n",
       "min           0.000000              0.000000           0.000000   \n",
       "25%           0.000000              0.000000           0.000000   \n",
       "50%           1.000000              0.000000           0.000000   \n",
       "75%           1.000000              0.000000           0.000000   \n",
       "max           1.000000              1.000000           1.000000   \n",
       "\n",
       "       GarageFinish_None  GarageCond_None  GarageQual_None  GarageType_None  \\\n",
       "count        2911.000000      2911.000000      2911.000000      2911.000000   \n",
       "mean            0.054277         0.054277         0.054277         0.053590   \n",
       "std             0.226602         0.226602         0.226602         0.225245   \n",
       "min             0.000000         0.000000         0.000000         0.000000   \n",
       "25%             0.000000         0.000000         0.000000         0.000000   \n",
       "50%             0.000000         0.000000         0.000000         0.000000   \n",
       "75%             0.000000         0.000000         0.000000         0.000000   \n",
       "max             1.000000         1.000000         1.000000         1.000000   \n",
       "\n",
       "       MSSubClass_30  LotShape_Reg  PavedDrive_N  Foundation_CBlock  \\\n",
       "count    2911.000000   2911.000000   2911.000000        2911.000000   \n",
       "mean        0.047750      0.637582      0.073514           0.423222   \n",
       "std         0.213273      0.480781      0.261024           0.494155   \n",
       "min         0.000000      0.000000      0.000000           0.000000   \n",
       "25%         0.000000      0.000000      0.000000           0.000000   \n",
       "50%         0.000000      1.000000      0.000000           0.000000   \n",
       "75%         0.000000      1.000000      0.000000           1.000000   \n",
       "max         1.000000      1.000000      1.000000           1.000000   \n",
       "\n",
       "       MSZoning_RM  HeatingQC_TA  CentralAir_N  GarageType_Detchd  \\\n",
       "count  2911.000000   2911.000000   2911.000000        2911.000000   \n",
       "mean      0.158708      0.293370      0.066644           0.266919   \n",
       "std       0.365467      0.455385      0.249447           0.442425   \n",
       "min       0.000000      0.000000      0.000000           0.000000   \n",
       "25%       0.000000      0.000000      0.000000           0.000000   \n",
       "50%       0.000000      0.000000      0.000000           0.000000   \n",
       "75%       0.000000      1.000000      0.000000           1.000000   \n",
       "max       1.000000      1.000000      1.000000           1.000000   \n",
       "\n",
       "       MasVnrType_None  GarageFinish_Unf  BsmtQual_TA  FireplaceQu_None  \\\n",
       "count      2911.000000       2911.000000  2911.000000       2911.000000   \n",
       "mean          0.605634          0.421848     0.439368          0.487118   \n",
       "std           0.488798          0.493939     0.496395          0.499920   \n",
       "min           0.000000          0.000000     0.000000          0.000000   \n",
       "25%           0.000000          0.000000     0.000000          0.000000   \n",
       "50%           1.000000          0.000000     0.000000          0.000000   \n",
       "75%           1.000000          1.000000     1.000000          1.000000   \n",
       "max           1.000000          1.000000     1.000000          1.000000   \n",
       "\n",
       "       KitchenQual_TA  ExterQual_TA  dummy_1  dummy_2  dummy_3  \n",
       "count     2911.000000   2911.000000   2911.0   2911.0   2911.0  \n",
       "mean         0.511852      0.616627      0.0      0.0      0.0  \n",
       "std          0.499945      0.486292      0.0      0.0      0.0  \n",
       "min          0.000000      0.000000      0.0      0.0      0.0  \n",
       "25%          0.000000      0.000000      0.0      0.0      0.0  \n",
       "50%          1.000000      1.000000      0.0      0.0      0.0  \n",
       "75%          1.000000      1.000000      0.0      0.0      0.0  \n",
       "max          1.000000      1.000000      0.0      0.0      0.0  \n",
       "\n",
       "[8 rows x 324 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop label column\n",
    "data.drop(['label'], axis=1, inplace=True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hi', 'mid', 'low', 'mid-hi', 'low-mid'], dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  5\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "'''one-hot encode the labels'''\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "# print(list(integer_encoded))\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "labels = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "print('Number of classes: ', len(labels[0]))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 18, 18)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_np = data.to_numpy()\n",
    "data_np.shape\n",
    "data_np = data_np.reshape(len(data), 18, 18)\n",
    "data_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data_np[0].shape\n",
    "\n",
    "nArray = np.array(data_np[99])\n",
    "\n",
    "\n",
    "a11=nArray.reshape(18,18)\n",
    "plt.imshow(a11)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 5)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 18, 18)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_examples = data_np\n",
    "all_examples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train test splitting\n",
    "- hold out 15% for testing\n",
    "- use 85% to train model with K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_samples = all_examples.shape[0] \n",
    "test_ratio = 0.15\n",
    "test_samples = int(test_ratio * all_examples.shape[0])\n",
    "\n",
    "train_examples = all_examples[:-1*test_samples]\n",
    "test_examples = all_examples[-1*test_samples:]\n",
    "\n",
    "train_labels = labels[:-1*test_samples]\n",
    "test_labels = labels[-1*test_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (2475, 18, 18)\n",
      "test:  (436, 18, 18)\n",
      "train label:  (2475, 5)\n",
      "test label:  (436, 5)\n"
     ]
    }
   ],
   "source": [
    "print('train: ', train_examples.shape)\n",
    "print('test: ', test_examples.shape)\n",
    "print('train label: ', train_labels.shape)\n",
    "print('test label: ', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainX = train_examples.reshape(ttl_samples-test_samples, 18,18,1)\n",
    "# trainY = train_labels\n",
    "\n",
    "# testX = test_examples.reshape(test_samples, 18,18,1)\n",
    "# testY = test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "\n",
    "def create_model(num_classes=5, lr=0.005):\n",
    "\n",
    "\t# Working\n",
    "\twith tf.device('/cpu:0'):\n",
    "\t\tdata_augmentation = tf.keras.Sequential([ \n",
    "\t\t\ttf.keras.layers.RandomFlip(\"horizontal\", input_shape=(18, 18, 1)),\n",
    "\t  \t\ttf.keras.layers.RandomRotation(0.1),\n",
    "\t\t    tf.keras.layers.RandomZoom(0.1)\n",
    "\t\t\t])\n",
    "\n",
    "\n",
    "\tmodel = tf.keras.Sequential([\n",
    "\t\t# data_augmentation,\n",
    "\t  \t# tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(18, 18, 1)),\n",
    "\t\ttf.keras.layers.AveragePooling2D((2, 2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.AveragePooling2D((2,2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Flatten(),\n",
    "\t\ttf.keras.layers.Dense(128, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\t])\n",
    "\n",
    "\t# opt = tf.keras.optimizers.SGD(lr=0.005, momentum=0.9)\n",
    "\topt = tf.keras.optimizers.Adam(lr=lr)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 18, 18, 32)        320       \n",
      "                                                                 \n",
      " average_pooling2d_12 (Avera  (None, 9, 9, 32)         0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 9, 9, 32)          0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 9, 9, 32)          9248      \n",
      "                                                                 \n",
      " average_pooling2d_13 (Avera  (None, 4, 4, 32)         0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 4, 4, 32)          0         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 75,877\n",
      "Trainable params: 75,877\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evantilu/miniforge3/envs/6998_DL_tf/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 26s - loss: 1.6138 - accuracy: 0.1562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:48:54.701288: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.1444 - accuracy: 0.4845\n",
      "Epoch 1: val_accuracy improved from -inf to 0.64516, saving model to ./ckpt/5_cls_lr005/val_acc_0.645.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1444 - accuracy: 0.4845 - val_loss: 0.9252 - val_accuracy: 0.6452\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 0.9431 - accuracy: 0.6429"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:48:55.714733: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/70 [============================>.] - ETA: 0s - loss: 0.9260 - accuracy: 0.6075\n",
      "Epoch 2: val_accuracy improved from 0.64516 to 0.71371, saving model to ./ckpt/5_cls_lr005/val_acc_0.714.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9323 - accuracy: 0.6040 - val_loss: 0.7963 - val_accuracy: 0.7137\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8684 - accuracy: 0.6073\n",
      "Epoch 3: val_accuracy did not improve from 0.71371\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.8690 - accuracy: 0.6080 - val_loss: 0.8240 - val_accuracy: 0.6573\n",
      "Epoch 4/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.7817 - accuracy: 0.6553\n",
      "Epoch 4: val_accuracy did not improve from 0.71371\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.7761 - accuracy: 0.6596 - val_loss: 0.7438 - val_accuracy: 0.6613\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7482 - accuracy: 0.6780\n",
      "Epoch 5: val_accuracy did not improve from 0.71371\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.7482 - accuracy: 0.6780 - val_loss: 0.6920 - val_accuracy: 0.7056\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6882 - accuracy: 0.6893\n",
      "Epoch 6: val_accuracy did not improve from 0.71371\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.6882 - accuracy: 0.6893 - val_loss: 0.6940 - val_accuracy: 0.6734\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6490 - accuracy: 0.7059\n",
      "Epoch 7: val_accuracy did not improve from 0.71371\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6490 - accuracy: 0.7059 - val_loss: 0.7248 - val_accuracy: 0.6613\n",
      "Epoch 8/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.6325 - accuracy: 0.7135\n",
      "Epoch 8: val_accuracy did not improve from 0.71371\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6348 - accuracy: 0.7144 - val_loss: 0.7073 - val_accuracy: 0.6855\n",
      "Epoch 9/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6383 - accuracy: 0.7141\n",
      "Epoch 9: val_accuracy improved from 0.71371 to 0.73387, saving model to ./ckpt/5_cls_lr005/val_acc_0.734.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6381 - accuracy: 0.7140 - val_loss: 0.5975 - val_accuracy: 0.7339\n",
      "Epoch 10/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5594 - accuracy: 0.7486\n",
      "Epoch 10: val_accuracy improved from 0.73387 to 0.78226, saving model to ./ckpt/5_cls_lr005/val_acc_0.782.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5571 - accuracy: 0.7503 - val_loss: 0.5359 - val_accuracy: 0.7823\n",
      "Epoch 11/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5641 - accuracy: 0.7603\n",
      "Epoch 11: val_accuracy did not improve from 0.78226\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5647 - accuracy: 0.7620 - val_loss: 0.5876 - val_accuracy: 0.7339\n",
      "Epoch 12/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5308 - accuracy: 0.7675\n",
      "Epoch 12: val_accuracy did not improve from 0.78226\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5318 - accuracy: 0.7661 - val_loss: 0.5830 - val_accuracy: 0.7419\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5437 - accuracy: 0.7661\n",
      "Epoch 13: val_accuracy did not improve from 0.78226\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5437 - accuracy: 0.7661 - val_loss: 0.5693 - val_accuracy: 0.7419\n",
      "Epoch 14/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5071 - accuracy: 0.7878\n",
      "Epoch 14: val_accuracy did not improve from 0.78226\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5074 - accuracy: 0.7876 - val_loss: 0.6495 - val_accuracy: 0.7016\n",
      "Epoch 15/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4850 - accuracy: 0.7997\n",
      "Epoch 15: val_accuracy did not improve from 0.78226\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4863 - accuracy: 0.7975 - val_loss: 0.5054 - val_accuracy: 0.7661\n",
      "Epoch 16/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.4824 - accuracy: 0.8029\n",
      "Epoch 16: val_accuracy improved from 0.78226 to 0.79032, saving model to ./ckpt/5_cls_lr005/val_acc_0.790.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4844 - accuracy: 0.7997 - val_loss: 0.5071 - val_accuracy: 0.7903\n",
      "Epoch 17/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4813 - accuracy: 0.7927\n",
      "Epoch 17: val_accuracy did not improve from 0.79032\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4835 - accuracy: 0.7921 - val_loss: 0.5623 - val_accuracy: 0.7460\n",
      "Epoch 18/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4498 - accuracy: 0.8143\n",
      "Epoch 18: val_accuracy did not improve from 0.79032\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4518 - accuracy: 0.8132 - val_loss: 0.5371 - val_accuracy: 0.7661\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4890 - accuracy: 0.7907\n",
      "Epoch 19: val_accuracy did not improve from 0.79032\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4890 - accuracy: 0.7907 - val_loss: 0.7080 - val_accuracy: 0.6855\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4477 - accuracy: 0.8075\n",
      "Epoch 20: val_accuracy did not improve from 0.79032\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4495 - accuracy: 0.8074 - val_loss: 0.5206 - val_accuracy: 0.7621\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3970 - accuracy: 0.8340\n",
      "Epoch 21: val_accuracy did not improve from 0.79032\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3973 - accuracy: 0.8316 - val_loss: 0.5283 - val_accuracy: 0.7863\n",
      "Epoch 22/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4015 - accuracy: 0.8293\n",
      "Epoch 22: val_accuracy did not improve from 0.79032\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4016 - accuracy: 0.8303 - val_loss: 0.5097 - val_accuracy: 0.7782\n",
      "Epoch 23/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.4009 - accuracy: 0.8389\n",
      "Epoch 23: val_accuracy improved from 0.79032 to 0.81048, saving model to ./ckpt/5_cls_lr005/val_acc_0.810.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3970 - accuracy: 0.8406 - val_loss: 0.5331 - val_accuracy: 0.8105\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3812 - accuracy: 0.8405\n",
      "Epoch 24: val_accuracy did not improve from 0.81048\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3860 - accuracy: 0.8383 - val_loss: 0.5473 - val_accuracy: 0.7540\n",
      "Epoch 25/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3696 - accuracy: 0.8545\n",
      "Epoch 25: val_accuracy did not improve from 0.81048\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3704 - accuracy: 0.8532 - val_loss: 0.5885 - val_accuracy: 0.7581\n",
      "Epoch 25: early stopping\n",
      "Epoch 1/120\n",
      " 6/70 [=>............................] - ETA: 0s - loss: 1.6077 - accuracy: 0.2083 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:49:14.652892: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.1612 - accuracy: 0.4733\n",
      "Epoch 1: val_accuracy improved from -inf to 0.57661, saving model to ./ckpt/5_cls_lr005/val_acc_0.577.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1612 - accuracy: 0.4733 - val_loss: 0.9600 - val_accuracy: 0.5766\n",
      "Epoch 2/120\n",
      " 6/70 [=>............................] - ETA: 0s - loss: 0.9067 - accuracy: 0.6042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:49:15.540048: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9437 - accuracy: 0.5729\n",
      "Epoch 2: val_accuracy improved from 0.57661 to 0.64516, saving model to ./ckpt/5_cls_lr005/val_acc_0.645.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.9418 - accuracy: 0.5734 - val_loss: 0.8490 - val_accuracy: 0.6452\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8530 - accuracy: 0.6082\n",
      "Epoch 3: val_accuracy did not improve from 0.64516\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.8551 - accuracy: 0.6062 - val_loss: 0.8154 - val_accuracy: 0.6411\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8097 - accuracy: 0.6354\n",
      "Epoch 4: val_accuracy improved from 0.64516 to 0.66129, saving model to ./ckpt/5_cls_lr005/val_acc_0.661.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.8097 - accuracy: 0.6354 - val_loss: 0.7142 - val_accuracy: 0.6613\n",
      "Epoch 5/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.7157 - accuracy: 0.6780\n",
      "Epoch 5: val_accuracy improved from 0.66129 to 0.70968, saving model to ./ckpt/5_cls_lr005/val_acc_0.710.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.7175 - accuracy: 0.6749 - val_loss: 0.7107 - val_accuracy: 0.7097\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7251 - accuracy: 0.6767\n",
      "Epoch 6: val_accuracy improved from 0.70968 to 0.76613, saving model to ./ckpt/5_cls_lr005/val_acc_0.766.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.7251 - accuracy: 0.6767 - val_loss: 0.6044 - val_accuracy: 0.7661\n",
      "Epoch 7/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6698 - accuracy: 0.7066\n",
      "Epoch 7: val_accuracy did not improve from 0.76613\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6719 - accuracy: 0.7081 - val_loss: 0.6513 - val_accuracy: 0.7258\n",
      "Epoch 8/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.6411 - accuracy: 0.7188\n",
      "Epoch 8: val_accuracy did not improve from 0.76613\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6409 - accuracy: 0.7185 - val_loss: 0.5702 - val_accuracy: 0.7460\n",
      "Epoch 9/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6044 - accuracy: 0.7416\n",
      "Epoch 9: val_accuracy did not improve from 0.76613\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6056 - accuracy: 0.7414 - val_loss: 0.5609 - val_accuracy: 0.7460\n",
      "Epoch 10/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.6022 - accuracy: 0.7269\n",
      "Epoch 10: val_accuracy did not improve from 0.76613\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6069 - accuracy: 0.7265 - val_loss: 0.5809 - val_accuracy: 0.7460\n",
      "Epoch 11/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.5759 - accuracy: 0.7510\n",
      "Epoch 11: val_accuracy did not improve from 0.76613\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5750 - accuracy: 0.7508 - val_loss: 0.5449 - val_accuracy: 0.7500\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5424 - accuracy: 0.7602\n",
      "Epoch 12: val_accuracy did not improve from 0.76613\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5424 - accuracy: 0.7602 - val_loss: 0.6325 - val_accuracy: 0.7056\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5600 - accuracy: 0.7517\n",
      "Epoch 13: val_accuracy did not improve from 0.76613\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5600 - accuracy: 0.7517 - val_loss: 0.5390 - val_accuracy: 0.7540\n",
      "Epoch 14/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5691 - accuracy: 0.7557\n",
      "Epoch 14: val_accuracy did not improve from 0.76613\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5691 - accuracy: 0.7553 - val_loss: 0.5979 - val_accuracy: 0.7661\n",
      "Epoch 15/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5685 - accuracy: 0.7539\n",
      "Epoch 15: val_accuracy did not improve from 0.76613\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5685 - accuracy: 0.7539 - val_loss: 0.5233 - val_accuracy: 0.7540\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5081 - accuracy: 0.7864\n",
      "Epoch 16: val_accuracy improved from 0.76613 to 0.78629, saving model to ./ckpt/5_cls_lr005/val_acc_0.786.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5162 - accuracy: 0.7813 - val_loss: 0.5165 - val_accuracy: 0.7863\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4733 - accuracy: 0.8033\n",
      "Epoch 17: val_accuracy did not improve from 0.78629\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4733 - accuracy: 0.8033 - val_loss: 0.5665 - val_accuracy: 0.7661\n",
      "Epoch 18/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4812 - accuracy: 0.7868\n",
      "Epoch 18: val_accuracy did not improve from 0.78629\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4832 - accuracy: 0.7858 - val_loss: 0.5541 - val_accuracy: 0.7460\n",
      "Epoch 19/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4947 - accuracy: 0.7906\n",
      "Epoch 19: val_accuracy did not improve from 0.78629\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4962 - accuracy: 0.7894 - val_loss: 0.5201 - val_accuracy: 0.7661\n",
      "Epoch 20/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4299 - accuracy: 0.8125\n",
      "Epoch 20: val_accuracy improved from 0.78629 to 0.79839, saving model to ./ckpt/5_cls_lr005/val_acc_0.798.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4298 - accuracy: 0.8123 - val_loss: 0.5117 - val_accuracy: 0.7984\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4296 - accuracy: 0.8128\n",
      "Epoch 21: val_accuracy did not improve from 0.79839\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4296 - accuracy: 0.8128 - val_loss: 0.5280 - val_accuracy: 0.7540\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4352 - accuracy: 0.8253\n",
      "Epoch 22: val_accuracy did not improve from 0.79839\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4335 - accuracy: 0.8262 - val_loss: 0.4929 - val_accuracy: 0.7903\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4109 - accuracy: 0.8311\n",
      "Epoch 23: val_accuracy did not improve from 0.79839\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4111 - accuracy: 0.8316 - val_loss: 0.5117 - val_accuracy: 0.7944\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4074 - accuracy: 0.8298\n",
      "Epoch 24: val_accuracy did not improve from 0.79839\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4063 - accuracy: 0.8298 - val_loss: 0.5463 - val_accuracy: 0.7742\n",
      "Epoch 25/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4058 - accuracy: 0.8284\n",
      "Epoch 25: val_accuracy did not improve from 0.79839\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4070 - accuracy: 0.8280 - val_loss: 0.5890 - val_accuracy: 0.7742\n",
      "Epoch 26/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.3826 - accuracy: 0.8346\n",
      "Epoch 26: val_accuracy did not improve from 0.79839\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3814 - accuracy: 0.8366 - val_loss: 0.5879 - val_accuracy: 0.7460\n",
      "Epoch 27/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3779 - accuracy: 0.8494\n",
      "Epoch 27: val_accuracy did not improve from 0.79839\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3771 - accuracy: 0.8482 - val_loss: 0.5485 - val_accuracy: 0.7621\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3826 - accuracy: 0.8392\n",
      "Epoch 28: val_accuracy did not improve from 0.79839\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3852 - accuracy: 0.8383 - val_loss: 0.5563 - val_accuracy: 0.7823\n",
      "Epoch 29/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3187 - accuracy: 0.8741\n",
      "Epoch 29: val_accuracy did not improve from 0.79839\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3238 - accuracy: 0.8716 - val_loss: 0.5853 - val_accuracy: 0.7944\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3649 - accuracy: 0.8501\n",
      "Epoch 30: val_accuracy did not improve from 0.79839\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.3644 - accuracy: 0.8500 - val_loss: 0.5412 - val_accuracy: 0.7581\n",
      "Epoch 31/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3247 - accuracy: 0.8636\n",
      "Epoch 31: val_accuracy did not improve from 0.79839\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.3245 - accuracy: 0.8639 - val_loss: 0.6072 - val_accuracy: 0.7419\n",
      "Epoch 32/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3089 - accuracy: 0.8648\n",
      "Epoch 32: val_accuracy did not improve from 0.79839\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3089 - accuracy: 0.8648 - val_loss: 0.6217 - val_accuracy: 0.7782\n",
      "Epoch 32: early stopping\n",
      "Epoch 1/120\n",
      " 6/70 [=>............................] - ETA: 0s - loss: 1.5965 - accuracy: 0.2135 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:49:39.361973: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.1654 - accuracy: 0.4809\n",
      "Epoch 1: val_accuracy improved from -inf to 0.55645, saving model to ./ckpt/5_cls_lr005/val_acc_0.556.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1654 - accuracy: 0.4809 - val_loss: 0.9764 - val_accuracy: 0.5565\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 1.0311 - accuracy: 0.5179"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:49:40.200576: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9550 - accuracy: 0.5690\n",
      "Epoch 2: val_accuracy improved from 0.55645 to 0.58065, saving model to ./ckpt/5_cls_lr005/val_acc_0.581.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.9517 - accuracy: 0.5730 - val_loss: 0.8964 - val_accuracy: 0.5806\n",
      "Epoch 3/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.8510 - accuracy: 0.6240\n",
      "Epoch 3: val_accuracy did not improve from 0.58065\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.8542 - accuracy: 0.6215 - val_loss: 0.8562 - val_accuracy: 0.5685\n",
      "Epoch 4/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.7626 - accuracy: 0.6553\n",
      "Epoch 4: val_accuracy improved from 0.58065 to 0.66532, saving model to ./ckpt/5_cls_lr005/val_acc_0.665.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.7651 - accuracy: 0.6542 - val_loss: 0.7470 - val_accuracy: 0.6653\n",
      "Epoch 5/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.6963 - accuracy: 0.6947\n",
      "Epoch 5: val_accuracy improved from 0.66532 to 0.77823, saving model to ./ckpt/5_cls_lr005/val_acc_0.778.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.6931 - accuracy: 0.6938 - val_loss: 0.6420 - val_accuracy: 0.7782\n",
      "Epoch 6/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6821 - accuracy: 0.7076\n",
      "Epoch 6: val_accuracy did not improve from 0.77823\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.6811 - accuracy: 0.7068 - val_loss: 0.6367 - val_accuracy: 0.7379\n",
      "Epoch 7/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6008 - accuracy: 0.7408\n",
      "Epoch 7: val_accuracy did not improve from 0.77823\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6048 - accuracy: 0.7373 - val_loss: 0.7216 - val_accuracy: 0.6976\n",
      "Epoch 8/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.6058 - accuracy: 0.7308\n",
      "Epoch 8: val_accuracy did not improve from 0.77823\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.6074 - accuracy: 0.7288 - val_loss: 0.8152 - val_accuracy: 0.6653\n",
      "Epoch 9/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5754 - accuracy: 0.7438\n",
      "Epoch 9: val_accuracy did not improve from 0.77823\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - accuracy: 0.7449 - val_loss: 0.5657 - val_accuracy: 0.7782\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5374 - accuracy: 0.7663\n",
      "Epoch 10: val_accuracy did not improve from 0.77823\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5349 - accuracy: 0.7678 - val_loss: 0.5907 - val_accuracy: 0.7661\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5333 - accuracy: 0.7668\n",
      "Epoch 11: val_accuracy did not improve from 0.77823\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5344 - accuracy: 0.7674 - val_loss: 0.5730 - val_accuracy: 0.7661\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5305 - accuracy: 0.7640\n",
      "Epoch 12: val_accuracy did not improve from 0.77823\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5290 - accuracy: 0.7670 - val_loss: 0.6757 - val_accuracy: 0.7419\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5072 - accuracy: 0.7813\n",
      "Epoch 13: val_accuracy did not improve from 0.77823\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5072 - accuracy: 0.7813 - val_loss: 0.6520 - val_accuracy: 0.7581\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4710 - accuracy: 0.7894\n",
      "Epoch 14: val_accuracy did not improve from 0.77823\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4710 - accuracy: 0.7894 - val_loss: 0.6631 - val_accuracy: 0.7258\n",
      "Epoch 15/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4803 - accuracy: 0.7865\n",
      "Epoch 15: val_accuracy did not improve from 0.77823\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4857 - accuracy: 0.7836 - val_loss: 0.5786 - val_accuracy: 0.7742\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4572 - accuracy: 0.8054\n",
      "Epoch 16: val_accuracy did not improve from 0.77823\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4577 - accuracy: 0.8051 - val_loss: 0.5797 - val_accuracy: 0.7581\n",
      "Epoch 17/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4402 - accuracy: 0.8222\n",
      "Epoch 17: val_accuracy improved from 0.77823 to 0.78629, saving model to ./ckpt/5_cls_lr005/val_acc_0.786.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4408 - accuracy: 0.8204 - val_loss: 0.5453 - val_accuracy: 0.7863\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4210 - accuracy: 0.8208\n",
      "Epoch 18: val_accuracy did not improve from 0.78629\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4210 - accuracy: 0.8208 - val_loss: 0.6364 - val_accuracy: 0.7056\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4124 - accuracy: 0.8253\n",
      "Epoch 19: val_accuracy did not improve from 0.78629\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4124 - accuracy: 0.8253 - val_loss: 0.8347 - val_accuracy: 0.7056\n",
      "Epoch 20/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4418 - accuracy: 0.8083\n",
      "Epoch 20: val_accuracy did not improve from 0.78629\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4405 - accuracy: 0.8101 - val_loss: 0.7476 - val_accuracy: 0.7500\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4024 - accuracy: 0.8335\n",
      "Epoch 21: val_accuracy did not improve from 0.78629\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4031 - accuracy: 0.8307 - val_loss: 0.5869 - val_accuracy: 0.7702\n",
      "Epoch 22/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.3872 - accuracy: 0.8279\n",
      "Epoch 22: val_accuracy did not improve from 0.78629\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3894 - accuracy: 0.8249 - val_loss: 0.6137 - val_accuracy: 0.7782\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3749 - accuracy: 0.8428\n",
      "Epoch 23: val_accuracy did not improve from 0.78629\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - accuracy: 0.8433 - val_loss: 0.7167 - val_accuracy: 0.7379\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3648 - accuracy: 0.8493\n",
      "Epoch 24: val_accuracy did not improve from 0.78629\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3611 - accuracy: 0.8505 - val_loss: 0.7232 - val_accuracy: 0.7218\n",
      "Epoch 25/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3777 - accuracy: 0.8428\n",
      "Epoch 25: val_accuracy did not improve from 0.78629\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3838 - accuracy: 0.8397 - val_loss: 0.7255 - val_accuracy: 0.7621\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3334 - accuracy: 0.8629\n",
      "Epoch 26: val_accuracy did not improve from 0.78629\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3351 - accuracy: 0.8617 - val_loss: 0.7507 - val_accuracy: 0.7460\n",
      "Epoch 27/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3193 - accuracy: 0.8693\n",
      "Epoch 27: val_accuracy did not improve from 0.78629\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3235 - accuracy: 0.8675 - val_loss: 0.6486 - val_accuracy: 0.7702\n",
      "Epoch 27: early stopping\n",
      "Epoch 1/120\n",
      " 6/70 [=>............................] - ETA: 0s - loss: 1.6082 - accuracy: 0.1823 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:49:59.401920: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 1.1970 - accuracy: 0.4497\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54839, saving model to ./ckpt/5_cls_lr005/val_acc_0.548.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.1938 - accuracy: 0.4522 - val_loss: 1.0626 - val_accuracy: 0.5484\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 0.9921 - accuracy: 0.5268"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:50:00.530664: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9726 - accuracy: 0.5555\n",
      "Epoch 2: val_accuracy improved from 0.54839 to 0.59274, saving model to ./ckpt/5_cls_lr005/val_acc_0.593.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.9686 - accuracy: 0.5590 - val_loss: 0.9489 - val_accuracy: 0.5927\n",
      "Epoch 3/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8785 - accuracy: 0.5971\n",
      "Epoch 3: val_accuracy improved from 0.59274 to 0.65726, saving model to ./ckpt/5_cls_lr005/val_acc_0.657.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.8810 - accuracy: 0.5968 - val_loss: 0.8486 - val_accuracy: 0.6573\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.7885 - accuracy: 0.6484\n",
      "Epoch 4: val_accuracy did not improve from 0.65726\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.7883 - accuracy: 0.6502 - val_loss: 0.7624 - val_accuracy: 0.6492\n",
      "Epoch 5/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.7181 - accuracy: 0.6823\n",
      "Epoch 5: val_accuracy improved from 0.65726 to 0.67339, saving model to ./ckpt/5_cls_lr005/val_acc_0.673.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.7228 - accuracy: 0.6807 - val_loss: 0.6976 - val_accuracy: 0.6734\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6736 - accuracy: 0.7004\n",
      "Epoch 6: val_accuracy did not improve from 0.67339\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6753 - accuracy: 0.6991 - val_loss: 0.7909 - val_accuracy: 0.6210\n",
      "Epoch 7/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6926 - accuracy: 0.6917\n",
      "Epoch 7: val_accuracy improved from 0.67339 to 0.67742, saving model to ./ckpt/5_cls_lr005/val_acc_0.677.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6899 - accuracy: 0.6947 - val_loss: 0.6701 - val_accuracy: 0.6774\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6407 - accuracy: 0.7237\n",
      "Epoch 8: val_accuracy did not improve from 0.67742\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6434 - accuracy: 0.7225 - val_loss: 0.6909 - val_accuracy: 0.6613\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5937 - accuracy: 0.7423\n",
      "Epoch 9: val_accuracy improved from 0.67742 to 0.68952, saving model to ./ckpt/5_cls_lr005/val_acc_0.690.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5937 - accuracy: 0.7423 - val_loss: 0.6423 - val_accuracy: 0.6895\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5578 - accuracy: 0.7613\n",
      "Epoch 10: val_accuracy improved from 0.68952 to 0.73387, saving model to ./ckpt/5_cls_lr005/val_acc_0.734.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5579 - accuracy: 0.7607 - val_loss: 0.5990 - val_accuracy: 0.7339\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5486 - accuracy: 0.7601\n",
      "Epoch 11: val_accuracy improved from 0.73387 to 0.75000, saving model to ./ckpt/5_cls_lr005/val_acc_0.750.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5495 - accuracy: 0.7602 - val_loss: 0.6138 - val_accuracy: 0.7500\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5623 - accuracy: 0.7584\n",
      "Epoch 12: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5623 - accuracy: 0.7584 - val_loss: 0.6188 - val_accuracy: 0.7097\n",
      "Epoch 13/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5457 - accuracy: 0.7661\n",
      "Epoch 13: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5465 - accuracy: 0.7656 - val_loss: 0.6206 - val_accuracy: 0.7218\n",
      "Epoch 14/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5197 - accuracy: 0.7718\n",
      "Epoch 14: val_accuracy improved from 0.75000 to 0.75806, saving model to ./ckpt/5_cls_lr005/val_acc_0.758.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5248 - accuracy: 0.7705 - val_loss: 0.5839 - val_accuracy: 0.7581\n",
      "Epoch 15/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5304 - accuracy: 0.7752\n",
      "Epoch 15: val_accuracy did not improve from 0.75806\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5274 - accuracy: 0.7750 - val_loss: 0.5835 - val_accuracy: 0.7500\n",
      "Epoch 16/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.5047 - accuracy: 0.7861\n",
      "Epoch 16: val_accuracy did not improve from 0.75806\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5065 - accuracy: 0.7840 - val_loss: 0.5994 - val_accuracy: 0.7419\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5159 - accuracy: 0.7767\n",
      "Epoch 17: val_accuracy did not improve from 0.75806\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5158 - accuracy: 0.7764 - val_loss: 0.6488 - val_accuracy: 0.7137\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4376 - accuracy: 0.8234\n",
      "Epoch 18: val_accuracy did not improve from 0.75806\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4385 - accuracy: 0.8226 - val_loss: 0.6413 - val_accuracy: 0.7339\n",
      "Epoch 19/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4504 - accuracy: 0.8097\n",
      "Epoch 19: val_accuracy did not improve from 0.75806\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4518 - accuracy: 0.8101 - val_loss: 0.6034 - val_accuracy: 0.7298\n",
      "Epoch 20/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4374 - accuracy: 0.8200\n",
      "Epoch 20: val_accuracy did not improve from 0.75806\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4384 - accuracy: 0.8195 - val_loss: 0.6056 - val_accuracy: 0.7298\n",
      "Epoch 21/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.4167 - accuracy: 0.8284\n",
      "Epoch 21: val_accuracy did not improve from 0.75806\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4209 - accuracy: 0.8258 - val_loss: 0.6396 - val_accuracy: 0.7460\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4416 - accuracy: 0.8128\n",
      "Epoch 22: val_accuracy did not improve from 0.75806\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4416 - accuracy: 0.8128 - val_loss: 0.5847 - val_accuracy: 0.7540\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3990 - accuracy: 0.8366\n",
      "Epoch 23: val_accuracy improved from 0.75806 to 0.77419, saving model to ./ckpt/5_cls_lr005/val_acc_0.774.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.3990 - accuracy: 0.8366 - val_loss: 0.6293 - val_accuracy: 0.7742\n",
      "Epoch 24/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.3943 - accuracy: 0.8341\n",
      "Epoch 24: val_accuracy did not improve from 0.77419\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3948 - accuracy: 0.8334 - val_loss: 0.5791 - val_accuracy: 0.7742\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3738 - accuracy: 0.8487\n",
      "Epoch 25: val_accuracy did not improve from 0.77419\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.3738 - accuracy: 0.8487 - val_loss: 0.6614 - val_accuracy: 0.7177\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4000 - accuracy: 0.8386\n",
      "Epoch 26: val_accuracy did not improve from 0.77419\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4008 - accuracy: 0.8357 - val_loss: 0.6073 - val_accuracy: 0.7702\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3701 - accuracy: 0.8498\n",
      "Epoch 27: val_accuracy did not improve from 0.77419\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - accuracy: 0.8491 - val_loss: 0.6545 - val_accuracy: 0.7621\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3658 - accuracy: 0.8542\n",
      "Epoch 28: val_accuracy did not improve from 0.77419\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - accuracy: 0.8545 - val_loss: 0.5806 - val_accuracy: 0.7702\n",
      "Epoch 29/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3584 - accuracy: 0.8549\n",
      "Epoch 29: val_accuracy did not improve from 0.77419\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3564 - accuracy: 0.8572 - val_loss: 0.6508 - val_accuracy: 0.7581\n",
      "Epoch 30/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3640 - accuracy: 0.8451\n",
      "Epoch 30: val_accuracy did not improve from 0.77419\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3636 - accuracy: 0.8455 - val_loss: 0.5958 - val_accuracy: 0.7742\n",
      "Epoch 31/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3375 - accuracy: 0.8577\n",
      "Epoch 31: val_accuracy did not improve from 0.77419\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3383 - accuracy: 0.8568 - val_loss: 0.6520 - val_accuracy: 0.7379\n",
      "Epoch 32/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3208 - accuracy: 0.8657\n",
      "Epoch 32: val_accuracy did not improve from 0.77419\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3261 - accuracy: 0.8630 - val_loss: 0.6582 - val_accuracy: 0.7540\n",
      "Epoch 33/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3154 - accuracy: 0.8703\n",
      "Epoch 33: val_accuracy did not improve from 0.77419\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3152 - accuracy: 0.8711 - val_loss: 0.6196 - val_accuracy: 0.7379\n",
      "Epoch 34/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.8777\n",
      "Epoch 34: val_accuracy did not improve from 0.77419\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3060 - accuracy: 0.8779 - val_loss: 0.8257 - val_accuracy: 0.7177\n",
      "Epoch 34: early stopping\n",
      "Epoch 1/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 1.5744 - accuracy: 0.3170 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:50:25.937337: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1419 - accuracy: 0.4827\n",
      "Epoch 1: val_accuracy improved from -inf to 0.56048, saving model to ./ckpt/5_cls_lr005/val_acc_0.560.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1396 - accuracy: 0.4832 - val_loss: 1.0415 - val_accuracy: 0.5605\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 0.9733 - accuracy: 0.5759"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:50:26.738275: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9431 - accuracy: 0.5772\n",
      "Epoch 2: val_accuracy improved from 0.56048 to 0.60887, saving model to ./ckpt/5_cls_lr005/val_acc_0.609.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.9400 - accuracy: 0.5802 - val_loss: 0.8957 - val_accuracy: 0.6089\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8407 - accuracy: 0.6287\n",
      "Epoch 3: val_accuracy improved from 0.60887 to 0.66129, saving model to ./ckpt/5_cls_lr005/val_acc_0.661.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.8327 - accuracy: 0.6327 - val_loss: 0.8269 - val_accuracy: 0.6613\n",
      "Epoch 4/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.7900 - accuracy: 0.6406\n",
      "Epoch 4: val_accuracy did not improve from 0.66129\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.7865 - accuracy: 0.6435 - val_loss: 0.8028 - val_accuracy: 0.6210\n",
      "Epoch 5/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.7142 - accuracy: 0.6832\n",
      "Epoch 5: val_accuracy did not improve from 0.66129\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.7128 - accuracy: 0.6848 - val_loss: 0.7438 - val_accuracy: 0.6452\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6540 - accuracy: 0.7086\n",
      "Epoch 6: val_accuracy improved from 0.66129 to 0.69758, saving model to ./ckpt/5_cls_lr005/val_acc_0.698.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.6540 - accuracy: 0.7086 - val_loss: 0.6894 - val_accuracy: 0.6976\n",
      "Epoch 7/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.6467 - accuracy: 0.7163\n",
      "Epoch 7: val_accuracy did not improve from 0.69758\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.6476 - accuracy: 0.7162 - val_loss: 0.7403 - val_accuracy: 0.6371\n",
      "Epoch 8/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6022 - accuracy: 0.7323\n",
      "Epoch 8: val_accuracy did not improve from 0.69758\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.6087 - accuracy: 0.7319 - val_loss: 0.6969 - val_accuracy: 0.6774\n",
      "Epoch 9/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5896 - accuracy: 0.7449\n",
      "Epoch 9: val_accuracy improved from 0.69758 to 0.70161, saving model to ./ckpt/5_cls_lr005/val_acc_0.702.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5905 - accuracy: 0.7432 - val_loss: 0.6684 - val_accuracy: 0.7016\n",
      "Epoch 10/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5739 - accuracy: 0.7523\n",
      "Epoch 10: val_accuracy did not improve from 0.70161\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5733 - accuracy: 0.7530 - val_loss: 0.6623 - val_accuracy: 0.6895\n",
      "Epoch 11/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5414 - accuracy: 0.7647\n",
      "Epoch 11: val_accuracy improved from 0.70161 to 0.72581, saving model to ./ckpt/5_cls_lr005/val_acc_0.726.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5456 - accuracy: 0.7611 - val_loss: 0.6132 - val_accuracy: 0.7258\n",
      "Epoch 12/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5346 - accuracy: 0.7751\n",
      "Epoch 12: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5386 - accuracy: 0.7737 - val_loss: 0.5948 - val_accuracy: 0.7016\n",
      "Epoch 13/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5246 - accuracy: 0.7727\n",
      "Epoch 13: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5213 - accuracy: 0.7746 - val_loss: 0.7349 - val_accuracy: 0.6371\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5129 - accuracy: 0.7822\n",
      "Epoch 14: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5129 - accuracy: 0.7822 - val_loss: 0.6799 - val_accuracy: 0.7258\n",
      "Epoch 15/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5155 - accuracy: 0.7760\n",
      "Epoch 15: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5118 - accuracy: 0.7764 - val_loss: 0.6266 - val_accuracy: 0.7097\n",
      "Epoch 16/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.5170 - accuracy: 0.7832\n",
      "Epoch 16: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5190 - accuracy: 0.7800 - val_loss: 0.7225 - val_accuracy: 0.6532\n",
      "Epoch 17/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5063 - accuracy: 0.7872\n",
      "Epoch 17: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5055 - accuracy: 0.7863 - val_loss: 0.6082 - val_accuracy: 0.7097\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4585 - accuracy: 0.8033\n",
      "Epoch 18: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4585 - accuracy: 0.8033 - val_loss: 0.6273 - val_accuracy: 0.6976\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4716 - accuracy: 0.8049\n",
      "Epoch 19: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4664 - accuracy: 0.8083 - val_loss: 0.6523 - val_accuracy: 0.7016\n",
      "Epoch 20/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4756 - accuracy: 0.7994\n",
      "Epoch 20: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4779 - accuracy: 0.7975 - val_loss: 0.5978 - val_accuracy: 0.7137\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4539 - accuracy: 0.8092\n",
      "Epoch 21: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4539 - accuracy: 0.8092 - val_loss: 0.5950 - val_accuracy: 0.7218\n",
      "Epoch 22/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4283 - accuracy: 0.8218\n",
      "Epoch 22: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4306 - accuracy: 0.8213 - val_loss: 0.5912 - val_accuracy: 0.7177\n",
      "Epoch 23/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4049 - accuracy: 0.8316\n",
      "Epoch 23: val_accuracy improved from 0.72581 to 0.75000, saving model to ./ckpt/5_cls_lr005/val_acc_0.750.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4096 - accuracy: 0.8294 - val_loss: 0.6354 - val_accuracy: 0.7500\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3814 - accuracy: 0.8465\n",
      "Epoch 24: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3817 - accuracy: 0.8455 - val_loss: 0.5784 - val_accuracy: 0.7339\n",
      "Epoch 25/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.3673 - accuracy: 0.8502\n",
      "Epoch 25: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.3706 - accuracy: 0.8482 - val_loss: 0.6224 - val_accuracy: 0.7177\n",
      "Epoch 26/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3734 - accuracy: 0.8433\n",
      "Epoch 26: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - accuracy: 0.8464 - val_loss: 0.6465 - val_accuracy: 0.7016\n",
      "Epoch 27/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.3718 - accuracy: 0.8466\n",
      "Epoch 27: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3784 - accuracy: 0.8446 - val_loss: 0.6596 - val_accuracy: 0.7218\n",
      "Epoch 28/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3658 - accuracy: 0.8617\n",
      "Epoch 28: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - accuracy: 0.8604 - val_loss: 0.6539 - val_accuracy: 0.7016\n",
      "Epoch 29/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3475 - accuracy: 0.8635\n",
      "Epoch 29: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3475 - accuracy: 0.8635 - val_loss: 0.6064 - val_accuracy: 0.7218\n",
      "Epoch 30/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3779 - accuracy: 0.8456\n",
      "Epoch 30: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3750 - accuracy: 0.8482 - val_loss: 0.6439 - val_accuracy: 0.7339\n",
      "Epoch 31/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.3230 - accuracy: 0.8653\n",
      "Epoch 31: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3224 - accuracy: 0.8648 - val_loss: 0.6619 - val_accuracy: 0.7460\n",
      "Epoch 32/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3381 - accuracy: 0.8591\n",
      "Epoch 32: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3402 - accuracy: 0.8559 - val_loss: 0.6676 - val_accuracy: 0.7298\n",
      "Epoch 33/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3274 - accuracy: 0.8589\n",
      "Epoch 33: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3301 - accuracy: 0.8586 - val_loss: 0.6211 - val_accuracy: 0.7419\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2789 - accuracy: 0.8940\n",
      "Epoch 34: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.2789 - accuracy: 0.8940 - val_loss: 0.7288 - val_accuracy: 0.7258\n",
      "Epoch 34: early stopping\n",
      "Epoch 1/120\n",
      " 6/70 [=>............................] - ETA: 0s - loss: 1.6080 - accuracy: 0.2135 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:50:51.882740: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.1587 - accuracy: 0.4811\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50202, saving model to ./ckpt/5_cls_lr005/val_acc_0.502.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1587 - accuracy: 0.4811 - val_loss: 1.0622 - val_accuracy: 0.5020\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 1.0346 - accuracy: 0.5848"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:50:52.875365: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/70 [==========================>...] - ETA: 0s - loss: 0.9620 - accuracy: 0.5721\n",
      "Epoch 2: val_accuracy improved from 0.50202 to 0.56680, saving model to ./ckpt/5_cls_lr005/val_acc_0.567.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.9591 - accuracy: 0.5718 - val_loss: 0.9633 - val_accuracy: 0.5668\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8682 - accuracy: 0.6157\n",
      "Epoch 3: val_accuracy improved from 0.56680 to 0.63968, saving model to ./ckpt/5_cls_lr005/val_acc_0.640.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.8644 - accuracy: 0.6176 - val_loss: 0.8699 - val_accuracy: 0.6397\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7711 - accuracy: 0.6567\n",
      "Epoch 4: val_accuracy improved from 0.63968 to 0.68016, saving model to ./ckpt/5_cls_lr005/val_acc_0.680.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.7731 - accuracy: 0.6566 - val_loss: 0.7634 - val_accuracy: 0.6802\n",
      "Epoch 5/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.7288 - accuracy: 0.6760\n",
      "Epoch 5: val_accuracy did not improve from 0.68016\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.7277 - accuracy: 0.6728 - val_loss: 0.7393 - val_accuracy: 0.6721\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6908 - accuracy: 0.6831\n",
      "Epoch 6: val_accuracy improved from 0.68016 to 0.71660, saving model to ./ckpt/5_cls_lr005/val_acc_0.717.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6908 - accuracy: 0.6831 - val_loss: 0.6904 - val_accuracy: 0.7166\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6318 - accuracy: 0.7224\n",
      "Epoch 7: val_accuracy did not improve from 0.71660\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6322 - accuracy: 0.7222 - val_loss: 0.6924 - val_accuracy: 0.7166\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5886 - accuracy: 0.7388\n",
      "Epoch 8: val_accuracy did not improve from 0.71660\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5886 - accuracy: 0.7388 - val_loss: 0.7451 - val_accuracy: 0.6680\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5879 - accuracy: 0.7518\n",
      "Epoch 9: val_accuracy did not improve from 0.71660\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5879 - accuracy: 0.7518 - val_loss: 0.7139 - val_accuracy: 0.6518\n",
      "Epoch 10/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5817 - accuracy: 0.7509\n",
      "Epoch 10: val_accuracy did not improve from 0.71660\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5899 - accuracy: 0.7464 - val_loss: 0.6785 - val_accuracy: 0.6923\n",
      "Epoch 11/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5342 - accuracy: 0.7747\n",
      "Epoch 11: val_accuracy did not improve from 0.71660\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5362 - accuracy: 0.7751 - val_loss: 1.0849 - val_accuracy: 0.5870\n",
      "Epoch 12/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5777 - accuracy: 0.7547\n",
      "Epoch 12: val_accuracy improved from 0.71660 to 0.73684, saving model to ./ckpt/5_cls_lr005/val_acc_0.737.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5758 - accuracy: 0.7567 - val_loss: 0.6390 - val_accuracy: 0.7368\n",
      "Epoch 13/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5218 - accuracy: 0.7808\n",
      "Epoch 13: val_accuracy did not improve from 0.73684\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5226 - accuracy: 0.7792 - val_loss: 0.7623 - val_accuracy: 0.6640\n",
      "Epoch 14/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5509 - accuracy: 0.7589\n",
      "Epoch 14: val_accuracy did not improve from 0.73684\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5536 - accuracy: 0.7581 - val_loss: 0.6495 - val_accuracy: 0.7126\n",
      "Epoch 15/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4878 - accuracy: 0.7909\n",
      "Epoch 15: val_accuracy improved from 0.73684 to 0.74899, saving model to ./ckpt/5_cls_lr005/val_acc_0.749.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4943 - accuracy: 0.7895 - val_loss: 0.6765 - val_accuracy: 0.7490\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5402 - accuracy: 0.7682\n",
      "Epoch 16: val_accuracy did not improve from 0.74899\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5468 - accuracy: 0.7653 - val_loss: 0.6502 - val_accuracy: 0.7368\n",
      "Epoch 17/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4995 - accuracy: 0.7910\n",
      "Epoch 17: val_accuracy did not improve from 0.74899\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4942 - accuracy: 0.7940 - val_loss: 0.6344 - val_accuracy: 0.7126\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4909 - accuracy: 0.7869\n",
      "Epoch 18: val_accuracy did not improve from 0.74899\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4876 - accuracy: 0.7882 - val_loss: 0.6157 - val_accuracy: 0.7449\n",
      "Epoch 19/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5005 - accuracy: 0.7840\n",
      "Epoch 19: val_accuracy did not improve from 0.74899\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5036 - accuracy: 0.7810 - val_loss: 0.6240 - val_accuracy: 0.7287\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4487 - accuracy: 0.8134\n",
      "Epoch 20: val_accuracy did not improve from 0.74899\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4515 - accuracy: 0.8119 - val_loss: 0.6222 - val_accuracy: 0.7409\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4470 - accuracy: 0.8162\n",
      "Epoch 21: val_accuracy did not improve from 0.74899\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4549 - accuracy: 0.8128 - val_loss: 0.6344 - val_accuracy: 0.7328\n",
      "Epoch 22/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4532 - accuracy: 0.8134\n",
      "Epoch 22: val_accuracy improved from 0.74899 to 0.75304, saving model to ./ckpt/5_cls_lr005/val_acc_0.753.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4533 - accuracy: 0.8146 - val_loss: 0.6246 - val_accuracy: 0.7530\n",
      "Epoch 23/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4635 - accuracy: 0.7892\n",
      "Epoch 23: val_accuracy did not improve from 0.75304\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4699 - accuracy: 0.7890 - val_loss: 0.7304 - val_accuracy: 0.7166\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4239 - accuracy: 0.8214\n",
      "Epoch 24: val_accuracy did not improve from 0.75304\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4237 - accuracy: 0.8232 - val_loss: 0.7214 - val_accuracy: 0.7206\n",
      "Epoch 25/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.8249\n",
      "Epoch 25: val_accuracy did not improve from 0.75304\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4203 - accuracy: 0.8263 - val_loss: 0.6690 - val_accuracy: 0.7409\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3816 - accuracy: 0.8419\n",
      "Epoch 26: val_accuracy improved from 0.75304 to 0.77733, saving model to ./ckpt/5_cls_lr005/val_acc_0.777.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3870 - accuracy: 0.8407 - val_loss: 0.6397 - val_accuracy: 0.7773\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3858 - accuracy: 0.8377\n",
      "Epoch 27: val_accuracy did not improve from 0.77733\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3807 - accuracy: 0.8407 - val_loss: 0.7289 - val_accuracy: 0.7287\n",
      "Epoch 28/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3829 - accuracy: 0.8451\n",
      "Epoch 28: val_accuracy did not improve from 0.77733\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3828 - accuracy: 0.8452 - val_loss: 0.7021 - val_accuracy: 0.7206\n",
      "Epoch 28: early stopping\n",
      "Epoch 1/120\n",
      " 6/70 [=>............................] - ETA: 0s - loss: 1.5878 - accuracy: 0.2292 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:51:13.274667: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/70 [============================>.] - ETA: 0s - loss: 1.1999 - accuracy: 0.4642\n",
      "Epoch 1: val_accuracy improved from -inf to 0.58704, saving model to ./ckpt/5_cls_lr005/val_acc_0.587.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1966 - accuracy: 0.4632 - val_loss: 0.9337 - val_accuracy: 0.5870\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 1.0001 - accuracy: 0.5536"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:51:14.156778: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9659 - accuracy: 0.5786\n",
      "Epoch 2: val_accuracy improved from 0.58704 to 0.67206, saving model to ./ckpt/5_cls_lr005/val_acc_0.672.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.9681 - accuracy: 0.5754 - val_loss: 0.8103 - val_accuracy: 0.6721\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8739 - accuracy: 0.6158\n",
      "Epoch 3: val_accuracy did not improve from 0.67206\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.8754 - accuracy: 0.6131 - val_loss: 0.7721 - val_accuracy: 0.6518\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.7988 - accuracy: 0.6429\n",
      "Epoch 4: val_accuracy did not improve from 0.67206\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8006 - accuracy: 0.6418 - val_loss: 0.7357 - val_accuracy: 0.6721\n",
      "Epoch 5/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.7221 - accuracy: 0.6827\n",
      "Epoch 5: val_accuracy improved from 0.67206 to 0.72470, saving model to ./ckpt/5_cls_lr005/val_acc_0.725.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.7302 - accuracy: 0.6764 - val_loss: 0.6842 - val_accuracy: 0.7247\n",
      "Epoch 6/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.6684 - accuracy: 0.7058\n",
      "Epoch 6: val_accuracy did not improve from 0.72470\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6777 - accuracy: 0.6975 - val_loss: 0.6883 - val_accuracy: 0.7166\n",
      "Epoch 7/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.6503 - accuracy: 0.7144\n",
      "Epoch 7: val_accuracy improved from 0.72470 to 0.77733, saving model to ./ckpt/5_cls_lr005/val_acc_0.777.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6452 - accuracy: 0.7172 - val_loss: 0.5820 - val_accuracy: 0.7773\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6183 - accuracy: 0.7301\n",
      "Epoch 8: val_accuracy did not improve from 0.77733\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6212 - accuracy: 0.7285 - val_loss: 0.6486 - val_accuracy: 0.7085\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5821 - accuracy: 0.7424\n",
      "Epoch 9: val_accuracy did not improve from 0.77733\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5821 - accuracy: 0.7424 - val_loss: 0.6086 - val_accuracy: 0.7652\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5683 - accuracy: 0.7572\n",
      "Epoch 10: val_accuracy did not improve from 0.77733\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5683 - accuracy: 0.7572 - val_loss: 0.6322 - val_accuracy: 0.7206\n",
      "Epoch 11/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5610 - accuracy: 0.7579\n",
      "Epoch 11: val_accuracy did not improve from 0.77733\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5628 - accuracy: 0.7563 - val_loss: 0.6392 - val_accuracy: 0.7409\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5403 - accuracy: 0.7704\n",
      "Epoch 12: val_accuracy did not improve from 0.77733\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5410 - accuracy: 0.7702 - val_loss: 0.5511 - val_accuracy: 0.7611\n",
      "Epoch 13/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5167 - accuracy: 0.7738\n",
      "Epoch 13: val_accuracy did not improve from 0.77733\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5181 - accuracy: 0.7729 - val_loss: 0.5586 - val_accuracy: 0.7571\n",
      "Epoch 14/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5288 - accuracy: 0.7789\n",
      "Epoch 14: val_accuracy improved from 0.77733 to 0.78543, saving model to ./ckpt/5_cls_lr005/val_acc_0.785.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5277 - accuracy: 0.7792 - val_loss: 0.5310 - val_accuracy: 0.7854\n",
      "Epoch 15/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.5341 - accuracy: 0.7649\n",
      "Epoch 15: val_accuracy did not improve from 0.78543\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5396 - accuracy: 0.7644 - val_loss: 0.5501 - val_accuracy: 0.7733\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5206 - accuracy: 0.7691\n",
      "Epoch 16: val_accuracy did not improve from 0.78543\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5220 - accuracy: 0.7711 - val_loss: 0.5808 - val_accuracy: 0.7449\n",
      "Epoch 17/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.4961 - accuracy: 0.7788\n",
      "Epoch 17: val_accuracy improved from 0.78543 to 0.78947, saving model to ./ckpt/5_cls_lr005/val_acc_0.789.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5048 - accuracy: 0.7756 - val_loss: 0.5274 - val_accuracy: 0.7895\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4719 - accuracy: 0.8002\n",
      "Epoch 18: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4696 - accuracy: 0.8016 - val_loss: 0.5725 - val_accuracy: 0.7733\n",
      "Epoch 19/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4795 - accuracy: 0.8007\n",
      "Epoch 19: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4784 - accuracy: 0.8007 - val_loss: 0.5536 - val_accuracy: 0.7652\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4095 - accuracy: 0.8352\n",
      "Epoch 20: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4141 - accuracy: 0.8344 - val_loss: 0.5899 - val_accuracy: 0.7530\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4161 - accuracy: 0.8270\n",
      "Epoch 21: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4204 - accuracy: 0.8263 - val_loss: 0.5716 - val_accuracy: 0.7652\n",
      "Epoch 22/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4113 - accuracy: 0.8228\n",
      "Epoch 22: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4151 - accuracy: 0.8205 - val_loss: 0.5913 - val_accuracy: 0.7733\n",
      "Epoch 23/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4056 - accuracy: 0.8279\n",
      "Epoch 23: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4082 - accuracy: 0.8254 - val_loss: 0.5707 - val_accuracy: 0.7733\n",
      "Epoch 24/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3915 - accuracy: 0.8352\n",
      "Epoch 24: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3980 - accuracy: 0.8299 - val_loss: 0.5896 - val_accuracy: 0.7733\n",
      "Epoch 25/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3884 - accuracy: 0.8414\n",
      "Epoch 25: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3926 - accuracy: 0.8398 - val_loss: 0.5853 - val_accuracy: 0.7611\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4068 - accuracy: 0.8218\n",
      "Epoch 26: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4110 - accuracy: 0.8200 - val_loss: 0.6383 - val_accuracy: 0.7247\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3870 - accuracy: 0.8414\n",
      "Epoch 27: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3883 - accuracy: 0.8402 - val_loss: 0.6231 - val_accuracy: 0.7490\n",
      "Epoch 27: early stopping\n",
      "Epoch 1/120\n",
      " 6/70 [=>............................] - ETA: 0s - loss: 1.6128 - accuracy: 0.2031 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:51:34.017850: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.1839 - accuracy: 0.4587\n",
      "Epoch 1: val_accuracy improved from -inf to 0.59919, saving model to ./ckpt/5_cls_lr005/val_acc_0.599.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1839 - accuracy: 0.4587 - val_loss: 0.9632 - val_accuracy: 0.5992\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 0.9612 - accuracy: 0.5759"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:51:34.855624: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9607 - accuracy: 0.5677\n",
      "Epoch 2: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.9535 - accuracy: 0.5723 - val_loss: 0.9040 - val_accuracy: 0.5628\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8669 - accuracy: 0.6082\n",
      "Epoch 3: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.8635 - accuracy: 0.6082 - val_loss: 0.8691 - val_accuracy: 0.5951\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8016 - accuracy: 0.6382\n",
      "Epoch 4: val_accuracy improved from 0.59919 to 0.70445, saving model to ./ckpt/5_cls_lr005/val_acc_0.704.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.8016 - accuracy: 0.6382 - val_loss: 0.7105 - val_accuracy: 0.7045\n",
      "Epoch 5/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.7662 - accuracy: 0.6558\n",
      "Epoch 5: val_accuracy did not improve from 0.70445\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.7620 - accuracy: 0.6593 - val_loss: 0.7703 - val_accuracy: 0.6802\n",
      "Epoch 6/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.6856\n",
      "Epoch 6: val_accuracy did not improve from 0.70445\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.6971 - accuracy: 0.6845 - val_loss: 0.8497 - val_accuracy: 0.6154\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6643 - accuracy: 0.7042\n",
      "Epoch 7: val_accuracy improved from 0.70445 to 0.72470, saving model to ./ckpt/5_cls_lr005/val_acc_0.725.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6643 - accuracy: 0.7042 - val_loss: 0.6641 - val_accuracy: 0.7247\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6126 - accuracy: 0.7271\n",
      "Epoch 8: val_accuracy improved from 0.72470 to 0.73684, saving model to ./ckpt/5_cls_lr005/val_acc_0.737.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.6126 - accuracy: 0.7271 - val_loss: 0.6292 - val_accuracy: 0.7368\n",
      "Epoch 9/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6118 - accuracy: 0.7332\n",
      "Epoch 9: val_accuracy did not improve from 0.73684\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.6124 - accuracy: 0.7316 - val_loss: 0.7740 - val_accuracy: 0.6437\n",
      "Epoch 10/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6045 - accuracy: 0.7463\n",
      "Epoch 10: val_accuracy did not improve from 0.73684\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6048 - accuracy: 0.7455 - val_loss: 0.6469 - val_accuracy: 0.7004\n",
      "Epoch 11/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5923 - accuracy: 0.7467\n",
      "Epoch 11: val_accuracy did not improve from 0.73684\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5924 - accuracy: 0.7419 - val_loss: 0.6715 - val_accuracy: 0.7004\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5636 - accuracy: 0.7523\n",
      "Epoch 12: val_accuracy did not improve from 0.73684\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5640 - accuracy: 0.7522 - val_loss: 0.7571 - val_accuracy: 0.6478\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5581 - accuracy: 0.7722\n",
      "Epoch 13: val_accuracy did not improve from 0.73684\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5566 - accuracy: 0.7729 - val_loss: 0.5878 - val_accuracy: 0.7368\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5379 - accuracy: 0.7702\n",
      "Epoch 14: val_accuracy did not improve from 0.73684\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5379 - accuracy: 0.7702 - val_loss: 0.6225 - val_accuracy: 0.7247\n",
      "Epoch 15/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.5040 - accuracy: 0.7889\n",
      "Epoch 15: val_accuracy did not improve from 0.73684\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5103 - accuracy: 0.7877 - val_loss: 0.6250 - val_accuracy: 0.7247\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5136 - accuracy: 0.7831\n",
      "Epoch 16: val_accuracy did not improve from 0.73684\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.5140 - accuracy: 0.7846 - val_loss: 0.6476 - val_accuracy: 0.7247\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5274 - accuracy: 0.7778\n",
      "Epoch 17: val_accuracy did not improve from 0.73684\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5274 - accuracy: 0.7778 - val_loss: 0.6107 - val_accuracy: 0.7328\n",
      "Epoch 18/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5042 - accuracy: 0.7854\n",
      "Epoch 18: val_accuracy did not improve from 0.73684\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5079 - accuracy: 0.7850 - val_loss: 0.6424 - val_accuracy: 0.7004\n",
      "Epoch 19/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.4859 - accuracy: 0.7971\n",
      "Epoch 19: val_accuracy did not improve from 0.73684\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4856 - accuracy: 0.7971 - val_loss: 0.6872 - val_accuracy: 0.7126\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4850 - accuracy: 0.7895\n",
      "Epoch 20: val_accuracy did not improve from 0.73684\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4850 - accuracy: 0.7895 - val_loss: 0.6272 - val_accuracy: 0.7247\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4685 - accuracy: 0.7917\n",
      "Epoch 21: val_accuracy improved from 0.73684 to 0.74089, saving model to ./ckpt/5_cls_lr005/val_acc_0.741.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4661 - accuracy: 0.7949 - val_loss: 0.6297 - val_accuracy: 0.7409\n",
      "Epoch 22/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4294 - accuracy: 0.8214\n",
      "Epoch 22: val_accuracy did not improve from 0.74089\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4298 - accuracy: 0.8209 - val_loss: 0.7397 - val_accuracy: 0.6964\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4445 - accuracy: 0.8035\n",
      "Epoch 23: val_accuracy improved from 0.74089 to 0.75304, saving model to ./ckpt/5_cls_lr005/val_acc_0.753.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4461 - accuracy: 0.8034 - val_loss: 0.6280 - val_accuracy: 0.7530\n",
      "Epoch 23: early stopping\n",
      "Epoch 1/120\n",
      " 6/70 [=>............................] - ETA: 0s - loss: 1.5960 - accuracy: 0.2500 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:51:51.901326: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/70 [============================>.] - ETA: 0s - loss: 1.1962 - accuracy: 0.4540\n",
      "Epoch 1: val_accuracy improved from -inf to 0.52227, saving model to ./ckpt/5_cls_lr005/val_acc_0.522.hdf5\n",
      "70/70 [==============================] - 2s 16ms/step - loss: 1.1942 - accuracy: 0.4574 - val_loss: 1.1185 - val_accuracy: 0.5223\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 0.9217 - accuracy: 0.5625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:51:52.941743: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9839 - accuracy: 0.5569\n",
      "Epoch 2: val_accuracy did not improve from 0.52227\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.9816 - accuracy: 0.5570 - val_loss: 0.9504 - val_accuracy: 0.5142\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8964 - accuracy: 0.6075\n",
      "Epoch 3: val_accuracy improved from 0.52227 to 0.54251, saving model to ./ckpt/5_cls_lr005/val_acc_0.543.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8985 - accuracy: 0.6050 - val_loss: 0.8969 - val_accuracy: 0.5425\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8394 - accuracy: 0.6295\n",
      "Epoch 4: val_accuracy improved from 0.54251 to 0.61943, saving model to ./ckpt/5_cls_lr005/val_acc_0.619.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8371 - accuracy: 0.6311 - val_loss: 0.8507 - val_accuracy: 0.6194\n",
      "Epoch 5/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.7217 - accuracy: 0.6889\n",
      "Epoch 5: val_accuracy improved from 0.61943 to 0.62348, saving model to ./ckpt/5_cls_lr005/val_acc_0.623.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.7228 - accuracy: 0.6881 - val_loss: 0.7498 - val_accuracy: 0.6235\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6985 - accuracy: 0.6893\n",
      "Epoch 6: val_accuracy improved from 0.62348 to 0.63968, saving model to ./ckpt/5_cls_lr005/val_acc_0.640.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.7012 - accuracy: 0.6876 - val_loss: 0.7292 - val_accuracy: 0.6397\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6437 - accuracy: 0.7197\n",
      "Epoch 7: val_accuracy improved from 0.63968 to 0.64372, saving model to ./ckpt/5_cls_lr005/val_acc_0.644.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6434 - accuracy: 0.7199 - val_loss: 0.7292 - val_accuracy: 0.6437\n",
      "Epoch 8/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6414 - accuracy: 0.7275\n",
      "Epoch 8: val_accuracy did not improve from 0.64372\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6418 - accuracy: 0.7271 - val_loss: 0.7787 - val_accuracy: 0.6235\n",
      "Epoch 9/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6333 - accuracy: 0.7421\n",
      "Epoch 9: val_accuracy improved from 0.64372 to 0.65992, saving model to ./ckpt/5_cls_lr005/val_acc_0.660.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6279 - accuracy: 0.7460 - val_loss: 0.7044 - val_accuracy: 0.6599\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5885 - accuracy: 0.7491\n",
      "Epoch 10: val_accuracy improved from 0.65992 to 0.66802, saving model to ./ckpt/5_cls_lr005/val_acc_0.668.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5884 - accuracy: 0.7487 - val_loss: 0.6843 - val_accuracy: 0.6680\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5532 - accuracy: 0.7684\n",
      "Epoch 11: val_accuracy improved from 0.66802 to 0.68826, saving model to ./ckpt/5_cls_lr005/val_acc_0.688.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5546 - accuracy: 0.7689 - val_loss: 0.6594 - val_accuracy: 0.6883\n",
      "Epoch 12/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5454 - accuracy: 0.7546\n",
      "Epoch 12: val_accuracy improved from 0.68826 to 0.70850, saving model to ./ckpt/5_cls_lr005/val_acc_0.709.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5458 - accuracy: 0.7572 - val_loss: 0.6705 - val_accuracy: 0.7085\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5664 - accuracy: 0.7522\n",
      "Epoch 13: val_accuracy did not improve from 0.70850\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.5664 - accuracy: 0.7522 - val_loss: 0.6337 - val_accuracy: 0.7045\n",
      "Epoch 14/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5367 - accuracy: 0.7670\n",
      "Epoch 14: val_accuracy did not improve from 0.70850\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5438 - accuracy: 0.7635 - val_loss: 0.6987 - val_accuracy: 0.6883\n",
      "Epoch 15/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5340 - accuracy: 0.7637\n",
      "Epoch 15: val_accuracy did not improve from 0.70850\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5321 - accuracy: 0.7635 - val_loss: 0.7186 - val_accuracy: 0.6761\n",
      "Epoch 16/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5119 - accuracy: 0.7757\n",
      "Epoch 16: val_accuracy improved from 0.70850 to 0.71255, saving model to ./ckpt/5_cls_lr005/val_acc_0.713.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5134 - accuracy: 0.7751 - val_loss: 0.6649 - val_accuracy: 0.7126\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4932 - accuracy: 0.7940\n",
      "Epoch 17: val_accuracy did not improve from 0.71255\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4932 - accuracy: 0.7940 - val_loss: 0.6875 - val_accuracy: 0.6964\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4928 - accuracy: 0.8034\n",
      "Epoch 18: val_accuracy improved from 0.71255 to 0.74494, saving model to ./ckpt/5_cls_lr005/val_acc_0.745.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4928 - accuracy: 0.8034 - val_loss: 0.6302 - val_accuracy: 0.7449\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4569 - accuracy: 0.8048\n",
      "Epoch 19: val_accuracy did not improve from 0.74494\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4569 - accuracy: 0.8048 - val_loss: 0.6837 - val_accuracy: 0.6842\n",
      "Epoch 20/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4469 - accuracy: 0.8093\n",
      "Epoch 20: val_accuracy did not improve from 0.74494\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4466 - accuracy: 0.8092 - val_loss: 0.7785 - val_accuracy: 0.6883\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4656 - accuracy: 0.8026\n",
      "Epoch 21: val_accuracy did not improve from 0.74494\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4656 - accuracy: 0.8034 - val_loss: 0.6474 - val_accuracy: 0.7247\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4400 - accuracy: 0.8175\n",
      "Epoch 22: val_accuracy did not improve from 0.74494\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4394 - accuracy: 0.8178 - val_loss: 0.6633 - val_accuracy: 0.6842\n",
      "Epoch 23/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4416 - accuracy: 0.8176\n",
      "Epoch 23: val_accuracy did not improve from 0.74494\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4405 - accuracy: 0.8182 - val_loss: 0.7321 - val_accuracy: 0.6964\n",
      "Epoch 24/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4310 - accuracy: 0.8168\n",
      "Epoch 24: val_accuracy did not improve from 0.74494\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4324 - accuracy: 0.8137 - val_loss: 0.6706 - val_accuracy: 0.7126\n",
      "Epoch 25/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4206 - accuracy: 0.8330\n",
      "Epoch 25: val_accuracy did not improve from 0.74494\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4198 - accuracy: 0.8330 - val_loss: 0.7724 - val_accuracy: 0.6964\n",
      "Epoch 26/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.3934 - accuracy: 0.8298\n",
      "Epoch 26: val_accuracy did not improve from 0.74494\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.3970 - accuracy: 0.8299 - val_loss: 0.7602 - val_accuracy: 0.7166\n",
      "Epoch 27/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4193 - accuracy: 0.8254\n",
      "Epoch 27: val_accuracy did not improve from 0.74494\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4193 - accuracy: 0.8254 - val_loss: 0.7261 - val_accuracy: 0.6721\n",
      "Epoch 28/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3799 - accuracy: 0.8465\n",
      "Epoch 28: val_accuracy did not improve from 0.74494\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3799 - accuracy: 0.8465 - val_loss: 0.7442 - val_accuracy: 0.6883\n",
      "Epoch 28: early stopping\n",
      "Epoch 1/120\n",
      " 5/70 [=>............................] - ETA: 0s - loss: 1.6258 - accuracy: 0.1750 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:52:15.168565: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 1.2049 - accuracy: 0.4348\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54251, saving model to ./ckpt/5_cls_lr005/val_acc_0.543.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.2012 - accuracy: 0.4367 - val_loss: 1.0087 - val_accuracy: 0.5425\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.0639 - accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:52:16.118385: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 0.9627 - accuracy: 0.5743\n",
      "Epoch 2: val_accuracy improved from 0.54251 to 0.55870, saving model to ./ckpt/5_cls_lr005/val_acc_0.559.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9615 - accuracy: 0.5750 - val_loss: 0.9473 - val_accuracy: 0.5587\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8706 - accuracy: 0.5993\n",
      "Epoch 3: val_accuracy improved from 0.55870 to 0.65587, saving model to ./ckpt/5_cls_lr005/val_acc_0.656.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8725 - accuracy: 0.5996 - val_loss: 0.7815 - val_accuracy: 0.6559\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8233 - accuracy: 0.6388\n",
      "Epoch 4: val_accuracy improved from 0.65587 to 0.69636, saving model to ./ckpt/5_cls_lr005/val_acc_0.696.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8239 - accuracy: 0.6396 - val_loss: 0.7006 - val_accuracy: 0.6964\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7300 - accuracy: 0.6827\n",
      "Epoch 5: val_accuracy did not improve from 0.69636\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.7300 - accuracy: 0.6827 - val_loss: 0.6985 - val_accuracy: 0.6964\n",
      "Epoch 6/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6776 - accuracy: 0.7150\n",
      "Epoch 6: val_accuracy did not improve from 0.69636\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6799 - accuracy: 0.7145 - val_loss: 0.7059 - val_accuracy: 0.6559\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6471 - accuracy: 0.7204\n",
      "Epoch 7: val_accuracy improved from 0.69636 to 0.71660, saving model to ./ckpt/5_cls_lr005/val_acc_0.717.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.6471 - accuracy: 0.7204 - val_loss: 0.5953 - val_accuracy: 0.7166\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6131 - accuracy: 0.7373\n",
      "Epoch 8: val_accuracy improved from 0.71660 to 0.76518, saving model to ./ckpt/5_cls_lr005/val_acc_0.765.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6162 - accuracy: 0.7352 - val_loss: 0.5344 - val_accuracy: 0.7652\n",
      "Epoch 9/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6132 - accuracy: 0.7346\n",
      "Epoch 9: val_accuracy did not improve from 0.76518\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.6089 - accuracy: 0.7374 - val_loss: 0.5897 - val_accuracy: 0.7247\n",
      "Epoch 10/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5723 - accuracy: 0.7612\n",
      "Epoch 10: val_accuracy improved from 0.76518 to 0.78138, saving model to ./ckpt/5_cls_lr005/val_acc_0.781.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5725 - accuracy: 0.7599 - val_loss: 0.5081 - val_accuracy: 0.7814\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7582\n",
      "Epoch 11: val_accuracy did not improve from 0.78138\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5665 - accuracy: 0.7576 - val_loss: 0.5159 - val_accuracy: 0.7571\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5557 - accuracy: 0.7559\n",
      "Epoch 12: val_accuracy did not improve from 0.78138\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5629 - accuracy: 0.7540 - val_loss: 0.5877 - val_accuracy: 0.7247\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5378 - accuracy: 0.7599\n",
      "Epoch 13: val_accuracy did not improve from 0.78138\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5378 - accuracy: 0.7599 - val_loss: 0.5036 - val_accuracy: 0.7652\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5299 - accuracy: 0.7639\n",
      "Epoch 14: val_accuracy did not improve from 0.78138\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5299 - accuracy: 0.7639 - val_loss: 0.5172 - val_accuracy: 0.7692\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5141 - accuracy: 0.7781\n",
      "Epoch 15: val_accuracy did not improve from 0.78138\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5146 - accuracy: 0.7787 - val_loss: 0.4961 - val_accuracy: 0.7733\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5489 - accuracy: 0.7598\n",
      "Epoch 16: val_accuracy did not improve from 0.78138\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5460 - accuracy: 0.7612 - val_loss: 0.5547 - val_accuracy: 0.7611\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5424 - accuracy: 0.7769\n",
      "Epoch 17: val_accuracy improved from 0.78138 to 0.78543, saving model to ./ckpt/5_cls_lr005/val_acc_0.785.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5424 - accuracy: 0.7769 - val_loss: 0.5329 - val_accuracy: 0.7854\n",
      "Epoch 18/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4870 - accuracy: 0.7966\n",
      "Epoch 18: val_accuracy did not improve from 0.78543\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4855 - accuracy: 0.7971 - val_loss: 0.4906 - val_accuracy: 0.7692\n",
      "Epoch 19/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4974 - accuracy: 0.7980\n",
      "Epoch 19: val_accuracy did not improve from 0.78543\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.5009 - accuracy: 0.7971 - val_loss: 0.5216 - val_accuracy: 0.7733\n",
      "Epoch 20/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4742 - accuracy: 0.8056\n",
      "Epoch 20: val_accuracy improved from 0.78543 to 0.78947, saving model to ./ckpt/5_cls_lr005/val_acc_0.789.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.4820 - accuracy: 0.8021 - val_loss: 0.5184 - val_accuracy: 0.7895\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4792 - accuracy: 0.7964\n",
      "Epoch 21: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4849 - accuracy: 0.7935 - val_loss: 0.4957 - val_accuracy: 0.7895\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4847 - accuracy: 0.8053\n",
      "Epoch 22: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4837 - accuracy: 0.8066 - val_loss: 0.5403 - val_accuracy: 0.7409\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4685 - accuracy: 0.8063\n",
      "Epoch 23: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4671 - accuracy: 0.8061 - val_loss: 0.5530 - val_accuracy: 0.7692\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4452 - accuracy: 0.8211\n",
      "Epoch 24: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4444 - accuracy: 0.8205 - val_loss: 0.5134 - val_accuracy: 0.7854\n",
      "Epoch 25/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.4093 - accuracy: 0.8385\n",
      "Epoch 25: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4200 - accuracy: 0.8344 - val_loss: 0.5038 - val_accuracy: 0.7773\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4280 - accuracy: 0.8245\n",
      "Epoch 26: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4280 - accuracy: 0.8245 - val_loss: 0.5473 - val_accuracy: 0.7895\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3941 - accuracy: 0.8400\n",
      "Epoch 27: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3999 - accuracy: 0.8375 - val_loss: 0.5661 - val_accuracy: 0.7409\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4215 - accuracy: 0.8342\n",
      "Epoch 28: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4244 - accuracy: 0.8321 - val_loss: 0.5685 - val_accuracy: 0.7530\n",
      "Epoch 28: early stopping\n",
      "Finish 10-fold cross validation\n",
      "Best performing model has 0.8105 validation accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# modify to save ckpt for each test\n",
    "ckpt_path = os.path.join(ckpt_path, \"val_acc_{val_accuracy:.3f}.hdf5\")\n",
    "\n",
    "# training params\n",
    "epochs = epochs\n",
    "num_classes = num_classes\n",
    "lr = lr\n",
    "\n",
    "# the k for k fold CV\n",
    "n_split = 10\n",
    "\n",
    "# for recording best performance\n",
    "max_acc = 0\n",
    "best_history = None\n",
    "\n",
    "'''\n",
    "k-fold cross validation\n",
    "Save the best model using validation accuracy as metric\n",
    "Print the global best performace when finished\n",
    "'''\n",
    "for train_index, test_index in KFold(n_split).split(train_examples):\n",
    "\n",
    "    x_train, x_vad = train_examples[train_index], train_examples[test_index]\n",
    "    y_train, y_vad = train_labels[train_index], train_labels[test_index]\n",
    "\n",
    "    model=create_model(num_classes, lr)\n",
    "  \n",
    "    # callbacks\n",
    "    checkpoint_filepath = ckpt_path\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "        mode='auto',\n",
    "    )\n",
    "\n",
    "    # Model weights are saved at the end of every epoch, if it's the best seen so far.\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_vad, y_vad),\n",
    "                        callbacks=[model_checkpoint_callback, early_stopping_callback])\n",
    "\n",
    "    val_acc = max(history.history['val_accuracy'])\n",
    "    \n",
    "    if val_acc > max_acc:\n",
    "        max_acc = val_acc\n",
    "        best_history = history\n",
    "        # print('Best acc so far. Saving params...\\n')\n",
    "\n",
    "print('Finish {}-fold cross validation'.format(n_split))\n",
    "print('Best performing model has {:.4f} validation accuracy'.format(max_acc))\n",
    "\n",
    "#CPU\n",
    "# with tf.device('/CPU:0'):\n",
    "#     history = model.fit(trainX, trainY, epochs=epochs,validation_data=(testX, testY), batch_size=64)\n",
    "\n",
    "# deafult go with GPU\n",
    "# history = model.fit(trainX, trainY, epochs=epochs,validation_data=(testX, testY), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "history = best_history\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# because of early stopping, can't just use \"epochs\"\n",
    "epochs_range = range(len(history.history['loss']))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6702 - accuracy: 0.7356"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:57:19.493771: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 10ms/step - loss: 0.6621 - accuracy: 0.7339\n",
      "\n",
      "evaluate on test set:\n",
      "loss = 0.66211\tacc = 73.395%\n"
     ]
    }
   ],
   "source": [
    "model = create_model(num_classes=num_classes)\n",
    "model.load_weights(os.path.join(ckpt_path, \"val_acc_0.810.hdf5\"))\n",
    "\n",
    "loss, acc = model.evaluate(test_examples, test_labels)\n",
    "\n",
    "print('\\nevaluate on test set:\\nloss = {:.5f}\\tacc = {:.3f}%'.format(loss, acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29cc21816e506614f017a9125cfdb1f5dc655865e499c52ef5f5406a40d25695"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
