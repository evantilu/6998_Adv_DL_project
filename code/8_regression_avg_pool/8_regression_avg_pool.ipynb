{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with avg pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and env settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.min_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables/parameters used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../../data/home_sale_data_324_features_raw_price.csv'\n",
    "\n",
    "ckpt_path = \"./ckpt/reg_lr005/\"\n",
    "os.makedirs(ckpt_path, exist_ok=True)\n",
    "\n",
    "lr = 0.005\n",
    "epochs = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>TotalSF</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>Total_Bathrooms</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>YrBltAndRemod</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>Foundation_PConc</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>hasfireplace</th>\n",
       "      <th>ExterQual_Gd</th>\n",
       "      <th>BsmtQual_Ex</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>HeatingQC_Ex</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>Total_porch_sf</th>\n",
       "      <th>BsmtFinType1_GLQ</th>\n",
       "      <th>KitchenQual_Ex</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>GarageFinish_Fin</th>\n",
       "      <th>...</th>\n",
       "      <th>BsmtExposure_No</th>\n",
       "      <th>Neighborhood_OldTown</th>\n",
       "      <th>Foundation_BrkTil</th>\n",
       "      <th>GarageFinish_None</th>\n",
       "      <th>GarageCond_None</th>\n",
       "      <th>GarageQual_None</th>\n",
       "      <th>GarageType_None</th>\n",
       "      <th>MSSubClass_30</th>\n",
       "      <th>LotShape_Reg</th>\n",
       "      <th>PavedDrive_N</th>\n",
       "      <th>Foundation_CBlock</th>\n",
       "      <th>MSZoning_RM</th>\n",
       "      <th>HeatingQC_TA</th>\n",
       "      <th>CentralAir_N</th>\n",
       "      <th>GarageType_Detchd</th>\n",
       "      <th>MasVnrType_None</th>\n",
       "      <th>GarageFinish_Unf</th>\n",
       "      <th>BsmtQual_TA</th>\n",
       "      <th>FireplaceQu_None</th>\n",
       "      <th>KitchenQual_TA</th>\n",
       "      <th>ExterQual_TA</th>\n",
       "      <th>dummy_1</th>\n",
       "      <th>dummy_2</th>\n",
       "      <th>dummy_3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.343848</td>\n",
       "      <td>0.171494</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.163872</td>\n",
       "      <td>0.282258</td>\n",
       "      <td>0.926316</td>\n",
       "      <td>0.264504</td>\n",
       "      <td>0.381677</td>\n",
       "      <td>0.949275</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.309164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361371</td>\n",
       "      <td>0.224093</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.483879</td>\n",
       "      <td>0.261800</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.163872</td>\n",
       "      <td>0.188172</td>\n",
       "      <td>0.642105</td>\n",
       "      <td>0.254523</td>\n",
       "      <td>0.365900</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>138571.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.381263</td>\n",
       "      <td>0.189291</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.163872</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.291953</td>\n",
       "      <td>0.422182</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.309164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.371257</td>\n",
       "      <td>0.111480</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.400287</td>\n",
       "      <td>0.198997</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.306925</td>\n",
       "      <td>0.442702</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114518.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.579871</td>\n",
       "      <td>0.317066</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.088659</td>\n",
       "      <td>0.295699</td>\n",
       "      <td>0.547368</td>\n",
       "      <td>0.489083</td>\n",
       "      <td>0.633984</td>\n",
       "      <td>0.688406</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307674</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.229601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148364.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OverallQual  GrLivArea   TotalSF  GarageCars  Total_Bathrooms  \\\n",
       "589      0.555556   0.343848  0.171494         0.4         0.163872   \n",
       "1520     0.555556   0.483879  0.261800         0.2         0.163872   \n",
       "1124     0.444444   0.381263  0.189291         0.4         0.163872   \n",
       "2696     0.444444   0.400287  0.198997         0.2         0.001104   \n",
       "1745     0.444444   0.579871  0.317066         0.4         0.088659   \n",
       "\n",
       "      GarageArea  YrBltAndRemod  TotalBsmtSF  1stFlrSF  YearBuilt  FullBath  \\\n",
       "589     0.282258       0.926316     0.264504  0.381677   0.949275      0.25   \n",
       "1520    0.188172       0.642105     0.254523  0.365900   0.550725      0.25   \n",
       "1124    0.451613       0.684211     0.291953  0.422182   0.782609      0.25   \n",
       "2696    0.208333       0.526316     0.306925  0.442702   0.673913      0.25   \n",
       "1745    0.295699       0.547368     0.489083  0.633984   0.688406      0.25   \n",
       "\n",
       "      YearRemodAdd  Foundation_PConc  TotRmsAbvGrd  hasfireplace  \\\n",
       "589       0.883333               1.0      0.309164           0.0   \n",
       "1520      0.900000               0.0      0.421336           0.0   \n",
       "1124      0.500000               0.0      0.309164           0.0   \n",
       "2696      0.250000               0.0      0.421336           0.0   \n",
       "1745      0.283333               0.0      0.600315           0.0   \n",
       "\n",
       "      ExterQual_Gd  BsmtQual_Ex  Fireplaces  HeatingQC_Ex  MasVnrArea  \\\n",
       "589            1.0          0.0         0.0           1.0    0.361371   \n",
       "1520           0.0          0.0         0.0           0.0    0.000000   \n",
       "1124           0.0          0.0         0.0           0.0    0.371257   \n",
       "2696           0.0          0.0         0.0           0.0    0.000000   \n",
       "1745           0.0          0.0         0.0           0.0    0.000000   \n",
       "\n",
       "      Total_porch_sf  BsmtFinType1_GLQ  KitchenQual_Ex  OpenPorchSF  \\\n",
       "589         0.224093               1.0             0.0     0.000000   \n",
       "1520        0.453207               0.0             0.0     0.000000   \n",
       "1124        0.111480               1.0             0.0     0.000000   \n",
       "2696        0.000000               0.0             0.0     0.000000   \n",
       "1745        0.307674               0.0             0.0     0.229601   \n",
       "\n",
       "      GarageFinish_Fin  ...  BsmtExposure_No  Neighborhood_OldTown  \\\n",
       "589                1.0  ...              0.0                   0.0   \n",
       "1520               0.0  ...              1.0                   0.0   \n",
       "1124               0.0  ...              0.0                   1.0   \n",
       "2696               0.0  ...              1.0                   0.0   \n",
       "1745               0.0  ...              1.0                   0.0   \n",
       "\n",
       "      Foundation_BrkTil  GarageFinish_None  GarageCond_None  GarageQual_None  \\\n",
       "589                 0.0                0.0              0.0              0.0   \n",
       "1520                0.0                0.0              0.0              0.0   \n",
       "1124                0.0                0.0              0.0              0.0   \n",
       "2696                0.0                0.0              0.0              0.0   \n",
       "1745                0.0                0.0              0.0              0.0   \n",
       "\n",
       "      GarageType_None  MSSubClass_30  LotShape_Reg  PavedDrive_N  \\\n",
       "589               0.0            0.0           1.0           0.0   \n",
       "1520              0.0            0.0           1.0           0.0   \n",
       "1124              0.0            0.0           1.0           0.0   \n",
       "2696              0.0            0.0           0.0           0.0   \n",
       "1745              0.0            0.0           1.0           0.0   \n",
       "\n",
       "      Foundation_CBlock  MSZoning_RM  HeatingQC_TA  CentralAir_N  \\\n",
       "589                 0.0          1.0           0.0           0.0   \n",
       "1520                1.0          0.0           1.0           0.0   \n",
       "1124                1.0          1.0           1.0           0.0   \n",
       "2696                1.0          0.0           1.0           0.0   \n",
       "1745                1.0          0.0           1.0           0.0   \n",
       "\n",
       "      GarageType_Detchd  MasVnrType_None  GarageFinish_Unf  BsmtQual_TA  \\\n",
       "589                 0.0              0.0               0.0          0.0   \n",
       "1520                1.0              1.0               1.0          1.0   \n",
       "1124                1.0              0.0               1.0          0.0   \n",
       "2696                0.0              1.0               1.0          1.0   \n",
       "1745                0.0              1.0               1.0          1.0   \n",
       "\n",
       "      FireplaceQu_None  KitchenQual_TA  ExterQual_TA  dummy_1  dummy_2  \\\n",
       "589                1.0             0.0           0.0      0.0      0.0   \n",
       "1520               1.0             1.0           1.0      0.0      0.0   \n",
       "1124               1.0             1.0           1.0      0.0      0.0   \n",
       "2696               1.0             1.0           1.0      0.0      0.0   \n",
       "1745               1.0             1.0           1.0      0.0      0.0   \n",
       "\n",
       "      dummy_3      label  \n",
       "589       0.0  140000.00  \n",
       "1520      0.0  138571.19  \n",
       "1124      0.0  140000.00  \n",
       "2696      0.0  114518.71  \n",
       "1745      0.0  148364.31  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(file)\n",
    "\n",
    "'''suffle rows randomly'''\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "labels = data['label']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>TotalSF</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>Total_Bathrooms</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>YrBltAndRemod</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>Foundation_PConc</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>hasfireplace</th>\n",
       "      <th>ExterQual_Gd</th>\n",
       "      <th>BsmtQual_Ex</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>HeatingQC_Ex</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>Total_porch_sf</th>\n",
       "      <th>BsmtFinType1_GLQ</th>\n",
       "      <th>KitchenQual_Ex</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>GarageFinish_Fin</th>\n",
       "      <th>...</th>\n",
       "      <th>Neighborhood_IDOTRR</th>\n",
       "      <th>BsmtExposure_No</th>\n",
       "      <th>Neighborhood_OldTown</th>\n",
       "      <th>Foundation_BrkTil</th>\n",
       "      <th>GarageFinish_None</th>\n",
       "      <th>GarageCond_None</th>\n",
       "      <th>GarageQual_None</th>\n",
       "      <th>GarageType_None</th>\n",
       "      <th>MSSubClass_30</th>\n",
       "      <th>LotShape_Reg</th>\n",
       "      <th>PavedDrive_N</th>\n",
       "      <th>Foundation_CBlock</th>\n",
       "      <th>MSZoning_RM</th>\n",
       "      <th>HeatingQC_TA</th>\n",
       "      <th>CentralAir_N</th>\n",
       "      <th>GarageType_Detchd</th>\n",
       "      <th>MasVnrType_None</th>\n",
       "      <th>GarageFinish_Unf</th>\n",
       "      <th>BsmtQual_TA</th>\n",
       "      <th>FireplaceQu_None</th>\n",
       "      <th>KitchenQual_TA</th>\n",
       "      <th>ExterQual_TA</th>\n",
       "      <th>dummy_1</th>\n",
       "      <th>dummy_2</th>\n",
       "      <th>dummy_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.0</td>\n",
       "      <td>2911.0</td>\n",
       "      <td>2911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.565136</td>\n",
       "      <td>0.542763</td>\n",
       "      <td>0.293431</td>\n",
       "      <td>0.353143</td>\n",
       "      <td>0.200863</td>\n",
       "      <td>0.317285</td>\n",
       "      <td>0.660811</td>\n",
       "      <td>0.326706</td>\n",
       "      <td>0.487996</td>\n",
       "      <td>0.719569</td>\n",
       "      <td>0.391876</td>\n",
       "      <td>0.570892</td>\n",
       "      <td>0.447956</td>\n",
       "      <td>0.542786</td>\n",
       "      <td>0.512882</td>\n",
       "      <td>0.335967</td>\n",
       "      <td>0.087255</td>\n",
       "      <td>0.171718</td>\n",
       "      <td>0.511508</td>\n",
       "      <td>0.159954</td>\n",
       "      <td>0.203170</td>\n",
       "      <td>0.290622</td>\n",
       "      <td>0.069392</td>\n",
       "      <td>0.176542</td>\n",
       "      <td>0.245620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031261</td>\n",
       "      <td>0.652697</td>\n",
       "      <td>0.082102</td>\n",
       "      <td>0.106493</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.053590</td>\n",
       "      <td>0.047750</td>\n",
       "      <td>0.637582</td>\n",
       "      <td>0.073514</td>\n",
       "      <td>0.423222</td>\n",
       "      <td>0.158708</td>\n",
       "      <td>0.293370</td>\n",
       "      <td>0.066644</td>\n",
       "      <td>0.266919</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.421848</td>\n",
       "      <td>0.439368</td>\n",
       "      <td>0.487118</td>\n",
       "      <td>0.511852</td>\n",
       "      <td>0.616627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.155988</td>\n",
       "      <td>0.125193</td>\n",
       "      <td>0.119772</td>\n",
       "      <td>0.152244</td>\n",
       "      <td>0.133043</td>\n",
       "      <td>0.143976</td>\n",
       "      <td>0.242686</td>\n",
       "      <td>0.131930</td>\n",
       "      <td>0.131369</td>\n",
       "      <td>0.219351</td>\n",
       "      <td>0.138140</td>\n",
       "      <td>0.348189</td>\n",
       "      <td>0.497369</td>\n",
       "      <td>0.128554</td>\n",
       "      <td>0.499920</td>\n",
       "      <td>0.472409</td>\n",
       "      <td>0.282257</td>\n",
       "      <td>0.181727</td>\n",
       "      <td>0.499953</td>\n",
       "      <td>0.218350</td>\n",
       "      <td>0.163524</td>\n",
       "      <td>0.454127</td>\n",
       "      <td>0.254163</td>\n",
       "      <td>0.183129</td>\n",
       "      <td>0.430528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174051</td>\n",
       "      <td>0.476195</td>\n",
       "      <td>0.274568</td>\n",
       "      <td>0.308520</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.225245</td>\n",
       "      <td>0.213273</td>\n",
       "      <td>0.480781</td>\n",
       "      <td>0.261024</td>\n",
       "      <td>0.494155</td>\n",
       "      <td>0.365467</td>\n",
       "      <td>0.455385</td>\n",
       "      <td>0.249447</td>\n",
       "      <td>0.442425</td>\n",
       "      <td>0.488798</td>\n",
       "      <td>0.493939</td>\n",
       "      <td>0.496395</td>\n",
       "      <td>0.499920</td>\n",
       "      <td>0.499945</td>\n",
       "      <td>0.486292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.451636</td>\n",
       "      <td>0.213557</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.088659</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.247193</td>\n",
       "      <td>0.395003</td>\n",
       "      <td>0.590580</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.547807</td>\n",
       "      <td>0.282326</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.164976</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.652632</td>\n",
       "      <td>0.308172</td>\n",
       "      <td>0.481664</td>\n",
       "      <td>0.731884</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516936</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.621247</td>\n",
       "      <td>0.356447</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.252530</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.405490</td>\n",
       "      <td>0.582574</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.353218</td>\n",
       "      <td>0.327089</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 324 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       OverallQual    GrLivArea      TotalSF   GarageCars  Total_Bathrooms  \\\n",
       "count  2911.000000  2911.000000  2911.000000  2911.000000      2911.000000   \n",
       "mean      0.565136     0.542763     0.293431     0.353143         0.200863   \n",
       "std       0.155988     0.125193     0.119772     0.152244         0.133043   \n",
       "min       0.000000     0.000000     0.000000     0.000000         0.000000   \n",
       "25%       0.444444     0.451636     0.213557     0.200000         0.088659   \n",
       "50%       0.555556     0.547807     0.282326     0.400000         0.164976   \n",
       "75%       0.666667     0.621247     0.356447     0.400000         0.252530   \n",
       "max       1.000000     1.000000     1.000000     1.000000         1.000000   \n",
       "\n",
       "        GarageArea  YrBltAndRemod  TotalBsmtSF     1stFlrSF    YearBuilt  \\\n",
       "count  2911.000000    2911.000000  2911.000000  2911.000000  2911.000000   \n",
       "mean      0.317285       0.660811     0.326706     0.487996     0.719569   \n",
       "std       0.143976       0.242686     0.131930     0.131369     0.219351   \n",
       "min       0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.215054       0.473684     0.247193     0.395003     0.590580   \n",
       "50%       0.322581       0.652632     0.308172     0.481664     0.731884   \n",
       "75%       0.387097       0.905263     0.405490     0.582574     0.934783   \n",
       "max       1.000000       1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          FullBath  YearRemodAdd  Foundation_PConc  TotRmsAbvGrd  \\\n",
       "count  2911.000000   2911.000000       2911.000000   2911.000000   \n",
       "mean      0.391876      0.570892          0.447956      0.542786   \n",
       "std       0.138140      0.348189          0.497369      0.128554   \n",
       "min       0.000000      0.000000          0.000000      0.000000   \n",
       "25%       0.250000      0.250000          0.000000      0.421336   \n",
       "50%       0.500000      0.716667          0.000000      0.516936   \n",
       "75%       0.500000      0.900000          1.000000      0.600315   \n",
       "max       1.000000      1.000000          1.000000      1.000000   \n",
       "\n",
       "       hasfireplace  ExterQual_Gd  BsmtQual_Ex   Fireplaces  HeatingQC_Ex  \\\n",
       "count   2911.000000   2911.000000  2911.000000  2911.000000   2911.000000   \n",
       "mean       0.512882      0.335967     0.087255     0.171718      0.511508   \n",
       "std        0.499920      0.472409     0.282257     0.181727      0.499953   \n",
       "min        0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "25%        0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "50%        1.000000      0.000000     0.000000     0.293793      1.000000   \n",
       "75%        1.000000      1.000000     0.000000     0.293793      1.000000   \n",
       "max        1.000000      1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "        MasVnrArea  Total_porch_sf  BsmtFinType1_GLQ  KitchenQual_Ex  \\\n",
       "count  2911.000000     2911.000000       2911.000000     2911.000000   \n",
       "mean      0.159954        0.203170          0.290622        0.069392   \n",
       "std       0.218350        0.163524          0.454127        0.254163   \n",
       "min       0.000000        0.000000          0.000000        0.000000   \n",
       "25%       0.000000        0.066964          0.000000        0.000000   \n",
       "50%       0.000000        0.179849          0.000000        0.000000   \n",
       "75%       0.353218        0.327089          1.000000        0.000000   \n",
       "max       1.000000        1.000000          1.000000        1.000000   \n",
       "\n",
       "       OpenPorchSF  GarageFinish_Fin  ...  Neighborhood_IDOTRR  \\\n",
       "count  2911.000000       2911.000000  ...          2911.000000   \n",
       "mean      0.176542          0.245620  ...             0.031261   \n",
       "std       0.183129          0.430528  ...             0.174051   \n",
       "min       0.000000          0.000000  ...             0.000000   \n",
       "25%       0.000000          0.000000  ...             0.000000   \n",
       "50%       0.180865          0.000000  ...             0.000000   \n",
       "75%       0.309510          0.000000  ...             0.000000   \n",
       "max       1.000000          1.000000  ...             1.000000   \n",
       "\n",
       "       BsmtExposure_No  Neighborhood_OldTown  Foundation_BrkTil  \\\n",
       "count      2911.000000           2911.000000        2911.000000   \n",
       "mean          0.652697              0.082102           0.106493   \n",
       "std           0.476195              0.274568           0.308520   \n",
       "min           0.000000              0.000000           0.000000   \n",
       "25%           0.000000              0.000000           0.000000   \n",
       "50%           1.000000              0.000000           0.000000   \n",
       "75%           1.000000              0.000000           0.000000   \n",
       "max           1.000000              1.000000           1.000000   \n",
       "\n",
       "       GarageFinish_None  GarageCond_None  GarageQual_None  GarageType_None  \\\n",
       "count        2911.000000      2911.000000      2911.000000      2911.000000   \n",
       "mean            0.054277         0.054277         0.054277         0.053590   \n",
       "std             0.226602         0.226602         0.226602         0.225245   \n",
       "min             0.000000         0.000000         0.000000         0.000000   \n",
       "25%             0.000000         0.000000         0.000000         0.000000   \n",
       "50%             0.000000         0.000000         0.000000         0.000000   \n",
       "75%             0.000000         0.000000         0.000000         0.000000   \n",
       "max             1.000000         1.000000         1.000000         1.000000   \n",
       "\n",
       "       MSSubClass_30  LotShape_Reg  PavedDrive_N  Foundation_CBlock  \\\n",
       "count    2911.000000   2911.000000   2911.000000        2911.000000   \n",
       "mean        0.047750      0.637582      0.073514           0.423222   \n",
       "std         0.213273      0.480781      0.261024           0.494155   \n",
       "min         0.000000      0.000000      0.000000           0.000000   \n",
       "25%         0.000000      0.000000      0.000000           0.000000   \n",
       "50%         0.000000      1.000000      0.000000           0.000000   \n",
       "75%         0.000000      1.000000      0.000000           1.000000   \n",
       "max         1.000000      1.000000      1.000000           1.000000   \n",
       "\n",
       "       MSZoning_RM  HeatingQC_TA  CentralAir_N  GarageType_Detchd  \\\n",
       "count  2911.000000   2911.000000   2911.000000        2911.000000   \n",
       "mean      0.158708      0.293370      0.066644           0.266919   \n",
       "std       0.365467      0.455385      0.249447           0.442425   \n",
       "min       0.000000      0.000000      0.000000           0.000000   \n",
       "25%       0.000000      0.000000      0.000000           0.000000   \n",
       "50%       0.000000      0.000000      0.000000           0.000000   \n",
       "75%       0.000000      1.000000      0.000000           1.000000   \n",
       "max       1.000000      1.000000      1.000000           1.000000   \n",
       "\n",
       "       MasVnrType_None  GarageFinish_Unf  BsmtQual_TA  FireplaceQu_None  \\\n",
       "count      2911.000000       2911.000000  2911.000000       2911.000000   \n",
       "mean          0.605634          0.421848     0.439368          0.487118   \n",
       "std           0.488798          0.493939     0.496395          0.499920   \n",
       "min           0.000000          0.000000     0.000000          0.000000   \n",
       "25%           0.000000          0.000000     0.000000          0.000000   \n",
       "50%           1.000000          0.000000     0.000000          0.000000   \n",
       "75%           1.000000          1.000000     1.000000          1.000000   \n",
       "max           1.000000          1.000000     1.000000          1.000000   \n",
       "\n",
       "       KitchenQual_TA  ExterQual_TA  dummy_1  dummy_2  dummy_3  \n",
       "count     2911.000000   2911.000000   2911.0   2911.0   2911.0  \n",
       "mean         0.511852      0.616627      0.0      0.0      0.0  \n",
       "std          0.499945      0.486292      0.0      0.0      0.0  \n",
       "min          0.000000      0.000000      0.0      0.0      0.0  \n",
       "25%          0.000000      0.000000      0.0      0.0      0.0  \n",
       "50%          1.000000      1.000000      0.0      0.0      0.0  \n",
       "75%          1.000000      1.000000      0.0      0.0      0.0  \n",
       "max          1.000000      1.000000      0.0      0.0      0.0  \n",
       "\n",
       "[8 rows x 324 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop label column\n",
    "data.drop(['label'], axis=1, inplace=True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      2911.000000\n",
       "mean     180161.314822\n",
       "std       78538.134736\n",
       "min       34900.000000\n",
       "25%      129082.035000\n",
       "50%      160200.000000\n",
       "75%      212000.000000\n",
       "max      755000.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 18, 18)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_np = data.to_numpy()\n",
    "data_np.shape\n",
    "data_np = data_np.reshape(len(data), 18, 18)\n",
    "data_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data_np[0].shape\n",
    "\n",
    "nArray = np.array(data_np[99])\n",
    "\n",
    "\n",
    "a11=nArray.reshape(18,18)\n",
    "plt.imshow(a11)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''because we don't use one-hot encoding in reggresion, need to change labels from pandas series to np array'''\n",
    "labels = labels.to_numpy()\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 18, 18)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_examples = data_np\n",
    "all_examples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train test splitting\n",
    "- hold out 15% for testing\n",
    "- use 85% to train model with K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_samples = all_examples.shape[0] \n",
    "test_ratio = 0.15\n",
    "test_samples = int(test_ratio * all_examples.shape[0])\n",
    "\n",
    "train_examples = all_examples[:-1*test_samples]\n",
    "test_examples = all_examples[-1*test_samples:]\n",
    "train_labels = labels[:-1*test_samples]\n",
    "test_labels = labels[-1*test_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (2475, 18, 18)\n",
      "test:  (436, 18, 18)\n",
      "train label:  (2475,)\n",
      "test label:  (436,)\n"
     ]
    }
   ],
   "source": [
    "print('train: ', train_examples.shape)\n",
    "print('test: ', test_examples.shape)\n",
    "print('train label: ', train_labels.shape)\n",
    "print('test label: ', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def create_reg_model(lr=0.005):\n",
    "\n",
    "\t# Working\n",
    "\twith tf.device('/cpu:0'):\n",
    "\t\tdata_augmentation = tf.keras.Sequential([ \n",
    "\t\t\ttf.keras.layers.RandomFlip(\"horizontal\", input_shape=(18, 18, 1)),\n",
    "\t  \t\ttf.keras.layers.RandomRotation(0.1),\n",
    "\t\t    tf.keras.layers.RandomZoom(0.1)\n",
    "\t\t\t])\n",
    "\n",
    "\n",
    "\tmodel = tf.keras.Sequential([\n",
    "\t\t# data_augmentation,\n",
    "\t  \t# tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(18, 18, 1)),\n",
    "\t\ttf.keras.layers.AveragePooling2D((2, 2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "\t\ttf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t  \ttf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.MaxPooling2D((2,2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Flatten(),\n",
    "\t\ttf.keras.layers.Dense(128, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(64, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(32, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "\t])\n",
    "\n",
    "\t# opt = tf.keras.optimizers.SGD(lr=0.005, momentum=0.9)\n",
    "\topt = tf.keras.optimizers.Adam(lr=lr)\n",
    "\tmodel.compile(optimizer=opt, loss=rmse, metrics=[rmse])\n",
    "\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trainging & visualizing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_reg_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(train_examples, train_labels, epochs=epochs, validation_data=(test_examples, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_rmse = history.history['rmse']\n",
    "# val_rmse = history.history['val_rmse']\n",
    "\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# epochs_range = range(epochs)\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(epochs_range, train_rmse, label='Training RMSE')\n",
    "# plt.plot(epochs_range, val_rmse, label='Validation RMSE')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "\n",
    "\n",
    "# '''How far are predictions from real values?'''\n",
    "# from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# def format_tick_labels(x, pos):\n",
    "#     return '{:.0f}k'.format(x/1000)\n",
    "\n",
    "# predictions = model.predict(test_examples)\n",
    "\n",
    "# xlims = [0, max(test_labels)*1.1]\n",
    "# ylims = [0, max(predictions)*1.1]\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.set_aspect('equal')\n",
    "# ax.scatter(test_labels, predictions)\n",
    "# ax.xaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "# ax.yaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "# ax.set_xlim(xlims)\n",
    "# ax.set_ylim(ylims)\n",
    "# ax.set_xlabel('Actual Price')\n",
    "# ax.set_ylabel('Predicted Price')\n",
    "\n",
    "# ax.plot(xlims, ylims, 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold CV Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evantilu/miniforge3/envs/6998_DL_tf/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2022-04-30 15:07:13.104212: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 105844.3203 - rmse: 105579.0859"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:07:16.700778: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 57201.09375, saving model to ./ckpt/reg_lr005/val_rmse_57395.hdf5\n",
      "70/70 [==============================] - 4s 25ms/step - loss: 105844.3203 - rmse: 105579.0859 - val_loss: 57201.0938 - val_rmse: 57394.9844\n",
      "Epoch 2/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 53112.5586 - rmse: 53112.5586\n",
      "Epoch 2: val_loss improved from 57201.09375 to 47690.07422, saving model to ./ckpt/reg_lr005/val_rmse_47712.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 53655.5000 - rmse: 53688.4727 - val_loss: 47690.0742 - val_rmse: 47711.6758\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 47692.5859 - rmse: 47692.5859\n",
      "Epoch 3: val_loss improved from 47690.07422 to 45922.51172, saving model to ./ckpt/reg_lr005/val_rmse_45911.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 47357.4180 - rmse: 47204.6367 - val_loss: 45922.5117 - val_rmse: 45910.5156\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 44079.1250 - rmse: 44079.1250\n",
      "Epoch 4: val_loss improved from 45922.51172 to 38772.96094, saving model to ./ckpt/reg_lr005/val_rmse_38564.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 44085.9219 - rmse: 44090.5000 - val_loss: 38772.9609 - val_rmse: 38563.6484\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 42093.5156 - rmse: 42093.5156\n",
      "Epoch 5: val_loss improved from 38772.96094 to 38607.31250, saving model to ./ckpt/reg_lr005/val_rmse_38537.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 41963.0547 - rmse: 41875.0664 - val_loss: 38607.3125 - val_rmse: 38536.7656\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 42698.6016 - rmse: 42698.6016\n",
      "Epoch 6: val_loss did not improve from 38607.31250\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 42468.6992 - rmse: 42368.7383 - val_loss: 49370.4062 - val_rmse: 49504.2344\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 40961.7969 - rmse: 40961.7969\n",
      "Epoch 7: val_loss did not improve from 38607.31250\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 41011.5625 - rmse: 41045.1211 - val_loss: 39032.7578 - val_rmse: 38807.0977\n",
      "Epoch 8/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 38097.4453 - rmse: 38097.4453\n",
      "Epoch 8: val_loss improved from 38607.31250 to 34931.26953, saving model to ./ckpt/reg_lr005/val_rmse_34896.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 38246.1406 - rmse: 38339.5938 - val_loss: 34931.2695 - val_rmse: 34895.6797\n",
      "Epoch 9/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 36367.0234 - rmse: 36367.0234\n",
      "Epoch 9: val_loss improved from 34931.26953 to 33097.46875, saving model to ./ckpt/reg_lr005/val_rmse_33054.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 36444.0312 - rmse: 36373.7891 - val_loss: 33097.4688 - val_rmse: 33054.4336\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 35251.3438 - rmse: 35266.6055\n",
      "Epoch 10: val_loss improved from 33097.46875 to 31728.99414, saving model to ./ckpt/reg_lr005/val_rmse_31633.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 35251.3438 - rmse: 35266.6055 - val_loss: 31728.9941 - val_rmse: 31632.6172\n",
      "Epoch 11/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 35699.3242 - rmse: 35699.3242\n",
      "Epoch 11: val_loss did not improve from 31728.99414\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 35646.5156 - rmse: 35657.4141 - val_loss: 35144.8867 - val_rmse: 34999.3867\n",
      "Epoch 12/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 33475.9844 - rmse: 33475.9844\n",
      "Epoch 12: val_loss improved from 31728.99414 to 28980.27344, saving model to ./ckpt/reg_lr005/val_rmse_28922.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 33653.7070 - rmse: 33662.9805 - val_loss: 28980.2734 - val_rmse: 28921.8203\n",
      "Epoch 13/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 31981.0957 - rmse: 31981.0957\n",
      "Epoch 13: val_loss improved from 28980.27344 to 27329.43555, saving model to ./ckpt/reg_lr005/val_rmse_27317.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 31669.5078 - rmse: 31625.1582 - val_loss: 27329.4355 - val_rmse: 27316.6445\n",
      "Epoch 14/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 30114.3867 - rmse: 30114.3867\n",
      "Epoch 14: val_loss did not improve from 27329.43555\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 29893.8984 - rmse: 29924.8906 - val_loss: 35928.8008 - val_rmse: 36114.1367\n",
      "Epoch 15/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 30836.4590 - rmse: 30843.9043\n",
      "Epoch 15: val_loss did not improve from 27329.43555\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 30836.4590 - rmse: 30843.9043 - val_loss: 32755.5840 - val_rmse: 32881.2305\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 28512.2812 - rmse: 28512.2812\n",
      "Epoch 16: val_loss improved from 27329.43555 to 25180.36523, saving model to ./ckpt/reg_lr005/val_rmse_25288.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 28676.3809 - rmse: 28680.7715 - val_loss: 25180.3652 - val_rmse: 25287.6602\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27669.5078 - rmse: 27669.5078\n",
      "Epoch 17: val_loss did not improve from 25180.36523\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27870.4629 - rmse: 27979.9961 - val_loss: 30928.2539 - val_rmse: 31044.2930\n",
      "Epoch 18/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 30201.6367 - rmse: 30201.6367\n",
      "Epoch 18: val_loss did not improve from 25180.36523\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 30269.2930 - rmse: 30255.7852 - val_loss: 28651.7969 - val_rmse: 28797.4902\n",
      "Epoch 19/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 26087.1230 - rmse: 26087.1230\n",
      "Epoch 19: val_loss improved from 25180.36523 to 22819.12500, saving model to ./ckpt/reg_lr005/val_rmse_22906.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26333.7188 - rmse: 26318.6914 - val_loss: 22819.1250 - val_rmse: 22906.2168\n",
      "Epoch 20/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 27296.2051 - rmse: 27296.2051\n",
      "Epoch 20: val_loss improved from 22819.12500 to 21631.35156, saving model to ./ckpt/reg_lr005/val_rmse_21656.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27132.9238 - rmse: 27093.5996 - val_loss: 21631.3516 - val_rmse: 21656.1543\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25750.3086 - rmse: 25750.3086\n",
      "Epoch 21: val_loss did not improve from 21631.35156\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25569.8086 - rmse: 25501.9980 - val_loss: 23560.2637 - val_rmse: 23628.5566\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 26317.5527 - rmse: 26317.5527\n",
      "Epoch 22: val_loss did not improve from 21631.35156\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 26328.9316 - rmse: 26341.1836 - val_loss: 34502.7656 - val_rmse: 34702.1797\n",
      "Epoch 23/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 26999.2148 - rmse: 26999.2148\n",
      "Epoch 23: val_loss improved from 21631.35156 to 20897.85352, saving model to ./ckpt/reg_lr005/val_rmse_20902.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26853.9473 - rmse: 26873.3496 - val_loss: 20897.8535 - val_rmse: 20902.2559\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23783.5371 - rmse: 23783.5371\n",
      "Epoch 24: val_loss did not improve from 20897.85352\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23861.8691 - rmse: 23935.6426 - val_loss: 26111.5898 - val_rmse: 26098.4883\n",
      "Epoch 25/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 24063.9648 - rmse: 24063.9648\n",
      "Epoch 25: val_loss improved from 20897.85352 to 20020.67188, saving model to ./ckpt/reg_lr005/val_rmse_20014.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24015.3301 - rmse: 24070.6777 - val_loss: 20020.6719 - val_rmse: 20014.2148\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23098.9355 - rmse: 23098.9355\n",
      "Epoch 26: val_loss improved from 20020.67188 to 19959.64258, saving model to ./ckpt/reg_lr005/val_rmse_19982.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23246.4941 - rmse: 23309.8555 - val_loss: 19959.6426 - val_rmse: 19982.2461\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23178.3789 - rmse: 23178.3789\n",
      "Epoch 27: val_loss did not improve from 19959.64258\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23332.7422 - rmse: 23326.2148 - val_loss: 29694.3770 - val_rmse: 29845.7695\n",
      "Epoch 28/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 26481.2930 - rmse: 26481.2930\n",
      "Epoch 28: val_loss did not improve from 19959.64258\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26280.1660 - rmse: 26292.2109 - val_loss: 25816.2500 - val_rmse: 25911.2285\n",
      "Epoch 29/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24561.5332 - rmse: 24561.5332\n",
      "Epoch 29: val_loss did not improve from 19959.64258\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 24816.7969 - rmse: 24869.3730 - val_loss: 31545.5254 - val_rmse: 31625.2793\n",
      "Epoch 30/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 26034.6621 - rmse: 26034.6621\n",
      "Epoch 30: val_loss improved from 19959.64258 to 18767.92969, saving model to ./ckpt/reg_lr005/val_rmse_18774.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25860.8223 - rmse: 25855.2617 - val_loss: 18767.9297 - val_rmse: 18773.8340\n",
      "Epoch 31/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21858.9199 - rmse: 21858.9199\n",
      "Epoch 31: val_loss did not improve from 18767.92969\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21787.9707 - rmse: 21754.1387 - val_loss: 21510.8340 - val_rmse: 21602.2617\n",
      "Epoch 32/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22715.5000 - rmse: 22715.5000\n",
      "Epoch 32: val_loss did not improve from 18767.92969\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22608.5312 - rmse: 22583.8711 - val_loss: 26305.6074 - val_rmse: 26389.1504\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22689.6211 - rmse: 22689.3633\n",
      "Epoch 33: val_loss did not improve from 18767.92969\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22689.6211 - rmse: 22689.3633 - val_loss: 23340.7285 - val_rmse: 23402.4355\n",
      "Epoch 34/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21916.7695 - rmse: 21916.7695\n",
      "Epoch 34: val_loss did not improve from 18767.92969\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21724.8242 - rmse: 21698.5645 - val_loss: 20502.2031 - val_rmse: 20535.4863\n",
      "Epoch 35/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21912.2441 - rmse: 21912.2441\n",
      "Epoch 35: val_loss did not improve from 18767.92969\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22160.3379 - rmse: 22156.0957 - val_loss: 18933.1035 - val_rmse: 18915.1426\n",
      "Epoch 36/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21762.6738 - rmse: 21762.6738\n",
      "Epoch 36: val_loss did not improve from 18767.92969\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21784.8359 - rmse: 21800.8066 - val_loss: 20874.2500 - val_rmse: 20935.6328\n",
      "Epoch 37/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23721.0781 - rmse: 23689.0117\n",
      "Epoch 37: val_loss did not improve from 18767.92969\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23721.0781 - rmse: 23689.0117 - val_loss: 19515.8047 - val_rmse: 19482.6309\n",
      "Epoch 38/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21560.5996 - rmse: 21546.3340\n",
      "Epoch 38: val_loss did not improve from 18767.92969\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21560.5996 - rmse: 21546.3340 - val_loss: 20833.5312 - val_rmse: 20814.0215\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19921.4160 - rmse: 19921.4160\n",
      "Epoch 39: val_loss improved from 18767.92969 to 18346.79297, saving model to ./ckpt/reg_lr005/val_rmse_18358.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19866.7324 - rmse: 19829.8516 - val_loss: 18346.7930 - val_rmse: 18357.5254\n",
      "Epoch 40/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22190.3516 - rmse: 22203.9238\n",
      "Epoch 40: val_loss improved from 18346.79297 to 17276.64453, saving model to ./ckpt/reg_lr005/val_rmse_17241.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22190.3516 - rmse: 22203.9238 - val_loss: 17276.6445 - val_rmse: 17241.4707\n",
      "Epoch 41/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19116.3594 - rmse: 19116.3594\n",
      "Epoch 41: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19257.1953 - rmse: 19328.3633 - val_loss: 21739.8340 - val_rmse: 21702.4785\n",
      "Epoch 42/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19732.9980 - rmse: 19751.1777\n",
      "Epoch 42: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19732.9980 - rmse: 19751.1777 - val_loss: 20632.9355 - val_rmse: 20682.7402\n",
      "Epoch 43/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20828.7676 - rmse: 20828.7676\n",
      "Epoch 43: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20784.4746 - rmse: 20781.3242 - val_loss: 26152.9160 - val_rmse: 26160.8066\n",
      "Epoch 44/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20993.9531 - rmse: 20993.9531\n",
      "Epoch 44: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21125.1250 - rmse: 21181.6543 - val_loss: 27637.9668 - val_rmse: 27778.6289\n",
      "Epoch 45/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20844.8555 - rmse: 20844.8555\n",
      "Epoch 45: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20789.8535 - rmse: 20752.7598 - val_loss: 22383.4453 - val_rmse: 22430.4258\n",
      "Epoch 46/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19568.7734 - rmse: 19568.7734\n",
      "Epoch 46: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19588.6875 - rmse: 19546.8242 - val_loss: 26583.8906 - val_rmse: 26689.7305\n",
      "Epoch 47/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20282.8047 - rmse: 20282.8047\n",
      "Epoch 47: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20594.2988 - rmse: 20765.4766 - val_loss: 27817.6035 - val_rmse: 27909.7109\n",
      "Epoch 48/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19897.8105 - rmse: 19897.8105\n",
      "Epoch 48: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19946.3340 - rmse: 19987.7969 - val_loss: 18439.5469 - val_rmse: 18414.8965\n",
      "Epoch 49/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20940.0879 - rmse: 20940.0879\n",
      "Epoch 49: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20862.6445 - rmse: 20873.4609 - val_loss: 18651.4961 - val_rmse: 18657.1016\n",
      "Epoch 50/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19253.1230 - rmse: 19253.1230\n",
      "Epoch 50: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19178.3594 - rmse: 19157.1621 - val_loss: 24487.9414 - val_rmse: 24537.0449\n",
      "Epoch 51/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19616.1836 - rmse: 19616.1836\n",
      "Epoch 51: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19579.3027 - rmse: 19578.5918 - val_loss: 33922.7812 - val_rmse: 33985.5547\n",
      "Epoch 52/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20213.4395 - rmse: 20213.4395\n",
      "Epoch 52: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20590.5605 - rmse: 20641.1230 - val_loss: 17514.5508 - val_rmse: 17494.1992\n",
      "Epoch 53/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18972.4531 - rmse: 18973.5234\n",
      "Epoch 53: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18972.4531 - rmse: 18973.5234 - val_loss: 25017.9980 - val_rmse: 25081.1758\n",
      "Epoch 54/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19318.9941 - rmse: 19318.9941\n",
      "Epoch 54: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19730.9688 - rmse: 19782.4844 - val_loss: 18116.6758 - val_rmse: 18096.7793\n",
      "Epoch 55/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19619.4004 - rmse: 19619.4004\n",
      "Epoch 55: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19582.1484 - rmse: 19606.4727 - val_loss: 33891.5000 - val_rmse: 33990.3828\n",
      "Epoch 56/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18614.7734 - rmse: 18614.7734\n",
      "Epoch 56: val_loss did not improve from 17276.64453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18865.4922 - rmse: 18897.0234 - val_loss: 17449.5957 - val_rmse: 17409.2832\n",
      "Epoch 57/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18922.1465 - rmse: 18922.1465\n",
      "Epoch 57: val_loss improved from 17276.64453 to 16551.55078, saving model to ./ckpt/reg_lr005/val_rmse_16509.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18953.7480 - rmse: 18960.7754 - val_loss: 16551.5508 - val_rmse: 16509.2012\n",
      "Epoch 58/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18297.5078 - rmse: 18297.5078\n",
      "Epoch 58: val_loss did not improve from 16551.55078\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18309.8945 - rmse: 18300.6016 - val_loss: 18254.2656 - val_rmse: 18231.8789\n",
      "Epoch 59/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20909.2617 - rmse: 20911.6270\n",
      "Epoch 59: val_loss did not improve from 16551.55078\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20909.2617 - rmse: 20911.6270 - val_loss: 30784.7129 - val_rmse: 30836.9590\n",
      "Epoch 60/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20156.9355 - rmse: 20156.9355\n",
      "Epoch 60: val_loss did not improve from 16551.55078\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20177.5547 - rmse: 20191.4609 - val_loss: 30957.5352 - val_rmse: 31100.9883\n",
      "Epoch 61/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19668.6348 - rmse: 19668.6348\n",
      "Epoch 61: val_loss did not improve from 16551.55078\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19527.0566 - rmse: 19531.6582 - val_loss: 22779.1758 - val_rmse: 22815.5938\n",
      "Epoch 62/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18018.5215 - rmse: 18018.5215\n",
      "Epoch 62: val_loss improved from 16551.55078 to 16262.21289, saving model to ./ckpt/reg_lr005/val_rmse_16229.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17982.7227 - rmse: 17958.5762 - val_loss: 16262.2129 - val_rmse: 16228.6113\n",
      "Epoch 63/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20553.6562 - rmse: 20553.6562\n",
      "Epoch 63: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20428.2207 - rmse: 20402.8652 - val_loss: 17563.7227 - val_rmse: 17543.3027\n",
      "Epoch 64/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19629.8633 - rmse: 19636.0430\n",
      "Epoch 64: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19629.8633 - rmse: 19636.0430 - val_loss: 21758.1523 - val_rmse: 21736.2969\n",
      "Epoch 65/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19212.4023 - rmse: 19223.4121\n",
      "Epoch 65: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19212.4023 - rmse: 19223.4121 - val_loss: 19623.9336 - val_rmse: 19613.7988\n",
      "Epoch 66/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18973.4531 - rmse: 18973.4531\n",
      "Epoch 66: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18955.3691 - rmse: 18943.1719 - val_loss: 17231.7305 - val_rmse: 17173.4199\n",
      "Epoch 67/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17601.5371 - rmse: 17552.7383\n",
      "Epoch 67: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17601.5371 - rmse: 17552.7383 - val_loss: 16567.7988 - val_rmse: 16511.5020\n",
      "Epoch 68/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17516.4238 - rmse: 17516.4238\n",
      "Epoch 68: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17689.1582 - rmse: 17694.0664 - val_loss: 16374.2500 - val_rmse: 16358.2656\n",
      "Epoch 69/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19049.1895 - rmse: 19030.4922\n",
      "Epoch 69: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19049.1895 - rmse: 19030.4922 - val_loss: 22016.1523 - val_rmse: 22057.3164\n",
      "Epoch 70/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17529.3945 - rmse: 17529.3945\n",
      "Epoch 70: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17448.8750 - rmse: 17410.8457 - val_loss: 16712.0430 - val_rmse: 16671.0781\n",
      "Epoch 71/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17896.5371 - rmse: 17896.5371\n",
      "Epoch 71: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18209.9844 - rmse: 18218.2969 - val_loss: 25269.7910 - val_rmse: 25310.4023\n",
      "Epoch 72/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19076.7500 - rmse: 19076.7500\n",
      "Epoch 72: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18826.3027 - rmse: 18800.4141 - val_loss: 18839.0137 - val_rmse: 18863.3770\n",
      "Epoch 73/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19178.3516 - rmse: 19178.3516\n",
      "Epoch 73: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19091.7949 - rmse: 19097.7598 - val_loss: 17739.1445 - val_rmse: 17691.4629\n",
      "Epoch 74/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18101.1992 - rmse: 18101.1992\n",
      "Epoch 74: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17962.8027 - rmse: 17935.6270 - val_loss: 18711.8340 - val_rmse: 18733.6504\n",
      "Epoch 75/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18858.4805 - rmse: 18858.4805\n",
      "Epoch 75: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18590.7168 - rmse: 18583.3125 - val_loss: 17766.3496 - val_rmse: 17737.6035\n",
      "Epoch 76/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18623.3691 - rmse: 18623.3691\n",
      "Epoch 76: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18391.5332 - rmse: 18356.8184 - val_loss: 16962.4355 - val_rmse: 16991.7637\n",
      "Epoch 77/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17553.5371 - rmse: 17550.5488\n",
      "Epoch 77: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17553.5371 - rmse: 17550.5488 - val_loss: 22111.8281 - val_rmse: 22185.3086\n",
      "Epoch 78/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17516.5312 - rmse: 17528.7480\n",
      "Epoch 78: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17516.5312 - rmse: 17528.7480 - val_loss: 21265.1641 - val_rmse: 21229.4395\n",
      "Epoch 79/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18775.2852 - rmse: 18775.2852\n",
      "Epoch 79: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18791.3125 - rmse: 18802.1230 - val_loss: 21876.2207 - val_rmse: 21862.6836\n",
      "Epoch 80/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16944.2910 - rmse: 16944.2910\n",
      "Epoch 80: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16952.3281 - rmse: 16916.0137 - val_loss: 18546.5840 - val_rmse: 18558.8047\n",
      "Epoch 81/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16331.8750 - rmse: 16331.8750\n",
      "Epoch 81: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16369.4756 - rmse: 16394.8340 - val_loss: 28143.2285 - val_rmse: 28226.3398\n",
      "Epoch 82/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19509.5059 - rmse: 19588.3164\n",
      "Epoch 82: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19509.5059 - rmse: 19588.3164 - val_loss: 28808.0234 - val_rmse: 28895.3496\n",
      "Epoch 83/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17357.1934 - rmse: 17357.1934\n",
      "Epoch 83: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17338.9434 - rmse: 17338.7520 - val_loss: 29319.7734 - val_rmse: 29452.2246\n",
      "Epoch 84/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18142.7168 - rmse: 18167.7832\n",
      "Epoch 84: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18142.7168 - rmse: 18167.7832 - val_loss: 17406.0527 - val_rmse: 17429.7422\n",
      "Epoch 85/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17573.7129 - rmse: 17573.7129\n",
      "Epoch 85: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17491.4336 - rmse: 17449.0430 - val_loss: 18507.4180 - val_rmse: 18523.7930\n",
      "Epoch 86/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18799.9785 - rmse: 18799.9785\n",
      "Epoch 86: val_loss did not improve from 16262.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18694.8750 - rmse: 18748.5000 - val_loss: 17051.3008 - val_rmse: 17077.5000\n",
      "Epoch 87/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17263.1973 - rmse: 17263.1973\n",
      "Epoch 87: val_loss improved from 16262.21289 to 16134.09863, saving model to ./ckpt/reg_lr005/val_rmse_16128.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17102.6992 - rmse: 17104.2754 - val_loss: 16134.0986 - val_rmse: 16127.9785\n",
      "Epoch 88/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17734.4297 - rmse: 17734.4297\n",
      "Epoch 88: val_loss did not improve from 16134.09863\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17665.3359 - rmse: 17659.3926 - val_loss: 16942.7832 - val_rmse: 16956.8672\n",
      "Epoch 89/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16800.1074 - rmse: 16800.1074\n",
      "Epoch 89: val_loss did not improve from 16134.09863\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16845.6562 - rmse: 16876.9023 - val_loss: 22363.5645 - val_rmse: 22388.2246\n",
      "Epoch 90/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17592.8379 - rmse: 17592.8379\n",
      "Epoch 90: val_loss did not improve from 16134.09863\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17554.9609 - rmse: 17566.7402 - val_loss: 16360.1406 - val_rmse: 16372.7930\n",
      "Epoch 91/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16916.2930 - rmse: 16916.2930\n",
      "Epoch 91: val_loss did not improve from 16134.09863\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17015.7852 - rmse: 16999.2051 - val_loss: 23240.9648 - val_rmse: 23275.8457\n",
      "Epoch 92/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17577.5488 - rmse: 17573.2754\n",
      "Epoch 92: val_loss did not improve from 16134.09863\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17577.5488 - rmse: 17573.2754 - val_loss: 18664.8887 - val_rmse: 18631.2988\n",
      "Epoch 93/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16801.5176 - rmse: 16801.5176\n",
      "Epoch 93: val_loss did not improve from 16134.09863\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16766.4434 - rmse: 16734.3301 - val_loss: 21720.6074 - val_rmse: 21733.5781\n",
      "Epoch 94/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15921.6006 - rmse: 15921.6006\n",
      "Epoch 94: val_loss improved from 16134.09863 to 15375.21484, saving model to ./ckpt/reg_lr005/val_rmse_15338.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16004.3008 - rmse: 16012.0107 - val_loss: 15375.2148 - val_rmse: 15338.4736\n",
      "Epoch 95/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16777.6309 - rmse: 16777.6309\n",
      "Epoch 95: val_loss did not improve from 15375.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16584.4238 - rmse: 16577.8965 - val_loss: 17429.3652 - val_rmse: 17375.8848\n",
      "Epoch 96/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15795.2383 - rmse: 15795.2383\n",
      "Epoch 96: val_loss did not improve from 15375.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15968.8301 - rmse: 16018.7412 - val_loss: 17234.1934 - val_rmse: 17221.4199\n",
      "Epoch 97/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15918.8857 - rmse: 15918.8857\n",
      "Epoch 97: val_loss did not improve from 15375.21484\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16044.3936 - rmse: 16042.1396 - val_loss: 18509.3633 - val_rmse: 18495.4258\n",
      "Epoch 98/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16275.0078 - rmse: 16275.0078\n",
      "Epoch 98: val_loss did not improve from 15375.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16252.8359 - rmse: 16237.8818 - val_loss: 23177.3691 - val_rmse: 23272.3320\n",
      "Epoch 99/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16212.6611 - rmse: 16212.6611\n",
      "Epoch 99: val_loss did not improve from 15375.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16165.3623 - rmse: 16133.4609 - val_loss: 20748.0273 - val_rmse: 20797.8164\n",
      "Epoch 100/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16031.5947 - rmse: 16031.5947\n",
      "Epoch 100: val_loss did not improve from 15375.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15867.2266 - rmse: 15847.3018 - val_loss: 20197.0840 - val_rmse: 20245.5840\n",
      "Epoch 101/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16564.8184 - rmse: 16601.2949\n",
      "Epoch 101: val_loss did not improve from 15375.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16564.8184 - rmse: 16601.2949 - val_loss: 26781.2383 - val_rmse: 26832.8887\n",
      "Epoch 102/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17115.6172 - rmse: 17110.4980\n",
      "Epoch 102: val_loss did not improve from 15375.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17115.6172 - rmse: 17110.4980 - val_loss: 23676.1016 - val_rmse: 23683.6074\n",
      "Epoch 103/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17325.0176 - rmse: 17325.0176\n",
      "Epoch 103: val_loss did not improve from 15375.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17297.9648 - rmse: 17279.7207 - val_loss: 22007.0059 - val_rmse: 22051.9297\n",
      "Epoch 104/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15944.8789 - rmse: 15944.8789\n",
      "Epoch 104: val_loss did not improve from 15375.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15853.8955 - rmse: 15841.5059 - val_loss: 16354.5088 - val_rmse: 16320.9521\n",
      "Epoch 105/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16430.2344 - rmse: 16430.2344\n",
      "Epoch 105: val_loss did not improve from 15375.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16360.4873 - rmse: 16376.1211 - val_loss: 21498.4629 - val_rmse: 21486.6719\n",
      "Epoch 106/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15938.4287 - rmse: 15938.4287\n",
      "Epoch 106: val_loss did not improve from 15375.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16133.7656 - rmse: 16159.5879 - val_loss: 23977.3047 - val_rmse: 24078.2539\n",
      "Epoch 107/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16222.6211 - rmse: 16222.6211\n",
      "Epoch 107: val_loss improved from 15375.21484 to 14720.66016, saving model to ./ckpt/reg_lr005/val_rmse_14718.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16499.5312 - rmse: 16516.9453 - val_loss: 14720.6602 - val_rmse: 14717.6719\n",
      "Epoch 108/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15803.4678 - rmse: 15803.4678\n",
      "Epoch 108: val_loss did not improve from 14720.66016\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15837.0586 - rmse: 15859.7129 - val_loss: 31795.6934 - val_rmse: 31876.5508\n",
      "Epoch 109/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16911.1309 - rmse: 16911.1309\n",
      "Epoch 109: val_loss did not improve from 14720.66016\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17013.0430 - rmse: 17081.7754 - val_loss: 20318.7715 - val_rmse: 20347.4766\n",
      "Epoch 110/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 14891.0771 - rmse: 14891.0771\n",
      "Epoch 110: val_loss did not improve from 14720.66016\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15016.8291 - rmse: 15048.8252 - val_loss: 18176.3223 - val_rmse: 18183.4238\n",
      "Epoch 111/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17070.6289 - rmse: 17070.6289\n",
      "Epoch 111: val_loss did not improve from 14720.66016\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16980.1016 - rmse: 16975.6016 - val_loss: 27278.9805 - val_rmse: 27426.7910\n",
      "Epoch 112/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16275.1602 - rmse: 16275.1602\n",
      "Epoch 112: val_loss did not improve from 14720.66016\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16215.5068 - rmse: 16175.2734 - val_loss: 18257.6973 - val_rmse: 18304.8438\n",
      "Epoch 113/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16706.7109 - rmse: 16706.7109\n",
      "Epoch 113: val_loss did not improve from 14720.66016\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16806.5176 - rmse: 16793.2109 - val_loss: 15328.9561 - val_rmse: 15329.0566\n",
      "Epoch 114/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15816.1162 - rmse: 15816.1162\n",
      "Epoch 114: val_loss did not improve from 14720.66016\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15962.3799 - rmse: 16024.1572 - val_loss: 15650.7783 - val_rmse: 15622.7939\n",
      "Epoch 115/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15149.6279 - rmse: 15151.3750\n",
      "Epoch 115: val_loss did not improve from 14720.66016\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15149.6279 - rmse: 15151.3750 - val_loss: 22208.1914 - val_rmse: 22286.2402\n",
      "Epoch 116/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15264.0029 - rmse: 15264.0029\n",
      "Epoch 116: val_loss did not improve from 14720.66016\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15210.1562 - rmse: 15204.2998 - val_loss: 25091.0859 - val_rmse: 25168.5195\n",
      "Epoch 117/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17181.5684 - rmse: 17181.5684\n",
      "Epoch 117: val_loss did not improve from 14720.66016\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16951.2539 - rmse: 16933.5898 - val_loss: 19098.8906 - val_rmse: 19145.4980\n",
      "Epoch 118/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15789.6133 - rmse: 15789.6133\n",
      "Epoch 118: val_loss did not improve from 14720.66016\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15941.6328 - rmse: 15941.0840 - val_loss: 16459.2129 - val_rmse: 16471.6094\n",
      "Epoch 119/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16782.5664 - rmse: 16782.5664\n",
      "Epoch 119: val_loss did not improve from 14720.66016\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16751.8730 - rmse: 16751.2539 - val_loss: 26987.5898 - val_rmse: 27133.6230\n",
      "Epoch 120/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15012.5879 - rmse: 15012.5879\n",
      "Epoch 120: val_loss did not improve from 14720.66016\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15036.2207 - rmse: 15020.3145 - val_loss: 23887.5605 - val_rmse: 23996.3359\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 25s - loss: 190820.2656 - rmse: 190820.2656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:08:36.472526: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 106370.8438 - rmse: 106047.5781\n",
      "Epoch 1: val_loss improved from inf to 53949.17188, saving model to ./ckpt/reg_lr005/val_rmse_53506.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 106370.8438 - rmse: 106047.5781 - val_loss: 53949.1719 - val_rmse: 53506.3906\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 63031.4414 - rmse: 63031.4414"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:08:37.350571: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 55644.7539 - rmse: 55644.7539\n",
      "Epoch 2: val_loss improved from 53949.17188 to 45021.09766, saving model to ./ckpt/reg_lr005/val_rmse_44639.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 55240.0859 - rmse: 55153.0859 - val_loss: 45021.0977 - val_rmse: 44639.1250\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 47239.3242 - rmse: 47239.3242\n",
      "Epoch 3: val_loss improved from 45021.09766 to 40119.09375, saving model to ./ckpt/reg_lr005/val_rmse_39686.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 47339.1328 - rmse: 47501.2148 - val_loss: 40119.0938 - val_rmse: 39685.7305\n",
      "Epoch 4/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 48165.6055 - rmse: 48165.6055\n",
      "Epoch 4: val_loss improved from 40119.09375 to 38360.61719, saving model to ./ckpt/reg_lr005/val_rmse_37954.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 47758.6680 - rmse: 47692.1914 - val_loss: 38360.6172 - val_rmse: 37954.3750\n",
      "Epoch 5/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 41671.5234 - rmse: 41671.5234\n",
      "Epoch 5: val_loss improved from 38360.61719 to 35659.95703, saving model to ./ckpt/reg_lr005/val_rmse_35407.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 41154.2383 - rmse: 41100.9492 - val_loss: 35659.9570 - val_rmse: 35406.9414\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 41020.1484 - rmse: 41020.1484\n",
      "Epoch 6: val_loss did not improve from 35659.95703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 41174.5547 - rmse: 41257.0234 - val_loss: 36334.9258 - val_rmse: 36154.4258\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 39350.4062 - rmse: 39350.4062\n",
      "Epoch 7: val_loss improved from 35659.95703 to 33385.81641, saving model to ./ckpt/reg_lr005/val_rmse_33132.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 38922.8125 - rmse: 38840.6992 - val_loss: 33385.8164 - val_rmse: 33131.6250\n",
      "Epoch 8/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 38458.6328 - rmse: 38458.6328\n",
      "Epoch 8: val_loss did not improve from 33385.81641\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 38364.4805 - rmse: 38299.6367 - val_loss: 37004.6875 - val_rmse: 36886.9141\n",
      "Epoch 9/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 36365.4023 - rmse: 36365.4023\n",
      "Epoch 9: val_loss improved from 33385.81641 to 31896.92578, saving model to ./ckpt/reg_lr005/val_rmse_31577.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 36320.2109 - rmse: 36283.1328 - val_loss: 31896.9258 - val_rmse: 31576.9766\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 35852.7109 - rmse: 35894.6445\n",
      "Epoch 10: val_loss did not improve from 31896.92578\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 35852.7109 - rmse: 35894.6445 - val_loss: 34652.8789 - val_rmse: 34568.3203\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 34554.2227 - rmse: 34554.2227\n",
      "Epoch 11: val_loss did not improve from 31896.92578\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 34590.5508 - rmse: 34615.0469 - val_loss: 35138.6211 - val_rmse: 34806.9258\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 35541.0898 - rmse: 35541.0898\n",
      "Epoch 12: val_loss did not improve from 31896.92578\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 35327.3945 - rmse: 35308.4570 - val_loss: 32230.1152 - val_rmse: 32142.5742\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 32506.2656 - rmse: 32506.2656\n",
      "Epoch 13: val_loss improved from 31896.92578 to 28249.78906, saving model to ./ckpt/reg_lr005/val_rmse_27999.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 32561.5918 - rmse: 32598.9043 - val_loss: 28249.7891 - val_rmse: 27998.8555\n",
      "Epoch 14/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 29326.8262 - rmse: 29326.8262\n",
      "Epoch 14: val_loss improved from 28249.78906 to 27318.51562, saving model to ./ckpt/reg_lr005/val_rmse_27216.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 30481.3125 - rmse: 30623.4902 - val_loss: 27318.5156 - val_rmse: 27215.6680\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 33525.3125 - rmse: 33525.3125\n",
      "Epoch 15: val_loss improved from 27318.51562 to 25319.43359, saving model to ./ckpt/reg_lr005/val_rmse_25105.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 33419.4492 - rmse: 33348.0547 - val_loss: 25319.4336 - val_rmse: 25105.0430\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 28278.9355 - rmse: 28278.9355\n",
      "Epoch 16: val_loss did not improve from 25319.43359\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 28900.4297 - rmse: 28939.3262 - val_loss: 27568.9258 - val_rmse: 27332.4336\n",
      "Epoch 17/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 27752.8438 - rmse: 27752.8438\n",
      "Epoch 17: val_loss did not improve from 25319.43359\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27836.1328 - rmse: 27888.1465 - val_loss: 28897.6289 - val_rmse: 28893.7871\n",
      "Epoch 18/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 28906.4668 - rmse: 28906.4668\n",
      "Epoch 18: val_loss did not improve from 25319.43359\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 28944.9238 - rmse: 28940.3105 - val_loss: 27678.9668 - val_rmse: 27498.6406\n",
      "Epoch 19/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 27807.0391 - rmse: 27807.0391\n",
      "Epoch 19: val_loss did not improve from 25319.43359\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 27982.3223 - rmse: 28027.8848 - val_loss: 36937.7188 - val_rmse: 36733.4414\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 26490.4727 - rmse: 26448.7031\n",
      "Epoch 20: val_loss improved from 25319.43359 to 23641.97461, saving model to ./ckpt/reg_lr005/val_rmse_23514.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 26490.4727 - rmse: 26448.7031 - val_loss: 23641.9746 - val_rmse: 23514.0449\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 25450.9316 - rmse: 25450.9316\n",
      "Epoch 21: val_loss did not improve from 23641.97461\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 25407.1816 - rmse: 25423.0352 - val_loss: 25777.0664 - val_rmse: 25811.9219\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 28482.3848 - rmse: 28493.7461\n",
      "Epoch 22: val_loss did not improve from 23641.97461\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 28482.3848 - rmse: 28493.7461 - val_loss: 29805.7598 - val_rmse: 29593.1738\n",
      "Epoch 23/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 25053.4238 - rmse: 25053.4238\n",
      "Epoch 23: val_loss improved from 23641.97461 to 21641.92188, saving model to ./ckpt/reg_lr005/val_rmse_21596.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 24895.5234 - rmse: 24829.8867 - val_loss: 21641.9219 - val_rmse: 21595.9648\n",
      "Epoch 24/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 25035.2676 - rmse: 25035.2676\n",
      "Epoch 24: val_loss did not improve from 21641.92188\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 24873.0781 - rmse: 24870.4492 - val_loss: 22425.8535 - val_rmse: 22335.9883\n",
      "Epoch 25/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24316.8418 - rmse: 24316.8418\n",
      "Epoch 25: val_loss did not improve from 21641.92188\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 24447.9238 - rmse: 24489.7852 - val_loss: 22195.8672 - val_rmse: 22026.7793\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23270.4062 - rmse: 23270.4062\n",
      "Epoch 26: val_loss improved from 21641.92188 to 20119.44336, saving model to ./ckpt/reg_lr005/val_rmse_20049.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23322.6895 - rmse: 23317.6992 - val_loss: 20119.4434 - val_rmse: 20048.5098\n",
      "Epoch 27/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22726.2500 - rmse: 22726.2500\n",
      "Epoch 27: val_loss did not improve from 20119.44336\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22703.6973 - rmse: 22672.6699 - val_loss: 24966.0684 - val_rmse: 24747.7500\n",
      "Epoch 28/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23172.7793 - rmse: 23172.7793\n",
      "Epoch 28: val_loss did not improve from 20119.44336\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22955.9004 - rmse: 22930.4160 - val_loss: 20817.2773 - val_rmse: 20645.2266\n",
      "Epoch 29/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22333.0176 - rmse: 22333.0176\n",
      "Epoch 29: val_loss did not improve from 20119.44336\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22289.2402 - rmse: 22272.6758 - val_loss: 21462.3398 - val_rmse: 21378.5898\n",
      "Epoch 30/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21693.1348 - rmse: 21693.1348\n",
      "Epoch 30: val_loss did not improve from 20119.44336\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21823.8164 - rmse: 21837.8496 - val_loss: 20961.6270 - val_rmse: 20775.3359\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22822.5293 - rmse: 22880.3379\n",
      "Epoch 31: val_loss did not improve from 20119.44336\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22822.5293 - rmse: 22880.3379 - val_loss: 20835.8945 - val_rmse: 20748.7969\n",
      "Epoch 32/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22323.2422 - rmse: 22323.2422\n",
      "Epoch 32: val_loss did not improve from 20119.44336\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22467.8789 - rmse: 22479.5293 - val_loss: 33564.1133 - val_rmse: 33321.3945\n",
      "Epoch 33/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22230.7109 - rmse: 22230.7109\n",
      "Epoch 33: val_loss improved from 20119.44336 to 18773.21680, saving model to ./ckpt/reg_lr005/val_rmse_18633.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22389.2266 - rmse: 22417.1387 - val_loss: 18773.2168 - val_rmse: 18632.6328\n",
      "Epoch 34/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21200.3340 - rmse: 21200.3340\n",
      "Epoch 34: val_loss did not improve from 18773.21680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21262.2676 - rmse: 21237.4609 - val_loss: 19544.6504 - val_rmse: 19306.0293\n",
      "Epoch 35/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21159.0762 - rmse: 21159.0762\n",
      "Epoch 35: val_loss did not improve from 18773.21680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21184.4238 - rmse: 21285.2402 - val_loss: 35823.8242 - val_rmse: 35611.0625\n",
      "Epoch 36/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23165.6953 - rmse: 23165.6953\n",
      "Epoch 36: val_loss improved from 18773.21680 to 18320.07031, saving model to ./ckpt/reg_lr005/val_rmse_18155.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23245.4922 - rmse: 23206.2207 - val_loss: 18320.0703 - val_rmse: 18155.2852\n",
      "Epoch 37/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20646.4316 - rmse: 20708.1777\n",
      "Epoch 37: val_loss did not improve from 18320.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20646.4316 - rmse: 20708.1777 - val_loss: 27340.2559 - val_rmse: 27108.5488\n",
      "Epoch 38/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20105.9980 - rmse: 20105.9980\n",
      "Epoch 38: val_loss did not improve from 18320.07031\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20417.3574 - rmse: 20490.6699 - val_loss: 18490.6406 - val_rmse: 18289.8555\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20460.0352 - rmse: 20460.0352\n",
      "Epoch 39: val_loss did not improve from 18320.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20618.0488 - rmse: 20724.6172 - val_loss: 19608.4395 - val_rmse: 19515.1543\n",
      "Epoch 40/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19564.3750 - rmse: 19551.6836\n",
      "Epoch 40: val_loss did not improve from 18320.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19564.3750 - rmse: 19551.6836 - val_loss: 21266.7012 - val_rmse: 21030.2500\n",
      "Epoch 41/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19491.8574 - rmse: 19482.9375\n",
      "Epoch 41: val_loss did not improve from 18320.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19491.8574 - rmse: 19482.9375 - val_loss: 23002.4980 - val_rmse: 22797.7285\n",
      "Epoch 42/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20109.1602 - rmse: 20109.1602\n",
      "Epoch 42: val_loss did not improve from 18320.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20102.9102 - rmse: 20120.5664 - val_loss: 23204.5684 - val_rmse: 22998.4316\n",
      "Epoch 43/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20023.2148 - rmse: 20055.9082\n",
      "Epoch 43: val_loss did not improve from 18320.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20023.2148 - rmse: 20055.9082 - val_loss: 18475.6445 - val_rmse: 18316.9355\n",
      "Epoch 44/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19541.2227 - rmse: 19541.2227\n",
      "Epoch 44: val_loss did not improve from 18320.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19345.7480 - rmse: 19351.2266 - val_loss: 19886.1660 - val_rmse: 19641.5664\n",
      "Epoch 45/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19343.9785 - rmse: 19343.9785\n",
      "Epoch 45: val_loss improved from 18320.07031 to 17811.30273, saving model to ./ckpt/reg_lr005/val_rmse_17651.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19492.2500 - rmse: 19484.7051 - val_loss: 17811.3027 - val_rmse: 17651.3438\n",
      "Epoch 46/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18557.8867 - rmse: 18557.8867\n",
      "Epoch 46: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18560.2637 - rmse: 18561.8652 - val_loss: 21250.1445 - val_rmse: 21047.8555\n",
      "Epoch 47/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21748.9121 - rmse: 21748.9121\n",
      "Epoch 47: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21815.9453 - rmse: 21871.4434 - val_loss: 18223.3438 - val_rmse: 18077.7500\n",
      "Epoch 48/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20291.1816 - rmse: 20291.1816\n",
      "Epoch 48: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20281.2383 - rmse: 20274.5332 - val_loss: 18903.0840 - val_rmse: 18719.2051\n",
      "Epoch 49/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18834.1660 - rmse: 18834.1660\n",
      "Epoch 49: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18878.6328 - rmse: 18943.4785 - val_loss: 32169.8613 - val_rmse: 31991.1504\n",
      "Epoch 50/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20111.6172 - rmse: 20111.6172\n",
      "Epoch 50: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20241.6543 - rmse: 20239.8809 - val_loss: 22807.6211 - val_rmse: 22607.4121\n",
      "Epoch 51/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19654.3262 - rmse: 19668.0176\n",
      "Epoch 51: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19654.3262 - rmse: 19668.0176 - val_loss: 19221.1973 - val_rmse: 19012.5977\n",
      "Epoch 52/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18284.6602 - rmse: 18284.6602\n",
      "Epoch 52: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18300.6074 - rmse: 18311.3652 - val_loss: 18650.8359 - val_rmse: 18446.3008\n",
      "Epoch 53/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19043.6777 - rmse: 19036.1504\n",
      "Epoch 53: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19043.6777 - rmse: 19036.1504 - val_loss: 20616.8301 - val_rmse: 20424.1035\n",
      "Epoch 54/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18970.8770 - rmse: 18970.8770\n",
      "Epoch 54: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19102.5000 - rmse: 19135.4180 - val_loss: 32234.2070 - val_rmse: 32021.1270\n",
      "Epoch 55/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20485.0664 - rmse: 20485.0664\n",
      "Epoch 55: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20613.4492 - rmse: 20743.8516 - val_loss: 19173.1172 - val_rmse: 18963.1836\n",
      "Epoch 56/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20888.0215 - rmse: 20888.0215\n",
      "Epoch 56: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21189.5625 - rmse: 21183.8320 - val_loss: 19914.7422 - val_rmse: 19664.3340\n",
      "Epoch 57/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21321.4062 - rmse: 21278.3027\n",
      "Epoch 57: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21321.4062 - rmse: 21278.3027 - val_loss: 18336.6895 - val_rmse: 18152.8906\n",
      "Epoch 58/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18932.1914 - rmse: 18932.1914\n",
      "Epoch 58: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19009.5371 - rmse: 19055.2090 - val_loss: 17963.3516 - val_rmse: 17827.4238\n",
      "Epoch 59/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18361.9824 - rmse: 18361.9824\n",
      "Epoch 59: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18355.9785 - rmse: 18351.9297 - val_loss: 21989.9453 - val_rmse: 21814.5312\n",
      "Epoch 60/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17593.6953 - rmse: 17593.6953\n",
      "Epoch 60: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17621.0078 - rmse: 17642.7676 - val_loss: 18675.3027 - val_rmse: 18511.6816\n",
      "Epoch 61/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19163.1621 - rmse: 19163.1621\n",
      "Epoch 61: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19102.7070 - rmse: 19088.5742 - val_loss: 17865.1875 - val_rmse: 17706.7207\n",
      "Epoch 62/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18259.7754 - rmse: 18259.7754\n",
      "Epoch 62: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18145.0840 - rmse: 18203.8672 - val_loss: 21353.7598 - val_rmse: 21163.5508\n",
      "Epoch 63/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19781.9785 - rmse: 19822.5742\n",
      "Epoch 63: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19781.9785 - rmse: 19822.5742 - val_loss: 19776.8418 - val_rmse: 19683.9395\n",
      "Epoch 64/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18533.2559 - rmse: 18533.2559\n",
      "Epoch 64: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18504.3848 - rmse: 18508.2070 - val_loss: 21375.3086 - val_rmse: 21141.0449\n",
      "Epoch 65/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17317.4414 - rmse: 17317.4414\n",
      "Epoch 65: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17351.5566 - rmse: 17349.5273 - val_loss: 18541.8672 - val_rmse: 18382.2305\n",
      "Epoch 66/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18632.9375 - rmse: 18632.9375\n",
      "Epoch 66: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18666.0801 - rmse: 18775.6836 - val_loss: 21453.5078 - val_rmse: 21288.2012\n",
      "Epoch 67/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20098.5000 - rmse: 20098.5000\n",
      "Epoch 67: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19904.5898 - rmse: 19891.6543 - val_loss: 21776.5801 - val_rmse: 21604.9805\n",
      "Epoch 68/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17601.1367 - rmse: 17601.1367\n",
      "Epoch 68: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17603.3691 - rmse: 17591.4277 - val_loss: 18254.5605 - val_rmse: 18091.6992\n",
      "Epoch 69/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18926.4141 - rmse: 18926.4141\n",
      "Epoch 69: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19016.8613 - rmse: 19109.9004 - val_loss: 18888.5000 - val_rmse: 18778.2500\n",
      "Epoch 70/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17507.0508 - rmse: 17507.0508\n",
      "Epoch 70: val_loss did not improve from 17811.30273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17493.1094 - rmse: 17532.0098 - val_loss: 18821.4980 - val_rmse: 18650.8711\n",
      "Epoch 70: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 34s - loss: 194703.7500 - rmse: 194703.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:09:22.440797: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 107123.0078 - rmse: 106788.5078\n",
      "Epoch 1: val_loss improved from inf to 47107.16406, saving model to ./ckpt/reg_lr005/val_rmse_47207.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 107123.0078 - rmse: 106788.5078 - val_loss: 47107.1641 - val_rmse: 47207.1719\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 65061.3125 - rmse: 65061.3125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:09:23.287306: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 54878.6406 - rmse: 54878.6406\n",
      "Epoch 2: val_loss improved from 47107.16406 to 41392.71094, saving model to ./ckpt/reg_lr005/val_rmse_41525.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 54538.4648 - rmse: 54529.8047 - val_loss: 41392.7109 - val_rmse: 41524.5859\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 48852.6172 - rmse: 48852.6172\n",
      "Epoch 3: val_loss improved from 41392.71094 to 41304.21094, saving model to ./ckpt/reg_lr005/val_rmse_41505.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 48903.7734 - rmse: 48790.4180 - val_loss: 41304.2109 - val_rmse: 41504.6133\n",
      "Epoch 4/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 44071.9141 - rmse: 44071.9141\n",
      "Epoch 4: val_loss improved from 41304.21094 to 36318.70703, saving model to ./ckpt/reg_lr005/val_rmse_36496.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 43999.9023 - rmse: 43943.7305 - val_loss: 36318.7070 - val_rmse: 36495.7891\n",
      "Epoch 5/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 42616.0039 - rmse: 42616.0039\n",
      "Epoch 5: val_loss improved from 36318.70703 to 35301.78125, saving model to ./ckpt/reg_lr005/val_rmse_35466.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 42408.2656 - rmse: 42459.0977 - val_loss: 35301.7812 - val_rmse: 35466.0859\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 40460.9102 - rmse: 40460.9102\n",
      "Epoch 6: val_loss did not improve from 35301.78125\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 40501.5703 - rmse: 40528.9883 - val_loss: 37865.2109 - val_rmse: 38018.9297\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 38283.8008 - rmse: 38320.8828\n",
      "Epoch 7: val_loss improved from 35301.78125 to 33743.87109, saving model to ./ckpt/reg_lr005/val_rmse_33875.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 38283.8008 - rmse: 38320.8828 - val_loss: 33743.8711 - val_rmse: 33875.1836\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 38552.1445 - rmse: 38535.3555\n",
      "Epoch 8: val_loss improved from 33743.87109 to 31105.80078, saving model to ./ckpt/reg_lr005/val_rmse_31214.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 38552.1445 - rmse: 38535.3555 - val_loss: 31105.8008 - val_rmse: 31214.4258\n",
      "Epoch 9/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 36389.6758 - rmse: 36389.6758\n",
      "Epoch 9: val_loss did not improve from 31105.80078\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 36059.4531 - rmse: 36008.6719 - val_loss: 31109.0879 - val_rmse: 31207.4570\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 35246.7305 - rmse: 35283.9727\n",
      "Epoch 10: val_loss improved from 31105.80078 to 30454.61328, saving model to ./ckpt/reg_lr005/val_rmse_30557.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 35246.7305 - rmse: 35283.9727 - val_loss: 30454.6133 - val_rmse: 30556.6953\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 35385.2305 - rmse: 35385.2305\n",
      "Epoch 11: val_loss improved from 30454.61328 to 27029.67969, saving model to ./ckpt/reg_lr005/val_rmse_27126.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 35371.5508 - rmse: 35376.8203 - val_loss: 27029.6797 - val_rmse: 27125.5645\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 32079.9746 - rmse: 32079.9746\n",
      "Epoch 12: val_loss improved from 27029.67969 to 25689.25391, saving model to ./ckpt/reg_lr005/val_rmse_25787.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 32007.6348 - rmse: 31958.8438 - val_loss: 25689.2539 - val_rmse: 25787.1582\n",
      "Epoch 13/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 30490.5527 - rmse: 30490.5527\n",
      "Epoch 13: val_loss improved from 25689.25391 to 24780.01367, saving model to ./ckpt/reg_lr005/val_rmse_24893.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 30625.1211 - rmse: 30742.9531 - val_loss: 24780.0137 - val_rmse: 24892.5508\n",
      "Epoch 14/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 30692.8496 - rmse: 30692.8496\n",
      "Epoch 14: val_loss did not improve from 24780.01367\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 30505.5859 - rmse: 30425.2285 - val_loss: 31368.5547 - val_rmse: 31459.7715\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 29134.1113 - rmse: 29134.1113\n",
      "Epoch 15: val_loss did not improve from 24780.01367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 29180.6719 - rmse: 29212.0742 - val_loss: 24790.0508 - val_rmse: 24922.5879\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 32688.6113 - rmse: 32688.6113\n",
      "Epoch 16: val_loss improved from 24780.01367 to 23925.44922, saving model to ./ckpt/reg_lr005/val_rmse_24037.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 32523.4316 - rmse: 32510.6719 - val_loss: 23925.4492 - val_rmse: 24037.3418\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 27788.3359 - rmse: 27788.3359\n",
      "Epoch 17: val_loss improved from 23925.44922 to 23326.40820, saving model to ./ckpt/reg_lr005/val_rmse_23451.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 27786.8691 - rmse: 27785.8789 - val_loss: 23326.4082 - val_rmse: 23451.4629\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 29152.4375 - rmse: 29152.4375\n",
      "Epoch 18: val_loss did not improve from 23326.40820\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 29261.3594 - rmse: 29383.7402 - val_loss: 32928.1055 - val_rmse: 33029.9844\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 26514.2988 - rmse: 26514.2988\n",
      "Epoch 19: val_loss did not improve from 23326.40820\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26564.9785 - rmse: 26586.7109 - val_loss: 26798.7031 - val_rmse: 26863.7031\n",
      "Epoch 20/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 29572.7441 - rmse: 29572.7441\n",
      "Epoch 20: val_loss did not improve from 23326.40820\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 29547.7637 - rmse: 29536.7285 - val_loss: 25390.8457 - val_rmse: 25504.3906\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 26754.4336 - rmse: 26754.4336\n",
      "Epoch 21: val_loss did not improve from 23326.40820\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 26624.3711 - rmse: 26596.8613 - val_loss: 29848.5391 - val_rmse: 29938.9512\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25286.1270 - rmse: 25290.4512\n",
      "Epoch 22: val_loss improved from 23326.40820 to 23256.31445, saving model to ./ckpt/reg_lr005/val_rmse_23354.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 25286.1270 - rmse: 25290.4512 - val_loss: 23256.3145 - val_rmse: 23353.9375\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27471.4395 - rmse: 27440.3027\n",
      "Epoch 23: val_loss improved from 23256.31445 to 21138.42383, saving model to ./ckpt/reg_lr005/val_rmse_21218.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 27471.4395 - rmse: 27440.3027 - val_loss: 21138.4238 - val_rmse: 21217.9551\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 24052.0430 - rmse: 24052.0430\n",
      "Epoch 24: val_loss improved from 21138.42383 to 20694.50586, saving model to ./ckpt/reg_lr005/val_rmse_20746.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 24191.3438 - rmse: 24285.2910 - val_loss: 20694.5059 - val_rmse: 20745.8418\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24933.5000 - rmse: 24893.6641\n",
      "Epoch 25: val_loss did not improve from 20694.50586\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24933.5000 - rmse: 24893.6641 - val_loss: 24032.7422 - val_rmse: 24115.3809\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23772.5117 - rmse: 23821.7832\n",
      "Epoch 26: val_loss improved from 20694.50586 to 20194.01367, saving model to ./ckpt/reg_lr005/val_rmse_20291.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23772.5117 - rmse: 23821.7832 - val_loss: 20194.0137 - val_rmse: 20290.7148\n",
      "Epoch 27/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 23048.5801 - rmse: 23048.5801\n",
      "Epoch 27: val_loss did not improve from 20194.01367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23423.9414 - rmse: 23445.3027 - val_loss: 26922.8027 - val_rmse: 26988.4902\n",
      "Epoch 28/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24669.9512 - rmse: 24669.9512\n",
      "Epoch 28: val_loss improved from 20194.01367 to 19881.82031, saving model to ./ckpt/reg_lr005/val_rmse_19961.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 24450.2773 - rmse: 24417.4375 - val_loss: 19881.8203 - val_rmse: 19960.8691\n",
      "Epoch 29/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22360.1406 - rmse: 22366.7188\n",
      "Epoch 29: val_loss did not improve from 19881.82031\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22360.1406 - rmse: 22366.7188 - val_loss: 24559.9199 - val_rmse: 24651.6328\n",
      "Epoch 30/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22324.4336 - rmse: 22329.5977\n",
      "Epoch 30: val_loss did not improve from 19881.82031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22324.4336 - rmse: 22329.5977 - val_loss: 25428.2812 - val_rmse: 25428.0859\n",
      "Epoch 31/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21065.2520 - rmse: 21065.2520\n",
      "Epoch 31: val_loss did not improve from 19881.82031\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21196.9395 - rmse: 21285.7539 - val_loss: 20139.5801 - val_rmse: 20185.2539\n",
      "Epoch 32/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23138.6602 - rmse: 23138.6602\n",
      "Epoch 32: val_loss improved from 19881.82031 to 19347.58008, saving model to ./ckpt/reg_lr005/val_rmse_19356.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22974.8906 - rmse: 22938.8105 - val_loss: 19347.5801 - val_rmse: 19355.7559\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21374.7520 - rmse: 21399.6816\n",
      "Epoch 33: val_loss did not improve from 19347.58008\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21374.7520 - rmse: 21399.6816 - val_loss: 20737.9062 - val_rmse: 20823.6289\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21056.0156 - rmse: 21050.7715\n",
      "Epoch 34: val_loss did not improve from 19347.58008\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21056.0156 - rmse: 21050.7715 - val_loss: 29675.5195 - val_rmse: 29758.9355\n",
      "Epoch 35/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20721.1504 - rmse: 20721.1504\n",
      "Epoch 35: val_loss did not improve from 19347.58008\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20708.6660 - rmse: 20753.9395 - val_loss: 22050.7305 - val_rmse: 22120.8359\n",
      "Epoch 36/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20572.9531 - rmse: 20572.9531\n",
      "Epoch 36: val_loss did not improve from 19347.58008\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20802.8164 - rmse: 20796.6191 - val_loss: 28787.4492 - val_rmse: 28834.8828\n",
      "Epoch 37/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21174.2695 - rmse: 21174.2695\n",
      "Epoch 37: val_loss improved from 19347.58008 to 18960.38867, saving model to ./ckpt/reg_lr005/val_rmse_19021.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21123.0430 - rmse: 21088.4922 - val_loss: 18960.3887 - val_rmse: 19021.4980\n",
      "Epoch 38/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19848.6211 - rmse: 19842.9941\n",
      "Epoch 38: val_loss did not improve from 18960.38867\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19848.6211 - rmse: 19842.9941 - val_loss: 21323.6777 - val_rmse: 21390.6816\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21423.1191 - rmse: 21423.1191\n",
      "Epoch 39: val_loss improved from 18960.38867 to 17870.25000, saving model to ./ckpt/reg_lr005/val_rmse_17991.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21380.1055 - rmse: 21351.0938 - val_loss: 17870.2500 - val_rmse: 17991.0156\n",
      "Epoch 40/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20351.8164 - rmse: 20351.8164\n",
      "Epoch 40: val_loss improved from 17870.25000 to 17320.03125, saving model to ./ckpt/reg_lr005/val_rmse_17398.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20302.7207 - rmse: 20316.8320 - val_loss: 17320.0312 - val_rmse: 17398.3496\n",
      "Epoch 41/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20956.6230 - rmse: 20956.6230\n",
      "Epoch 41: val_loss did not improve from 17320.03125\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20949.2031 - rmse: 20976.7402 - val_loss: 20577.0332 - val_rmse: 20656.5645\n",
      "Epoch 42/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20638.2832 - rmse: 20638.2832\n",
      "Epoch 42: val_loss did not improve from 17320.03125\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20468.8047 - rmse: 20481.9473 - val_loss: 19646.8184 - val_rmse: 19737.8789\n",
      "Epoch 43/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19563.0195 - rmse: 19577.5449\n",
      "Epoch 43: val_loss did not improve from 17320.03125\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19563.0195 - rmse: 19577.5449 - val_loss: 17435.7598 - val_rmse: 17530.6309\n",
      "Epoch 44/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19825.7227 - rmse: 19825.7227\n",
      "Epoch 44: val_loss did not improve from 17320.03125\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19668.0996 - rmse: 19658.6211 - val_loss: 19699.6465 - val_rmse: 19778.4531\n",
      "Epoch 45/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20047.9043 - rmse: 20047.9043\n",
      "Epoch 45: val_loss did not improve from 17320.03125\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20203.6836 - rmse: 20179.0859 - val_loss: 17331.1250 - val_rmse: 17432.8438\n",
      "Epoch 46/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20824.8262 - rmse: 20824.8262\n",
      "Epoch 46: val_loss did not improve from 17320.03125\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20518.3242 - rmse: 20489.0312 - val_loss: 21989.5625 - val_rmse: 22068.5898\n",
      "Epoch 47/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19755.2578 - rmse: 19755.2578\n",
      "Epoch 47: val_loss improved from 17320.03125 to 17118.71094, saving model to ./ckpt/reg_lr005/val_rmse_17212.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19538.2871 - rmse: 19511.6406 - val_loss: 17118.7109 - val_rmse: 17212.1328\n",
      "Epoch 48/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20013.4199 - rmse: 20013.4199\n",
      "Epoch 48: val_loss did not improve from 17118.71094\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20231.9316 - rmse: 20199.8750 - val_loss: 29365.5137 - val_rmse: 29426.3242\n",
      "Epoch 49/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19336.0371 - rmse: 19336.0371\n",
      "Epoch 49: val_loss did not improve from 17118.71094\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19285.7422 - rmse: 19258.8281 - val_loss: 22162.3828 - val_rmse: 22238.1328\n",
      "Epoch 50/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19434.2227 - rmse: 19434.2227\n",
      "Epoch 50: val_loss did not improve from 17118.71094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19186.3828 - rmse: 19165.3906 - val_loss: 24857.2051 - val_rmse: 24915.1953\n",
      "Epoch 51/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18527.6973 - rmse: 18527.6973\n",
      "Epoch 51: val_loss did not improve from 17118.71094\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18552.4121 - rmse: 18534.7617 - val_loss: 17269.9434 - val_rmse: 17355.0234\n",
      "Epoch 52/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20062.9434 - rmse: 20062.9434\n",
      "Epoch 52: val_loss did not improve from 17118.71094\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20058.6973 - rmse: 20055.8340 - val_loss: 23788.0703 - val_rmse: 23840.8477\n",
      "Epoch 53/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18809.0215 - rmse: 18809.0215\n",
      "Epoch 53: val_loss did not improve from 17118.71094\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18756.4648 - rmse: 18764.6504 - val_loss: 26089.9629 - val_rmse: 26154.7734\n",
      "Epoch 54/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18791.1934 - rmse: 18791.1934\n",
      "Epoch 54: val_loss did not improve from 17118.71094\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18800.0059 - rmse: 18779.6816 - val_loss: 20585.5234 - val_rmse: 20671.3867\n",
      "Epoch 55/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20013.7695 - rmse: 20013.7695\n",
      "Epoch 55: val_loss did not improve from 17118.71094\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20136.7363 - rmse: 20088.3438 - val_loss: 17379.9414 - val_rmse: 17474.9141\n",
      "Epoch 56/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20856.8652 - rmse: 20856.8652\n",
      "Epoch 56: val_loss did not improve from 17118.71094\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20972.9219 - rmse: 21079.9336 - val_loss: 22017.6094 - val_rmse: 22093.8574\n",
      "Epoch 57/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18870.9824 - rmse: 18870.9824\n",
      "Epoch 57: val_loss did not improve from 17118.71094\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18855.4492 - rmse: 18844.9727 - val_loss: 33121.9570 - val_rmse: 33155.7852\n",
      "Epoch 58/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20450.9707 - rmse: 20450.9707\n",
      "Epoch 58: val_loss improved from 17118.71094 to 17076.80273, saving model to ./ckpt/reg_lr005/val_rmse_17177.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20538.6914 - rmse: 20533.4238 - val_loss: 17076.8027 - val_rmse: 17176.9922\n",
      "Epoch 59/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23215.9844 - rmse: 23226.2773\n",
      "Epoch 59: val_loss did not improve from 17076.80273\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23215.9844 - rmse: 23226.2773 - val_loss: 27184.3789 - val_rmse: 27234.3203\n",
      "Epoch 60/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19177.0918 - rmse: 19177.0918\n",
      "Epoch 60: val_loss did not improve from 17076.80273\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18979.0586 - rmse: 18970.4082 - val_loss: 23055.5605 - val_rmse: 23114.8242\n",
      "Epoch 61/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18524.8633 - rmse: 18524.8633\n",
      "Epoch 61: val_loss did not improve from 17076.80273\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18484.2031 - rmse: 18456.7188 - val_loss: 19385.0801 - val_rmse: 19468.5020\n",
      "Epoch 62/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17385.8457 - rmse: 17389.0684\n",
      "Epoch 62: val_loss did not improve from 17076.80273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17385.8457 - rmse: 17389.0684 - val_loss: 21168.9336 - val_rmse: 21253.1328\n",
      "Epoch 63/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18399.8594 - rmse: 18399.8594\n",
      "Epoch 63: val_loss did not improve from 17076.80273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18379.9785 - rmse: 18377.5078 - val_loss: 18940.4707 - val_rmse: 19014.9902\n",
      "Epoch 64/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17427.7949 - rmse: 17427.7949\n",
      "Epoch 64: val_loss did not improve from 17076.80273\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17469.2012 - rmse: 17472.0195 - val_loss: 22212.0801 - val_rmse: 22295.7285\n",
      "Epoch 65/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19932.9551 - rmse: 19932.9551\n",
      "Epoch 65: val_loss improved from 17076.80273 to 16522.87500, saving model to ./ckpt/reg_lr005/val_rmse_16665.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19746.3027 - rmse: 19733.7520 - val_loss: 16522.8750 - val_rmse: 16665.4395\n",
      "Epoch 66/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18571.1289 - rmse: 18571.1289\n",
      "Epoch 66: val_loss did not improve from 16522.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18593.1367 - rmse: 18593.8730 - val_loss: 17137.6914 - val_rmse: 17213.9473\n",
      "Epoch 67/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17229.7676 - rmse: 17229.7676\n",
      "Epoch 67: val_loss did not improve from 16522.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17539.5156 - rmse: 17748.4199 - val_loss: 20443.5020 - val_rmse: 20540.7793\n",
      "Epoch 68/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18459.4395 - rmse: 18459.4395\n",
      "Epoch 68: val_loss did not improve from 16522.87500\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18347.1895 - rmse: 18344.1152 - val_loss: 20758.7422 - val_rmse: 20837.9004\n",
      "Epoch 69/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17262.7344 - rmse: 17262.7344\n",
      "Epoch 69: val_loss improved from 16522.87500 to 16230.25391, saving model to ./ckpt/reg_lr005/val_rmse_16374.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17297.7793 - rmse: 17321.4141 - val_loss: 16230.2539 - val_rmse: 16374.2812\n",
      "Epoch 70/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18642.8281 - rmse: 18642.8281\n",
      "Epoch 70: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18484.6836 - rmse: 18434.5801 - val_loss: 20006.7031 - val_rmse: 20116.3438\n",
      "Epoch 71/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17622.8340 - rmse: 17627.0605\n",
      "Epoch 71: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17622.8340 - rmse: 17627.0605 - val_loss: 17046.6816 - val_rmse: 17125.6094\n",
      "Epoch 72/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16705.0527 - rmse: 16705.0527\n",
      "Epoch 72: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16614.8105 - rmse: 16617.4043 - val_loss: 20106.2051 - val_rmse: 20238.3203\n",
      "Epoch 73/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18774.3945 - rmse: 18774.3945\n",
      "Epoch 73: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18710.0234 - rmse: 18691.7402 - val_loss: 22873.7344 - val_rmse: 22929.7539\n",
      "Epoch 74/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19395.3496 - rmse: 19395.3496\n",
      "Epoch 74: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19302.7598 - rmse: 19307.3945 - val_loss: 20161.0020 - val_rmse: 20226.2109\n",
      "Epoch 75/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18039.7598 - rmse: 18039.7598\n",
      "Epoch 75: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18065.9844 - rmse: 18083.6680 - val_loss: 25708.7441 - val_rmse: 25767.3984\n",
      "Epoch 76/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17051.6367 - rmse: 17051.6367\n",
      "Epoch 76: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17112.7910 - rmse: 17166.1719 - val_loss: 20664.9395 - val_rmse: 20744.9180\n",
      "Epoch 77/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21784.0234 - rmse: 21784.0234\n",
      "Epoch 77: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21469.8691 - rmse: 21442.1895 - val_loss: 21772.3887 - val_rmse: 21846.1328\n",
      "Epoch 78/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17062.8223 - rmse: 17062.8223\n",
      "Epoch 78: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17136.9551 - rmse: 17145.6074 - val_loss: 17610.3066 - val_rmse: 17686.0703\n",
      "Epoch 79/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18256.4316 - rmse: 18263.3496\n",
      "Epoch 79: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18256.4316 - rmse: 18263.3496 - val_loss: 17543.6484 - val_rmse: 17685.4238\n",
      "Epoch 80/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19168.2559 - rmse: 19168.2559\n",
      "Epoch 80: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19112.2656 - rmse: 19100.8125 - val_loss: 26110.1367 - val_rmse: 26178.9336\n",
      "Epoch 81/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16973.0195 - rmse: 16973.0195\n",
      "Epoch 81: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16922.1641 - rmse: 16887.8652 - val_loss: 21987.2793 - val_rmse: 22058.6035\n",
      "Epoch 82/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17767.1562 - rmse: 17733.0859\n",
      "Epoch 82: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17767.1562 - rmse: 17733.0859 - val_loss: 25914.0820 - val_rmse: 25977.5508\n",
      "Epoch 83/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17702.6289 - rmse: 17702.6289\n",
      "Epoch 83: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17770.9844 - rmse: 17792.4277 - val_loss: 16842.1250 - val_rmse: 16972.0820\n",
      "Epoch 84/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17979.4434 - rmse: 17979.4434\n",
      "Epoch 84: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17974.3633 - rmse: 17974.1016 - val_loss: 20233.4551 - val_rmse: 20318.4004\n",
      "Epoch 85/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16586.1094 - rmse: 16586.1094\n",
      "Epoch 85: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17003.4922 - rmse: 17073.7090 - val_loss: 16543.7168 - val_rmse: 16665.7969\n",
      "Epoch 86/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16137.7461 - rmse: 16137.7461\n",
      "Epoch 86: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16381.0732 - rmse: 16389.8789 - val_loss: 16572.2090 - val_rmse: 16671.4238\n",
      "Epoch 87/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15655.1289 - rmse: 15655.1289\n",
      "Epoch 87: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15696.1172 - rmse: 15708.7305 - val_loss: 17147.1660 - val_rmse: 17300.5176\n",
      "Epoch 88/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16886.0996 - rmse: 16886.0996\n",
      "Epoch 88: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16900.2441 - rmse: 16886.5762 - val_loss: 17257.4336 - val_rmse: 17357.7031\n",
      "Epoch 89/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18132.9844 - rmse: 18132.9844\n",
      "Epoch 89: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18335.0156 - rmse: 18396.9121 - val_loss: 16787.0332 - val_rmse: 16892.3086\n",
      "Epoch 90/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17793.0156 - rmse: 17793.0156\n",
      "Epoch 90: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18044.0605 - rmse: 18075.1680 - val_loss: 26364.3066 - val_rmse: 26412.9512\n",
      "Epoch 91/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15587.8486 - rmse: 15587.8486\n",
      "Epoch 91: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15681.7393 - rmse: 15703.7646 - val_loss: 17342.7109 - val_rmse: 17427.3145\n",
      "Epoch 92/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16680.6445 - rmse: 16680.6445\n",
      "Epoch 92: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16606.4922 - rmse: 16607.9434 - val_loss: 17770.9414 - val_rmse: 17843.3301\n",
      "Epoch 93/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16804.0781 - rmse: 16804.0781\n",
      "Epoch 93: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16841.2891 - rmse: 16861.5703 - val_loss: 25178.0723 - val_rmse: 25240.5898\n",
      "Epoch 94/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16080.5186 - rmse: 16080.5186\n",
      "Epoch 94: val_loss did not improve from 16230.25391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16118.9463 - rmse: 16159.1895 - val_loss: 17715.9609 - val_rmse: 17821.5879\n",
      "Epoch 94: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 28s - loss: 209813.7188 - rmse: 209813.7188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:10:24.820725: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 106134.3359 - rmse: 105818.0234\n",
      "Epoch 1: val_loss improved from inf to 52749.95703, saving model to ./ckpt/reg_lr005/val_rmse_52500.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 106134.3359 - rmse: 105818.0234 - val_loss: 52749.9570 - val_rmse: 52500.0156\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 63444.4297 - rmse: 63444.4297"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:10:25.706104: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/70 [==========================>...] - ETA: 0s - loss: 52316.4336 - rmse: 52316.4336\n",
      "Epoch 2: val_loss improved from 52749.95703 to 41557.66016, saving model to ./ckpt/reg_lr005/val_rmse_41446.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 52493.8711 - rmse: 52472.7344 - val_loss: 41557.6602 - val_rmse: 41446.2539\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 46836.7227 - rmse: 46836.7227\n",
      "Epoch 3: val_loss improved from 41557.66016 to 38731.60547, saving model to ./ckpt/reg_lr005/val_rmse_38587.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 46599.2734 - rmse: 46523.7539 - val_loss: 38731.6055 - val_rmse: 38586.9023\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 45616.7773 - rmse: 45616.7773\n",
      "Epoch 4: val_loss improved from 38731.60547 to 34403.22656, saving model to ./ckpt/reg_lr005/val_rmse_34271.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 45509.2500 - rmse: 45543.2305 - val_loss: 34403.2266 - val_rmse: 34270.6055\n",
      "Epoch 5/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 42255.0859 - rmse: 42255.0859\n",
      "Epoch 5: val_loss improved from 34403.22656 to 33084.17188, saving model to ./ckpt/reg_lr005/val_rmse_32954.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 41981.9336 - rmse: 41863.8633 - val_loss: 33084.1719 - val_rmse: 32954.0234\n",
      "Epoch 6/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 40899.2852 - rmse: 40899.2852\n",
      "Epoch 6: val_loss improved from 33084.17188 to 32164.09961, saving model to ./ckpt/reg_lr005/val_rmse_32023.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 41179.7188 - rmse: 41144.1797 - val_loss: 32164.0996 - val_rmse: 32023.1172\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 38692.7812 - rmse: 38692.7812\n",
      "Epoch 7: val_loss did not improve from 32164.09961\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 38677.7109 - rmse: 38667.5508 - val_loss: 32713.8770 - val_rmse: 32636.6699\n",
      "Epoch 8/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 38125.3555 - rmse: 38125.3555\n",
      "Epoch 8: val_loss improved from 32164.09961 to 31519.19922, saving model to ./ckpt/reg_lr005/val_rmse_31455.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 38347.6875 - rmse: 38412.3047 - val_loss: 31519.1992 - val_rmse: 31455.1953\n",
      "Epoch 9/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 36216.5117 - rmse: 36216.5117\n",
      "Epoch 9: val_loss did not improve from 31519.19922\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 36695.0078 - rmse: 36642.1367 - val_loss: 40608.3164 - val_rmse: 40633.5898\n",
      "Epoch 10/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 36481.8477 - rmse: 36481.8477\n",
      "Epoch 10: val_loss improved from 31519.19922 to 29696.10742, saving model to ./ckpt/reg_lr005/val_rmse_29656.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 36589.0586 - rmse: 36608.5508 - val_loss: 29696.1074 - val_rmse: 29655.9961\n",
      "Epoch 11/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 33686.3711 - rmse: 33686.3711\n",
      "Epoch 11: val_loss improved from 29696.10742 to 27955.62891, saving model to ./ckpt/reg_lr005/val_rmse_27899.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 33890.6211 - rmse: 33777.3242 - val_loss: 27955.6289 - val_rmse: 27899.4395\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 33523.0195 - rmse: 33523.0195\n",
      "Epoch 12: val_loss did not improve from 27955.62891\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 33218.1562 - rmse: 33163.3594 - val_loss: 28063.0117 - val_rmse: 28080.5781\n",
      "Epoch 13/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 32846.9609 - rmse: 32846.9609\n",
      "Epoch 13: val_loss did not improve from 27955.62891\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 32741.0801 - rmse: 32827.3867 - val_loss: 31080.7734 - val_rmse: 30980.6562\n",
      "Epoch 14/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 30382.6816 - rmse: 30382.6816\n",
      "Epoch 14: val_loss did not improve from 27955.62891\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 30719.5488 - rmse: 30796.4863 - val_loss: 28192.7598 - val_rmse: 28258.1289\n",
      "Epoch 15/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 29130.5098 - rmse: 29130.5098\n",
      "Epoch 15: val_loss did not improve from 27955.62891\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 29126.0391 - rmse: 29083.3301 - val_loss: 29020.3750 - val_rmse: 29087.7676\n",
      "Epoch 16/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 31160.2539 - rmse: 31160.2539\n",
      "Epoch 16: val_loss improved from 27955.62891 to 24458.77734, saving model to ./ckpt/reg_lr005/val_rmse_24503.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 31142.8184 - rmse: 31144.5430 - val_loss: 24458.7773 - val_rmse: 24503.3828\n",
      "Epoch 17/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 28674.4297 - rmse: 28674.4297\n",
      "Epoch 17: val_loss did not improve from 24458.77734\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 29140.0137 - rmse: 29277.0742 - val_loss: 47260.7852 - val_rmse: 47411.6094\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 30435.4199 - rmse: 30386.9473\n",
      "Epoch 18: val_loss improved from 24458.77734 to 23566.66992, saving model to ./ckpt/reg_lr005/val_rmse_23622.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 30435.4199 - rmse: 30386.9473 - val_loss: 23566.6699 - val_rmse: 23622.0996\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27594.5059 - rmse: 27665.2402\n",
      "Epoch 19: val_loss improved from 23566.66992 to 23395.04688, saving model to ./ckpt/reg_lr005/val_rmse_23434.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 27594.5059 - rmse: 27665.2402 - val_loss: 23395.0469 - val_rmse: 23433.8242\n",
      "Epoch 20/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 27195.6191 - rmse: 27195.6191\n",
      "Epoch 20: val_loss did not improve from 23395.04688\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 27150.1562 - rmse: 27125.5000 - val_loss: 25058.0664 - val_rmse: 25121.9297\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 26911.6328 - rmse: 26911.6328\n",
      "Epoch 21: val_loss did not improve from 23395.04688\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26776.8828 - rmse: 26738.0234 - val_loss: 24588.1973 - val_rmse: 24557.4551\n",
      "Epoch 22/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 25580.7715 - rmse: 25580.7715\n",
      "Epoch 22: val_loss did not improve from 23395.04688\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25510.7832 - rmse: 25503.3672 - val_loss: 26961.6211 - val_rmse: 26939.0703\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 26493.1367 - rmse: 26493.1367\n",
      "Epoch 23: val_loss improved from 23395.04688 to 22895.56055, saving model to ./ckpt/reg_lr005/val_rmse_22944.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26340.4766 - rmse: 26321.7441 - val_loss: 22895.5605 - val_rmse: 22944.2148\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 26230.2676 - rmse: 26230.2676\n",
      "Epoch 24: val_loss improved from 22895.56055 to 21278.87891, saving model to ./ckpt/reg_lr005/val_rmse_21273.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26156.5703 - rmse: 26207.7422 - val_loss: 21278.8789 - val_rmse: 21272.5957\n",
      "Epoch 25/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 24986.9473 - rmse: 24986.9473\n",
      "Epoch 25: val_loss did not improve from 21278.87891\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 24946.9180 - rmse: 24899.3652 - val_loss: 25030.4160 - val_rmse: 24973.5078\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 26291.1797 - rmse: 26291.1797\n",
      "Epoch 26: val_loss did not improve from 21278.87891\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26264.3047 - rmse: 26246.1797 - val_loss: 21723.0352 - val_rmse: 21697.6289\n",
      "Epoch 27/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23564.5977 - rmse: 23564.5977\n",
      "Epoch 27: val_loss did not improve from 21278.87891\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24132.6621 - rmse: 24127.6465 - val_loss: 38516.4492 - val_rmse: 38471.8359\n",
      "Epoch 28/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24588.9199 - rmse: 24588.9199\n",
      "Epoch 28: val_loss improved from 21278.87891 to 20213.69336, saving model to ./ckpt/reg_lr005/val_rmse_20219.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 24677.9941 - rmse: 24610.2305 - val_loss: 20213.6934 - val_rmse: 20219.0000\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 27090.9531 - rmse: 27090.9531\n",
      "Epoch 29: val_loss did not improve from 20213.69336\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 27070.9258 - rmse: 27057.4160 - val_loss: 22373.7598 - val_rmse: 22381.4121\n",
      "Epoch 30/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22884.1055 - rmse: 22884.1055\n",
      "Epoch 30: val_loss did not improve from 20213.69336\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22841.2734 - rmse: 22878.4043 - val_loss: 21881.4961 - val_rmse: 21884.1172\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22528.7852 - rmse: 22513.1152\n",
      "Epoch 31: val_loss did not improve from 20213.69336\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22528.7852 - rmse: 22513.1152 - val_loss: 23740.0020 - val_rmse: 23725.0430\n",
      "Epoch 32/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21362.5430 - rmse: 21362.5430\n",
      "Epoch 32: val_loss improved from 20213.69336 to 19801.84570, saving model to ./ckpt/reg_lr005/val_rmse_19839.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21356.9336 - rmse: 21353.1484 - val_loss: 19801.8457 - val_rmse: 19839.0410\n",
      "Epoch 33/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22028.1582 - rmse: 22028.1582\n",
      "Epoch 33: val_loss did not improve from 19801.84570\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22356.9160 - rmse: 22366.2402 - val_loss: 22248.9492 - val_rmse: 22261.7852\n",
      "Epoch 34/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21160.2031 - rmse: 21160.2031\n",
      "Epoch 34: val_loss did not improve from 19801.84570\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21240.7344 - rmse: 21250.6270 - val_loss: 21786.8438 - val_rmse: 21749.6113\n",
      "Epoch 35/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21932.1230 - rmse: 21922.8496\n",
      "Epoch 35: val_loss did not improve from 19801.84570\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21932.1230 - rmse: 21922.8496 - val_loss: 22744.9395 - val_rmse: 22772.0449\n",
      "Epoch 36/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21586.3184 - rmse: 21586.3184\n",
      "Epoch 36: val_loss did not improve from 19801.84570\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21514.8066 - rmse: 21515.5625 - val_loss: 20154.8672 - val_rmse: 20157.7012\n",
      "Epoch 37/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19757.3262 - rmse: 19757.3262\n",
      "Epoch 37: val_loss did not improve from 19801.84570\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19549.6035 - rmse: 19521.8711 - val_loss: 23546.6816 - val_rmse: 23520.0586\n",
      "Epoch 38/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20379.5938 - rmse: 20379.5938\n",
      "Epoch 38: val_loss improved from 19801.84570 to 19165.53125, saving model to ./ckpt/reg_lr005/val_rmse_19193.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20283.1055 - rmse: 20326.2207 - val_loss: 19165.5312 - val_rmse: 19192.9531\n",
      "Epoch 39/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22431.1699 - rmse: 22431.1699\n",
      "Epoch 39: val_loss did not improve from 19165.53125\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22249.7188 - rmse: 22278.4570 - val_loss: 19761.6953 - val_rmse: 19761.6484\n",
      "Epoch 40/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20198.1133 - rmse: 20198.1133\n",
      "Epoch 40: val_loss did not improve from 19165.53125\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20394.7676 - rmse: 20381.3281 - val_loss: 27167.7656 - val_rmse: 27135.0820\n",
      "Epoch 41/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21234.2500 - rmse: 21234.2500\n",
      "Epoch 41: val_loss improved from 19165.53125 to 18790.91211, saving model to ./ckpt/reg_lr005/val_rmse_18764.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21284.7402 - rmse: 21318.7930 - val_loss: 18790.9121 - val_rmse: 18763.9414\n",
      "Epoch 42/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20289.7031 - rmse: 20289.7031\n",
      "Epoch 42: val_loss improved from 18790.91211 to 17658.41406, saving model to ./ckpt/reg_lr005/val_rmse_17599.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20395.5215 - rmse: 20466.8887 - val_loss: 17658.4141 - val_rmse: 17598.7910\n",
      "Epoch 43/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21326.6348 - rmse: 21349.8730\n",
      "Epoch 43: val_loss did not improve from 17658.41406\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21326.6348 - rmse: 21349.8730 - val_loss: 19599.5371 - val_rmse: 19585.4102\n",
      "Epoch 44/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19902.7305 - rmse: 19902.7305\n",
      "Epoch 44: val_loss did not improve from 17658.41406\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19839.6758 - rmse: 19846.5059 - val_loss: 20782.4746 - val_rmse: 20746.8535\n",
      "Epoch 45/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19409.8164 - rmse: 19409.8164\n",
      "Epoch 45: val_loss improved from 17658.41406 to 17193.40820, saving model to ./ckpt/reg_lr005/val_rmse_17161.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19363.7637 - rmse: 19332.7031 - val_loss: 17193.4082 - val_rmse: 17161.4707\n",
      "Epoch 46/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20068.2559 - rmse: 20054.2168\n",
      "Epoch 46: val_loss did not improve from 17193.40820\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20068.2559 - rmse: 20054.2168 - val_loss: 18287.7520 - val_rmse: 18297.4375\n",
      "Epoch 47/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20557.0449 - rmse: 20557.0449\n",
      "Epoch 47: val_loss improved from 17193.40820 to 16584.92578, saving model to ./ckpt/reg_lr005/val_rmse_16551.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20627.4551 - rmse: 20674.9414 - val_loss: 16584.9258 - val_rmse: 16550.7285\n",
      "Epoch 48/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19091.8359 - rmse: 19091.8359\n",
      "Epoch 48: val_loss did not improve from 16584.92578\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19216.8242 - rmse: 19199.6406 - val_loss: 16606.5625 - val_rmse: 16573.4668\n",
      "Epoch 49/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20843.3379 - rmse: 20843.3379\n",
      "Epoch 49: val_loss did not improve from 16584.92578\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20790.0801 - rmse: 20754.1602 - val_loss: 17981.0781 - val_rmse: 17933.9824\n",
      "Epoch 50/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19769.4883 - rmse: 19769.4883\n",
      "Epoch 50: val_loss did not improve from 16584.92578\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19705.9102 - rmse: 19705.0020 - val_loss: 22136.4473 - val_rmse: 22144.7422\n",
      "Epoch 51/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19266.6270 - rmse: 19266.6270\n",
      "Epoch 51: val_loss improved from 16584.92578 to 16510.71289, saving model to ./ckpt/reg_lr005/val_rmse_16502.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19245.6699 - rmse: 19275.9336 - val_loss: 16510.7129 - val_rmse: 16502.0918\n",
      "Epoch 52/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18958.8555 - rmse: 18967.4316\n",
      "Epoch 52: val_loss did not improve from 16510.71289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18958.8555 - rmse: 18967.4316 - val_loss: 19908.5781 - val_rmse: 19912.6797\n",
      "Epoch 53/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18878.2148 - rmse: 18878.2148\n",
      "Epoch 53: val_loss did not improve from 16510.71289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18942.8223 - rmse: 18986.3965 - val_loss: 16668.6113 - val_rmse: 16634.2031\n",
      "Epoch 54/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19157.3027 - rmse: 19157.3027\n",
      "Epoch 54: val_loss did not improve from 16510.71289\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19398.0371 - rmse: 19458.8652 - val_loss: 18661.6797 - val_rmse: 18619.7070\n",
      "Epoch 55/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18594.2520 - rmse: 18604.9844\n",
      "Epoch 55: val_loss did not improve from 16510.71289\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18594.2520 - rmse: 18604.9844 - val_loss: 23515.2285 - val_rmse: 23508.8184\n",
      "Epoch 56/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19506.3125 - rmse: 19506.3125\n",
      "Epoch 56: val_loss did not improve from 16510.71289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19304.7910 - rmse: 19268.3320 - val_loss: 18122.3711 - val_rmse: 18114.1152\n",
      "Epoch 57/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17756.2637 - rmse: 17756.2637\n",
      "Epoch 57: val_loss did not improve from 16510.71289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17955.0078 - rmse: 18089.0488 - val_loss: 16855.7559 - val_rmse: 16791.4219\n",
      "Epoch 58/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19063.2266 - rmse: 19063.2266\n",
      "Epoch 58: val_loss did not improve from 16510.71289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19396.4258 - rmse: 19509.8574 - val_loss: 16873.6797 - val_rmse: 16861.2070\n",
      "Epoch 59/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18519.2539 - rmse: 18519.2539\n",
      "Epoch 59: val_loss did not improve from 16510.71289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18437.3008 - rmse: 18403.4902 - val_loss: 21095.0938 - val_rmse: 21124.5859\n",
      "Epoch 60/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18764.5195 - rmse: 18764.5195\n",
      "Epoch 60: val_loss did not improve from 16510.71289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18804.6055 - rmse: 18863.3594 - val_loss: 21775.9648 - val_rmse: 21784.3438\n",
      "Epoch 61/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18300.4473 - rmse: 18300.4473\n",
      "Epoch 61: val_loss improved from 16510.71289 to 15614.78223, saving model to ./ckpt/reg_lr005/val_rmse_15589.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18299.9434 - rmse: 18312.6660 - val_loss: 15614.7822 - val_rmse: 15588.7441\n",
      "Epoch 62/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17905.1602 - rmse: 17905.1602\n",
      "Epoch 62: val_loss did not improve from 15614.78223\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17820.2148 - rmse: 17833.0176 - val_loss: 18189.0195 - val_rmse: 18194.1836\n",
      "Epoch 63/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19663.5195 - rmse: 19663.5195\n",
      "Epoch 63: val_loss did not improve from 15614.78223\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19714.8457 - rmse: 19718.2070 - val_loss: 16239.6367 - val_rmse: 16173.0566\n",
      "Epoch 64/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19031.3262 - rmse: 19031.3262\n",
      "Epoch 64: val_loss did not improve from 15614.78223\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18940.3047 - rmse: 18934.3203 - val_loss: 16145.0957 - val_rmse: 16087.8662\n",
      "Epoch 65/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18536.2383 - rmse: 18536.2383\n",
      "Epoch 65: val_loss did not improve from 15614.78223\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18520.5391 - rmse: 18509.9512 - val_loss: 16411.9062 - val_rmse: 16423.1016\n",
      "Epoch 66/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18043.0156 - rmse: 18043.0156\n",
      "Epoch 66: val_loss did not improve from 15614.78223\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18317.9473 - rmse: 18359.9238 - val_loss: 25021.7812 - val_rmse: 24966.2090\n",
      "Epoch 67/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21253.9141 - rmse: 21253.9141\n",
      "Epoch 67: val_loss improved from 15614.78223 to 15375.10352, saving model to ./ckpt/reg_lr005/val_rmse_15368.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21296.5039 - rmse: 21389.7148 - val_loss: 15375.1035 - val_rmse: 15367.5010\n",
      "Epoch 68/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18749.0645 - rmse: 18749.0645\n",
      "Epoch 68: val_loss did not improve from 15375.10352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18680.0625 - rmse: 18669.9473 - val_loss: 16885.9590 - val_rmse: 16801.9180\n",
      "Epoch 69/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17537.0273 - rmse: 17537.0273\n",
      "Epoch 69: val_loss did not improve from 15375.10352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17446.8555 - rmse: 17474.8535 - val_loss: 18239.1445 - val_rmse: 18264.2578\n",
      "Epoch 70/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17376.7871 - rmse: 17376.7871\n",
      "Epoch 70: val_loss did not improve from 15375.10352\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17583.4941 - rmse: 17611.5879 - val_loss: 19690.6855 - val_rmse: 19712.9082\n",
      "Epoch 71/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17886.7051 - rmse: 17886.7051\n",
      "Epoch 71: val_loss did not improve from 15375.10352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17908.0254 - rmse: 17908.6660 - val_loss: 16444.6191 - val_rmse: 16437.1855\n",
      "Epoch 72/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18592.7070 - rmse: 18592.7070\n",
      "Epoch 72: val_loss did not improve from 15375.10352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18673.9629 - rmse: 18772.2578 - val_loss: 18194.0176 - val_rmse: 18159.6250\n",
      "Epoch 73/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18956.3867 - rmse: 18956.3867\n",
      "Epoch 73: val_loss improved from 15375.10352 to 15256.53711, saving model to ./ckpt/reg_lr005/val_rmse_15244.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18987.4785 - rmse: 18959.9414 - val_loss: 15256.5371 - val_rmse: 15244.3359\n",
      "Epoch 74/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17919.9062 - rmse: 17919.9062\n",
      "Epoch 74: val_loss did not improve from 15256.53711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17932.2012 - rmse: 17941.1758 - val_loss: 23419.9766 - val_rmse: 23459.4199\n",
      "Epoch 75/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17382.6465 - rmse: 17382.6465\n",
      "Epoch 75: val_loss did not improve from 15256.53711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17338.3984 - rmse: 17340.5508 - val_loss: 19383.4414 - val_rmse: 19399.8047\n",
      "Epoch 76/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18988.8184 - rmse: 18988.8652\n",
      "Epoch 76: val_loss did not improve from 15256.53711\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18988.8184 - rmse: 18988.8652 - val_loss: 15335.7197 - val_rmse: 15299.1094\n",
      "Epoch 77/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18069.8945 - rmse: 18069.8945\n",
      "Epoch 77: val_loss did not improve from 15256.53711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18111.9512 - rmse: 18166.4395 - val_loss: 15407.0576 - val_rmse: 15393.1719\n",
      "Epoch 78/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18139.7500 - rmse: 18145.3906\n",
      "Epoch 78: val_loss did not improve from 15256.53711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18139.7500 - rmse: 18145.3906 - val_loss: 26440.2969 - val_rmse: 26474.9492\n",
      "Epoch 79/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18684.4707 - rmse: 18684.4707\n",
      "Epoch 79: val_loss did not improve from 15256.53711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18719.2617 - rmse: 18700.5898 - val_loss: 17486.3008 - val_rmse: 17512.7461\n",
      "Epoch 80/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18430.5059 - rmse: 18430.5059\n",
      "Epoch 80: val_loss did not improve from 15256.53711\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18281.4668 - rmse: 18255.5352 - val_loss: 17991.1152 - val_rmse: 18008.5195\n",
      "Epoch 81/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17670.5938 - rmse: 17648.9492\n",
      "Epoch 81: val_loss did not improve from 15256.53711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17670.5938 - rmse: 17648.9492 - val_loss: 21386.9766 - val_rmse: 21396.1016\n",
      "Epoch 82/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16166.8389 - rmse: 16166.8389\n",
      "Epoch 82: val_loss did not improve from 15256.53711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16358.1973 - rmse: 16387.3535 - val_loss: 15619.0977 - val_rmse: 15611.6133\n",
      "Epoch 83/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16875.7090 - rmse: 16913.7617\n",
      "Epoch 83: val_loss did not improve from 15256.53711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16875.7090 - rmse: 16913.7617 - val_loss: 21161.1641 - val_rmse: 21174.5527\n",
      "Epoch 84/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16729.8340 - rmse: 16729.8340\n",
      "Epoch 84: val_loss improved from 15256.53711 to 14839.33203, saving model to ./ckpt/reg_lr005/val_rmse_14784.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16813.8320 - rmse: 16900.0430 - val_loss: 14839.3320 - val_rmse: 14783.9834\n",
      "Epoch 85/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17241.0000 - rmse: 17227.5020\n",
      "Epoch 85: val_loss did not improve from 14839.33203\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 17241.0000 - rmse: 17227.5020 - val_loss: 18835.2109 - val_rmse: 18845.2207\n",
      "Epoch 86/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17966.5645 - rmse: 17966.5645\n",
      "Epoch 86: val_loss did not improve from 14839.33203\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17920.8320 - rmse: 17889.9902 - val_loss: 15482.8135 - val_rmse: 15471.1602\n",
      "Epoch 87/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17833.9668 - rmse: 17833.9668\n",
      "Epoch 87: val_loss did not improve from 14839.33203\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17940.8789 - rmse: 17919.0352 - val_loss: 15725.4004 - val_rmse: 15644.4580\n",
      "Epoch 88/120\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 17340.3164 - rmse: 17340.3164\n",
      "Epoch 88: val_loss did not improve from 14839.33203\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17174.2930 - rmse: 17240.7383 - val_loss: 19767.7715 - val_rmse: 19790.5664\n",
      "Epoch 89/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15755.4463 - rmse: 15755.4463\n",
      "Epoch 89: val_loss did not improve from 14839.33203\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15728.2266 - rmse: 15712.7178 - val_loss: 23184.0625 - val_rmse: 23217.1367\n",
      "Epoch 90/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16368.7754 - rmse: 16368.7754\n",
      "Epoch 90: val_loss improved from 14839.33203 to 14585.30176, saving model to ./ckpt/reg_lr005/val_rmse_14575.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16313.3350 - rmse: 16313.4590 - val_loss: 14585.3018 - val_rmse: 14574.6074\n",
      "Epoch 91/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17408.5234 - rmse: 17408.5234\n",
      "Epoch 91: val_loss did not improve from 14585.30176\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17360.4375 - rmse: 17411.1074 - val_loss: 20386.4492 - val_rmse: 20443.3828\n",
      "Epoch 92/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18425.8262 - rmse: 18425.8262\n",
      "Epoch 92: val_loss did not improve from 14585.30176\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18390.7402 - rmse: 18367.0742 - val_loss: 14809.0088 - val_rmse: 14732.0049\n",
      "Epoch 93/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17160.0703 - rmse: 17160.0703\n",
      "Epoch 93: val_loss did not improve from 14585.30176\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17095.7695 - rmse: 17086.5137 - val_loss: 18134.0879 - val_rmse: 18130.7930\n",
      "Epoch 94/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17751.7344 - rmse: 17751.7344\n",
      "Epoch 94: val_loss did not improve from 14585.30176\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17779.3086 - rmse: 17797.9082 - val_loss: 23424.5684 - val_rmse: 23439.2168\n",
      "Epoch 95/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17757.1973 - rmse: 17757.1973\n",
      "Epoch 95: val_loss did not improve from 14585.30176\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17663.9707 - rmse: 17627.3379 - val_loss: 14799.6826 - val_rmse: 14769.4121\n",
      "Epoch 96/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15739.6230 - rmse: 15739.6230\n",
      "Epoch 96: val_loss improved from 14585.30176 to 14235.56445, saving model to ./ckpt/reg_lr005/val_rmse_14184.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15743.8496 - rmse: 15747.8408 - val_loss: 14235.5645 - val_rmse: 14184.2002\n",
      "Epoch 97/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16925.6816 - rmse: 16925.6816\n",
      "Epoch 97: val_loss did not improve from 14235.56445\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16924.5195 - rmse: 16948.8867 - val_loss: 19455.2051 - val_rmse: 19490.0977\n",
      "Epoch 98/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15375.4229 - rmse: 15375.4229\n",
      "Epoch 98: val_loss did not improve from 14235.56445\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15457.0654 - rmse: 15462.8232 - val_loss: 17721.1680 - val_rmse: 17743.9121\n",
      "Epoch 99/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18565.9941 - rmse: 18565.9941\n",
      "Epoch 99: val_loss did not improve from 14235.56445\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18577.9277 - rmse: 18588.1777 - val_loss: 19511.1562 - val_rmse: 19537.4746\n",
      "Epoch 100/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17068.0898 - rmse: 17068.0898\n",
      "Epoch 100: val_loss did not improve from 14235.56445\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17138.7793 - rmse: 17130.2363 - val_loss: 18050.4180 - val_rmse: 18074.6582\n",
      "Epoch 101/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16343.6689 - rmse: 16343.6689\n",
      "Epoch 101: val_loss did not improve from 14235.56445\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16644.3164 - rmse: 16645.9961 - val_loss: 18686.2090 - val_rmse: 18729.4863\n",
      "Epoch 102/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16134.1953 - rmse: 16134.1953\n",
      "Epoch 102: val_loss did not improve from 14235.56445\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16221.4463 - rmse: 16297.0361 - val_loss: 16266.1221 - val_rmse: 16194.2959\n",
      "Epoch 103/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16613.3906 - rmse: 16613.3906\n",
      "Epoch 103: val_loss improved from 14235.56445 to 14136.14160, saving model to ./ckpt/reg_lr005/val_rmse_14109.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16624.8086 - rmse: 16632.5117 - val_loss: 14136.1416 - val_rmse: 14108.6152\n",
      "Epoch 104/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16073.1045 - rmse: 16073.1045\n",
      "Epoch 104: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16170.5977 - rmse: 16200.0059 - val_loss: 14884.6865 - val_rmse: 14867.4111\n",
      "Epoch 105/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16890.0020 - rmse: 16890.0020\n",
      "Epoch 105: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17032.4199 - rmse: 17065.2559 - val_loss: 21236.7090 - val_rmse: 21269.7109\n",
      "Epoch 106/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16688.4180 - rmse: 16688.4180\n",
      "Epoch 106: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16709.3359 - rmse: 16733.3809 - val_loss: 14455.7881 - val_rmse: 14385.4248\n",
      "Epoch 107/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19214.0391 - rmse: 19175.8789\n",
      "Epoch 107: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19214.0391 - rmse: 19175.8789 - val_loss: 14819.1221 - val_rmse: 14793.7109\n",
      "Epoch 108/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16631.2734 - rmse: 16631.2734\n",
      "Epoch 108: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16639.9629 - rmse: 16770.0430 - val_loss: 14873.5215 - val_rmse: 14879.2695\n",
      "Epoch 109/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16176.8242 - rmse: 16176.8242\n",
      "Epoch 109: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16079.6758 - rmse: 16080.5859 - val_loss: 14704.3613 - val_rmse: 14709.4795\n",
      "Epoch 110/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15273.8213 - rmse: 15273.4746\n",
      "Epoch 110: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15273.8213 - rmse: 15273.4746 - val_loss: 22040.6328 - val_rmse: 22079.0234\n",
      "Epoch 111/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15156.1787 - rmse: 15156.1787\n",
      "Epoch 111: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 15369.9189 - rmse: 15459.5176 - val_loss: 23269.3164 - val_rmse: 23308.9512\n",
      "Epoch 112/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16509.5566 - rmse: 16509.5566\n",
      "Epoch 112: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16403.5117 - rmse: 16377.1592 - val_loss: 17009.4473 - val_rmse: 17027.2422\n",
      "Epoch 113/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15410.8213 - rmse: 15410.8213\n",
      "Epoch 113: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15401.3184 - rmse: 15397.0166 - val_loss: 19567.9746 - val_rmse: 19584.6543\n",
      "Epoch 114/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15117.7744 - rmse: 15117.7744\n",
      "Epoch 114: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15045.7207 - rmse: 15026.5498 - val_loss: 16943.6191 - val_rmse: 16960.0254\n",
      "Epoch 115/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15986.9414 - rmse: 15986.9414\n",
      "Epoch 115: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16010.4609 - rmse: 16028.4736 - val_loss: 24476.1270 - val_rmse: 24548.2305\n",
      "Epoch 116/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16076.0947 - rmse: 16076.0947\n",
      "Epoch 116: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16173.3945 - rmse: 16239.0166 - val_loss: 19663.8984 - val_rmse: 19701.9453\n",
      "Epoch 117/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16543.4219 - rmse: 16543.4219\n",
      "Epoch 117: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16475.7656 - rmse: 16457.1113 - val_loss: 14156.1719 - val_rmse: 14149.1816\n",
      "Epoch 118/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15531.6289 - rmse: 15531.6289\n",
      "Epoch 118: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15887.8926 - rmse: 15988.8682 - val_loss: 19241.9609 - val_rmse: 19281.0781\n",
      "Epoch 119/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15029.9434 - rmse: 15029.9434\n",
      "Epoch 119: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15184.5928 - rmse: 15189.7695 - val_loss: 21497.2578 - val_rmse: 21554.6387\n",
      "Epoch 120/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16265.9600 - rmse: 16265.9600\n",
      "Epoch 120: val_loss did not improve from 14136.14160\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16278.5039 - rmse: 16256.9658 - val_loss: 26528.7266 - val_rmse: 26567.2148\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:11:45.694844: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 106293.4531 - rmse: 105920.6328\n",
      "Epoch 1: val_loss improved from inf to 58538.94531, saving model to ./ckpt/reg_lr005/val_rmse_57692.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 106293.4531 - rmse: 105920.6328 - val_loss: 58538.9453 - val_rmse: 57692.1211\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 50938.8750 - rmse: 50938.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:11:46.722650: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/70 [============================>.] - ETA: 0s - loss: 56138.8008 - rmse: 56138.8008\n",
      "Epoch 2: val_loss improved from 58538.94531 to 46554.19922, saving model to ./ckpt/reg_lr005/val_rmse_45949.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 55741.5664 - rmse: 55615.1562 - val_loss: 46554.1992 - val_rmse: 45948.8125\n",
      "Epoch 3/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 45672.0547 - rmse: 45551.3711\n",
      "Epoch 3: val_loss did not improve from 46554.19922\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 45672.0547 - rmse: 45551.3711 - val_loss: 50380.4492 - val_rmse: 49701.9102\n",
      "Epoch 4/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 44440.4219 - rmse: 44440.4219\n",
      "Epoch 4: val_loss improved from 46554.19922 to 40958.52734, saving model to ./ckpt/reg_lr005/val_rmse_40396.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 44346.2031 - rmse: 44318.6172 - val_loss: 40958.5273 - val_rmse: 40396.4062\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 42001.1758 - rmse: 41931.7109\n",
      "Epoch 5: val_loss improved from 40958.52734 to 38634.59375, saving model to ./ckpt/reg_lr005/val_rmse_38242.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 42001.1758 - rmse: 41931.7109 - val_loss: 38634.5938 - val_rmse: 38241.9961\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 39532.2734 - rmse: 39484.8164\n",
      "Epoch 6: val_loss improved from 38634.59375 to 36687.46094, saving model to ./ckpt/reg_lr005/val_rmse_36202.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 39532.2734 - rmse: 39484.8164 - val_loss: 36687.4609 - val_rmse: 36202.0039\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 38772.0469 - rmse: 38772.0469\n",
      "Epoch 7: val_loss improved from 36687.46094 to 35722.02344, saving model to ./ckpt/reg_lr005/val_rmse_35260.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 38733.8203 - rmse: 38708.0352 - val_loss: 35722.0234 - val_rmse: 35260.3828\n",
      "Epoch 8/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 37402.2227 - rmse: 37402.2227\n",
      "Epoch 8: val_loss improved from 35722.02344 to 34362.98438, saving model to ./ckpt/reg_lr005/val_rmse_33995.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 37318.1914 - rmse: 37295.2734 - val_loss: 34362.9844 - val_rmse: 33995.1914\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 35583.9648 - rmse: 35550.8789\n",
      "Epoch 9: val_loss did not improve from 34362.98438\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 35583.9648 - rmse: 35550.8789 - val_loss: 36738.6953 - val_rmse: 36324.0195\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 34081.3711 - rmse: 34087.5508\n",
      "Epoch 10: val_loss improved from 34362.98438 to 32004.02930, saving model to ./ckpt/reg_lr005/val_rmse_31698.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 34081.3711 - rmse: 34087.5508 - val_loss: 32004.0293 - val_rmse: 31697.6465\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 33508.7188 - rmse: 33449.3125\n",
      "Epoch 11: val_loss improved from 32004.02930 to 31131.72266, saving model to ./ckpt/reg_lr005/val_rmse_30852.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 33508.7188 - rmse: 33449.3125 - val_loss: 31131.7227 - val_rmse: 30851.6211\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 34024.4336 - rmse: 34024.4336\n",
      "Epoch 12: val_loss improved from 31131.72266 to 29854.89844, saving model to ./ckpt/reg_lr005/val_rmse_29601.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 33858.2773 - rmse: 33746.2148 - val_loss: 29854.8984 - val_rmse: 29600.6465\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 29647.3535 - rmse: 29647.3535\n",
      "Epoch 13: val_loss improved from 29854.89844 to 28991.97852, saving model to ./ckpt/reg_lr005/val_rmse_28795.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 29662.7812 - rmse: 29673.1836 - val_loss: 28991.9785 - val_rmse: 28795.3496\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 30945.0898 - rmse: 30949.3242\n",
      "Epoch 14: val_loss did not improve from 28991.97852\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 30945.0898 - rmse: 30949.3242 - val_loss: 30552.1426 - val_rmse: 30283.9141\n",
      "Epoch 15/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 29675.6855 - rmse: 29675.6855\n",
      "Epoch 15: val_loss did not improve from 28991.97852\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 30039.4004 - rmse: 29992.8398 - val_loss: 41155.6055 - val_rmse: 40796.2422\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 28530.3281 - rmse: 28530.3281\n",
      "Epoch 16: val_loss did not improve from 28991.97852\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 28287.5996 - rmse: 28280.9219 - val_loss: 29369.7598 - val_rmse: 29177.8008\n",
      "Epoch 17/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 26268.2148 - rmse: 26268.2148\n",
      "Epoch 17: val_loss improved from 28991.97852 to 25928.98438, saving model to ./ckpt/reg_lr005/val_rmse_25819.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26240.1660 - rmse: 26287.5957 - val_loss: 25928.9844 - val_rmse: 25819.2383\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27705.9082 - rmse: 27705.9082\n",
      "Epoch 18: val_loss improved from 25928.98438 to 24070.52148, saving model to ./ckpt/reg_lr005/val_rmse_24036.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27614.4570 - rmse: 27622.4668 - val_loss: 24070.5215 - val_rmse: 24035.6445\n",
      "Epoch 19/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25324.2168 - rmse: 25324.2168\n",
      "Epoch 19: val_loss did not improve from 24070.52148\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25433.4512 - rmse: 25361.8770 - val_loss: 27050.3398 - val_rmse: 26935.0938\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25443.9219 - rmse: 25550.9102\n",
      "Epoch 20: val_loss improved from 24070.52148 to 23074.43945, saving model to ./ckpt/reg_lr005/val_rmse_23076.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25443.9219 - rmse: 25550.9102 - val_loss: 23074.4395 - val_rmse: 23076.2070\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 28225.1758 - rmse: 28225.1758\n",
      "Epoch 21: val_loss improved from 23074.43945 to 22749.99414, saving model to ./ckpt/reg_lr005/val_rmse_22722.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 28082.0957 - rmse: 28051.3906 - val_loss: 22749.9941 - val_rmse: 22722.1211\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24702.9863 - rmse: 24795.3633\n",
      "Epoch 22: val_loss did not improve from 22749.99414\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24702.9863 - rmse: 24795.3633 - val_loss: 22878.4551 - val_rmse: 22845.7305\n",
      "Epoch 23/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 24434.9258 - rmse: 24434.9258\n",
      "Epoch 23: val_loss did not improve from 22749.99414\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 24502.2910 - rmse: 24554.5215 - val_loss: 30624.4707 - val_rmse: 30470.4824\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23434.0977 - rmse: 23434.0977\n",
      "Epoch 24: val_loss improved from 22749.99414 to 21937.42773, saving model to ./ckpt/reg_lr005/val_rmse_21959.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23433.8008 - rmse: 23407.8027 - val_loss: 21937.4277 - val_rmse: 21959.2266\n",
      "Epoch 25/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23314.9922 - rmse: 23314.9922\n",
      "Epoch 25: val_loss did not improve from 21937.42773\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23418.3613 - rmse: 23460.9941 - val_loss: 29179.3613 - val_rmse: 29049.9805\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24855.7754 - rmse: 24855.7754\n",
      "Epoch 26: val_loss did not improve from 21937.42773\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24717.0430 - rmse: 24757.5977 - val_loss: 27501.5820 - val_rmse: 27340.3672\n",
      "Epoch 27/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22865.8789 - rmse: 22865.8789\n",
      "Epoch 27: val_loss did not improve from 21937.42773\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22870.2363 - rmse: 22873.1758 - val_loss: 24988.7715 - val_rmse: 24871.7246\n",
      "Epoch 28/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 26207.6348 - rmse: 26207.6348\n",
      "Epoch 28: val_loss did not improve from 21937.42773\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26153.4863 - rmse: 26126.9531 - val_loss: 29477.7266 - val_rmse: 29332.0039\n",
      "Epoch 29/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23155.3770 - rmse: 23155.3770\n",
      "Epoch 29: val_loss improved from 21937.42773 to 21021.81836, saving model to ./ckpt/reg_lr005/val_rmse_21050.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23216.6113 - rmse: 23260.7344 - val_loss: 21021.8184 - val_rmse: 21049.9844\n",
      "Epoch 30/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24220.6875 - rmse: 24233.5918\n",
      "Epoch 30: val_loss did not improve from 21021.81836\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24220.6875 - rmse: 24233.5918 - val_loss: 29452.8105 - val_rmse: 29264.7402\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22715.0762 - rmse: 22758.6641\n",
      "Epoch 31: val_loss did not improve from 21021.81836\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22715.0762 - rmse: 22758.6641 - val_loss: 22272.7324 - val_rmse: 22290.8320\n",
      "Epoch 32/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22066.0684 - rmse: 22066.0684\n",
      "Epoch 32: val_loss did not improve from 21021.81836\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22021.1074 - rmse: 21990.7832 - val_loss: 21137.9512 - val_rmse: 21070.2129\n",
      "Epoch 33/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 22368.0234 - rmse: 22368.0234\n",
      "Epoch 33: val_loss improved from 21021.81836 to 19905.24219, saving model to ./ckpt/reg_lr005/val_rmse_19871.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22307.1172 - rmse: 22293.5625 - val_loss: 19905.2422 - val_rmse: 19870.8672\n",
      "Epoch 34/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20921.5215 - rmse: 20921.5215\n",
      "Epoch 34: val_loss did not improve from 19905.24219\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20853.8730 - rmse: 20852.8496 - val_loss: 20214.5820 - val_rmse: 20209.9492\n",
      "Epoch 35/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22637.1895 - rmse: 22637.1895\n",
      "Epoch 35: val_loss did not improve from 19905.24219\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22611.6562 - rmse: 22582.9492 - val_loss: 27280.9141 - val_rmse: 27115.8730\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21750.9336 - rmse: 21781.9375\n",
      "Epoch 36: val_loss did not improve from 19905.24219\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21750.9336 - rmse: 21781.9375 - val_loss: 39175.6641 - val_rmse: 38916.8945\n",
      "Epoch 37/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20617.9727 - rmse: 20617.9727\n",
      "Epoch 37: val_loss did not improve from 19905.24219\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20634.2656 - rmse: 20655.9902 - val_loss: 20344.8672 - val_rmse: 20301.3926\n",
      "Epoch 38/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20828.7188 - rmse: 20828.7188\n",
      "Epoch 38: val_loss improved from 19905.24219 to 19272.16016, saving model to ./ckpt/reg_lr005/val_rmse_19241.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20759.1465 - rmse: 20804.3418 - val_loss: 19272.1602 - val_rmse: 19240.7578\n",
      "Epoch 39/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19735.5195 - rmse: 19764.5508\n",
      "Epoch 39: val_loss did not improve from 19272.16016\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19735.5195 - rmse: 19764.5508 - val_loss: 26177.5234 - val_rmse: 26034.4434\n",
      "Epoch 40/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20268.0195 - rmse: 20268.0195\n",
      "Epoch 40: val_loss did not improve from 19272.16016\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20258.4648 - rmse: 20276.9473 - val_loss: 21166.8418 - val_rmse: 21065.9258\n",
      "Epoch 41/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19211.9434 - rmse: 19211.9434\n",
      "Epoch 41: val_loss did not improve from 19272.16016\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19274.1113 - rmse: 19322.3008 - val_loss: 22900.4238 - val_rmse: 22789.2500\n",
      "Epoch 42/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20094.7031 - rmse: 20094.7031\n",
      "Epoch 42: val_loss did not improve from 19272.16016\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19993.1582 - rmse: 19981.7637 - val_loss: 22481.0664 - val_rmse: 22356.0762\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19615.6445 - rmse: 19615.6445\n",
      "Epoch 43: val_loss improved from 19272.16016 to 18950.71680, saving model to ./ckpt/reg_lr005/val_rmse_18928.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19696.8438 - rmse: 19751.6035 - val_loss: 18950.7168 - val_rmse: 18928.1406\n",
      "Epoch 44/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19709.7148 - rmse: 19709.7148\n",
      "Epoch 44: val_loss did not improve from 18950.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19589.2559 - rmse: 19567.6602 - val_loss: 20379.6406 - val_rmse: 20327.9043\n",
      "Epoch 45/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19654.3867 - rmse: 19654.3867\n",
      "Epoch 45: val_loss did not improve from 18950.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19916.8594 - rmse: 19931.0410 - val_loss: 19029.8672 - val_rmse: 18989.7051\n",
      "Epoch 46/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19966.2988 - rmse: 19959.0508\n",
      "Epoch 46: val_loss did not improve from 18950.71680\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19966.2988 - rmse: 19959.0508 - val_loss: 19528.3164 - val_rmse: 19456.0547\n",
      "Epoch 47/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18705.7383 - rmse: 18713.0742\n",
      "Epoch 47: val_loss did not improve from 18950.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18705.7383 - rmse: 18713.0742 - val_loss: 20269.0527 - val_rmse: 20168.0234\n",
      "Epoch 48/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18548.2812 - rmse: 18548.2812\n",
      "Epoch 48: val_loss did not improve from 18950.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18590.9824 - rmse: 18574.8262 - val_loss: 21209.5801 - val_rmse: 21117.1172\n",
      "Epoch 49/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19551.7891 - rmse: 19551.7891\n",
      "Epoch 49: val_loss did not improve from 18950.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19613.8066 - rmse: 19608.9727 - val_loss: 31004.4180 - val_rmse: 30788.5078\n",
      "Epoch 50/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18958.3906 - rmse: 18958.3906\n",
      "Epoch 50: val_loss did not improve from 18950.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18926.0645 - rmse: 18927.6367 - val_loss: 19113.4102 - val_rmse: 19016.2910\n",
      "Epoch 51/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20251.4746 - rmse: 20251.4746\n",
      "Epoch 51: val_loss improved from 18950.71680 to 18557.22461, saving model to ./ckpt/reg_lr005/val_rmse_18484.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20315.9551 - rmse: 20332.5059 - val_loss: 18557.2246 - val_rmse: 18484.4512\n",
      "Epoch 52/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18840.4531 - rmse: 18840.4531\n",
      "Epoch 52: val_loss did not improve from 18557.22461\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18870.6836 - rmse: 18895.6719 - val_loss: 22853.7070 - val_rmse: 22752.0176\n",
      "Epoch 53/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18879.9551 - rmse: 18879.9551\n",
      "Epoch 53: val_loss did not improve from 18557.22461\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18785.0078 - rmse: 18834.0703 - val_loss: 21949.3457 - val_rmse: 21830.7578\n",
      "Epoch 54/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18277.6348 - rmse: 18277.6348\n",
      "Epoch 54: val_loss did not improve from 18557.22461\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18362.5117 - rmse: 18454.1992 - val_loss: 27261.5098 - val_rmse: 27091.2812\n",
      "Epoch 55/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19312.2832 - rmse: 19312.2832\n",
      "Epoch 55: val_loss did not improve from 18557.22461\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19242.0039 - rmse: 19225.2207 - val_loss: 22326.7871 - val_rmse: 22133.3164\n",
      "Epoch 56/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19312.7109 - rmse: 19312.7109\n",
      "Epoch 56: val_loss did not improve from 18557.22461\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19280.9062 - rmse: 19259.6035 - val_loss: 25348.9395 - val_rmse: 25128.4961\n",
      "Epoch 57/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18009.4473 - rmse: 18009.4473\n",
      "Epoch 57: val_loss did not improve from 18557.22461\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17908.1699 - rmse: 17894.9277 - val_loss: 21647.8711 - val_rmse: 21534.5234\n",
      "Epoch 58/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18456.3184 - rmse: 18456.3184\n",
      "Epoch 58: val_loss did not improve from 18557.22461\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18728.8535 - rmse: 18740.2852 - val_loss: 26693.1074 - val_rmse: 26519.3809\n",
      "Epoch 59/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18297.6582 - rmse: 18297.6582\n",
      "Epoch 59: val_loss did not improve from 18557.22461\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18302.1191 - rmse: 18326.8691 - val_loss: 25246.9062 - val_rmse: 25079.1758\n",
      "Epoch 60/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18368.8477 - rmse: 18368.8477\n",
      "Epoch 60: val_loss did not improve from 18557.22461\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18422.8203 - rmse: 18412.5762 - val_loss: 20627.2734 - val_rmse: 20508.6875\n",
      "Epoch 61/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17960.5996 - rmse: 17960.5996\n",
      "Epoch 61: val_loss did not improve from 18557.22461\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18172.2422 - rmse: 18182.2539 - val_loss: 21643.1621 - val_rmse: 21489.1406\n",
      "Epoch 62/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18087.7793 - rmse: 18087.7793\n",
      "Epoch 62: val_loss improved from 18557.22461 to 18220.46484, saving model to ./ckpt/reg_lr005/val_rmse_18151.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18306.0332 - rmse: 18384.1055 - val_loss: 18220.4648 - val_rmse: 18151.0723\n",
      "Epoch 63/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18238.3184 - rmse: 18238.3184\n",
      "Epoch 63: val_loss did not improve from 18220.46484\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18298.0742 - rmse: 18277.7070 - val_loss: 18471.3340 - val_rmse: 18403.9395\n",
      "Epoch 64/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17936.6328 - rmse: 17936.6328\n",
      "Epoch 64: val_loss did not improve from 18220.46484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17994.3770 - rmse: 18033.3223 - val_loss: 20030.6445 - val_rmse: 19898.2148\n",
      "Epoch 65/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18056.4082 - rmse: 18056.4082\n",
      "Epoch 65: val_loss did not improve from 18220.46484\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18064.2324 - rmse: 18068.6992 - val_loss: 22661.7422 - val_rmse: 22489.5117\n",
      "Epoch 66/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17641.2637 - rmse: 17641.2637\n",
      "Epoch 66: val_loss did not improve from 18220.46484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17741.3965 - rmse: 17779.9824 - val_loss: 23324.2129 - val_rmse: 23175.8145\n",
      "Epoch 67/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17840.0234 - rmse: 17840.0234\n",
      "Epoch 67: val_loss did not improve from 18220.46484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17870.4883 - rmse: 17854.6582 - val_loss: 25621.9160 - val_rmse: 25430.3750\n",
      "Epoch 68/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17910.3027 - rmse: 17910.3027\n",
      "Epoch 68: val_loss improved from 18220.46484 to 17904.47656, saving model to ./ckpt/reg_lr005/val_rmse_17811.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17960.4062 - rmse: 17993.6582 - val_loss: 17904.4766 - val_rmse: 17810.9902\n",
      "Epoch 69/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18027.4336 - rmse: 18027.4336\n",
      "Epoch 69: val_loss did not improve from 17904.47656\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17984.8926 - rmse: 17990.3652 - val_loss: 20684.2871 - val_rmse: 20545.9688\n",
      "Epoch 70/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17927.5430 - rmse: 17927.5430\n",
      "Epoch 70: val_loss did not improve from 17904.47656\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17900.9941 - rmse: 17883.0879 - val_loss: 24219.5000 - val_rmse: 24034.9219\n",
      "Epoch 71/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17660.3867 - rmse: 17660.3867\n",
      "Epoch 71: val_loss did not improve from 17904.47656\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17921.8301 - rmse: 17991.8066 - val_loss: 32879.4531 - val_rmse: 32645.9727\n",
      "Epoch 72/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18873.5820 - rmse: 18873.5820\n",
      "Epoch 72: val_loss did not improve from 17904.47656\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19130.1895 - rmse: 19232.8750 - val_loss: 20256.1191 - val_rmse: 20216.0430\n",
      "Epoch 73/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17103.3691 - rmse: 17103.3691\n",
      "Epoch 73: val_loss did not improve from 17904.47656\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16807.4980 - rmse: 16803.8242 - val_loss: 21444.8164 - val_rmse: 21282.9141\n",
      "Epoch 74/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16675.7695 - rmse: 16675.7695\n",
      "Epoch 74: val_loss did not improve from 17904.47656\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16564.5566 - rmse: 16543.5391 - val_loss: 18313.9160 - val_rmse: 18219.0156\n",
      "Epoch 75/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16609.8262 - rmse: 16605.0645\n",
      "Epoch 75: val_loss did not improve from 17904.47656\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16609.8262 - rmse: 16605.0645 - val_loss: 22481.9355 - val_rmse: 22332.0566\n",
      "Epoch 76/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17527.3301 - rmse: 17527.3301\n",
      "Epoch 76: val_loss did not improve from 17904.47656\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 17477.3730 - rmse: 17515.7070 - val_loss: 19216.9531 - val_rmse: 19098.5410\n",
      "Epoch 77/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16627.8457 - rmse: 16627.8457\n",
      "Epoch 77: val_loss improved from 17904.47656 to 17678.31250, saving model to ./ckpt/reg_lr005/val_rmse_17587.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16661.7207 - rmse: 16650.9590 - val_loss: 17678.3125 - val_rmse: 17587.3164\n",
      "Epoch 78/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17556.0664 - rmse: 17556.0664\n",
      "Epoch 78: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17540.2832 - rmse: 17552.5332 - val_loss: 18761.6445 - val_rmse: 18704.7012\n",
      "Epoch 79/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16797.6797 - rmse: 16797.6797\n",
      "Epoch 79: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16893.5977 - rmse: 16912.6367 - val_loss: 22360.8008 - val_rmse: 22213.1035\n",
      "Epoch 80/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16781.3262 - rmse: 16781.3262\n",
      "Epoch 80: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16852.9395 - rmse: 16841.0664 - val_loss: 19064.8730 - val_rmse: 18912.9863\n",
      "Epoch 81/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17164.6113 - rmse: 17164.6113\n",
      "Epoch 81: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17137.6719 - rmse: 17140.6523 - val_loss: 19367.9453 - val_rmse: 19265.5098\n",
      "Epoch 82/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15586.3867 - rmse: 15586.3867\n",
      "Epoch 82: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15860.9277 - rmse: 16009.2266 - val_loss: 17743.8086 - val_rmse: 17645.9883\n",
      "Epoch 83/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16273.3975 - rmse: 16273.3975\n",
      "Epoch 83: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16323.7383 - rmse: 16335.0537 - val_loss: 23426.0078 - val_rmse: 23249.5469\n",
      "Epoch 84/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15821.7246 - rmse: 15821.7246\n",
      "Epoch 84: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15848.3760 - rmse: 15900.0879 - val_loss: 20982.0176 - val_rmse: 20800.4082\n",
      "Epoch 85/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16155.5020 - rmse: 16155.5020\n",
      "Epoch 85: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16324.4336 - rmse: 16390.7441 - val_loss: 18241.9629 - val_rmse: 18078.2520\n",
      "Epoch 86/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17161.4160 - rmse: 17161.4160\n",
      "Epoch 86: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17091.6094 - rmse: 17074.5801 - val_loss: 18775.8145 - val_rmse: 18606.5645\n",
      "Epoch 87/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16413.4395 - rmse: 16413.4395\n",
      "Epoch 87: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16427.7949 - rmse: 16437.4785 - val_loss: 18534.5020 - val_rmse: 18396.2578\n",
      "Epoch 88/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16783.3359 - rmse: 16783.3359\n",
      "Epoch 88: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16866.7031 - rmse: 16882.8750 - val_loss: 26215.5508 - val_rmse: 26044.8398\n",
      "Epoch 89/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16281.7793 - rmse: 16281.7793\n",
      "Epoch 89: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16175.2188 - rmse: 16162.4854 - val_loss: 19356.4512 - val_rmse: 19200.9316\n",
      "Epoch 90/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16860.0469 - rmse: 16860.0469\n",
      "Epoch 90: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16903.5156 - rmse: 16949.2773 - val_loss: 21602.4141 - val_rmse: 21419.3359\n",
      "Epoch 91/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15512.1172 - rmse: 15512.1172\n",
      "Epoch 91: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16101.4443 - rmse: 16174.2021 - val_loss: 33755.1602 - val_rmse: 33571.0625\n",
      "Epoch 92/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17529.0625 - rmse: 17529.0625\n",
      "Epoch 92: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17361.1191 - rmse: 17333.9297 - val_loss: 21721.6953 - val_rmse: 21564.2441\n",
      "Epoch 93/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15344.8838 - rmse: 15344.8838\n",
      "Epoch 93: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15372.5928 - rmse: 15375.0059 - val_loss: 20506.7012 - val_rmse: 20346.8379\n",
      "Epoch 94/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16144.2549 - rmse: 16144.2549\n",
      "Epoch 94: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16078.8096 - rmse: 16066.4980 - val_loss: 19924.3770 - val_rmse: 19762.6523\n",
      "Epoch 95/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16264.9756 - rmse: 16264.9756\n",
      "Epoch 95: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16257.7090 - rmse: 16257.7070 - val_loss: 19427.4961 - val_rmse: 19280.4766\n",
      "Epoch 96/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15286.3789 - rmse: 15286.3789\n",
      "Epoch 96: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15287.6699 - rmse: 15302.1357 - val_loss: 22870.0879 - val_rmse: 22774.9922\n",
      "Epoch 97/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15432.6484 - rmse: 15432.6484\n",
      "Epoch 97: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15599.8779 - rmse: 15600.0664 - val_loss: 20573.8945 - val_rmse: 20404.8613\n",
      "Epoch 98/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15675.1953 - rmse: 15675.1953\n",
      "Epoch 98: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15648.0918 - rmse: 15636.8574 - val_loss: 23704.6270 - val_rmse: 23528.7500\n",
      "Epoch 99/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16476.6270 - rmse: 16476.6270\n",
      "Epoch 99: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16456.8770 - rmse: 16440.5820 - val_loss: 19655.1348 - val_rmse: 19525.8125\n",
      "Epoch 100/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15306.6318 - rmse: 15306.6318\n",
      "Epoch 100: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15308.2373 - rmse: 15280.6768 - val_loss: 17806.2070 - val_rmse: 17683.3711\n",
      "Epoch 101/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 14309.7139 - rmse: 14309.7139\n",
      "Epoch 101: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14380.3945 - rmse: 14412.8701 - val_loss: 18553.4531 - val_rmse: 18437.3105\n",
      "Epoch 102/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14777.3760 - rmse: 14776.8994\n",
      "Epoch 102: val_loss did not improve from 17678.31250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14777.3760 - rmse: 14776.8994 - val_loss: 18771.5918 - val_rmse: 18646.1855\n",
      "Epoch 102: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 29s - loss: 190162.1562 - rmse: 190162.1562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:12:58.185624: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 107500.3125 - rmse: 107197.8828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:12:59.258642: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 74193.12500, saving model to ./ckpt/reg_lr005/val_rmse_73072.hdf5\n",
      "70/70 [==============================] - 2s 17ms/step - loss: 107500.3125 - rmse: 107197.8828 - val_loss: 74193.1250 - val_rmse: 73071.8125\n",
      "Epoch 2/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 53182.1992 - rmse: 53182.1992\n",
      "Epoch 2: val_loss improved from 74193.12500 to 60492.51562, saving model to ./ckpt/reg_lr005/val_rmse_59572.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 53263.8516 - rmse: 53281.5977 - val_loss: 60492.5156 - val_rmse: 59572.1719\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 45521.8984 - rmse: 45521.8984\n",
      "Epoch 3: val_loss improved from 60492.51562 to 56642.93359, saving model to ./ckpt/reg_lr005/val_rmse_55808.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 45122.6953 - rmse: 45002.2383 - val_loss: 56642.9336 - val_rmse: 55808.3359\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 43433.4570 - rmse: 43433.4570\n",
      "Epoch 4: val_loss improved from 56642.93359 to 55295.08203, saving model to ./ckpt/reg_lr005/val_rmse_54837.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 43396.5977 - rmse: 43394.7305 - val_loss: 55295.0820 - val_rmse: 54836.9180\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 41188.2383 - rmse: 41180.7773\n",
      "Epoch 5: val_loss improved from 55295.08203 to 54844.42578, saving model to ./ckpt/reg_lr005/val_rmse_53999.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 41188.2383 - rmse: 41180.7773 - val_loss: 54844.4258 - val_rmse: 53998.9219\n",
      "Epoch 6/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 39453.4805 - rmse: 39453.4805\n",
      "Epoch 6: val_loss improved from 54844.42578 to 49557.36328, saving model to ./ckpt/reg_lr005/val_rmse_49004.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 39574.0000 - rmse: 39629.1992 - val_loss: 49557.3633 - val_rmse: 49003.5820\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 39663.1680 - rmse: 39663.1680\n",
      "Epoch 7: val_loss improved from 49557.36328 to 48257.76562, saving model to ./ckpt/reg_lr005/val_rmse_47477.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 39203.0117 - rmse: 39120.8633 - val_loss: 48257.7656 - val_rmse: 47477.4258\n",
      "Epoch 8/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 38285.0664 - rmse: 38285.0664\n",
      "Epoch 8: val_loss did not improve from 48257.76562\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 38526.7500 - rmse: 38687.2500 - val_loss: 54212.8047 - val_rmse: 53353.8047\n",
      "Epoch 9/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 35922.6211 - rmse: 35922.6211\n",
      "Epoch 9: val_loss improved from 48257.76562 to 44314.34766, saving model to ./ckpt/reg_lr005/val_rmse_43668.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 36103.8164 - rmse: 36124.5312 - val_loss: 44314.3477 - val_rmse: 43668.1445\n",
      "Epoch 10/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 36154.8086 - rmse: 36154.8086\n",
      "Epoch 10: val_loss did not improve from 44314.34766\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 36500.2344 - rmse: 36512.8320 - val_loss: 45802.2578 - val_rmse: 45424.4883\n",
      "Epoch 11/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 33716.2227 - rmse: 33716.2227\n",
      "Epoch 11: val_loss did not improve from 44314.34766\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 33740.9648 - rmse: 33686.0625 - val_loss: 47156.6172 - val_rmse: 46354.3516\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 33293.2344 - rmse: 33293.2344\n",
      "Epoch 12: val_loss improved from 44314.34766 to 40519.24609, saving model to ./ckpt/reg_lr005/val_rmse_39854.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 33270.8945 - rmse: 33257.6797 - val_loss: 40519.2461 - val_rmse: 39854.1914\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 32576.8711 - rmse: 32576.8711\n",
      "Epoch 13: val_loss did not improve from 40519.24609\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 32576.4434 - rmse: 32576.1895 - val_loss: 40854.4258 - val_rmse: 40169.0391\n",
      "Epoch 14/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 29746.9160 - rmse: 29746.9160\n",
      "Epoch 14: val_loss improved from 40519.24609 to 35887.93750, saving model to ./ckpt/reg_lr005/val_rmse_35455.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 29821.8613 - rmse: 29808.6230 - val_loss: 35887.9375 - val_rmse: 35455.1680\n",
      "Epoch 15/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 29803.0117 - rmse: 29803.0117\n",
      "Epoch 15: val_loss improved from 35887.93750 to 34679.96875, saving model to ./ckpt/reg_lr005/val_rmse_34190.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 29675.9844 - rmse: 29642.4648 - val_loss: 34679.9688 - val_rmse: 34190.3203\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 28078.2812 - rmse: 28078.2812\n",
      "Epoch 16: val_loss improved from 34679.96875 to 33007.87891, saving model to ./ckpt/reg_lr005/val_rmse_32603.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27756.8906 - rmse: 27726.9414 - val_loss: 33007.8789 - val_rmse: 32603.0410\n",
      "Epoch 17/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 26122.8281 - rmse: 26122.8281\n",
      "Epoch 17: val_loss improved from 33007.87891 to 31577.60938, saving model to ./ckpt/reg_lr005/val_rmse_31217.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26006.1641 - rmse: 25958.1172 - val_loss: 31577.6094 - val_rmse: 31217.1621\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27172.7695 - rmse: 27259.3906\n",
      "Epoch 18: val_loss did not improve from 31577.60938\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 27172.7695 - rmse: 27259.3906 - val_loss: 33049.8945 - val_rmse: 32828.6094\n",
      "Epoch 19/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 25372.2441 - rmse: 25372.2441\n",
      "Epoch 19: val_loss improved from 31577.60938 to 30016.77148, saving model to ./ckpt/reg_lr005/val_rmse_29733.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25460.0215 - rmse: 25485.8672 - val_loss: 30016.7715 - val_rmse: 29733.2051\n",
      "Epoch 20/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 25191.6367 - rmse: 25191.6367\n",
      "Epoch 20: val_loss did not improve from 30016.77148\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 25387.2480 - rmse: 25368.2422 - val_loss: 31515.2852 - val_rmse: 31059.9922\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27158.3652 - rmse: 27158.3652\n",
      "Epoch 21: val_loss did not improve from 30016.77148\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27518.8965 - rmse: 27549.2344 - val_loss: 31941.9570 - val_rmse: 31519.7461\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 26341.4551 - rmse: 26345.0000\n",
      "Epoch 22: val_loss improved from 30016.77148 to 28471.38281, saving model to ./ckpt/reg_lr005/val_rmse_28139.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 26341.4551 - rmse: 26345.0000 - val_loss: 28471.3828 - val_rmse: 28138.8867\n",
      "Epoch 23/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24039.9004 - rmse: 24039.9004\n",
      "Epoch 23: val_loss did not improve from 28471.38281\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 24049.2109 - rmse: 24060.9160 - val_loss: 29517.7871 - val_rmse: 29238.9609\n",
      "Epoch 24/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23153.7930 - rmse: 23174.6152\n",
      "Epoch 24: val_loss improved from 28471.38281 to 27899.49414, saving model to ./ckpt/reg_lr005/val_rmse_27668.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23153.7930 - rmse: 23174.6152 - val_loss: 27899.4941 - val_rmse: 27667.8770\n",
      "Epoch 25/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 23790.8027 - rmse: 23790.8027\n",
      "Epoch 25: val_loss did not improve from 27899.49414\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23815.7637 - rmse: 23792.1875 - val_loss: 29220.0039 - val_rmse: 28774.2539\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23199.0352 - rmse: 23199.0352\n",
      "Epoch 26: val_loss improved from 27899.49414 to 27179.30664, saving model to ./ckpt/reg_lr005/val_rmse_26856.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23048.9395 - rmse: 23066.9902 - val_loss: 27179.3066 - val_rmse: 26855.5508\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22317.6738 - rmse: 22317.6738\n",
      "Epoch 27: val_loss improved from 27179.30664 to 26279.48633, saving model to ./ckpt/reg_lr005/val_rmse_25962.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22338.8867 - rmse: 22343.5332 - val_loss: 26279.4863 - val_rmse: 25961.9414\n",
      "Epoch 28/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 21474.5762 - rmse: 21474.5762\n",
      "Epoch 28: val_loss did not improve from 26279.48633\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22191.5508 - rmse: 22258.8672 - val_loss: 36898.7148 - val_rmse: 36449.2969\n",
      "Epoch 29/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23018.9941 - rmse: 23018.9941\n",
      "Epoch 29: val_loss did not improve from 26279.48633\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22858.8184 - rmse: 22894.4941 - val_loss: 27186.0625 - val_rmse: 26780.8496\n",
      "Epoch 30/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21150.7520 - rmse: 21208.7305\n",
      "Epoch 30: val_loss improved from 26279.48633 to 25644.21484, saving model to ./ckpt/reg_lr005/val_rmse_25420.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21150.7520 - rmse: 21208.7305 - val_loss: 25644.2148 - val_rmse: 25420.4609\n",
      "Epoch 31/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22557.6816 - rmse: 22557.6816\n",
      "Epoch 31: val_loss improved from 25644.21484 to 25132.38086, saving model to ./ckpt/reg_lr005/val_rmse_24780.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22486.1211 - rmse: 22499.0938 - val_loss: 25132.3809 - val_rmse: 24780.0859\n",
      "Epoch 32/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21128.6582 - rmse: 21128.6582\n",
      "Epoch 32: val_loss improved from 25132.38086 to 24288.35156, saving model to ./ckpt/reg_lr005/val_rmse_24128.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21067.9824 - rmse: 21089.3301 - val_loss: 24288.3516 - val_rmse: 24127.6328\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20560.4824 - rmse: 20550.5078\n",
      "Epoch 33: val_loss did not improve from 24288.35156\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20560.4824 - rmse: 20550.5078 - val_loss: 24601.2969 - val_rmse: 24274.2988\n",
      "Epoch 34/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21664.1934 - rmse: 21664.1934\n",
      "Epoch 34: val_loss did not improve from 24288.35156\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21631.2324 - rmse: 21611.7383 - val_loss: 24335.8965 - val_rmse: 24010.1387\n",
      "Epoch 35/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22063.6172 - rmse: 22095.0293\n",
      "Epoch 35: val_loss did not improve from 24288.35156\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22063.6172 - rmse: 22095.0293 - val_loss: 29103.4980 - val_rmse: 28731.0000\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20415.3496 - rmse: 20440.5879\n",
      "Epoch 36: val_loss improved from 24288.35156 to 22491.44336, saving model to ./ckpt/reg_lr005/val_rmse_22400.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20415.3496 - rmse: 20440.5879 - val_loss: 22491.4434 - val_rmse: 22399.7207\n",
      "Epoch 37/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21804.4336 - rmse: 21804.4336\n",
      "Epoch 37: val_loss did not improve from 22491.44336\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21556.6406 - rmse: 21549.7148 - val_loss: 25791.6133 - val_rmse: 25516.1660\n",
      "Epoch 38/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20438.8496 - rmse: 20438.8496\n",
      "Epoch 38: val_loss did not improve from 22491.44336\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20307.6602 - rmse: 20277.6348 - val_loss: 23281.8555 - val_rmse: 23038.5801\n",
      "Epoch 39/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20690.3867 - rmse: 20690.3867\n",
      "Epoch 39: val_loss did not improve from 22491.44336\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20555.2637 - rmse: 20540.8867 - val_loss: 24348.9512 - val_rmse: 24031.8145\n",
      "Epoch 40/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19381.5664 - rmse: 19381.5664\n",
      "Epoch 40: val_loss improved from 22491.44336 to 22005.53906, saving model to ./ckpt/reg_lr005/val_rmse_21854.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19460.3906 - rmse: 19459.5117 - val_loss: 22005.5391 - val_rmse: 21853.6523\n",
      "Epoch 41/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18850.1348 - rmse: 18850.1348\n",
      "Epoch 41: val_loss did not improve from 22005.53906\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19007.6016 - rmse: 19100.7324 - val_loss: 39921.6328 - val_rmse: 39467.1641\n",
      "Epoch 42/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19293.2988 - rmse: 19293.2988\n",
      "Epoch 42: val_loss did not improve from 22005.53906\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19468.5176 - rmse: 19572.1465 - val_loss: 25442.9297 - val_rmse: 25131.5605\n",
      "Epoch 43/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21275.0430 - rmse: 21243.1836\n",
      "Epoch 43: val_loss did not improve from 22005.53906\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21275.0430 - rmse: 21243.1836 - val_loss: 24729.8633 - val_rmse: 24702.4805\n",
      "Epoch 44/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20863.9746 - rmse: 20863.9746\n",
      "Epoch 44: val_loss did not improve from 22005.53906\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20804.4980 - rmse: 20820.4004 - val_loss: 25528.4043 - val_rmse: 25513.5254\n",
      "Epoch 45/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19860.0078 - rmse: 19900.4688\n",
      "Epoch 45: val_loss did not improve from 22005.53906\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19860.0078 - rmse: 19900.4688 - val_loss: 28192.8633 - val_rmse: 27802.1074\n",
      "Epoch 46/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20011.4941 - rmse: 20011.4941\n",
      "Epoch 46: val_loss did not improve from 22005.53906\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19864.1738 - rmse: 19839.0098 - val_loss: 24110.3262 - val_rmse: 23802.4863\n",
      "Epoch 47/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19795.7539 - rmse: 19795.7539\n",
      "Epoch 47: val_loss did not improve from 22005.53906\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19837.4590 - rmse: 19869.3242 - val_loss: 28766.8945 - val_rmse: 28392.5723\n",
      "Epoch 48/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18122.2344 - rmse: 18132.5820\n",
      "Epoch 48: val_loss did not improve from 22005.53906\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18122.2344 - rmse: 18132.5820 - val_loss: 23662.5273 - val_rmse: 23398.3750\n",
      "Epoch 49/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21015.9980 - rmse: 21015.9980\n",
      "Epoch 49: val_loss did not improve from 22005.53906\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21099.6875 - rmse: 21149.1836 - val_loss: 33953.3398 - val_rmse: 33540.1680\n",
      "Epoch 50/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18500.0879 - rmse: 18500.0879\n",
      "Epoch 50: val_loss did not improve from 22005.53906\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18923.0547 - rmse: 18946.6992 - val_loss: 22978.3535 - val_rmse: 22724.9102\n",
      "Epoch 51/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19441.9121 - rmse: 19441.9121\n",
      "Epoch 51: val_loss improved from 22005.53906 to 20737.93164, saving model to ./ckpt/reg_lr005/val_rmse_20598.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19373.1680 - rmse: 19353.9941 - val_loss: 20737.9316 - val_rmse: 20598.3203\n",
      "Epoch 52/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18957.8809 - rmse: 18957.8809\n",
      "Epoch 52: val_loss did not improve from 20737.93164\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19049.1543 - rmse: 19071.9707 - val_loss: 25043.5996 - val_rmse: 24708.3145\n",
      "Epoch 53/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19613.9863 - rmse: 19613.9863\n",
      "Epoch 53: val_loss did not improve from 20737.93164\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19802.5293 - rmse: 19914.0391 - val_loss: 26052.7676 - val_rmse: 25768.0820\n",
      "Epoch 54/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19033.3965 - rmse: 19033.3965\n",
      "Epoch 54: val_loss did not improve from 20737.93164\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18913.2305 - rmse: 18889.4863 - val_loss: 20960.2832 - val_rmse: 20760.4688\n",
      "Epoch 55/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18086.3652 - rmse: 18086.3652\n",
      "Epoch 55: val_loss did not improve from 20737.93164\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18217.0020 - rmse: 18276.8750 - val_loss: 21701.1992 - val_rmse: 21542.1602\n",
      "Epoch 56/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19594.0605 - rmse: 19594.0605\n",
      "Epoch 56: val_loss did not improve from 20737.93164\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19553.9219 - rmse: 19543.7988 - val_loss: 23400.3848 - val_rmse: 23067.8555\n",
      "Epoch 57/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18193.5430 - rmse: 18193.5430\n",
      "Epoch 57: val_loss did not improve from 20737.93164\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18503.3086 - rmse: 18538.5332 - val_loss: 34052.5430 - val_rmse: 33740.2852\n",
      "Epoch 58/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20023.7285 - rmse: 20023.7285\n",
      "Epoch 58: val_loss did not improve from 20737.93164\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19905.3945 - rmse: 19934.2617 - val_loss: 23611.0371 - val_rmse: 23334.8672\n",
      "Epoch 59/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18366.4512 - rmse: 18366.4512\n",
      "Epoch 59: val_loss did not improve from 20737.93164\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18669.5547 - rmse: 18812.6621 - val_loss: 26857.0781 - val_rmse: 26518.5820\n",
      "Epoch 60/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18322.3027 - rmse: 18322.3027\n",
      "Epoch 60: val_loss did not improve from 20737.93164\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18433.7500 - rmse: 18485.5742 - val_loss: 30833.7500 - val_rmse: 30521.3164\n",
      "Epoch 61/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18215.1973 - rmse: 18215.1973\n",
      "Epoch 61: val_loss did not improve from 20737.93164\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18437.6992 - rmse: 18518.1250 - val_loss: 26215.5039 - val_rmse: 26255.6250\n",
      "Epoch 62/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19246.9590 - rmse: 19246.9590\n",
      "Epoch 62: val_loss did not improve from 20737.93164\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19551.2930 - rmse: 19595.1465 - val_loss: 33562.6797 - val_rmse: 33203.1445\n",
      "Epoch 63/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18305.4355 - rmse: 18303.2207\n",
      "Epoch 63: val_loss improved from 20737.93164 to 19720.89453, saving model to ./ckpt/reg_lr005/val_rmse_19631.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18305.4355 - rmse: 18303.2207 - val_loss: 19720.8945 - val_rmse: 19631.1934\n",
      "Epoch 64/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18782.8203 - rmse: 18782.8203\n",
      "Epoch 64: val_loss did not improve from 19720.89453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18880.3926 - rmse: 18959.2070 - val_loss: 19897.5684 - val_rmse: 19740.4727\n",
      "Epoch 65/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18405.8203 - rmse: 18405.8203\n",
      "Epoch 65: val_loss did not improve from 19720.89453\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18339.2246 - rmse: 18299.8379 - val_loss: 20226.1914 - val_rmse: 20051.5938\n",
      "Epoch 66/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16978.7441 - rmse: 16978.7441\n",
      "Epoch 66: val_loss did not improve from 19720.89453\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17116.2598 - rmse: 17134.8535 - val_loss: 22674.0996 - val_rmse: 22426.6992\n",
      "Epoch 67/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17350.3770 - rmse: 17350.3770\n",
      "Epoch 67: val_loss did not improve from 19720.89453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17479.9082 - rmse: 17529.3516 - val_loss: 25477.6309 - val_rmse: 25211.3086\n",
      "Epoch 68/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18316.8535 - rmse: 18316.8535\n",
      "Epoch 68: val_loss did not improve from 19720.89453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18271.3047 - rmse: 18265.1445 - val_loss: 20794.6973 - val_rmse: 20612.3223\n",
      "Epoch 69/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17782.2461 - rmse: 17782.2461\n",
      "Epoch 69: val_loss did not improve from 19720.89453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17857.3945 - rmse: 17857.5996 - val_loss: 23026.7324 - val_rmse: 22800.6953\n",
      "Epoch 70/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17534.5469 - rmse: 17534.5469\n",
      "Epoch 70: val_loss did not improve from 19720.89453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17811.2383 - rmse: 17854.7402 - val_loss: 31131.8359 - val_rmse: 30780.1270\n",
      "Epoch 71/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18441.9746 - rmse: 18441.9746\n",
      "Epoch 71: val_loss did not improve from 19720.89453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18437.6387 - rmse: 18433.8242 - val_loss: 20558.7988 - val_rmse: 20467.1074\n",
      "Epoch 72/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19402.5566 - rmse: 19402.5566\n",
      "Epoch 72: val_loss did not improve from 19720.89453\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19301.6875 - rmse: 19298.3457 - val_loss: 21471.2617 - val_rmse: 21247.9316\n",
      "Epoch 73/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17668.5254 - rmse: 17668.5254\n",
      "Epoch 73: val_loss improved from 19720.89453 to 18963.74609, saving model to ./ckpt/reg_lr005/val_rmse_18887.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17591.7793 - rmse: 17605.4414 - val_loss: 18963.7461 - val_rmse: 18887.4121\n",
      "Epoch 74/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20906.7656 - rmse: 20906.7656\n",
      "Epoch 74: val_loss did not improve from 18963.74609\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20704.2500 - rmse: 20689.5312 - val_loss: 20904.1602 - val_rmse: 20714.8086\n",
      "Epoch 75/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17859.4609 - rmse: 17873.4746\n",
      "Epoch 75: val_loss did not improve from 18963.74609\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17859.4609 - rmse: 17873.4746 - val_loss: 20989.6211 - val_rmse: 20939.7500\n",
      "Epoch 76/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17602.5137 - rmse: 17602.5137\n",
      "Epoch 76: val_loss did not improve from 18963.74609\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17671.5000 - rmse: 17685.3203 - val_loss: 24904.6523 - val_rmse: 24632.6699\n",
      "Epoch 77/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17315.7578 - rmse: 17315.7578\n",
      "Epoch 77: val_loss did not improve from 18963.74609\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17208.4961 - rmse: 17246.6348 - val_loss: 29825.2031 - val_rmse: 29478.5156\n",
      "Epoch 78/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16273.9160 - rmse: 16273.9160\n",
      "Epoch 78: val_loss did not improve from 18963.74609\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16298.3184 - rmse: 16302.9980 - val_loss: 20939.3359 - val_rmse: 20746.1523\n",
      "Epoch 79/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17848.3574 - rmse: 17848.3574\n",
      "Epoch 79: val_loss did not improve from 18963.74609\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17819.4160 - rmse: 17811.2344 - val_loss: 22882.1309 - val_rmse: 22593.5020\n",
      "Epoch 80/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17557.4316 - rmse: 17557.4316\n",
      "Epoch 80: val_loss did not improve from 18963.74609\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17423.3750 - rmse: 17415.5938 - val_loss: 27193.0664 - val_rmse: 26912.0566\n",
      "Epoch 81/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17301.3613 - rmse: 17301.3613\n",
      "Epoch 81: val_loss did not improve from 18963.74609\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17482.1211 - rmse: 17542.6934 - val_loss: 31575.5859 - val_rmse: 31258.3438\n",
      "Epoch 82/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16288.3340 - rmse: 16288.3340\n",
      "Epoch 82: val_loss did not improve from 18963.74609\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16301.0938 - rmse: 16308.6396 - val_loss: 23744.5996 - val_rmse: 23474.7988\n",
      "Epoch 83/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15806.3203 - rmse: 15806.3203\n",
      "Epoch 83: val_loss did not improve from 18963.74609\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15925.0684 - rmse: 15941.5430 - val_loss: 24341.0215 - val_rmse: 24078.7422\n",
      "Epoch 84/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16439.8320 - rmse: 16439.8320\n",
      "Epoch 84: val_loss did not improve from 18963.74609\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16267.9336 - rmse: 16234.1572 - val_loss: 20285.2734 - val_rmse: 20082.4805\n",
      "Epoch 85/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15778.7422 - rmse: 15778.7422\n",
      "Epoch 85: val_loss did not improve from 18963.74609\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15829.3867 - rmse: 15842.6611 - val_loss: 21030.8945 - val_rmse: 20802.9004\n",
      "Epoch 86/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16618.3594 - rmse: 16618.3594\n",
      "Epoch 86: val_loss improved from 18963.74609 to 18617.71875, saving model to ./ckpt/reg_lr005/val_rmse_18477.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16628.4023 - rmse: 16630.6719 - val_loss: 18617.7188 - val_rmse: 18476.7344\n",
      "Epoch 87/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18356.0918 - rmse: 18356.0918\n",
      "Epoch 87: val_loss did not improve from 18617.71875\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18224.2148 - rmse: 18192.2812 - val_loss: 21834.5820 - val_rmse: 21564.8633\n",
      "Epoch 88/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17476.2520 - rmse: 17498.9473\n",
      "Epoch 88: val_loss did not improve from 18617.71875\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17476.2520 - rmse: 17498.9473 - val_loss: 25674.3496 - val_rmse: 25370.7480\n",
      "Epoch 89/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17475.5527 - rmse: 17475.5527\n",
      "Epoch 89: val_loss did not improve from 18617.71875\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 17466.9727 - rmse: 17497.9160 - val_loss: 27011.0879 - val_rmse: 26699.3906\n",
      "Epoch 90/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16837.9668 - rmse: 16837.9668\n",
      "Epoch 90: val_loss did not improve from 18617.71875\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16956.1934 - rmse: 16957.9062 - val_loss: 20584.2383 - val_rmse: 20488.3457\n",
      "Epoch 91/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16507.6035 - rmse: 16507.6035\n",
      "Epoch 91: val_loss did not improve from 18617.71875\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16619.7559 - rmse: 16631.8203 - val_loss: 27226.6055 - val_rmse: 26931.8223\n",
      "Epoch 92/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17047.5977 - rmse: 17047.5977\n",
      "Epoch 92: val_loss improved from 18617.71875 to 18345.21484, saving model to ./ckpt/reg_lr005/val_rmse_18376.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17128.0840 - rmse: 17138.8027 - val_loss: 18345.2148 - val_rmse: 18375.7383\n",
      "Epoch 93/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17390.8105 - rmse: 17390.8105\n",
      "Epoch 93: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17542.4844 - rmse: 17600.4121 - val_loss: 21866.3555 - val_rmse: 21602.3438\n",
      "Epoch 94/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16984.6191 - rmse: 16984.6191\n",
      "Epoch 94: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16916.3008 - rmse: 16875.8945 - val_loss: 24295.9473 - val_rmse: 24077.3281\n",
      "Epoch 95/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17492.0117 - rmse: 17492.0117\n",
      "Epoch 95: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 17311.1934 - rmse: 17325.2148 - val_loss: 24991.2695 - val_rmse: 24694.9023\n",
      "Epoch 96/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16657.2793 - rmse: 16657.2793\n",
      "Epoch 96: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16588.1309 - rmse: 16572.1699 - val_loss: 21498.0254 - val_rmse: 21210.4395\n",
      "Epoch 97/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15607.2920 - rmse: 15607.2920\n",
      "Epoch 97: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15702.4551 - rmse: 15759.7783 - val_loss: 22757.3301 - val_rmse: 22515.7070\n",
      "Epoch 98/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16783.6250 - rmse: 16783.6250\n",
      "Epoch 98: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16866.7500 - rmse: 16915.9121 - val_loss: 18388.2500 - val_rmse: 18308.9512\n",
      "Epoch 99/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16035.9248 - rmse: 16091.6943\n",
      "Epoch 99: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16035.9248 - rmse: 16091.6943 - val_loss: 21646.8770 - val_rmse: 21670.5020\n",
      "Epoch 100/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18404.4141 - rmse: 18404.4141\n",
      "Epoch 100: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18505.8281 - rmse: 18478.7441 - val_loss: 27086.9805 - val_rmse: 26752.9355\n",
      "Epoch 101/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15972.1807 - rmse: 15972.1807\n",
      "Epoch 101: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15859.0127 - rmse: 15830.7520 - val_loss: 27134.8613 - val_rmse: 26774.2285\n",
      "Epoch 102/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17421.2676 - rmse: 17433.3789\n",
      "Epoch 102: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17421.2676 - rmse: 17433.3789 - val_loss: 19236.7285 - val_rmse: 19117.1250\n",
      "Epoch 103/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15980.7471 - rmse: 15980.7471\n",
      "Epoch 103: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16029.4688 - rmse: 16058.2842 - val_loss: 19675.8262 - val_rmse: 19505.6602\n",
      "Epoch 104/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16420.6367 - rmse: 16420.6367\n",
      "Epoch 104: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16477.7891 - rmse: 16499.1426 - val_loss: 30437.0312 - val_rmse: 30078.4277\n",
      "Epoch 105/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15143.3965 - rmse: 15143.3965\n",
      "Epoch 105: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15120.8545 - rmse: 15143.4121 - val_loss: 21393.6387 - val_rmse: 21150.8789\n",
      "Epoch 106/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15538.5547 - rmse: 15538.5547\n",
      "Epoch 106: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15551.3955 - rmse: 15558.9893 - val_loss: 23392.6562 - val_rmse: 23073.7734\n",
      "Epoch 107/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15953.0225 - rmse: 15953.0225\n",
      "Epoch 107: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15933.6826 - rmse: 15922.2451 - val_loss: 24278.3652 - val_rmse: 23978.7227\n",
      "Epoch 108/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15660.3252 - rmse: 15660.3252\n",
      "Epoch 108: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15672.3809 - rmse: 15667.0576 - val_loss: 21573.7656 - val_rmse: 21275.1836\n",
      "Epoch 109/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15647.0732 - rmse: 15647.0732\n",
      "Epoch 109: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15576.9658 - rmse: 15568.7393 - val_loss: 21044.5215 - val_rmse: 20783.3398\n",
      "Epoch 110/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15721.0332 - rmse: 15721.0332\n",
      "Epoch 110: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15840.6914 - rmse: 15882.2783 - val_loss: 27563.0059 - val_rmse: 27157.3594\n",
      "Epoch 111/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 14939.1670 - rmse: 14939.1670\n",
      "Epoch 111: val_loss did not improve from 18345.21484\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15016.1055 - rmse: 15059.6289 - val_loss: 23209.7969 - val_rmse: 22885.6250\n",
      "Epoch 112/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15611.1475 - rmse: 15611.1475\n",
      "Epoch 112: val_loss improved from 18345.21484 to 18099.68750, saving model to ./ckpt/reg_lr005/val_rmse_17998.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15546.4512 - rmse: 15518.0605 - val_loss: 18099.6875 - val_rmse: 17998.3008\n",
      "Epoch 113/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15587.6650 - rmse: 15587.6650\n",
      "Epoch 113: val_loss did not improve from 18099.68750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15616.4297 - rmse: 15633.4414 - val_loss: 25522.5566 - val_rmse: 25173.2188\n",
      "Epoch 114/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 14953.5029 - rmse: 14953.5029\n",
      "Epoch 114: val_loss did not improve from 18099.68750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14918.8877 - rmse: 14935.0371 - val_loss: 26539.4219 - val_rmse: 26188.3281\n",
      "Epoch 115/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15504.6689 - rmse: 15504.6689\n",
      "Epoch 115: val_loss did not improve from 18099.68750\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 15595.9180 - rmse: 15614.1719 - val_loss: 19949.9414 - val_rmse: 19728.7812\n",
      "Epoch 116/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15150.1963 - rmse: 15150.1963\n",
      "Epoch 116: val_loss did not improve from 18099.68750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15251.6934 - rmse: 15260.6377 - val_loss: 29742.5605 - val_rmse: 29319.1641\n",
      "Epoch 117/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15387.2529 - rmse: 15387.2529\n",
      "Epoch 117: val_loss did not improve from 18099.68750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15416.9160 - rmse: 15411.1680 - val_loss: 25760.9648 - val_rmse: 25382.2168\n",
      "Epoch 118/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 14542.2227 - rmse: 14542.2227\n",
      "Epoch 118: val_loss did not improve from 18099.68750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14527.0625 - rmse: 14521.0068 - val_loss: 19772.4473 - val_rmse: 19535.7500\n",
      "Epoch 119/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 14574.8545 - rmse: 14574.8545\n",
      "Epoch 119: val_loss did not improve from 18099.68750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14565.9629 - rmse: 14578.9355 - val_loss: 25342.9160 - val_rmse: 24998.7832\n",
      "Epoch 120/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 14301.2158 - rmse: 14301.2158\n",
      "Epoch 120: val_loss did not improve from 18099.68750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14372.1436 - rmse: 14370.1797 - val_loss: 19709.0332 - val_rmse: 19506.0547\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 52s - loss: 206728.2031 - rmse: 206728.2031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:14:20.368933: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 100476.8906 - rmse: 100196.6562\n",
      "Epoch 1: val_loss improved from inf to 57452.93359, saving model to ./ckpt/reg_lr005/val_rmse_58129.hdf5\n",
      "70/70 [==============================] - 2s 12ms/step - loss: 100476.8906 - rmse: 100196.6562 - val_loss: 57452.9336 - val_rmse: 58129.3672\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 70315.7969 - rmse: 70315.7969"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:14:21.277606: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 49993.8594 - rmse: 49993.8594\n",
      "Epoch 2: val_loss improved from 57452.93359 to 46512.82812, saving model to ./ckpt/reg_lr005/val_rmse_47003.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 50199.2383 - rmse: 50182.3750 - val_loss: 46512.8281 - val_rmse: 47003.2422\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 44612.4883 - rmse: 44612.4883\n",
      "Epoch 3: val_loss improved from 46512.82812 to 43387.23047, saving model to ./ckpt/reg_lr005/val_rmse_43720.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 44794.0781 - rmse: 44764.2422 - val_loss: 43387.2305 - val_rmse: 43719.7188\n",
      "Epoch 4/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 41734.6836 - rmse: 41734.6836\n",
      "Epoch 4: val_loss did not improve from 43387.23047\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 42054.6602 - rmse: 42077.2734 - val_loss: 53919.9141 - val_rmse: 54121.0586\n",
      "Epoch 5/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 44854.6250 - rmse: 44854.6250\n",
      "Epoch 5: val_loss improved from 43387.23047 to 41194.17969, saving model to ./ckpt/reg_lr005/val_rmse_41403.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 44837.3398 - rmse: 44824.4258 - val_loss: 41194.1797 - val_rmse: 41402.6562\n",
      "Epoch 6/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 40733.3789 - rmse: 40733.3789\n",
      "Epoch 6: val_loss improved from 41194.17969 to 38568.17188, saving model to ./ckpt/reg_lr005/val_rmse_38764.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 41245.7695 - rmse: 41246.5781 - val_loss: 38568.1719 - val_rmse: 38763.7461\n",
      "Epoch 7/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 38472.4453 - rmse: 38472.4453\n",
      "Epoch 7: val_loss improved from 38568.17188 to 36830.44531, saving model to ./ckpt/reg_lr005/val_rmse_36966.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 38204.5469 - rmse: 38137.7305 - val_loss: 36830.4453 - val_rmse: 36965.5352\n",
      "Epoch 8/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 37913.1367 - rmse: 37913.1367\n",
      "Epoch 8: val_loss improved from 36830.44531 to 36333.90625, saving model to ./ckpt/reg_lr005/val_rmse_36472.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 38554.2305 - rmse: 38890.8711 - val_loss: 36333.9062 - val_rmse: 36472.3281\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 38270.3359 - rmse: 38311.6289\n",
      "Epoch 9: val_loss improved from 36333.90625 to 34440.09766, saving model to ./ckpt/reg_lr005/val_rmse_34506.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 38270.3359 - rmse: 38311.6289 - val_loss: 34440.0977 - val_rmse: 34505.8320\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 35542.3867 - rmse: 35510.2188\n",
      "Epoch 10: val_loss did not improve from 34440.09766\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 35542.3867 - rmse: 35510.2188 - val_loss: 39595.3125 - val_rmse: 39662.9219\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 33296.7852 - rmse: 33296.7852\n",
      "Epoch 11: val_loss improved from 34440.09766 to 32302.72266, saving model to ./ckpt/reg_lr005/val_rmse_32319.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 33224.1133 - rmse: 33181.1289 - val_loss: 32302.7227 - val_rmse: 32318.5723\n",
      "Epoch 12/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 35282.3125 - rmse: 35282.3125\n",
      "Epoch 12: val_loss improved from 32302.72266 to 31627.54297, saving model to ./ckpt/reg_lr005/val_rmse_31646.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 35127.8789 - rmse: 35100.7812 - val_loss: 31627.5430 - val_rmse: 31646.0137\n",
      "Epoch 13/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 31208.8496 - rmse: 31208.8496\n",
      "Epoch 13: val_loss did not improve from 31627.54297\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 31215.9473 - rmse: 31259.1719 - val_loss: 33574.1055 - val_rmse: 33519.6445\n",
      "Epoch 14/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 30926.4141 - rmse: 30926.4141\n",
      "Epoch 14: val_loss improved from 31627.54297 to 28769.91992, saving model to ./ckpt/reg_lr005/val_rmse_28683.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 30993.8652 - rmse: 31033.7578 - val_loss: 28769.9199 - val_rmse: 28683.1445\n",
      "Epoch 15/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 30233.0645 - rmse: 30233.0645\n",
      "Epoch 15: val_loss did not improve from 28769.91992\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 29990.0508 - rmse: 29962.1367 - val_loss: 32603.8262 - val_rmse: 32569.9043\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 29350.6387 - rmse: 29350.6387\n",
      "Epoch 16: val_loss did not improve from 28769.91992\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 29151.2285 - rmse: 29145.3691 - val_loss: 30697.9863 - val_rmse: 30625.4902\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 29655.0586 - rmse: 29655.0586\n",
      "Epoch 17: val_loss did not improve from 28769.91992\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 29603.1367 - rmse: 29572.4297 - val_loss: 31457.5371 - val_rmse: 31395.4590\n",
      "Epoch 18/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 26975.8184 - rmse: 26975.8184\n",
      "Epoch 18: val_loss improved from 28769.91992 to 27330.32617, saving model to ./ckpt/reg_lr005/val_rmse_27287.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26944.0234 - rmse: 26925.2188 - val_loss: 27330.3262 - val_rmse: 27286.6895\n",
      "Epoch 19/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 27041.0625 - rmse: 27041.0625\n",
      "Epoch 19: val_loss improved from 27330.32617 to 26063.23438, saving model to ./ckpt/reg_lr005/val_rmse_25962.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27312.3164 - rmse: 27310.6855 - val_loss: 26063.2344 - val_rmse: 25961.5977\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 26363.3184 - rmse: 26378.3398\n",
      "Epoch 20: val_loss improved from 26063.23438 to 25099.47852, saving model to ./ckpt/reg_lr005/val_rmse_25004.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 26363.3184 - rmse: 26378.3398 - val_loss: 25099.4785 - val_rmse: 25004.4492\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 28650.8809 - rmse: 28679.4375\n",
      "Epoch 21: val_loss did not improve from 25099.47852\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 28650.8809 - rmse: 28679.4375 - val_loss: 25131.7305 - val_rmse: 25022.6230\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 25627.5938 - rmse: 25627.5938\n",
      "Epoch 22: val_loss improved from 25099.47852 to 24178.62500, saving model to ./ckpt/reg_lr005/val_rmse_24032.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25608.1191 - rmse: 25596.5996 - val_loss: 24178.6250 - val_rmse: 24032.0605\n",
      "Epoch 23/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 25972.4844 - rmse: 25972.4844\n",
      "Epoch 23: val_loss did not improve from 24178.62500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25597.3809 - rmse: 25569.2617 - val_loss: 24199.8223 - val_rmse: 24153.9043\n",
      "Epoch 24/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 24707.9492 - rmse: 24707.9492\n",
      "Epoch 24: val_loss improved from 24178.62500 to 23744.48242, saving model to ./ckpt/reg_lr005/val_rmse_23649.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 24579.8633 - rmse: 24563.4648 - val_loss: 23744.4824 - val_rmse: 23648.9453\n",
      "Epoch 25/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25492.6270 - rmse: 25492.6270\n",
      "Epoch 25: val_loss did not improve from 23744.48242\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25368.5703 - rmse: 25374.5371 - val_loss: 25048.3125 - val_rmse: 24851.7891\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24017.1641 - rmse: 24017.1641\n",
      "Epoch 26: val_loss did not improve from 23744.48242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23868.4824 - rmse: 23883.9766 - val_loss: 26767.1738 - val_rmse: 26761.6562\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25743.6895 - rmse: 25743.6895\n",
      "Epoch 27: val_loss improved from 23744.48242 to 22876.03320, saving model to ./ckpt/reg_lr005/val_rmse_22781.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 25561.6211 - rmse: 25504.2559 - val_loss: 22876.0332 - val_rmse: 22781.2754\n",
      "Epoch 28/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23916.5332 - rmse: 23916.5332\n",
      "Epoch 28: val_loss improved from 22876.03320 to 22403.99023, saving model to ./ckpt/reg_lr005/val_rmse_22223.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23902.8652 - rmse: 23898.8613 - val_loss: 22403.9902 - val_rmse: 22222.9844\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22248.0938 - rmse: 22248.0938\n",
      "Epoch 29: val_loss did not improve from 22403.99023\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22261.3027 - rmse: 22269.1152 - val_loss: 23572.5996 - val_rmse: 23475.4902\n",
      "Epoch 30/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22494.4219 - rmse: 22483.9102\n",
      "Epoch 30: val_loss did not improve from 22403.99023\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22494.4219 - rmse: 22483.9102 - val_loss: 32096.8828 - val_rmse: 32083.3359\n",
      "Epoch 31/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21133.0391 - rmse: 21133.0391\n",
      "Epoch 31: val_loss improved from 22403.99023 to 22293.15234, saving model to ./ckpt/reg_lr005/val_rmse_22282.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21149.3145 - rmse: 21158.9395 - val_loss: 22293.1523 - val_rmse: 22282.4805\n",
      "Epoch 32/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20766.7148 - rmse: 20833.9746\n",
      "Epoch 32: val_loss did not improve from 22293.15234\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20766.7148 - rmse: 20833.9746 - val_loss: 28490.2773 - val_rmse: 28356.4473\n",
      "Epoch 33/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20794.3809 - rmse: 20794.3809\n",
      "Epoch 33: val_loss improved from 22293.15234 to 21027.24414, saving model to ./ckpt/reg_lr005/val_rmse_20788.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20752.3184 - rmse: 20739.3965 - val_loss: 21027.2441 - val_rmse: 20787.6914\n",
      "Epoch 34/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20788.1035 - rmse: 20788.1035\n",
      "Epoch 34: val_loss did not improve from 21027.24414\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20789.6074 - rmse: 20765.8809 - val_loss: 24106.8223 - val_rmse: 23908.2676\n",
      "Epoch 35/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22930.9688 - rmse: 22961.9707\n",
      "Epoch 35: val_loss did not improve from 21027.24414\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22930.9688 - rmse: 22961.9707 - val_loss: 26844.4375 - val_rmse: 26881.9219\n",
      "Epoch 36/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 21306.9355 - rmse: 21306.9355\n",
      "Epoch 36: val_loss did not improve from 21027.24414\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21452.7129 - rmse: 21586.7266 - val_loss: 28111.9590 - val_rmse: 28103.5195\n",
      "Epoch 37/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20622.1230 - rmse: 20622.1230\n",
      "Epoch 37: val_loss improved from 21027.24414 to 20338.33789, saving model to ./ckpt/reg_lr005/val_rmse_20241.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20490.0723 - rmse: 20477.2520 - val_loss: 20338.3379 - val_rmse: 20240.8242\n",
      "Epoch 38/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19651.0723 - rmse: 19651.0723\n",
      "Epoch 38: val_loss did not improve from 20338.33789\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19722.5664 - rmse: 19745.8242 - val_loss: 20417.4277 - val_rmse: 20213.2871\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19743.1816 - rmse: 19743.1816\n",
      "Epoch 39: val_loss improved from 20338.33789 to 20088.47656, saving model to ./ckpt/reg_lr005/val_rmse_19927.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19713.4531 - rmse: 19695.8691 - val_loss: 20088.4766 - val_rmse: 19927.2910\n",
      "Epoch 40/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21459.4551 - rmse: 21459.4551\n",
      "Epoch 40: val_loss did not improve from 20088.47656\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21403.0996 - rmse: 21435.2461 - val_loss: 20699.9199 - val_rmse: 20643.0234\n",
      "Epoch 41/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19290.1230 - rmse: 19290.1230\n",
      "Epoch 41: val_loss did not improve from 20088.47656\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19326.6953 - rmse: 19348.3242 - val_loss: 24926.9805 - val_rmse: 24873.4160\n",
      "Epoch 42/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19392.1328 - rmse: 19410.9629\n",
      "Epoch 42: val_loss did not improve from 20088.47656\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19392.1328 - rmse: 19410.9629 - val_loss: 33976.9961 - val_rmse: 34061.2773\n",
      "Epoch 43/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20002.5742 - rmse: 20002.5742\n",
      "Epoch 43: val_loss improved from 20088.47656 to 19983.47852, saving model to ./ckpt/reg_lr005/val_rmse_19874.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20005.1055 - rmse: 19998.8438 - val_loss: 19983.4785 - val_rmse: 19873.7969\n",
      "Epoch 44/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19808.0293 - rmse: 19808.0293\n",
      "Epoch 44: val_loss did not improve from 19983.47852\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19721.6211 - rmse: 19696.3066 - val_loss: 21075.9355 - val_rmse: 20877.2070\n",
      "Epoch 45/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19270.9727 - rmse: 19303.4043\n",
      "Epoch 45: val_loss did not improve from 19983.47852\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19270.9727 - rmse: 19303.4043 - val_loss: 30134.5254 - val_rmse: 30214.5957\n",
      "Epoch 46/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21621.7656 - rmse: 21621.7656\n",
      "Epoch 46: val_loss did not improve from 19983.47852\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21535.5996 - rmse: 21530.2754 - val_loss: 20969.3965 - val_rmse: 20957.8320\n",
      "Epoch 47/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18706.1094 - rmse: 18706.1094\n",
      "Epoch 47: val_loss did not improve from 19983.47852\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18713.7539 - rmse: 18704.7246 - val_loss: 22675.7695 - val_rmse: 22678.5938\n",
      "Epoch 48/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 21345.5781 - rmse: 21345.5781\n",
      "Epoch 48: val_loss did not improve from 19983.47852\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21178.6387 - rmse: 21135.5430 - val_loss: 19986.4453 - val_rmse: 19836.3418\n",
      "Epoch 49/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19465.2129 - rmse: 19485.1641\n",
      "Epoch 49: val_loss did not improve from 19983.47852\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19465.2129 - rmse: 19485.1641 - val_loss: 20879.8672 - val_rmse: 20686.7949\n",
      "Epoch 50/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20783.4062 - rmse: 20783.4062\n",
      "Epoch 50: val_loss improved from 19983.47852 to 19683.29297, saving model to ./ckpt/reg_lr005/val_rmse_19573.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20721.6035 - rmse: 20694.6055 - val_loss: 19683.2930 - val_rmse: 19572.8535\n",
      "Epoch 51/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18372.9355 - rmse: 18372.9355\n",
      "Epoch 51: val_loss did not improve from 19683.29297\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18500.1562 - rmse: 18506.8770 - val_loss: 20620.7969 - val_rmse: 20386.3496\n",
      "Epoch 52/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18607.8320 - rmse: 18607.8320\n",
      "Epoch 52: val_loss improved from 19683.29297 to 19667.82227, saving model to ./ckpt/reg_lr005/val_rmse_19535.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18567.4121 - rmse: 18543.5039 - val_loss: 19667.8223 - val_rmse: 19534.5703\n",
      "Epoch 53/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20229.1602 - rmse: 20195.8418\n",
      "Epoch 53: val_loss did not improve from 19667.82227\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20229.1602 - rmse: 20195.8418 - val_loss: 20439.6895 - val_rmse: 20353.9629\n",
      "Epoch 54/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17943.9922 - rmse: 17943.9922\n",
      "Epoch 54: val_loss did not improve from 19667.82227\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18132.1172 - rmse: 18157.9492 - val_loss: 23462.2559 - val_rmse: 23483.1152\n",
      "Epoch 55/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18442.8301 - rmse: 18442.8301\n",
      "Epoch 55: val_loss did not improve from 19667.82227\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18581.6543 - rmse: 18587.5957 - val_loss: 27827.7793 - val_rmse: 27871.2383\n",
      "Epoch 56/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19519.4297 - rmse: 19519.4297\n",
      "Epoch 56: val_loss did not improve from 19667.82227\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19775.7285 - rmse: 19836.6211 - val_loss: 22397.9883 - val_rmse: 22170.9512\n",
      "Epoch 57/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19205.8145 - rmse: 19205.8145\n",
      "Epoch 57: val_loss did not improve from 19667.82227\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19249.1816 - rmse: 19257.4883 - val_loss: 26458.8535 - val_rmse: 26538.2363\n",
      "Epoch 58/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17632.9141 - rmse: 17632.9141\n",
      "Epoch 58: val_loss did not improve from 19667.82227\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17712.6504 - rmse: 17739.5195 - val_loss: 24446.5879 - val_rmse: 24524.8262\n",
      "Epoch 59/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17263.5586 - rmse: 17263.5586\n",
      "Epoch 59: val_loss improved from 19667.82227 to 19227.71680, saving model to ./ckpt/reg_lr005/val_rmse_19085.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17197.9844 - rmse: 17190.5039 - val_loss: 19227.7168 - val_rmse: 19085.1367\n",
      "Epoch 60/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17263.7871 - rmse: 17263.7871\n",
      "Epoch 60: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17537.1426 - rmse: 17631.5898 - val_loss: 19549.0859 - val_rmse: 19468.8770\n",
      "Epoch 61/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21133.1758 - rmse: 21133.1758\n",
      "Epoch 61: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21016.3828 - rmse: 21010.2637 - val_loss: 30535.9355 - val_rmse: 30552.9434\n",
      "Epoch 62/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18260.1855 - rmse: 18260.1855\n",
      "Epoch 62: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18151.8809 - rmse: 18134.2051 - val_loss: 23646.3281 - val_rmse: 23575.7812\n",
      "Epoch 63/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17582.3691 - rmse: 17669.3711\n",
      "Epoch 63: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17582.3691 - rmse: 17669.3711 - val_loss: 24068.3496 - val_rmse: 24059.4629\n",
      "Epoch 64/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17521.1738 - rmse: 17521.1738\n",
      "Epoch 64: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17346.6309 - rmse: 17343.7070 - val_loss: 20563.1035 - val_rmse: 20487.6836\n",
      "Epoch 65/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19113.9277 - rmse: 19113.9277\n",
      "Epoch 65: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19388.8398 - rmse: 19397.1719 - val_loss: 29882.3926 - val_rmse: 29908.6465\n",
      "Epoch 66/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16832.1016 - rmse: 16832.1016\n",
      "Epoch 66: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16791.2109 - rmse: 16838.7090 - val_loss: 19919.1504 - val_rmse: 19780.6348\n",
      "Epoch 67/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18165.2324 - rmse: 18165.2324\n",
      "Epoch 67: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18152.7734 - rmse: 18179.6465 - val_loss: 21766.2578 - val_rmse: 21715.1289\n",
      "Epoch 68/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19928.1758 - rmse: 19928.1758\n",
      "Epoch 68: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19888.1270 - rmse: 19864.4395 - val_loss: 23966.1719 - val_rmse: 23886.3457\n",
      "Epoch 69/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16764.0527 - rmse: 16764.0527\n",
      "Epoch 69: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16881.8535 - rmse: 16951.5234 - val_loss: 24425.8086 - val_rmse: 24449.4648\n",
      "Epoch 70/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17203.1641 - rmse: 17203.1641\n",
      "Epoch 70: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17323.9961 - rmse: 17367.7520 - val_loss: 21407.9375 - val_rmse: 21187.3047\n",
      "Epoch 71/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17511.3926 - rmse: 17511.3926\n",
      "Epoch 71: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17752.0000 - rmse: 17894.3027 - val_loss: 19596.0000 - val_rmse: 19418.2285\n",
      "Epoch 72/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16397.8945 - rmse: 16397.8945\n",
      "Epoch 72: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16560.1426 - rmse: 16530.5625 - val_loss: 20218.3965 - val_rmse: 20022.4258\n",
      "Epoch 73/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18746.4863 - rmse: 18746.4863\n",
      "Epoch 73: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19010.6133 - rmse: 19033.6582 - val_loss: 20149.1680 - val_rmse: 19906.9531\n",
      "Epoch 74/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17358.9941 - rmse: 17358.9941\n",
      "Epoch 74: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17375.2520 - rmse: 17405.5098 - val_loss: 21640.9590 - val_rmse: 21560.6250\n",
      "Epoch 75/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15673.1191 - rmse: 15673.1191\n",
      "Epoch 75: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15758.1367 - rmse: 15729.6748 - val_loss: 23710.8223 - val_rmse: 23717.5488\n",
      "Epoch 76/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16433.1309 - rmse: 16433.1309\n",
      "Epoch 76: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16366.5605 - rmse: 16357.5000 - val_loss: 22034.3516 - val_rmse: 21958.7305\n",
      "Epoch 77/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17314.1270 - rmse: 17314.1270\n",
      "Epoch 77: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17315.5605 - rmse: 17332.9160 - val_loss: 20790.0156 - val_rmse: 20630.3145\n",
      "Epoch 78/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16599.2090 - rmse: 16599.2090\n",
      "Epoch 78: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16551.3652 - rmse: 16535.7402 - val_loss: 19878.3828 - val_rmse: 19760.7695\n",
      "Epoch 79/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16846.4277 - rmse: 16846.4277\n",
      "Epoch 79: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16917.2949 - rmse: 16921.6641 - val_loss: 22546.7402 - val_rmse: 22473.3457\n",
      "Epoch 80/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16227.2295 - rmse: 16227.2295\n",
      "Epoch 80: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16365.0742 - rmse: 16387.3223 - val_loss: 19487.4414 - val_rmse: 19363.7344\n",
      "Epoch 81/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16237.8975 - rmse: 16237.8975\n",
      "Epoch 81: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16295.6250 - rmse: 16303.6719 - val_loss: 20944.0957 - val_rmse: 20863.8789\n",
      "Epoch 82/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16338.1875 - rmse: 16338.1875\n",
      "Epoch 82: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16329.6338 - rmse: 16296.5498 - val_loss: 22645.4102 - val_rmse: 22571.0898\n",
      "Epoch 83/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16238.9463 - rmse: 16238.9463\n",
      "Epoch 83: val_loss did not improve from 19227.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16296.9121 - rmse: 16331.1943 - val_loss: 22923.4961 - val_rmse: 22846.1270\n",
      "Epoch 84/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17993.7031 - rmse: 17993.7031\n",
      "Epoch 84: val_loss improved from 19227.71680 to 19126.86523, saving model to ./ckpt/reg_lr005/val_rmse_19015.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17870.0684 - rmse: 17856.5469 - val_loss: 19126.8652 - val_rmse: 19014.9316\n",
      "Epoch 85/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16700.5371 - rmse: 16700.5371\n",
      "Epoch 85: val_loss did not improve from 19126.86523\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16655.6914 - rmse: 16640.9492 - val_loss: 20226.4844 - val_rmse: 20169.8242\n",
      "Epoch 86/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16205.1162 - rmse: 16205.1162\n",
      "Epoch 86: val_loss did not improve from 19126.86523\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16476.8516 - rmse: 16589.5586 - val_loss: 29725.0527 - val_rmse: 29694.8086\n",
      "Epoch 87/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17610.3809 - rmse: 17610.3809\n",
      "Epoch 87: val_loss did not improve from 19126.86523\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17425.4219 - rmse: 17418.1152 - val_loss: 20052.9082 - val_rmse: 19924.3496\n",
      "Epoch 88/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16667.2422 - rmse: 16667.2422\n",
      "Epoch 88: val_loss did not improve from 19126.86523\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16692.4355 - rmse: 16686.7344 - val_loss: 22756.6758 - val_rmse: 22678.9180\n",
      "Epoch 89/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15775.7070 - rmse: 15775.7070\n",
      "Epoch 89: val_loss did not improve from 19126.86523\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15629.0107 - rmse: 15610.0410 - val_loss: 22691.7480 - val_rmse: 22712.1152\n",
      "Epoch 90/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15403.8750 - rmse: 15403.8750\n",
      "Epoch 90: val_loss did not improve from 19126.86523\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15447.5371 - rmse: 15473.3594 - val_loss: 25454.5586 - val_rmse: 25486.8691\n",
      "Epoch 91/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15404.0498 - rmse: 15404.0498\n",
      "Epoch 91: val_loss improved from 19126.86523 to 18900.86914, saving model to ./ckpt/reg_lr005/val_rmse_18706.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15451.9355 - rmse: 15476.0068 - val_loss: 18900.8691 - val_rmse: 18705.5352\n",
      "Epoch 92/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15958.2539 - rmse: 15958.2539\n",
      "Epoch 92: val_loss did not improve from 18900.86914\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16076.6895 - rmse: 16146.7354 - val_loss: 22517.6133 - val_rmse: 22456.4453\n",
      "Epoch 93/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15842.4727 - rmse: 15856.3193\n",
      "Epoch 93: val_loss did not improve from 18900.86914\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15842.4727 - rmse: 15856.3193 - val_loss: 19294.7520 - val_rmse: 19176.9805\n",
      "Epoch 94/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16236.6270 - rmse: 16236.6270\n",
      "Epoch 94: val_loss did not improve from 18900.86914\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16128.8545 - rmse: 16112.2070 - val_loss: 27051.6719 - val_rmse: 27105.3594\n",
      "Epoch 95/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16608.4980 - rmse: 16608.4980\n",
      "Epoch 95: val_loss did not improve from 18900.86914\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16551.3477 - rmse: 16539.8398 - val_loss: 20387.8711 - val_rmse: 20293.4805\n",
      "Epoch 96/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15712.6689 - rmse: 15712.6689\n",
      "Epoch 96: val_loss did not improve from 18900.86914\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15762.5947 - rmse: 15803.5967 - val_loss: 19665.0332 - val_rmse: 19546.6777\n",
      "Epoch 97/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15700.1543 - rmse: 15700.1543\n",
      "Epoch 97: val_loss did not improve from 18900.86914\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15730.0654 - rmse: 15775.9932 - val_loss: 22121.7129 - val_rmse: 22066.6328\n",
      "Epoch 98/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15730.9316 - rmse: 15730.9316\n",
      "Epoch 98: val_loss did not improve from 18900.86914\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15868.4834 - rmse: 15834.7109 - val_loss: 19493.2871 - val_rmse: 19486.7793\n",
      "Epoch 99/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16013.2305 - rmse: 16002.7393\n",
      "Epoch 99: val_loss did not improve from 18900.86914\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16013.2305 - rmse: 16002.7393 - val_loss: 20512.3184 - val_rmse: 20454.7168\n",
      "Epoch 100/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15668.9932 - rmse: 15668.9932\n",
      "Epoch 100: val_loss did not improve from 18900.86914\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15670.1875 - rmse: 15645.7979 - val_loss: 19972.3320 - val_rmse: 19916.8477\n",
      "Epoch 101/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16371.8916 - rmse: 16371.8916\n",
      "Epoch 101: val_loss did not improve from 18900.86914\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16313.8066 - rmse: 16328.0176 - val_loss: 20843.7988 - val_rmse: 20776.4453\n",
      "Epoch 102/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16217.0439 - rmse: 16217.0439\n",
      "Epoch 102: val_loss improved from 18900.86914 to 18649.02539, saving model to ./ckpt/reg_lr005/val_rmse_18487.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16142.8926 - rmse: 16169.0469 - val_loss: 18649.0254 - val_rmse: 18486.7754\n",
      "Epoch 103/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15517.5186 - rmse: 15517.5186\n",
      "Epoch 103: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15494.3125 - rmse: 15482.3555 - val_loss: 28416.7090 - val_rmse: 28506.5645\n",
      "Epoch 104/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16117.4287 - rmse: 16117.4287\n",
      "Epoch 104: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16253.5713 - rmse: 16361.9053 - val_loss: 20249.0645 - val_rmse: 20064.3945\n",
      "Epoch 105/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15837.4707 - rmse: 15834.7949\n",
      "Epoch 105: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15837.4707 - rmse: 15834.7949 - val_loss: 22611.4336 - val_rmse: 22592.2949\n",
      "Epoch 106/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 14165.0283 - rmse: 14165.0283\n",
      "Epoch 106: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14179.6426 - rmse: 14188.2852 - val_loss: 20005.2207 - val_rmse: 19937.8789\n",
      "Epoch 107/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15199.6729 - rmse: 15199.6729\n",
      "Epoch 107: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15223.9355 - rmse: 15227.8604 - val_loss: 24101.0742 - val_rmse: 24127.0879\n",
      "Epoch 108/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 14881.6807 - rmse: 14881.6807\n",
      "Epoch 108: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14999.0645 - rmse: 15068.4873 - val_loss: 20651.5352 - val_rmse: 20496.8398\n",
      "Epoch 109/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15249.7666 - rmse: 15249.7666\n",
      "Epoch 109: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15361.2695 - rmse: 15402.3789 - val_loss: 19993.2598 - val_rmse: 19769.9141\n",
      "Epoch 110/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16865.8164 - rmse: 16865.8164\n",
      "Epoch 110: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17221.0391 - rmse: 17292.7617 - val_loss: 28490.0879 - val_rmse: 28523.5508\n",
      "Epoch 111/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15166.3418 - rmse: 15166.3418\n",
      "Epoch 111: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15135.2803 - rmse: 15151.2305 - val_loss: 20838.3516 - val_rmse: 20841.0469\n",
      "Epoch 112/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 14732.2109 - rmse: 14732.2109\n",
      "Epoch 112: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14744.1289 - rmse: 14763.2998 - val_loss: 19370.0938 - val_rmse: 19300.8789\n",
      "Epoch 113/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14932.8633 - rmse: 14988.3193\n",
      "Epoch 113: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14932.8633 - rmse: 14988.3193 - val_loss: 26147.2402 - val_rmse: 26175.1719\n",
      "Epoch 114/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 14182.0225 - rmse: 14182.0225\n",
      "Epoch 114: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14264.8096 - rmse: 14315.3545 - val_loss: 26051.3047 - val_rmse: 26031.5156\n",
      "Epoch 115/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17674.3164 - rmse: 17674.3164\n",
      "Epoch 115: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17659.2402 - rmse: 17721.3027 - val_loss: 24013.3906 - val_rmse: 24061.0898\n",
      "Epoch 116/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15898.9492 - rmse: 15898.9492\n",
      "Epoch 116: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15927.4238 - rmse: 15944.2646 - val_loss: 18982.4980 - val_rmse: 18769.8730\n",
      "Epoch 117/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 14700.3721 - rmse: 14700.3721\n",
      "Epoch 117: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14638.0889 - rmse: 14648.6660 - val_loss: 22829.0703 - val_rmse: 22868.6660\n",
      "Epoch 118/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16088.1318 - rmse: 16088.1318\n",
      "Epoch 118: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16145.1904 - rmse: 16119.1826 - val_loss: 18730.8301 - val_rmse: 18509.6445\n",
      "Epoch 119/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 14461.8057 - rmse: 14461.8057\n",
      "Epoch 119: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 14445.1191 - rmse: 14467.6787 - val_loss: 19449.7754 - val_rmse: 19440.1953\n",
      "Epoch 120/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 14450.4268 - rmse: 14450.4268\n",
      "Epoch 120: val_loss did not improve from 18649.02539\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14510.4541 - rmse: 14504.5078 - val_loss: 23006.9355 - val_rmse: 23029.9941\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:15:42.710395: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 105119.3516 - rmse: 104750.5078\n",
      "Epoch 1: val_loss improved from inf to 70626.94531, saving model to ./ckpt/reg_lr005/val_rmse_71158.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 105119.3516 - rmse: 104750.5078 - val_loss: 70626.9453 - val_rmse: 71158.1250\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 81538.6094 - rmse: 81538.6094  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:15:43.667165: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 55957.8867 - rmse: 56000.2344\n",
      "Epoch 2: val_loss improved from 70626.94531 to 53536.30469, saving model to ./ckpt/reg_lr005/val_rmse_54140.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 55957.8867 - rmse: 56000.2344 - val_loss: 53536.3047 - val_rmse: 54140.0078\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 48543.7969 - rmse: 48543.7969\n",
      "Epoch 3: val_loss improved from 53536.30469 to 47339.95312, saving model to ./ckpt/reg_lr005/val_rmse_47810.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 48483.3672 - rmse: 48481.3086 - val_loss: 47339.9531 - val_rmse: 47810.4297\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 45930.5039 - rmse: 45925.3281\n",
      "Epoch 4: val_loss did not improve from 47339.95312\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 45930.5039 - rmse: 45925.3281 - val_loss: 51259.0430 - val_rmse: 51783.5781\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 41941.8398 - rmse: 41941.8398\n",
      "Epoch 5: val_loss improved from 47339.95312 to 42369.44141, saving model to ./ckpt/reg_lr005/val_rmse_42665.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 42040.0352 - rmse: 42098.1055 - val_loss: 42369.4414 - val_rmse: 42664.6758\n",
      "Epoch 6/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 39402.1992 - rmse: 39402.1992\n",
      "Epoch 6: val_loss did not improve from 42369.44141\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 39870.9375 - rmse: 39819.5234 - val_loss: 42552.6680 - val_rmse: 42926.1406\n",
      "Epoch 7/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 39367.8125 - rmse: 39367.8125\n",
      "Epoch 7: val_loss improved from 42369.44141 to 41353.36719, saving model to ./ckpt/reg_lr005/val_rmse_41681.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 39151.1836 - rmse: 39079.6133 - val_loss: 41353.3672 - val_rmse: 41681.4844\n",
      "Epoch 8/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 40818.8828 - rmse: 40818.8828\n",
      "Epoch 8: val_loss improved from 41353.36719 to 41256.57031, saving model to ./ckpt/reg_lr005/val_rmse_41565.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 40636.2500 - rmse: 40608.1445 - val_loss: 41256.5703 - val_rmse: 41564.7148\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 36650.3984 - rmse: 36670.0195\n",
      "Epoch 9: val_loss improved from 41256.57031 to 37800.96875, saving model to ./ckpt/reg_lr005/val_rmse_37848.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 36650.3984 - rmse: 36670.0195 - val_loss: 37800.9688 - val_rmse: 37848.3867\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 36730.0742 - rmse: 36730.0742\n",
      "Epoch 10: val_loss improved from 37800.96875 to 35459.42188, saving model to ./ckpt/reg_lr005/val_rmse_35616.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 36655.8984 - rmse: 36612.0234 - val_loss: 35459.4219 - val_rmse: 35616.3398\n",
      "Epoch 11/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 34026.6875 - rmse: 34026.6875\n",
      "Epoch 11: val_loss did not improve from 35459.42188\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 34955.8555 - rmse: 35197.3242 - val_loss: 42588.4609 - val_rmse: 42403.3984\n",
      "Epoch 12/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 34307.8398 - rmse: 34307.8398\n",
      "Epoch 12: val_loss improved from 35459.42188 to 32137.80469, saving model to ./ckpt/reg_lr005/val_rmse_32130.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 34136.3281 - rmse: 34101.7734 - val_loss: 32137.8047 - val_rmse: 32130.2461\n",
      "Epoch 13/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 32219.4062 - rmse: 32219.4062\n",
      "Epoch 13: val_loss did not improve from 32137.80469\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 32138.5762 - rmse: 32138.9531 - val_loss: 32354.7090 - val_rmse: 32367.6426\n",
      "Epoch 14/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 31513.3379 - rmse: 31513.3379\n",
      "Epoch 14: val_loss did not improve from 32137.80469\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 31200.9961 - rmse: 31202.1641 - val_loss: 37092.9492 - val_rmse: 36879.2188\n",
      "Epoch 15/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 30061.7441 - rmse: 30061.7441\n",
      "Epoch 15: val_loss improved from 32137.80469 to 29036.53906, saving model to ./ckpt/reg_lr005/val_rmse_28875.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 29987.5918 - rmse: 30011.3359 - val_loss: 29036.5391 - val_rmse: 28875.1660\n",
      "Epoch 16/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 27444.2891 - rmse: 27444.2891\n",
      "Epoch 16: val_loss did not improve from 29036.53906\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 27586.2305 - rmse: 27588.3262 - val_loss: 33487.1523 - val_rmse: 33377.3945\n",
      "Epoch 17/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 29312.2363 - rmse: 29312.2363\n",
      "Epoch 17: val_loss did not improve from 29036.53906\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 29239.0684 - rmse: 29309.4922 - val_loss: 36495.7773 - val_rmse: 36374.6094\n",
      "Epoch 18/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 26220.4512 - rmse: 26220.4512\n",
      "Epoch 18: val_loss did not improve from 29036.53906\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 26239.0117 - rmse: 26249.9883 - val_loss: 32491.7715 - val_rmse: 32391.7637\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27895.0430 - rmse: 28006.7461\n",
      "Epoch 19: val_loss did not improve from 29036.53906\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27895.0430 - rmse: 28006.7461 - val_loss: 31879.3691 - val_rmse: 31604.7891\n",
      "Epoch 20/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 27103.8516 - rmse: 27103.8516\n",
      "Epoch 20: val_loss improved from 29036.53906 to 26374.86133, saving model to ./ckpt/reg_lr005/val_rmse_26077.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27217.9609 - rmse: 27224.6816 - val_loss: 26374.8613 - val_rmse: 26077.4297\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25275.7207 - rmse: 25275.7207\n",
      "Epoch 21: val_loss did not improve from 26374.86133\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 25384.0195 - rmse: 25419.3555 - val_loss: 42799.3086 - val_rmse: 42682.9492\n",
      "Epoch 22/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 29657.0137 - rmse: 29657.0137\n",
      "Epoch 22: val_loss did not improve from 26374.86133\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 29811.8398 - rmse: 29825.3125 - val_loss: 26827.5762 - val_rmse: 26690.3887\n",
      "Epoch 23/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 24599.9551 - rmse: 24599.9551\n",
      "Epoch 23: val_loss did not improve from 26374.86133\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24522.7539 - rmse: 24538.3867 - val_loss: 29550.1992 - val_rmse: 29421.1250\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24696.1445 - rmse: 24696.1445\n",
      "Epoch 24: val_loss improved from 26374.86133 to 25830.41992, saving model to ./ckpt/reg_lr005/val_rmse_25632.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24716.9531 - rmse: 24767.7168 - val_loss: 25830.4199 - val_rmse: 25631.8262\n",
      "Epoch 25/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23079.5508 - rmse: 23079.5508\n",
      "Epoch 25: val_loss did not improve from 25830.41992\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23373.8789 - rmse: 23348.4902 - val_loss: 26007.8613 - val_rmse: 25825.0430\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 27817.7988 - rmse: 27817.7988\n",
      "Epoch 26: val_loss improved from 25830.41992 to 23980.25586, saving model to ./ckpt/reg_lr005/val_rmse_23755.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27877.9434 - rmse: 27913.5117 - val_loss: 23980.2559 - val_rmse: 23755.0938\n",
      "Epoch 27/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23201.6562 - rmse: 23201.6562\n",
      "Epoch 27: val_loss did not improve from 23980.25586\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23211.3613 - rmse: 23203.4004 - val_loss: 25018.9219 - val_rmse: 24727.5020\n",
      "Epoch 28/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22769.3945 - rmse: 22769.3945\n",
      "Epoch 28: val_loss did not improve from 23980.25586\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22779.7734 - rmse: 22767.5957 - val_loss: 24763.4863 - val_rmse: 24616.2715\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22434.7422 - rmse: 22434.7422\n",
      "Epoch 29: val_loss did not improve from 23980.25586\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22378.5488 - rmse: 22345.3125 - val_loss: 34921.7383 - val_rmse: 34753.9336\n",
      "Epoch 30/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22330.3223 - rmse: 22330.3223\n",
      "Epoch 30: val_loss did not improve from 23980.25586\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22186.2344 - rmse: 22209.9160 - val_loss: 34611.0469 - val_rmse: 34462.3633\n",
      "Epoch 31/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21786.8613 - rmse: 21786.8613\n",
      "Epoch 31: val_loss did not improve from 23980.25586\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21994.3008 - rmse: 22004.3984 - val_loss: 25233.8672 - val_rmse: 25070.8594\n",
      "Epoch 32/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 21519.5566 - rmse: 21519.5566\n",
      "Epoch 32: val_loss improved from 23980.25586 to 22511.85742, saving model to ./ckpt/reg_lr005/val_rmse_22226.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21655.9688 - rmse: 21616.7285 - val_loss: 22511.8574 - val_rmse: 22225.6855\n",
      "Epoch 33/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20635.2969 - rmse: 20635.2969\n",
      "Epoch 33: val_loss did not improve from 22511.85742\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20823.3281 - rmse: 20911.4707 - val_loss: 35504.6914 - val_rmse: 35351.4336\n",
      "Epoch 34/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21399.1055 - rmse: 21399.1055\n",
      "Epoch 34: val_loss did not improve from 22511.85742\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21535.6895 - rmse: 21557.5879 - val_loss: 43359.5469 - val_rmse: 43270.3711\n",
      "Epoch 35/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22748.1953 - rmse: 22748.1953\n",
      "Epoch 35: val_loss did not improve from 22511.85742\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22748.1523 - rmse: 22694.4941 - val_loss: 24818.9980 - val_rmse: 24693.4746\n",
      "Epoch 36/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19587.1152 - rmse: 19587.1152\n",
      "Epoch 36: val_loss did not improve from 22511.85742\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19661.6602 - rmse: 19656.2480 - val_loss: 33054.4375 - val_rmse: 32890.2812\n",
      "Epoch 37/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21050.6836 - rmse: 21050.6836\n",
      "Epoch 37: val_loss improved from 22511.85742 to 20506.61133, saving model to ./ckpt/reg_lr005/val_rmse_20282.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20882.3262 - rmse: 20868.8281 - val_loss: 20506.6113 - val_rmse: 20281.5078\n",
      "Epoch 38/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20743.5410 - rmse: 20743.5410\n",
      "Epoch 38: val_loss did not improve from 20506.61133\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20660.7441 - rmse: 20653.4668 - val_loss: 25116.5820 - val_rmse: 24879.8457\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19976.4492 - rmse: 19976.4492\n",
      "Epoch 39: val_loss did not improve from 20506.61133\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19949.5117 - rmse: 19933.5801 - val_loss: 26829.6953 - val_rmse: 26662.5898\n",
      "Epoch 40/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20016.6738 - rmse: 20016.6738\n",
      "Epoch 40: val_loss did not improve from 20506.61133\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20095.6191 - rmse: 20141.5898 - val_loss: 25070.5000 - val_rmse: 24865.5430\n",
      "Epoch 41/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20252.0254 - rmse: 20252.0254\n",
      "Epoch 41: val_loss did not improve from 20506.61133\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20578.9590 - rmse: 20650.3027 - val_loss: 24051.8711 - val_rmse: 23916.7773\n",
      "Epoch 42/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20860.5801 - rmse: 20860.5801\n",
      "Epoch 42: val_loss did not improve from 20506.61133\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20791.0840 - rmse: 20810.9297 - val_loss: 24496.3770 - val_rmse: 24313.8008\n",
      "Epoch 43/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20456.7305 - rmse: 20456.7305\n",
      "Epoch 43: val_loss did not improve from 20506.61133\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20348.8633 - rmse: 20361.5547 - val_loss: 27980.9688 - val_rmse: 27825.7461\n",
      "Epoch 44/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20152.2832 - rmse: 20152.2832\n",
      "Epoch 44: val_loss did not improve from 20506.61133\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20098.7969 - rmse: 20102.0156 - val_loss: 30302.6250 - val_rmse: 30216.7266\n",
      "Epoch 45/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21207.5176 - rmse: 21207.5176\n",
      "Epoch 45: val_loss did not improve from 20506.61133\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21405.4297 - rmse: 21400.1152 - val_loss: 21078.7051 - val_rmse: 20870.4082\n",
      "Epoch 46/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20024.3574 - rmse: 20024.3574\n",
      "Epoch 46: val_loss did not improve from 20506.61133\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19868.3105 - rmse: 19856.8945 - val_loss: 29872.7266 - val_rmse: 29767.7930\n",
      "Epoch 47/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18988.9863 - rmse: 18988.9863\n",
      "Epoch 47: val_loss improved from 20506.61133 to 19456.40820, saving model to ./ckpt/reg_lr005/val_rmse_19221.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19366.3633 - rmse: 19367.5078 - val_loss: 19456.4082 - val_rmse: 19220.7734\n",
      "Epoch 48/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18090.2734 - rmse: 18090.2734\n",
      "Epoch 48: val_loss did not improve from 19456.40820\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18152.0488 - rmse: 18194.4727 - val_loss: 26549.2930 - val_rmse: 26434.4629\n",
      "Epoch 49/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19674.7734 - rmse: 19712.0312\n",
      "Epoch 49: val_loss did not improve from 19456.40820\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19674.7734 - rmse: 19712.0312 - val_loss: 20059.6777 - val_rmse: 19869.4961\n",
      "Epoch 50/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19114.9199 - rmse: 19114.9199\n",
      "Epoch 50: val_loss did not improve from 19456.40820\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19423.2266 - rmse: 19439.9883 - val_loss: 33023.7773 - val_rmse: 32934.3164\n",
      "Epoch 51/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19564.6152 - rmse: 19564.6152\n",
      "Epoch 51: val_loss did not improve from 19456.40820\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19591.3457 - rmse: 19570.4355 - val_loss: 26018.7832 - val_rmse: 25878.1914\n",
      "Epoch 52/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19469.6504 - rmse: 19469.6504\n",
      "Epoch 52: val_loss did not improve from 19456.40820\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19364.2246 - rmse: 19392.1602 - val_loss: 23017.9727 - val_rmse: 22909.2500\n",
      "Epoch 53/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17836.3281 - rmse: 17836.3281\n",
      "Epoch 53: val_loss did not improve from 19456.40820\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17950.9062 - rmse: 18014.0625 - val_loss: 19608.9648 - val_rmse: 19499.8848\n",
      "Epoch 54/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18119.9258 - rmse: 18119.9258\n",
      "Epoch 54: val_loss improved from 19456.40820 to 19207.85352, saving model to ./ckpt/reg_lr005/val_rmse_19085.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18142.9922 - rmse: 18163.2852 - val_loss: 19207.8535 - val_rmse: 19084.7793\n",
      "Epoch 55/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18486.1035 - rmse: 18486.1035\n",
      "Epoch 55: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18439.1719 - rmse: 18469.5156 - val_loss: 32341.2266 - val_rmse: 32240.5645\n",
      "Epoch 56/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20216.1270 - rmse: 20216.1270\n",
      "Epoch 56: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20160.6816 - rmse: 20157.1934 - val_loss: 31638.5039 - val_rmse: 31548.6758\n",
      "Epoch 57/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19278.7148 - rmse: 19312.5059\n",
      "Epoch 57: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19278.7148 - rmse: 19312.5059 - val_loss: 25429.0879 - val_rmse: 25354.6699\n",
      "Epoch 58/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18471.0020 - rmse: 18471.0020\n",
      "Epoch 58: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18383.5859 - rmse: 18367.6992 - val_loss: 19336.8906 - val_rmse: 19193.4727\n",
      "Epoch 59/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18513.9902 - rmse: 18513.9902\n",
      "Epoch 59: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18413.5176 - rmse: 18378.5352 - val_loss: 20000.6309 - val_rmse: 19889.2773\n",
      "Epoch 60/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18909.2773 - rmse: 18909.2773\n",
      "Epoch 60: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18804.5293 - rmse: 18800.4727 - val_loss: 38971.4922 - val_rmse: 38969.4883\n",
      "Epoch 61/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17817.8477 - rmse: 17817.8477\n",
      "Epoch 61: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17850.5352 - rmse: 17875.2852 - val_loss: 24144.0547 - val_rmse: 24027.6719\n",
      "Epoch 62/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17922.7324 - rmse: 17922.7324\n",
      "Epoch 62: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17848.2148 - rmse: 17808.5234 - val_loss: 22034.0234 - val_rmse: 21939.2812\n",
      "Epoch 63/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17298.7695 - rmse: 17298.7695\n",
      "Epoch 63: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17383.5703 - rmse: 17414.5742 - val_loss: 19930.8828 - val_rmse: 19834.1094\n",
      "Epoch 64/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17811.5996 - rmse: 17811.5996\n",
      "Epoch 64: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17699.9883 - rmse: 17693.0996 - val_loss: 28809.4609 - val_rmse: 28758.5000\n",
      "Epoch 65/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18273.3008 - rmse: 18273.3008\n",
      "Epoch 65: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18252.5059 - rmse: 18265.2891 - val_loss: 32004.6660 - val_rmse: 31926.6172\n",
      "Epoch 66/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19377.7480 - rmse: 19377.7480\n",
      "Epoch 66: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19311.2598 - rmse: 19270.1230 - val_loss: 21895.7773 - val_rmse: 21860.2969\n",
      "Epoch 67/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17553.9883 - rmse: 17553.9883\n",
      "Epoch 67: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17521.0762 - rmse: 17501.6094 - val_loss: 29780.8770 - val_rmse: 29765.5859\n",
      "Epoch 68/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18035.8848 - rmse: 18035.8848\n",
      "Epoch 68: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17931.1504 - rmse: 17923.7344 - val_loss: 21554.9766 - val_rmse: 21454.2676\n",
      "Epoch 69/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17748.3047 - rmse: 17748.3047\n",
      "Epoch 69: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17779.5312 - rmse: 17762.5410 - val_loss: 26664.1426 - val_rmse: 26628.6289\n",
      "Epoch 70/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17944.3125 - rmse: 17944.3125\n",
      "Epoch 70: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17959.1895 - rmse: 17969.3730 - val_loss: 26532.3926 - val_rmse: 26473.0859\n",
      "Epoch 71/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18593.6484 - rmse: 18593.6484\n",
      "Epoch 71: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18594.4180 - rmse: 18564.9180 - val_loss: 21498.2559 - val_rmse: 21404.3867\n",
      "Epoch 72/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17497.0801 - rmse: 17497.0801\n",
      "Epoch 72: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17655.4121 - rmse: 17662.1113 - val_loss: 19590.9746 - val_rmse: 19510.0645\n",
      "Epoch 73/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16971.7559 - rmse: 16971.7559\n",
      "Epoch 73: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17108.7891 - rmse: 17155.8379 - val_loss: 24139.3926 - val_rmse: 24033.5273\n",
      "Epoch 74/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17916.6562 - rmse: 17916.6562\n",
      "Epoch 74: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17780.6484 - rmse: 17806.7832 - val_loss: 22362.8691 - val_rmse: 22325.8848\n",
      "Epoch 75/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17065.5469 - rmse: 17065.5469\n",
      "Epoch 75: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16913.2402 - rmse: 16912.9609 - val_loss: 22507.4629 - val_rmse: 22409.6426\n",
      "Epoch 76/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17998.0840 - rmse: 17998.0840\n",
      "Epoch 76: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17934.2168 - rmse: 17920.1895 - val_loss: 28535.1621 - val_rmse: 28489.4395\n",
      "Epoch 77/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18672.9258 - rmse: 18672.9258\n",
      "Epoch 77: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18741.8379 - rmse: 18775.1992 - val_loss: 23703.1426 - val_rmse: 23694.6289\n",
      "Epoch 78/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16435.5371 - rmse: 16435.5371\n",
      "Epoch 78: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16782.4668 - rmse: 16859.0020 - val_loss: 19449.5820 - val_rmse: 19340.2031\n",
      "Epoch 79/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17378.3633 - rmse: 17378.3633\n",
      "Epoch 79: val_loss did not improve from 19207.85352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17250.0156 - rmse: 17247.6016 - val_loss: 19556.8574 - val_rmse: 19466.2793\n",
      "Epoch 79: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:16:37.240327: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 105019.0234 - rmse: 104605.8125\n",
      "Epoch 1: val_loss improved from inf to 62311.33203, saving model to ./ckpt/reg_lr005/val_rmse_62735.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 105019.0234 - rmse: 104605.8125 - val_loss: 62311.3320 - val_rmse: 62734.9648\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 59185.7500 - rmse: 59185.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:16:38.244239: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 51459.2969 - rmse: 51459.2969\n",
      "Epoch 2: val_loss improved from 62311.33203 to 50268.28125, saving model to ./ckpt/reg_lr005/val_rmse_50587.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 51047.6367 - rmse: 51002.2227 - val_loss: 50268.2812 - val_rmse: 50587.3633\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 46325.6719 - rmse: 46325.6719\n",
      "Epoch 3: val_loss improved from 50268.28125 to 47136.42188, saving model to ./ckpt/reg_lr005/val_rmse_47234.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 46471.3359 - rmse: 46633.4375 - val_loss: 47136.4219 - val_rmse: 47234.1094\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 43973.5938 - rmse: 43864.6797\n",
      "Epoch 4: val_loss improved from 47136.42188 to 46592.00391, saving model to ./ckpt/reg_lr005/val_rmse_46806.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 43973.5938 - rmse: 43864.6797 - val_loss: 46592.0039 - val_rmse: 46805.8516\n",
      "Epoch 5/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 41125.2188 - rmse: 41125.2188\n",
      "Epoch 5: val_loss improved from 46592.00391 to 41820.26953, saving model to ./ckpt/reg_lr005/val_rmse_41833.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 41266.1406 - rmse: 41215.6680 - val_loss: 41820.2695 - val_rmse: 41832.6914\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 40479.3789 - rmse: 40479.3789\n",
      "Epoch 6: val_loss improved from 41820.26953 to 39686.90625, saving model to ./ckpt/reg_lr005/val_rmse_39743.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 40401.2852 - rmse: 40403.9062 - val_loss: 39686.9062 - val_rmse: 39743.4023\n",
      "Epoch 7/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 39960.1250 - rmse: 39960.1250\n",
      "Epoch 7: val_loss did not improve from 39686.90625\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 39732.6484 - rmse: 39678.1836 - val_loss: 42517.7422 - val_rmse: 42658.5312\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 38038.7969 - rmse: 38038.7969\n",
      "Epoch 8: val_loss improved from 39686.90625 to 37285.64062, saving model to ./ckpt/reg_lr005/val_rmse_37346.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 38017.5391 - rmse: 38004.9648 - val_loss: 37285.6406 - val_rmse: 37345.5938\n",
      "Epoch 9/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 36727.5625 - rmse: 36727.5625\n",
      "Epoch 9: val_loss did not improve from 37285.64062\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 36784.5938 - rmse: 36743.5547 - val_loss: 43861.3867 - val_rmse: 44002.3359\n",
      "Epoch 10/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 35637.9219 - rmse: 35637.9219\n",
      "Epoch 10: val_loss did not improve from 37285.64062\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 35499.4961 - rmse: 35562.4414 - val_loss: 51715.6641 - val_rmse: 51856.6094\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 35643.2969 - rmse: 35643.2969\n",
      "Epoch 11: val_loss did not improve from 37285.64062\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 35402.5781 - rmse: 35350.8711 - val_loss: 39896.8438 - val_rmse: 39998.6016\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 32383.5176 - rmse: 32383.5176\n",
      "Epoch 12: val_loss improved from 37285.64062 to 32080.48242, saving model to ./ckpt/reg_lr005/val_rmse_32096.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 32409.6777 - rmse: 32370.6387 - val_loss: 32080.4824 - val_rmse: 32095.9883\n",
      "Epoch 13/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 31971.9062 - rmse: 31971.9062\n",
      "Epoch 13: val_loss improved from 32080.48242 to 31174.06641, saving model to ./ckpt/reg_lr005/val_rmse_31201.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 31867.4219 - rmse: 31856.1465 - val_loss: 31174.0664 - val_rmse: 31201.1035\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 29879.9375 - rmse: 29894.3711\n",
      "Epoch 14: val_loss improved from 31174.06641 to 28775.30078, saving model to ./ckpt/reg_lr005/val_rmse_28772.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 29879.9375 - rmse: 29894.3711 - val_loss: 28775.3008 - val_rmse: 28771.5195\n",
      "Epoch 15/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 29444.0527 - rmse: 29444.0527\n",
      "Epoch 15: val_loss improved from 28775.30078 to 27865.51953, saving model to ./ckpt/reg_lr005/val_rmse_27813.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 29626.2148 - rmse: 29594.9180 - val_loss: 27865.5195 - val_rmse: 27812.6562\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 29801.0273 - rmse: 29801.0273\n",
      "Epoch 16: val_loss did not improve from 27865.51953\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 29771.3145 - rmse: 29753.7422 - val_loss: 29055.0879 - val_rmse: 28981.8281\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 31522.4023 - rmse: 31522.4023\n",
      "Epoch 17: val_loss did not improve from 27865.51953\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 32125.5840 - rmse: 32123.2285 - val_loss: 32309.2773 - val_rmse: 32251.1914\n",
      "Epoch 18/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 31445.5000 - rmse: 31445.5000\n",
      "Epoch 18: val_loss improved from 27865.51953 to 27750.19531, saving model to ./ckpt/reg_lr005/val_rmse_27831.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 31535.1133 - rmse: 31811.0898 - val_loss: 27750.1953 - val_rmse: 27831.3008\n",
      "Epoch 19/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 27564.7598 - rmse: 27564.7598\n",
      "Epoch 19: val_loss improved from 27750.19531 to 26994.48047, saving model to ./ckpt/reg_lr005/val_rmse_27066.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27558.4277 - rmse: 27589.8672 - val_loss: 26994.4805 - val_rmse: 27066.2090\n",
      "Epoch 20/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 26046.5234 - rmse: 26046.5234\n",
      "Epoch 20: val_loss improved from 26994.48047 to 25034.09570, saving model to ./ckpt/reg_lr005/val_rmse_25053.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26035.4629 - rmse: 26073.1172 - val_loss: 25034.0957 - val_rmse: 25053.3906\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27301.0293 - rmse: 27363.3047\n",
      "Epoch 21: val_loss did not improve from 25034.09570\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27301.0293 - rmse: 27363.3047 - val_loss: 26943.2949 - val_rmse: 26896.5410\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25300.1055 - rmse: 25336.5723\n",
      "Epoch 22: val_loss did not improve from 25034.09570\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25300.1055 - rmse: 25336.5723 - val_loss: 41080.9297 - val_rmse: 41192.6406\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 26494.8184 - rmse: 26576.6016\n",
      "Epoch 23: val_loss did not improve from 25034.09570\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26494.8184 - rmse: 26576.6016 - val_loss: 27819.6035 - val_rmse: 27879.0898\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 26418.1934 - rmse: 26418.1934\n",
      "Epoch 24: val_loss improved from 25034.09570 to 24774.58008, saving model to ./ckpt/reg_lr005/val_rmse_24803.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 26532.8574 - rmse: 26489.8457 - val_loss: 24774.5801 - val_rmse: 24802.9004\n",
      "Epoch 25/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24905.7656 - rmse: 24905.7656\n",
      "Epoch 25: val_loss did not improve from 24774.58008\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24689.0254 - rmse: 24708.6719 - val_loss: 26154.5449 - val_rmse: 26188.8789\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 28717.4297 - rmse: 28717.4297\n",
      "Epoch 26: val_loss improved from 24774.58008 to 23360.28320, saving model to ./ckpt/reg_lr005/val_rmse_23333.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 28774.8574 - rmse: 28818.3730 - val_loss: 23360.2832 - val_rmse: 23333.2188\n",
      "Epoch 27/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 23482.4512 - rmse: 23482.4512\n",
      "Epoch 27: val_loss did not improve from 23360.28320\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23386.5957 - rmse: 23402.3164 - val_loss: 25755.0527 - val_rmse: 25841.1758\n",
      "Epoch 28/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23173.2676 - rmse: 23173.2676\n",
      "Epoch 28: val_loss improved from 23360.28320 to 22445.48438, saving model to ./ckpt/reg_lr005/val_rmse_22513.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23221.5664 - rmse: 23207.4785 - val_loss: 22445.4844 - val_rmse: 22513.2617\n",
      "Epoch 29/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22423.0254 - rmse: 22423.0254\n",
      "Epoch 29: val_loss did not improve from 22445.48438\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22368.4492 - rmse: 22362.2773 - val_loss: 26353.3242 - val_rmse: 26421.5762\n",
      "Epoch 30/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21592.7910 - rmse: 21592.7910\n",
      "Epoch 30: val_loss improved from 22445.48438 to 21344.06445, saving model to ./ckpt/reg_lr005/val_rmse_21403.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21378.3887 - rmse: 21353.9629 - val_loss: 21344.0645 - val_rmse: 21403.1602\n",
      "Epoch 31/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21769.1719 - rmse: 21769.1719\n",
      "Epoch 31: val_loss did not improve from 21344.06445\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21872.5059 - rmse: 21933.6172 - val_loss: 36837.3945 - val_rmse: 36876.5625\n",
      "Epoch 32/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 22016.5449 - rmse: 22016.5449\n",
      "Epoch 32: val_loss did not improve from 21344.06445\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21773.2773 - rmse: 21814.4199 - val_loss: 25183.4746 - val_rmse: 25230.8867\n",
      "Epoch 33/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21940.9922 - rmse: 21940.9922\n",
      "Epoch 33: val_loss improved from 21344.06445 to 20297.04883, saving model to ./ckpt/reg_lr005/val_rmse_20329.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21906.9160 - rmse: 21957.3184 - val_loss: 20297.0488 - val_rmse: 20328.9062\n",
      "Epoch 34/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21574.9160 - rmse: 21574.9160\n",
      "Epoch 34: val_loss did not improve from 20297.04883\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21474.2285 - rmse: 21464.9355 - val_loss: 22093.5996 - val_rmse: 22168.8750\n",
      "Epoch 35/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 22149.2949 - rmse: 22149.2949\n",
      "Epoch 35: val_loss did not improve from 20297.04883\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22379.1680 - rmse: 22374.2734 - val_loss: 39737.2969 - val_rmse: 39788.9258\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23714.3164 - rmse: 23717.1992\n",
      "Epoch 36: val_loss did not improve from 20297.04883\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23714.3164 - rmse: 23717.1992 - val_loss: 24926.4160 - val_rmse: 24972.0371\n",
      "Epoch 37/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20824.8926 - rmse: 20832.5273\n",
      "Epoch 37: val_loss did not improve from 20297.04883\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20824.8926 - rmse: 20832.5273 - val_loss: 25149.2344 - val_rmse: 25202.8281\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20009.0234 - rmse: 20009.0234\n",
      "Epoch 38: val_loss did not improve from 20297.04883\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19989.8164 - rmse: 19978.4531 - val_loss: 28102.0195 - val_rmse: 28124.7090\n",
      "Epoch 39/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21323.2773 - rmse: 21323.2773\n",
      "Epoch 39: val_loss improved from 20297.04883 to 19829.15820, saving model to ./ckpt/reg_lr005/val_rmse_19875.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21262.1250 - rmse: 21248.1641 - val_loss: 19829.1582 - val_rmse: 19874.5508\n",
      "Epoch 40/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21622.8770 - rmse: 21622.8770\n",
      "Epoch 40: val_loss did not improve from 19829.15820\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21535.4629 - rmse: 21552.0625 - val_loss: 21656.3691 - val_rmse: 21651.0879\n",
      "Epoch 41/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19189.4609 - rmse: 19189.4609\n",
      "Epoch 41: val_loss did not improve from 19829.15820\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19230.9453 - rmse: 19255.4805 - val_loss: 25982.0273 - val_rmse: 25988.1426\n",
      "Epoch 42/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20082.0430 - rmse: 20082.0430\n",
      "Epoch 42: val_loss did not improve from 19829.15820\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20298.4141 - rmse: 20264.4043 - val_loss: 21172.4688 - val_rmse: 21260.6895\n",
      "Epoch 43/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20188.2441 - rmse: 20188.2441\n",
      "Epoch 43: val_loss did not improve from 19829.15820\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20279.9902 - rmse: 20255.2578 - val_loss: 22688.6328 - val_rmse: 22693.5176\n",
      "Epoch 44/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19601.4453 - rmse: 19601.4453\n",
      "Epoch 44: val_loss did not improve from 19829.15820\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19521.8555 - rmse: 19590.1914 - val_loss: 20029.6504 - val_rmse: 20055.2441\n",
      "Epoch 45/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19263.1895 - rmse: 19263.1895\n",
      "Epoch 45: val_loss did not improve from 19829.15820\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19202.7871 - rmse: 19221.1582 - val_loss: 25945.0352 - val_rmse: 25932.2461\n",
      "Epoch 46/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19295.9668 - rmse: 19295.9668\n",
      "Epoch 46: val_loss did not improve from 19829.15820\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19197.7617 - rmse: 19189.3535 - val_loss: 26904.7227 - val_rmse: 26917.3086\n",
      "Epoch 47/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19712.1621 - rmse: 19712.1621\n",
      "Epoch 47: val_loss improved from 19829.15820 to 19222.24414, saving model to ./ckpt/reg_lr005/val_rmse_19234.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19765.1055 - rmse: 19813.0176 - val_loss: 19222.2441 - val_rmse: 19233.9570\n",
      "Epoch 48/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18927.8359 - rmse: 18927.8359\n",
      "Epoch 48: val_loss did not improve from 19222.24414\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18867.1074 - rmse: 18849.7324 - val_loss: 21918.0957 - val_rmse: 21926.0703\n",
      "Epoch 49/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19140.2168 - rmse: 19140.2168\n",
      "Epoch 49: val_loss did not improve from 19222.24414\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19059.6250 - rmse: 19029.3320 - val_loss: 30041.4883 - val_rmse: 30091.9727\n",
      "Epoch 50/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18338.5801 - rmse: 18338.5801\n",
      "Epoch 50: val_loss did not improve from 19222.24414\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18308.0215 - rmse: 18289.9492 - val_loss: 20500.0898 - val_rmse: 20543.2832\n",
      "Epoch 51/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17917.3418 - rmse: 17917.3418\n",
      "Epoch 51: val_loss did not improve from 19222.24414\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17872.0645 - rmse: 17886.6934 - val_loss: 21722.1699 - val_rmse: 21724.8086\n",
      "Epoch 52/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20782.3633 - rmse: 20782.3633\n",
      "Epoch 52: val_loss did not improve from 19222.24414\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21039.3711 - rmse: 21070.9961 - val_loss: 32777.9023 - val_rmse: 32788.5898\n",
      "Epoch 53/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19326.0586 - rmse: 19326.0586\n",
      "Epoch 53: val_loss did not improve from 19222.24414\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19199.4824 - rmse: 19197.4023 - val_loss: 32733.2812 - val_rmse: 32749.4004\n",
      "Epoch 54/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18503.1523 - rmse: 18503.1523\n",
      "Epoch 54: val_loss improved from 19222.24414 to 19069.56055, saving model to ./ckpt/reg_lr005/val_rmse_19113.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18503.5566 - rmse: 18513.4941 - val_loss: 19069.5605 - val_rmse: 19113.0332\n",
      "Epoch 55/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19025.7090 - rmse: 19051.5742\n",
      "Epoch 55: val_loss did not improve from 19069.56055\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19025.7090 - rmse: 19051.5742 - val_loss: 29000.8105 - val_rmse: 28987.7773\n",
      "Epoch 56/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19220.6426 - rmse: 19220.6426\n",
      "Epoch 56: val_loss did not improve from 19069.56055\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19186.7520 - rmse: 19185.7227 - val_loss: 23813.2520 - val_rmse: 23828.1484\n",
      "Epoch 57/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18822.5801 - rmse: 18822.5801\n",
      "Epoch 57: val_loss did not improve from 19069.56055\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18746.0059 - rmse: 18751.4121 - val_loss: 23536.5859 - val_rmse: 23574.2500\n",
      "Epoch 58/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19005.8574 - rmse: 19005.8574\n",
      "Epoch 58: val_loss did not improve from 19069.56055\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19076.9863 - rmse: 19058.9570 - val_loss: 28053.7715 - val_rmse: 28066.2988\n",
      "Epoch 59/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17927.8555 - rmse: 17927.8555\n",
      "Epoch 59: val_loss did not improve from 19069.56055\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17998.7480 - rmse: 18031.2930 - val_loss: 19536.9043 - val_rmse: 19568.7715\n",
      "Epoch 60/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19506.5879 - rmse: 19506.5879\n",
      "Epoch 60: val_loss did not improve from 19069.56055\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19294.7754 - rmse: 19265.7109 - val_loss: 29906.1484 - val_rmse: 29923.9629\n",
      "Epoch 61/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17293.7305 - rmse: 17293.7305\n",
      "Epoch 61: val_loss did not improve from 19069.56055\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17193.1074 - rmse: 17188.1484 - val_loss: 23324.6191 - val_rmse: 23323.2773\n",
      "Epoch 62/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17208.3574 - rmse: 17208.3574\n",
      "Epoch 62: val_loss did not improve from 19069.56055\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17333.7871 - rmse: 17422.7402 - val_loss: 28769.9570 - val_rmse: 28684.5508\n",
      "Epoch 63/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18781.7383 - rmse: 18781.7383\n",
      "Epoch 63: val_loss did not improve from 19069.56055\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18729.1641 - rmse: 18723.6934 - val_loss: 20214.8379 - val_rmse: 20209.9844\n",
      "Epoch 64/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18034.3594 - rmse: 18034.3594\n",
      "Epoch 64: val_loss did not improve from 19069.56055\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18039.6074 - rmse: 18060.3555 - val_loss: 28602.9160 - val_rmse: 28615.5781\n",
      "Epoch 65/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17806.1875 - rmse: 17806.1875\n",
      "Epoch 65: val_loss improved from 19069.56055 to 18388.23242, saving model to ./ckpt/reg_lr005/val_rmse_18372.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17983.2617 - rmse: 18068.6328 - val_loss: 18388.2324 - val_rmse: 18372.0117\n",
      "Epoch 66/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17377.1406 - rmse: 17377.1406\n",
      "Epoch 66: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17283.8047 - rmse: 17268.5195 - val_loss: 19613.8164 - val_rmse: 19555.6523\n",
      "Epoch 67/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18007.0117 - rmse: 18042.5410\n",
      "Epoch 67: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18007.0117 - rmse: 18042.5410 - val_loss: 22891.3320 - val_rmse: 22935.0801\n",
      "Epoch 68/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17422.0430 - rmse: 17422.0430\n",
      "Epoch 68: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 17434.8320 - rmse: 17495.6016 - val_loss: 40155.8477 - val_rmse: 40186.3516\n",
      "Epoch 69/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18036.7930 - rmse: 18036.7930\n",
      "Epoch 69: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18032.9941 - rmse: 18030.7461 - val_loss: 32776.0547 - val_rmse: 32752.6250\n",
      "Epoch 70/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17931.6035 - rmse: 17931.6035\n",
      "Epoch 70: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17840.6660 - rmse: 17838.6152 - val_loss: 23703.7617 - val_rmse: 23707.7207\n",
      "Epoch 71/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17749.3340 - rmse: 17749.3340\n",
      "Epoch 71: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17988.0918 - rmse: 17997.0117 - val_loss: 26178.8086 - val_rmse: 26131.7363\n",
      "Epoch 72/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17046.9141 - rmse: 17046.9141\n",
      "Epoch 72: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16971.8320 - rmse: 17002.1465 - val_loss: 20232.0117 - val_rmse: 20199.3359\n",
      "Epoch 73/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16909.2207 - rmse: 16909.2207\n",
      "Epoch 73: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16879.7949 - rmse: 16867.0410 - val_loss: 28478.7969 - val_rmse: 28514.0137\n",
      "Epoch 74/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18350.4961 - rmse: 18350.4961\n",
      "Epoch 74: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18253.9707 - rmse: 18244.2539 - val_loss: 33469.6328 - val_rmse: 33446.0312\n",
      "Epoch 75/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18327.1016 - rmse: 18327.1016\n",
      "Epoch 75: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18331.5176 - rmse: 18319.4355 - val_loss: 21712.6152 - val_rmse: 21637.7129\n",
      "Epoch 76/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18176.6660 - rmse: 18176.6660\n",
      "Epoch 76: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18132.0781 - rmse: 18124.9922 - val_loss: 29804.3496 - val_rmse: 29860.9062\n",
      "Epoch 77/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17833.4844 - rmse: 17833.4844\n",
      "Epoch 77: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17734.1914 - rmse: 17727.9238 - val_loss: 21830.3633 - val_rmse: 21795.1309\n",
      "Epoch 78/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16328.7363 - rmse: 16328.7363\n",
      "Epoch 78: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16165.8105 - rmse: 16177.0732 - val_loss: 21756.2012 - val_rmse: 21733.3613\n",
      "Epoch 79/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16459.9883 - rmse: 16459.9883\n",
      "Epoch 79: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16455.3789 - rmse: 16472.0684 - val_loss: 19032.6348 - val_rmse: 18998.6562\n",
      "Epoch 80/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16561.9688 - rmse: 16561.9688\n",
      "Epoch 80: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16558.0645 - rmse: 16555.7539 - val_loss: 22961.9512 - val_rmse: 22932.1973\n",
      "Epoch 81/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16367.5723 - rmse: 16367.5723\n",
      "Epoch 81: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16275.6826 - rmse: 16280.8252 - val_loss: 25192.8652 - val_rmse: 25213.7051\n",
      "Epoch 82/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17009.1094 - rmse: 17009.1094\n",
      "Epoch 82: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17030.1504 - rmse: 17036.1758 - val_loss: 32640.7188 - val_rmse: 32628.2695\n",
      "Epoch 83/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17941.3691 - rmse: 17941.3691\n",
      "Epoch 83: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17853.5918 - rmse: 17842.5137 - val_loss: 20599.1582 - val_rmse: 20569.7109\n",
      "Epoch 84/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16211.5625 - rmse: 16211.5625\n",
      "Epoch 84: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16202.6660 - rmse: 16190.5000 - val_loss: 29763.4551 - val_rmse: 29760.9141\n",
      "Epoch 85/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16637.9277 - rmse: 16637.9277\n",
      "Epoch 85: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16656.4844 - rmse: 16671.0820 - val_loss: 20100.3711 - val_rmse: 20063.0020\n",
      "Epoch 86/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19134.2910 - rmse: 19134.2910\n",
      "Epoch 86: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19064.2715 - rmse: 19051.4609 - val_loss: 35965.1992 - val_rmse: 35945.4180\n",
      "Epoch 87/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17382.5586 - rmse: 17382.5586\n",
      "Epoch 87: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17301.0156 - rmse: 17294.4277 - val_loss: 20384.4199 - val_rmse: 20339.6836\n",
      "Epoch 88/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16178.4004 - rmse: 16185.4873\n",
      "Epoch 88: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16178.4004 - rmse: 16185.4873 - val_loss: 18833.3633 - val_rmse: 18781.8965\n",
      "Epoch 89/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16568.6816 - rmse: 16568.6816\n",
      "Epoch 89: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16496.5430 - rmse: 16489.6328 - val_loss: 20010.8750 - val_rmse: 20014.3809\n",
      "Epoch 90/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16531.9473 - rmse: 16531.9473\n",
      "Epoch 90: val_loss did not improve from 18388.23242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16559.1406 - rmse: 16580.6270 - val_loss: 29021.1836 - val_rmse: 28988.6152\n",
      "Epoch 90: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:17:40.016582: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 106744.1484 - rmse: 106751.9688\n",
      "Epoch 1: val_loss improved from inf to 61024.97266, saving model to ./ckpt/reg_lr005/val_rmse_60168.hdf5\n",
      "70/70 [==============================] - 2s 20ms/step - loss: 106744.1484 - rmse: 106751.9688 - val_loss: 61024.9727 - val_rmse: 60168.0117\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 58422.2852 - rmse: 58422.2852"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:17:41.529579: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 53800.0938 - rmse: 53800.0938\n",
      "Epoch 2: val_loss improved from 61024.97266 to 47552.58984, saving model to ./ckpt/reg_lr005/val_rmse_46751.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 53976.9727 - rmse: 53954.0781 - val_loss: 47552.5898 - val_rmse: 46750.7148\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 46391.2695 - rmse: 46391.2695\n",
      "Epoch 3: val_loss improved from 47552.58984 to 43133.95703, saving model to ./ckpt/reg_lr005/val_rmse_42572.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 46279.9023 - rmse: 46239.8008 - val_loss: 43133.9570 - val_rmse: 42572.1523\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 43250.1914 - rmse: 43250.1914\n",
      "Epoch 4: val_loss improved from 43133.95703 to 40873.59766, saving model to ./ckpt/reg_lr005/val_rmse_40463.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 43165.4844 - rmse: 43195.9258 - val_loss: 40873.5977 - val_rmse: 40462.7617\n",
      "Epoch 5/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 42791.8477 - rmse: 42791.8477\n",
      "Epoch 5: val_loss improved from 40873.59766 to 39290.38281, saving model to ./ckpt/reg_lr005/val_rmse_38890.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 43054.7109 - rmse: 43171.2656 - val_loss: 39290.3828 - val_rmse: 38890.1055\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 41795.6953 - rmse: 41750.3789\n",
      "Epoch 6: val_loss did not improve from 39290.38281\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 41795.6953 - rmse: 41750.3789 - val_loss: 42401.3320 - val_rmse: 41803.0664\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 39875.2578 - rmse: 39875.2578\n",
      "Epoch 7: val_loss improved from 39290.38281 to 35551.13281, saving model to ./ckpt/reg_lr005/val_rmse_35154.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 39899.1367 - rmse: 39783.4648 - val_loss: 35551.1328 - val_rmse: 35154.1250\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 37089.5703 - rmse: 37089.5703\n",
      "Epoch 8: val_loss did not improve from 35551.13281\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 36975.9375 - rmse: 36908.7305 - val_loss: 36024.3945 - val_rmse: 35737.1836\n",
      "Epoch 9/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 36083.0938 - rmse: 36083.0938\n",
      "Epoch 9: val_loss improved from 35551.13281 to 32404.33594, saving model to ./ckpt/reg_lr005/val_rmse_32081.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 36078.6562 - rmse: 36076.0352 - val_loss: 32404.3359 - val_rmse: 32081.3262\n",
      "Epoch 10/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 36179.1992 - rmse: 36179.1992\n",
      "Epoch 10: val_loss did not improve from 32404.33594\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 36041.8789 - rmse: 36028.4844 - val_loss: 43048.5312 - val_rmse: 42583.2070\n",
      "Epoch 11/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 35518.1953 - rmse: 35518.1953\n",
      "Epoch 11: val_loss did not improve from 32404.33594\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 35579.6758 - rmse: 35544.0547 - val_loss: 34680.3203 - val_rmse: 34482.5391\n",
      "Epoch 12/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 35114.2148 - rmse: 35114.2148\n",
      "Epoch 12: val_loss did not improve from 32404.33594\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 35525.5391 - rmse: 35587.6953 - val_loss: 33217.7383 - val_rmse: 33054.9297\n",
      "Epoch 13/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 34850.0430 - rmse: 34850.0430\n",
      "Epoch 13: val_loss improved from 32404.33594 to 28421.45898, saving model to ./ckpt/reg_lr005/val_rmse_28169.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 34756.3281 - rmse: 34733.2617 - val_loss: 28421.4590 - val_rmse: 28168.7441\n",
      "Epoch 14/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 30060.9375 - rmse: 30060.9375\n",
      "Epoch 14: val_loss improved from 28421.45898 to 26851.19141, saving model to ./ckpt/reg_lr005/val_rmse_26583.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 30669.1465 - rmse: 30730.2422 - val_loss: 26851.1914 - val_rmse: 26582.5312\n",
      "Epoch 15/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 31462.6250 - rmse: 31462.6250\n",
      "Epoch 15: val_loss did not improve from 26851.19141\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 31483.0664 - rmse: 31566.2402 - val_loss: 35107.5430 - val_rmse: 34776.3984\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 30296.9922 - rmse: 30296.9922\n",
      "Epoch 16: val_loss did not improve from 26851.19141\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 30223.0918 - rmse: 30179.3828 - val_loss: 30621.5723 - val_rmse: 30328.0840\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 30599.1582 - rmse: 30599.1582\n",
      "Epoch 17: val_loss improved from 26851.19141 to 24430.88086, saving model to ./ckpt/reg_lr005/val_rmse_24220.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 30339.6094 - rmse: 30370.2969 - val_loss: 24430.8809 - val_rmse: 24219.9336\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 28309.9980 - rmse: 28342.8047\n",
      "Epoch 18: val_loss did not improve from 24430.88086\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 28309.9980 - rmse: 28342.8047 - val_loss: 24772.9609 - val_rmse: 24528.8203\n",
      "Epoch 19/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 29747.6973 - rmse: 29747.6973\n",
      "Epoch 19: val_loss improved from 24430.88086 to 23914.22266, saving model to ./ckpt/reg_lr005/val_rmse_23684.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 29619.9062 - rmse: 29608.0957 - val_loss: 23914.2227 - val_rmse: 23684.4844\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 26017.9082 - rmse: 26017.9082\n",
      "Epoch 20: val_loss did not improve from 23914.22266\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26543.2578 - rmse: 26550.2949 - val_loss: 30569.1309 - val_rmse: 30475.1445\n",
      "Epoch 21/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 30314.5938 - rmse: 30314.5938\n",
      "Epoch 21: val_loss did not improve from 23914.22266\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 30344.4707 - rmse: 30382.0508 - val_loss: 30017.8164 - val_rmse: 29717.9004\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27597.1426 - rmse: 27597.1426\n",
      "Epoch 22: val_loss improved from 23914.22266 to 22315.27148, saving model to ./ckpt/reg_lr005/val_rmse_22130.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27563.5410 - rmse: 27524.0762 - val_loss: 22315.2715 - val_rmse: 22130.0820\n",
      "Epoch 23/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25483.2500 - rmse: 25483.2500\n",
      "Epoch 23: val_loss did not improve from 22315.27148\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 25687.6738 - rmse: 25715.2559 - val_loss: 25320.3652 - val_rmse: 25192.8945\n",
      "Epoch 24/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 29378.2129 - rmse: 29378.2129\n",
      "Epoch 24: val_loss did not improve from 22315.27148\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 29191.7383 - rmse: 29139.1992 - val_loss: 26380.7598 - val_rmse: 26109.0742\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24688.1523 - rmse: 24730.8203\n",
      "Epoch 25: val_loss did not improve from 22315.27148\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24688.1523 - rmse: 24730.8203 - val_loss: 25951.2480 - val_rmse: 25680.2578\n",
      "Epoch 26/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 24152.8281 - rmse: 24152.8281\n",
      "Epoch 26: val_loss did not improve from 22315.27148\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 24114.5508 - rmse: 24136.6367 - val_loss: 23329.5449 - val_rmse: 23134.9824\n",
      "Epoch 27/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 24519.1406 - rmse: 24519.1406\n",
      "Epoch 27: val_loss did not improve from 22315.27148\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 24557.0156 - rmse: 24547.0879 - val_loss: 23589.9961 - val_rmse: 23420.3848\n",
      "Epoch 28/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25021.1348 - rmse: 25021.1348\n",
      "Epoch 28: val_loss improved from 22315.27148 to 20104.32617, saving model to ./ckpt/reg_lr005/val_rmse_19954.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 24931.8184 - rmse: 24958.6680 - val_loss: 20104.3262 - val_rmse: 19953.9453\n",
      "Epoch 29/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22423.4668 - rmse: 22423.4668\n",
      "Epoch 29: val_loss did not improve from 20104.32617\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22445.5547 - rmse: 22464.2871 - val_loss: 22126.9004 - val_rmse: 21957.0762\n",
      "Epoch 30/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 24464.7422 - rmse: 24464.7422\n",
      "Epoch 30: val_loss did not improve from 20104.32617\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 24219.2305 - rmse: 24188.7559 - val_loss: 27894.2285 - val_rmse: 27659.9941\n",
      "Epoch 31/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23611.0586 - rmse: 23611.0586\n",
      "Epoch 31: val_loss improved from 20104.32617 to 19500.73047, saving model to ./ckpt/reg_lr005/val_rmse_19379.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23583.5547 - rmse: 23637.1406 - val_loss: 19500.7305 - val_rmse: 19379.3652\n",
      "Epoch 32/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22710.5020 - rmse: 22727.8926\n",
      "Epoch 32: val_loss did not improve from 19500.73047\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22710.5020 - rmse: 22727.8926 - val_loss: 21581.9980 - val_rmse: 21399.3438\n",
      "Epoch 33/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23369.6367 - rmse: 23369.6367\n",
      "Epoch 33: val_loss did not improve from 19500.73047\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23387.8848 - rmse: 23369.7578 - val_loss: 21929.5195 - val_rmse: 21785.9785\n",
      "Epoch 34/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21227.5098 - rmse: 21227.5098\n",
      "Epoch 34: val_loss improved from 19500.73047 to 18285.89062, saving model to ./ckpt/reg_lr005/val_rmse_18202.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21128.2754 - rmse: 21120.5527 - val_loss: 18285.8906 - val_rmse: 18202.4531\n",
      "Epoch 35/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22232.8105 - rmse: 22232.8105\n",
      "Epoch 35: val_loss did not improve from 18285.89062\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22317.3125 - rmse: 22298.1797 - val_loss: 24210.2461 - val_rmse: 24060.0059\n",
      "Epoch 36/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21461.8770 - rmse: 21461.8770\n",
      "Epoch 36: val_loss did not improve from 18285.89062\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21490.7090 - rmse: 21507.7617 - val_loss: 21644.9766 - val_rmse: 21460.6133\n",
      "Epoch 37/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21599.3418 - rmse: 21599.3418\n",
      "Epoch 37: val_loss did not improve from 18285.89062\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21564.6953 - rmse: 21544.2051 - val_loss: 21996.9961 - val_rmse: 21817.6523\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22154.4277 - rmse: 22154.4277\n",
      "Epoch 38: val_loss improved from 18285.89062 to 17601.32812, saving model to ./ckpt/reg_lr005/val_rmse_17534.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22130.6387 - rmse: 22116.5664 - val_loss: 17601.3281 - val_rmse: 17534.2246\n",
      "Epoch 39/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21265.7500 - rmse: 21265.7500\n",
      "Epoch 39: val_loss did not improve from 17601.32812\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21170.5977 - rmse: 21185.2012 - val_loss: 21146.4355 - val_rmse: 21034.5586\n",
      "Epoch 40/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21982.1719 - rmse: 21982.1719\n",
      "Epoch 40: val_loss did not improve from 17601.32812\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21955.8633 - rmse: 21940.3027 - val_loss: 31348.9844 - val_rmse: 31185.5605\n",
      "Epoch 41/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21578.7129 - rmse: 21578.7129\n",
      "Epoch 41: val_loss improved from 17601.32812 to 17429.05469, saving model to ./ckpt/reg_lr005/val_rmse_17355.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21604.5059 - rmse: 21604.4824 - val_loss: 17429.0547 - val_rmse: 17355.4258\n",
      "Epoch 42/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22537.3535 - rmse: 22537.3535\n",
      "Epoch 42: val_loss did not improve from 17429.05469\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22514.0723 - rmse: 22500.3027 - val_loss: 19172.3203 - val_rmse: 19110.4688\n",
      "Epoch 43/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20009.6113 - rmse: 20009.6113\n",
      "Epoch 43: val_loss improved from 17429.05469 to 17087.08594, saving model to ./ckpt/reg_lr005/val_rmse_17032.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20011.8066 - rmse: 20029.4355 - val_loss: 17087.0859 - val_rmse: 17031.6582\n",
      "Epoch 44/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19531.2246 - rmse: 19531.2246\n",
      "Epoch 44: val_loss did not improve from 17087.08594\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19474.4590 - rmse: 19477.3789 - val_loss: 17353.0703 - val_rmse: 17298.8477\n",
      "Epoch 45/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19961.4863 - rmse: 19961.4863\n",
      "Epoch 45: val_loss did not improve from 17087.08594\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19742.8691 - rmse: 19730.6191 - val_loss: 24519.8262 - val_rmse: 24376.4902\n",
      "Epoch 46/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20801.8379 - rmse: 20801.8379\n",
      "Epoch 46: val_loss did not improve from 17087.08594\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20750.6250 - rmse: 20737.3027 - val_loss: 22101.0176 - val_rmse: 21972.7168\n",
      "Epoch 47/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19673.4375 - rmse: 19673.4375\n",
      "Epoch 47: val_loss did not improve from 17087.08594\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19876.2168 - rmse: 19950.9121 - val_loss: 18842.0254 - val_rmse: 18787.4336\n",
      "Epoch 48/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19048.0039 - rmse: 19048.0039\n",
      "Epoch 48: val_loss did not improve from 17087.08594\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18946.6035 - rmse: 18906.9336 - val_loss: 20797.7344 - val_rmse: 20671.8379\n",
      "Epoch 49/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19802.4258 - rmse: 19802.4258\n",
      "Epoch 49: val_loss did not improve from 17087.08594\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19768.6094 - rmse: 19799.5625 - val_loss: 17724.4297 - val_rmse: 17614.5898\n",
      "Epoch 50/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18892.7031 - rmse: 18892.7031\n",
      "Epoch 50: val_loss did not improve from 17087.08594\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18874.2520 - rmse: 18863.3398 - val_loss: 27689.5723 - val_rmse: 27540.9980\n",
      "Epoch 51/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20753.9492 - rmse: 20753.9492\n",
      "Epoch 51: val_loss did not improve from 17087.08594\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20797.6289 - rmse: 20813.0918 - val_loss: 18455.2598 - val_rmse: 18368.2246\n",
      "Epoch 52/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21996.9043 - rmse: 21996.9043\n",
      "Epoch 52: val_loss did not improve from 17087.08594\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21879.5371 - rmse: 21889.9492 - val_loss: 18538.8789 - val_rmse: 18406.6738\n",
      "Epoch 53/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17996.1816 - rmse: 17996.1816\n",
      "Epoch 53: val_loss did not improve from 17087.08594\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18058.8770 - rmse: 18070.6797 - val_loss: 18291.4336 - val_rmse: 18180.2129\n",
      "Epoch 54/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18904.1270 - rmse: 18904.1270\n",
      "Epoch 54: val_loss improved from 17087.08594 to 16675.26367, saving model to ./ckpt/reg_lr005/val_rmse_16590.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18888.1016 - rmse: 18878.6230 - val_loss: 16675.2637 - val_rmse: 16590.3008\n",
      "Epoch 55/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18462.2109 - rmse: 18463.1465\n",
      "Epoch 55: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18462.2109 - rmse: 18463.1465 - val_loss: 23556.2520 - val_rmse: 23458.9316\n",
      "Epoch 56/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21121.0176 - rmse: 21121.0176\n",
      "Epoch 56: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21174.1289 - rmse: 21188.2480 - val_loss: 19243.0332 - val_rmse: 19117.7695\n",
      "Epoch 57/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20088.5566 - rmse: 20088.5566\n",
      "Epoch 57: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19962.2734 - rmse: 19949.2832 - val_loss: 19977.5430 - val_rmse: 19867.2148\n",
      "Epoch 58/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18873.4590 - rmse: 18873.4590\n",
      "Epoch 58: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19128.6348 - rmse: 19313.0078 - val_loss: 20947.7930 - val_rmse: 20818.4648\n",
      "Epoch 59/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18927.8184 - rmse: 18927.8184\n",
      "Epoch 59: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18885.4863 - rmse: 18890.9160 - val_loss: 17417.6738 - val_rmse: 17272.7969\n",
      "Epoch 60/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17903.6855 - rmse: 17903.6855\n",
      "Epoch 60: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17904.4707 - rmse: 17928.2793 - val_loss: 17094.6797 - val_rmse: 16968.7441\n",
      "Epoch 61/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19418.5742 - rmse: 19418.5742\n",
      "Epoch 61: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19476.5508 - rmse: 19510.8359 - val_loss: 16995.9434 - val_rmse: 16908.5762\n",
      "Epoch 62/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19094.9082 - rmse: 19094.9082\n",
      "Epoch 62: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19119.9121 - rmse: 19108.5879 - val_loss: 21192.3672 - val_rmse: 21107.3066\n",
      "Epoch 63/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18200.1523 - rmse: 18200.1523\n",
      "Epoch 63: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18292.0742 - rmse: 18329.3848 - val_loss: 28186.8301 - val_rmse: 28048.4375\n",
      "Epoch 64/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19123.4121 - rmse: 19123.4121\n",
      "Epoch 64: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19176.8340 - rmse: 19197.3301 - val_loss: 18297.0117 - val_rmse: 18133.9336\n",
      "Epoch 65/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18445.0195 - rmse: 18445.0195\n",
      "Epoch 65: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18422.3418 - rmse: 18408.8770 - val_loss: 19316.4062 - val_rmse: 19206.5762\n",
      "Epoch 66/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18425.4727 - rmse: 18425.4727\n",
      "Epoch 66: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18463.5312 - rmse: 18486.0410 - val_loss: 17312.5352 - val_rmse: 17289.4922\n",
      "Epoch 67/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18173.2676 - rmse: 18173.2676\n",
      "Epoch 67: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18099.4805 - rmse: 18085.5332 - val_loss: 18993.6465 - val_rmse: 18862.2441\n",
      "Epoch 68/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18706.4551 - rmse: 18699.8730\n",
      "Epoch 68: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18706.4551 - rmse: 18699.8730 - val_loss: 16825.8008 - val_rmse: 16736.6211\n",
      "Epoch 69/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17852.9941 - rmse: 17852.9941\n",
      "Epoch 69: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17806.4707 - rmse: 17800.8418 - val_loss: 17515.1309 - val_rmse: 17370.7754\n",
      "Epoch 70/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16939.8203 - rmse: 16939.8203\n",
      "Epoch 70: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16902.8848 - rmse: 16873.3887 - val_loss: 18553.1758 - val_rmse: 18430.9102\n",
      "Epoch 71/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17997.6426 - rmse: 17997.6426\n",
      "Epoch 71: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18005.6035 - rmse: 17970.0703 - val_loss: 25646.5215 - val_rmse: 25527.2812\n",
      "Epoch 72/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17738.8457 - rmse: 17738.8457\n",
      "Epoch 72: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17730.4160 - rmse: 17703.8418 - val_loss: 18118.4355 - val_rmse: 18024.1074\n",
      "Epoch 73/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19501.5117 - rmse: 19501.5117\n",
      "Epoch 73: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19327.7949 - rmse: 19299.9551 - val_loss: 25526.7539 - val_rmse: 25417.8086\n",
      "Epoch 74/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17230.2441 - rmse: 17230.2441\n",
      "Epoch 74: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17244.1172 - rmse: 17252.3203 - val_loss: 21324.8027 - val_rmse: 21283.8047\n",
      "Epoch 75/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17420.1113 - rmse: 17420.1113\n",
      "Epoch 75: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17275.9629 - rmse: 17334.6211 - val_loss: 18525.9980 - val_rmse: 18382.3652\n",
      "Epoch 76/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16417.0742 - rmse: 16417.0742\n",
      "Epoch 76: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16419.7598 - rmse: 16421.3477 - val_loss: 19170.0410 - val_rmse: 18994.1348\n",
      "Epoch 77/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16669.1426 - rmse: 16669.1426\n",
      "Epoch 77: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16608.2617 - rmse: 16606.6172 - val_loss: 17701.9082 - val_rmse: 17650.3809\n",
      "Epoch 78/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17633.6875 - rmse: 17633.6875\n",
      "Epoch 78: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17827.6934 - rmse: 17873.3516 - val_loss: 22573.9531 - val_rmse: 22464.9961\n",
      "Epoch 79/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17360.3105 - rmse: 17360.3105\n",
      "Epoch 79: val_loss did not improve from 16675.26367\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17374.5801 - rmse: 17395.2070 - val_loss: 22026.0859 - val_rmse: 21982.6621\n",
      "Epoch 79: early stopping\n",
      "Finish 10-fold cross validation\n",
      "Best performing model has 47107.1640625 validation loss (RMSE)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# modify to save ckpt for each test\n",
    "ckpt = os.path.join(ckpt_path, \"val_rmse_{val_rmse:.0f}.hdf5\")\n",
    "\n",
    "# training params\n",
    "epochs = epochs\n",
    "lr = lr\n",
    "\n",
    "# the k for k fold CV\n",
    "n_split = 10\n",
    "\n",
    "# for recording best performance\n",
    "min_loss = np.inf\n",
    "best_history = None\n",
    "\n",
    "'''\n",
    "k-fold cross validation\n",
    "Save the best model using validation accuracy as metric\n",
    "Print the global best performace when finished\n",
    "'''\n",
    "for train_index, test_index in KFold(n_split).split(train_examples):\n",
    "\n",
    "    x_train, x_vad = train_examples[train_index], train_examples[test_index]\n",
    "    y_train, y_vad = train_labels[train_index], train_labels[test_index]\n",
    "\n",
    "    model=create_reg_model(lr)\n",
    "  \n",
    "    # callbacks\n",
    "    checkpoint_filepath = ckpt\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='auto',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=25,\n",
    "        verbose=1,\n",
    "        mode='auto',\n",
    "    )\n",
    "\n",
    "    # Model weights are saved at the end of every epoch, if it's the best seen so far.\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_vad, y_vad),\n",
    "                        callbacks=[model_checkpoint_callback, early_stopping_callback])\n",
    "\n",
    "    val_loss = max(history.history['val_loss'])\n",
    "    \n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        best_history = history\n",
    "        # print('Best acc so far. Saving params...\\n')\n",
    "\n",
    "print('Finish {}-fold cross validation'.format(n_split))\n",
    "print('Best performing model has {} validation loss (RMSE)'.format(min_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:34:25.475588: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/evantilu/miniforge3/envs/6998_DL_tf/lib/python3.8/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x294d3d0d0>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = best_history\n",
    "\n",
    "train_rmse = history.history['rmse']\n",
    "val_rmse = history.history['val_rmse']\n",
    "\n",
    "epochs_range = range(len(history.history['loss']))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_rmse, label='Training RMSE')\n",
    "plt.plot(epochs_range, val_rmse, label='Validation RMSE')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "\n",
    "'''How far are predictions from real values?'''\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def format_tick_labels(x, pos):\n",
    "    return '{:.0f}k'.format(x/1000)\n",
    "\n",
    "predictions = model.predict(test_examples)\n",
    "\n",
    "xlims = [0, max(test_labels)*1.1]\n",
    "ylims = [0, max(predictions)*1.1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(test_labels, predictions)\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "ax.set_xlim(xlims)\n",
    "ax.set_ylim(ylims)\n",
    "ax.set_xlabel('Actual Price')\n",
    "ax.set_ylabel('Predicted Price')\n",
    "\n",
    "ax.plot(xlims, ylims, 'r-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14 [======================>.......] - ETA: 0s - loss: 16660.2539 - rmse: 16660.2539"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:35:11.097369: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 11ms/step - loss: 16477.6797 - rmse: 16472.5762\n",
      "\n",
      "evaluation on test set:\n",
      "loss (RMSE) = 16477.67969\n"
     ]
    }
   ],
   "source": [
    "model = create_reg_model()\n",
    "model.load_weights(os.path.join(ckpt_path, \"val_rmse_14109.hdf5\"))\n",
    "\n",
    "loss, acc = model.evaluate(test_examples, test_labels)\n",
    "\n",
    "print('\\nevaluation on test set:\\nloss (RMSE) = {:.5f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29cc21816e506614f017a9125cfdb1f5dc655865e499c52ef5f5406a40d25695"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
