{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold CV with 3-layer CNN avg pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and env settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.min_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables/parameters used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../../data/home_sale_data_324_features_10_class.csv'\n",
    "\n",
    "ckpt_path = \"./ckpt/10_class_lr005/\"\n",
    "os.makedirs(ckpt_path, exist_ok=True)\n",
    "\n",
    "num_classes = 10\n",
    "lr = 0.005\n",
    "epochs = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>TotalSF</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>Total_Bathrooms</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>YrBltAndRemod</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>Foundation_PConc</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>hasfireplace</th>\n",
       "      <th>ExterQual_Gd</th>\n",
       "      <th>BsmtQual_Ex</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>HeatingQC_Ex</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>Total_porch_sf</th>\n",
       "      <th>BsmtFinType1_GLQ</th>\n",
       "      <th>KitchenQual_Ex</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>GarageFinish_Fin</th>\n",
       "      <th>...</th>\n",
       "      <th>BsmtExposure_No</th>\n",
       "      <th>Neighborhood_OldTown</th>\n",
       "      <th>Foundation_BrkTil</th>\n",
       "      <th>GarageFinish_None</th>\n",
       "      <th>GarageCond_None</th>\n",
       "      <th>GarageQual_None</th>\n",
       "      <th>GarageType_None</th>\n",
       "      <th>MSSubClass_30</th>\n",
       "      <th>LotShape_Reg</th>\n",
       "      <th>PavedDrive_N</th>\n",
       "      <th>Foundation_CBlock</th>\n",
       "      <th>MSZoning_RM</th>\n",
       "      <th>HeatingQC_TA</th>\n",
       "      <th>CentralAir_N</th>\n",
       "      <th>GarageType_Detchd</th>\n",
       "      <th>MasVnrType_None</th>\n",
       "      <th>GarageFinish_Unf</th>\n",
       "      <th>BsmtQual_TA</th>\n",
       "      <th>FireplaceQu_None</th>\n",
       "      <th>KitchenQual_TA</th>\n",
       "      <th>ExterQual_TA</th>\n",
       "      <th>dummy_1</th>\n",
       "      <th>dummy_2</th>\n",
       "      <th>dummy_3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.584815</td>\n",
       "      <td>0.111736</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.164976</td>\n",
       "      <td>0.456989</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495831</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600315</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.282859</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>p7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.543753</td>\n",
       "      <td>0.231395</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.059256</td>\n",
       "      <td>0.384409</td>\n",
       "      <td>0.615789</td>\n",
       "      <td>0.356831</td>\n",
       "      <td>0.595861</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600315</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.258795</td>\n",
       "      <td>0.404908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.390999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>p3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.515934</td>\n",
       "      <td>0.268954</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.164976</td>\n",
       "      <td>0.293683</td>\n",
       "      <td>0.963158</td>\n",
       "      <td>0.414847</td>\n",
       "      <td>0.566380</td>\n",
       "      <td>0.971015</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.516936</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.168717</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>p3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.350917</td>\n",
       "      <td>0.174730</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.163872</td>\n",
       "      <td>0.491936</td>\n",
       "      <td>0.484211</td>\n",
       "      <td>0.269495</td>\n",
       "      <td>0.389345</td>\n",
       "      <td>0.644928</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.657933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>p7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.684929</td>\n",
       "      <td>0.455353</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.252530</td>\n",
       "      <td>0.313844</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.332190</td>\n",
       "      <td>0.475164</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.740833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>p2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OverallQual  GrLivArea   TotalSF  GarageCars  Total_Bathrooms  \\\n",
       "369      0.333333   0.584815  0.111736         0.4         0.164976   \n",
       "576      0.555556   0.543753  0.231395         0.4         0.059256   \n",
       "2532     0.555556   0.515934  0.268954         0.4         0.164976   \n",
       "1724     0.444444   0.350917  0.174730         0.4         0.163872   \n",
       "2842     0.666667   0.684929  0.455353         0.4         0.252530   \n",
       "\n",
       "      GarageArea  YrBltAndRemod  TotalBsmtSF  1stFlrSF  YearBuilt  FullBath  \\\n",
       "369     0.456989       0.463158     0.000000  0.495831   0.630435      0.50   \n",
       "576     0.384409       0.615789     0.356831  0.595861   0.637681      0.25   \n",
       "2532    0.293683       0.963158     0.414847  0.566380   0.971015      0.50   \n",
       "1724    0.491936       0.484211     0.269495  0.389345   0.644928      0.25   \n",
       "2842    0.313844       0.857895     0.332190  0.475164   0.898551      0.50   \n",
       "\n",
       "      YearRemodAdd  Foundation_PConc  TotRmsAbvGrd  hasfireplace  \\\n",
       "369       0.150000               0.0      0.600315           1.0   \n",
       "576       0.616667               0.0      0.600315           1.0   \n",
       "2532      0.950000               1.0      0.516936           1.0   \n",
       "1724      0.183333               0.0      0.421336           0.0   \n",
       "2842      0.783333               1.0      0.740833           1.0   \n",
       "\n",
       "      ExterQual_Gd  BsmtQual_Ex  Fireplaces  HeatingQC_Ex  MasVnrArea  \\\n",
       "369            0.0          0.0    0.293793           0.0    0.000000   \n",
       "576            0.0          0.0    0.549061           1.0    0.258795   \n",
       "2532           1.0          0.0    0.293793           1.0    0.000000   \n",
       "1724           0.0          0.0    0.000000           1.0    0.000000   \n",
       "2842           1.0          0.0    0.293793           1.0    0.000000   \n",
       "\n",
       "      Total_porch_sf  BsmtFinType1_GLQ  KitchenQual_Ex  OpenPorchSF  \\\n",
       "369         0.077734               0.0             0.0     0.282859   \n",
       "576         0.404908               0.0             0.0     0.390999   \n",
       "2532        0.046366               0.0             0.0     0.168717   \n",
       "1724        0.180810               1.0             0.0     0.657933   \n",
       "2842        0.272142               0.0             0.0     0.253387   \n",
       "\n",
       "      GarageFinish_Fin  ...  BsmtExposure_No  Neighborhood_OldTown  \\\n",
       "369                1.0  ...              0.0                   0.0   \n",
       "576                0.0  ...              1.0                   0.0   \n",
       "2532               1.0  ...              1.0                   0.0   \n",
       "1724               0.0  ...              1.0                   0.0   \n",
       "2842               0.0  ...              1.0                   0.0   \n",
       "\n",
       "      Foundation_BrkTil  GarageFinish_None  GarageCond_None  GarageQual_None  \\\n",
       "369                 0.0                0.0              0.0              0.0   \n",
       "576                 0.0                0.0              0.0              0.0   \n",
       "2532                0.0                0.0              0.0              0.0   \n",
       "1724                0.0                0.0              0.0              0.0   \n",
       "2842                0.0                0.0              0.0              0.0   \n",
       "\n",
       "      GarageType_None  MSSubClass_30  LotShape_Reg  PavedDrive_N  \\\n",
       "369               0.0            0.0           1.0           1.0   \n",
       "576               0.0            0.0           0.0           0.0   \n",
       "2532              0.0            0.0           1.0           0.0   \n",
       "1724              0.0            0.0           0.0           0.0   \n",
       "2842              0.0            0.0           0.0           0.0   \n",
       "\n",
       "      Foundation_CBlock  MSZoning_RM  HeatingQC_TA  CentralAir_N  \\\n",
       "369                 1.0          0.0           1.0           0.0   \n",
       "576                 1.0          0.0           0.0           0.0   \n",
       "2532                0.0          0.0           0.0           0.0   \n",
       "1724                1.0          0.0           0.0           0.0   \n",
       "2842                0.0          0.0           0.0           0.0   \n",
       "\n",
       "      GarageType_Detchd  MasVnrType_None  GarageFinish_Unf  BsmtQual_TA  \\\n",
       "369                 1.0              1.0               0.0          0.0   \n",
       "576                 0.0              0.0               1.0          1.0   \n",
       "2532                0.0              1.0               0.0          0.0   \n",
       "1724                1.0              1.0               1.0          1.0   \n",
       "2842                0.0              1.0               1.0          0.0   \n",
       "\n",
       "      FireplaceQu_None  KitchenQual_TA  ExterQual_TA  dummy_1  dummy_2  \\\n",
       "369                0.0             1.0           1.0      0.0      0.0   \n",
       "576                0.0             0.0           1.0      0.0      0.0   \n",
       "2532               0.0             0.0           0.0      0.0      0.0   \n",
       "1724               1.0             1.0           1.0      0.0      0.0   \n",
       "2842               0.0             0.0           0.0      0.0      0.0   \n",
       "\n",
       "      dummy_3  label  \n",
       "369       0.0     p7  \n",
       "576       0.0     p3  \n",
       "2532      0.0     p3  \n",
       "1724      0.0     p7  \n",
       "2842      0.0     p2  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(file)\n",
    "\n",
    "'''suffle rows randomly'''\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "labels = data['label']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>TotalSF</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>Total_Bathrooms</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>YrBltAndRemod</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>Foundation_PConc</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>hasfireplace</th>\n",
       "      <th>ExterQual_Gd</th>\n",
       "      <th>BsmtQual_Ex</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>HeatingQC_Ex</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>Total_porch_sf</th>\n",
       "      <th>BsmtFinType1_GLQ</th>\n",
       "      <th>KitchenQual_Ex</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>GarageFinish_Fin</th>\n",
       "      <th>...</th>\n",
       "      <th>Neighborhood_IDOTRR</th>\n",
       "      <th>BsmtExposure_No</th>\n",
       "      <th>Neighborhood_OldTown</th>\n",
       "      <th>Foundation_BrkTil</th>\n",
       "      <th>GarageFinish_None</th>\n",
       "      <th>GarageCond_None</th>\n",
       "      <th>GarageQual_None</th>\n",
       "      <th>GarageType_None</th>\n",
       "      <th>MSSubClass_30</th>\n",
       "      <th>LotShape_Reg</th>\n",
       "      <th>PavedDrive_N</th>\n",
       "      <th>Foundation_CBlock</th>\n",
       "      <th>MSZoning_RM</th>\n",
       "      <th>HeatingQC_TA</th>\n",
       "      <th>CentralAir_N</th>\n",
       "      <th>GarageType_Detchd</th>\n",
       "      <th>MasVnrType_None</th>\n",
       "      <th>GarageFinish_Unf</th>\n",
       "      <th>BsmtQual_TA</th>\n",
       "      <th>FireplaceQu_None</th>\n",
       "      <th>KitchenQual_TA</th>\n",
       "      <th>ExterQual_TA</th>\n",
       "      <th>dummy_1</th>\n",
       "      <th>dummy_2</th>\n",
       "      <th>dummy_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.0</td>\n",
       "      <td>2911.0</td>\n",
       "      <td>2911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.565136</td>\n",
       "      <td>0.542763</td>\n",
       "      <td>0.293431</td>\n",
       "      <td>0.353143</td>\n",
       "      <td>0.200863</td>\n",
       "      <td>0.317285</td>\n",
       "      <td>0.660811</td>\n",
       "      <td>0.326706</td>\n",
       "      <td>0.487996</td>\n",
       "      <td>0.719569</td>\n",
       "      <td>0.391876</td>\n",
       "      <td>0.570892</td>\n",
       "      <td>0.447956</td>\n",
       "      <td>0.542786</td>\n",
       "      <td>0.512882</td>\n",
       "      <td>0.335967</td>\n",
       "      <td>0.087255</td>\n",
       "      <td>0.171718</td>\n",
       "      <td>0.511508</td>\n",
       "      <td>0.159954</td>\n",
       "      <td>0.203170</td>\n",
       "      <td>0.290622</td>\n",
       "      <td>0.069392</td>\n",
       "      <td>0.176542</td>\n",
       "      <td>0.245620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031261</td>\n",
       "      <td>0.652697</td>\n",
       "      <td>0.082102</td>\n",
       "      <td>0.106493</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.053590</td>\n",
       "      <td>0.047750</td>\n",
       "      <td>0.637582</td>\n",
       "      <td>0.073514</td>\n",
       "      <td>0.423222</td>\n",
       "      <td>0.158708</td>\n",
       "      <td>0.293370</td>\n",
       "      <td>0.066644</td>\n",
       "      <td>0.266919</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.421848</td>\n",
       "      <td>0.439368</td>\n",
       "      <td>0.487118</td>\n",
       "      <td>0.511852</td>\n",
       "      <td>0.616627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.155988</td>\n",
       "      <td>0.125193</td>\n",
       "      <td>0.119772</td>\n",
       "      <td>0.152244</td>\n",
       "      <td>0.133043</td>\n",
       "      <td>0.143976</td>\n",
       "      <td>0.242686</td>\n",
       "      <td>0.131930</td>\n",
       "      <td>0.131369</td>\n",
       "      <td>0.219351</td>\n",
       "      <td>0.138140</td>\n",
       "      <td>0.348189</td>\n",
       "      <td>0.497369</td>\n",
       "      <td>0.128554</td>\n",
       "      <td>0.499920</td>\n",
       "      <td>0.472409</td>\n",
       "      <td>0.282257</td>\n",
       "      <td>0.181727</td>\n",
       "      <td>0.499953</td>\n",
       "      <td>0.218350</td>\n",
       "      <td>0.163524</td>\n",
       "      <td>0.454127</td>\n",
       "      <td>0.254163</td>\n",
       "      <td>0.183129</td>\n",
       "      <td>0.430528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174051</td>\n",
       "      <td>0.476195</td>\n",
       "      <td>0.274568</td>\n",
       "      <td>0.308520</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.225245</td>\n",
       "      <td>0.213273</td>\n",
       "      <td>0.480781</td>\n",
       "      <td>0.261024</td>\n",
       "      <td>0.494155</td>\n",
       "      <td>0.365467</td>\n",
       "      <td>0.455385</td>\n",
       "      <td>0.249447</td>\n",
       "      <td>0.442425</td>\n",
       "      <td>0.488798</td>\n",
       "      <td>0.493939</td>\n",
       "      <td>0.496395</td>\n",
       "      <td>0.499920</td>\n",
       "      <td>0.499945</td>\n",
       "      <td>0.486292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.451636</td>\n",
       "      <td>0.213557</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.088659</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.247193</td>\n",
       "      <td>0.395003</td>\n",
       "      <td>0.590580</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.547807</td>\n",
       "      <td>0.282326</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.164976</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.652632</td>\n",
       "      <td>0.308172</td>\n",
       "      <td>0.481664</td>\n",
       "      <td>0.731884</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516936</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.621247</td>\n",
       "      <td>0.356447</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.252530</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.405490</td>\n",
       "      <td>0.582574</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.353218</td>\n",
       "      <td>0.327089</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 324 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       OverallQual    GrLivArea      TotalSF   GarageCars  Total_Bathrooms  \\\n",
       "count  2911.000000  2911.000000  2911.000000  2911.000000      2911.000000   \n",
       "mean      0.565136     0.542763     0.293431     0.353143         0.200863   \n",
       "std       0.155988     0.125193     0.119772     0.152244         0.133043   \n",
       "min       0.000000     0.000000     0.000000     0.000000         0.000000   \n",
       "25%       0.444444     0.451636     0.213557     0.200000         0.088659   \n",
       "50%       0.555556     0.547807     0.282326     0.400000         0.164976   \n",
       "75%       0.666667     0.621247     0.356447     0.400000         0.252530   \n",
       "max       1.000000     1.000000     1.000000     1.000000         1.000000   \n",
       "\n",
       "        GarageArea  YrBltAndRemod  TotalBsmtSF     1stFlrSF    YearBuilt  \\\n",
       "count  2911.000000    2911.000000  2911.000000  2911.000000  2911.000000   \n",
       "mean      0.317285       0.660811     0.326706     0.487996     0.719569   \n",
       "std       0.143976       0.242686     0.131930     0.131369     0.219351   \n",
       "min       0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.215054       0.473684     0.247193     0.395003     0.590580   \n",
       "50%       0.322581       0.652632     0.308172     0.481664     0.731884   \n",
       "75%       0.387097       0.905263     0.405490     0.582574     0.934783   \n",
       "max       1.000000       1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          FullBath  YearRemodAdd  Foundation_PConc  TotRmsAbvGrd  \\\n",
       "count  2911.000000   2911.000000       2911.000000   2911.000000   \n",
       "mean      0.391876      0.570892          0.447956      0.542786   \n",
       "std       0.138140      0.348189          0.497369      0.128554   \n",
       "min       0.000000      0.000000          0.000000      0.000000   \n",
       "25%       0.250000      0.250000          0.000000      0.421336   \n",
       "50%       0.500000      0.716667          0.000000      0.516936   \n",
       "75%       0.500000      0.900000          1.000000      0.600315   \n",
       "max       1.000000      1.000000          1.000000      1.000000   \n",
       "\n",
       "       hasfireplace  ExterQual_Gd  BsmtQual_Ex   Fireplaces  HeatingQC_Ex  \\\n",
       "count   2911.000000   2911.000000  2911.000000  2911.000000   2911.000000   \n",
       "mean       0.512882      0.335967     0.087255     0.171718      0.511508   \n",
       "std        0.499920      0.472409     0.282257     0.181727      0.499953   \n",
       "min        0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "25%        0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "50%        1.000000      0.000000     0.000000     0.293793      1.000000   \n",
       "75%        1.000000      1.000000     0.000000     0.293793      1.000000   \n",
       "max        1.000000      1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "        MasVnrArea  Total_porch_sf  BsmtFinType1_GLQ  KitchenQual_Ex  \\\n",
       "count  2911.000000     2911.000000       2911.000000     2911.000000   \n",
       "mean      0.159954        0.203170          0.290622        0.069392   \n",
       "std       0.218350        0.163524          0.454127        0.254163   \n",
       "min       0.000000        0.000000          0.000000        0.000000   \n",
       "25%       0.000000        0.066964          0.000000        0.000000   \n",
       "50%       0.000000        0.179849          0.000000        0.000000   \n",
       "75%       0.353218        0.327089          1.000000        0.000000   \n",
       "max       1.000000        1.000000          1.000000        1.000000   \n",
       "\n",
       "       OpenPorchSF  GarageFinish_Fin  ...  Neighborhood_IDOTRR  \\\n",
       "count  2911.000000       2911.000000  ...          2911.000000   \n",
       "mean      0.176542          0.245620  ...             0.031261   \n",
       "std       0.183129          0.430528  ...             0.174051   \n",
       "min       0.000000          0.000000  ...             0.000000   \n",
       "25%       0.000000          0.000000  ...             0.000000   \n",
       "50%       0.180865          0.000000  ...             0.000000   \n",
       "75%       0.309510          0.000000  ...             0.000000   \n",
       "max       1.000000          1.000000  ...             1.000000   \n",
       "\n",
       "       BsmtExposure_No  Neighborhood_OldTown  Foundation_BrkTil  \\\n",
       "count      2911.000000           2911.000000        2911.000000   \n",
       "mean          0.652697              0.082102           0.106493   \n",
       "std           0.476195              0.274568           0.308520   \n",
       "min           0.000000              0.000000           0.000000   \n",
       "25%           0.000000              0.000000           0.000000   \n",
       "50%           1.000000              0.000000           0.000000   \n",
       "75%           1.000000              0.000000           0.000000   \n",
       "max           1.000000              1.000000           1.000000   \n",
       "\n",
       "       GarageFinish_None  GarageCond_None  GarageQual_None  GarageType_None  \\\n",
       "count        2911.000000      2911.000000      2911.000000      2911.000000   \n",
       "mean            0.054277         0.054277         0.054277         0.053590   \n",
       "std             0.226602         0.226602         0.226602         0.225245   \n",
       "min             0.000000         0.000000         0.000000         0.000000   \n",
       "25%             0.000000         0.000000         0.000000         0.000000   \n",
       "50%             0.000000         0.000000         0.000000         0.000000   \n",
       "75%             0.000000         0.000000         0.000000         0.000000   \n",
       "max             1.000000         1.000000         1.000000         1.000000   \n",
       "\n",
       "       MSSubClass_30  LotShape_Reg  PavedDrive_N  Foundation_CBlock  \\\n",
       "count    2911.000000   2911.000000   2911.000000        2911.000000   \n",
       "mean        0.047750      0.637582      0.073514           0.423222   \n",
       "std         0.213273      0.480781      0.261024           0.494155   \n",
       "min         0.000000      0.000000      0.000000           0.000000   \n",
       "25%         0.000000      0.000000      0.000000           0.000000   \n",
       "50%         0.000000      1.000000      0.000000           0.000000   \n",
       "75%         0.000000      1.000000      0.000000           1.000000   \n",
       "max         1.000000      1.000000      1.000000           1.000000   \n",
       "\n",
       "       MSZoning_RM  HeatingQC_TA  CentralAir_N  GarageType_Detchd  \\\n",
       "count  2911.000000   2911.000000   2911.000000        2911.000000   \n",
       "mean      0.158708      0.293370      0.066644           0.266919   \n",
       "std       0.365467      0.455385      0.249447           0.442425   \n",
       "min       0.000000      0.000000      0.000000           0.000000   \n",
       "25%       0.000000      0.000000      0.000000           0.000000   \n",
       "50%       0.000000      0.000000      0.000000           0.000000   \n",
       "75%       0.000000      1.000000      0.000000           1.000000   \n",
       "max       1.000000      1.000000      1.000000           1.000000   \n",
       "\n",
       "       MasVnrType_None  GarageFinish_Unf  BsmtQual_TA  FireplaceQu_None  \\\n",
       "count      2911.000000       2911.000000  2911.000000       2911.000000   \n",
       "mean          0.605634          0.421848     0.439368          0.487118   \n",
       "std           0.488798          0.493939     0.496395          0.499920   \n",
       "min           0.000000          0.000000     0.000000          0.000000   \n",
       "25%           0.000000          0.000000     0.000000          0.000000   \n",
       "50%           1.000000          0.000000     0.000000          0.000000   \n",
       "75%           1.000000          1.000000     1.000000          1.000000   \n",
       "max           1.000000          1.000000     1.000000          1.000000   \n",
       "\n",
       "       KitchenQual_TA  ExterQual_TA  dummy_1  dummy_2  dummy_3  \n",
       "count     2911.000000   2911.000000   2911.0   2911.0   2911.0  \n",
       "mean         0.511852      0.616627      0.0      0.0      0.0  \n",
       "std          0.499945      0.486292      0.0      0.0      0.0  \n",
       "min          0.000000      0.000000      0.0      0.0      0.0  \n",
       "25%          0.000000      0.000000      0.0      0.0      0.0  \n",
       "50%          1.000000      1.000000      0.0      0.0      0.0  \n",
       "75%          1.000000      1.000000      0.0      0.0      0.0  \n",
       "max          1.000000      1.000000      0.0      0.0      0.0  \n",
       "\n",
       "[8 rows x 324 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop label column\n",
    "data.drop(['label'], axis=1, inplace=True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['p7', 'p3', 'p2', 'p8', 'p9', 'p0', 'p4', 'p5', 'p6', 'p1'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  10\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "'''one-hot encode the labels'''\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "# print(list(integer_encoded))\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "labels = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "print('Number of classes: ', len(labels[0]))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 18, 18)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_np = data.to_numpy()\n",
    "data_np.shape\n",
    "data_np = data_np.reshape(len(data), 18, 18)\n",
    "data_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data_np[0].shape\n",
    "\n",
    "nArray = np.array(data_np[99])\n",
    "\n",
    "\n",
    "a11=nArray.reshape(18,18)\n",
    "plt.imshow(a11)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 10)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 18, 18)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_examples = data_np\n",
    "all_examples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train test splitting\n",
    "- hold out 15% for testing\n",
    "- use 85% to train model with K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_samples = all_examples.shape[0] \n",
    "test_ratio = 0.15\n",
    "test_samples = int(test_ratio * all_examples.shape[0])\n",
    "\n",
    "train_examples = all_examples[:-1*test_samples]\n",
    "test_examples = all_examples[-1*test_samples:]\n",
    "\n",
    "train_labels = labels[:-1*test_samples]\n",
    "test_labels = labels[-1*test_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (2475, 18, 18)\n",
      "test:  (436, 18, 18)\n",
      "train label:  (2475, 10)\n",
      "test label:  (436, 10)\n"
     ]
    }
   ],
   "source": [
    "print('train: ', train_examples.shape)\n",
    "print('test: ', test_examples.shape)\n",
    "print('train label: ', train_labels.shape)\n",
    "print('test label: ', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainX = train_examples.reshape(ttl_samples-test_samples, 18,18,1)\n",
    "# trainY = train_labels\n",
    "\n",
    "# testX = test_examples.reshape(test_samples, 18,18,1)\n",
    "# testY = test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "\n",
    "def create_model(num_classes=5, lr=0.005):\n",
    "\n",
    "\t# Working\n",
    "\twith tf.device('/cpu:0'):\n",
    "\t\tdata_augmentation = tf.keras.Sequential([ \n",
    "\t\t\ttf.keras.layers.RandomFlip(\"horizontal\", input_shape=(18, 18, 1)),\n",
    "\t  \t\ttf.keras.layers.RandomRotation(0.1),\n",
    "\t\t    tf.keras.layers.RandomZoom(0.1)\n",
    "\t\t\t])\n",
    "\n",
    "\n",
    "\tmodel = tf.keras.Sequential([\n",
    "\t\t# data_augmentation,\n",
    "\t  \t# tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(18, 18, 1)),\n",
    "\t\ttf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.MaxPooling2D((2,2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t  \ttf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.MaxPooling2D(),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Flatten(),\n",
    "\t\ttf.keras.layers.Dense(128, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "\t])\n",
    "\n",
    "\t# opt = tf.keras.optimizers.SGD(lr=0.005, momentum=0.9)\n",
    "\topt = tf.keras.optimizers.Adam(lr=lr)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\treturn model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_27 (Conv2D)          (None, 18, 18, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPoolin  (None, 9, 9, 16)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 9, 9, 16)          0         \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 9, 9, 32)          4640      \n",
      "                                                                 \n",
      " max_pooling2d_28 (MaxPoolin  (None, 4, 4, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 4, 4, 32)          0         \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 4, 4, 64)          18496     \n",
      "                                                                 \n",
      " max_pooling2d_29 (MaxPoolin  (None, 2, 2, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 2, 2, 64)          0         \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,837\n",
      "Trainable params: 56,837\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evantilu/miniforge3/envs/6998_DL_tf/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:55:23.714938: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-04-30 11:55:24.004055: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.1972 - accuracy: 0.4603"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:55:28.427078: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.56855, saving model to ./ckpt/5_class_lr005_new/val_acc_0.569.hdf5\n",
      "70/70 [==============================] - 5s 30ms/step - loss: 1.1972 - accuracy: 0.4603 - val_loss: 1.0213 - val_accuracy: 0.5685\n",
      "Epoch 2/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9591 - accuracy: 0.5793\n",
      "Epoch 2: val_accuracy improved from 0.56855 to 0.57661, saving model to ./ckpt/5_class_lr005_new/val_acc_0.577.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9565 - accuracy: 0.5806 - val_loss: 0.9917 - val_accuracy: 0.5766\n",
      "Epoch 3/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8840 - accuracy: 0.6071\n",
      "Epoch 3: val_accuracy improved from 0.57661 to 0.60484, saving model to ./ckpt/5_class_lr005_new/val_acc_0.605.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.8840 - accuracy: 0.6071 - val_loss: 0.8394 - val_accuracy: 0.6048\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.7946 - accuracy: 0.6535\n",
      "Epoch 4: val_accuracy improved from 0.60484 to 0.62097, saving model to ./ckpt/5_class_lr005_new/val_acc_0.621.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.7968 - accuracy: 0.6533 - val_loss: 0.8275 - val_accuracy: 0.6210\n",
      "Epoch 5/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.7169 - accuracy: 0.6870\n",
      "Epoch 5: val_accuracy improved from 0.62097 to 0.62903, saving model to ./ckpt/5_class_lr005_new/val_acc_0.629.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.7135 - accuracy: 0.6902 - val_loss: 0.8475 - val_accuracy: 0.6290\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6989 - accuracy: 0.6951\n",
      "Epoch 6: val_accuracy improved from 0.62903 to 0.66935, saving model to ./ckpt/5_class_lr005_new/val_acc_0.669.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.6989 - accuracy: 0.6951 - val_loss: 0.6849 - val_accuracy: 0.6694\n",
      "Epoch 7/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6318 - accuracy: 0.7197\n",
      "Epoch 7: val_accuracy did not improve from 0.66935\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.6278 - accuracy: 0.7225 - val_loss: 0.7587 - val_accuracy: 0.6452\n",
      "Epoch 8/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6290 - accuracy: 0.7261\n",
      "Epoch 8: val_accuracy improved from 0.66935 to 0.68145, saving model to ./ckpt/5_class_lr005_new/val_acc_0.681.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6279 - accuracy: 0.7252 - val_loss: 0.6847 - val_accuracy: 0.6815\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6243 - accuracy: 0.7301\n",
      "Epoch 9: val_accuracy improved from 0.68145 to 0.69355, saving model to ./ckpt/5_class_lr005_new/val_acc_0.694.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6243 - accuracy: 0.7301 - val_loss: 0.6884 - val_accuracy: 0.6935\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5766 - accuracy: 0.7530\n",
      "Epoch 10: val_accuracy improved from 0.69355 to 0.72177, saving model to ./ckpt/5_class_lr005_new/val_acc_0.722.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.5766 - accuracy: 0.7530 - val_loss: 0.6239 - val_accuracy: 0.7218\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5747 - accuracy: 0.7530\n",
      "Epoch 11: val_accuracy did not improve from 0.72177\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5747 - accuracy: 0.7530 - val_loss: 0.6965 - val_accuracy: 0.6774\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5491 - accuracy: 0.7763\n",
      "Epoch 12: val_accuracy did not improve from 0.72177\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5499 - accuracy: 0.7755 - val_loss: 0.6205 - val_accuracy: 0.7177\n",
      "Epoch 13/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5520 - accuracy: 0.7590\n",
      "Epoch 13: val_accuracy did not improve from 0.72177\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5520 - accuracy: 0.7598 - val_loss: 0.7103 - val_accuracy: 0.6694\n",
      "Epoch 14/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4993 - accuracy: 0.7884\n",
      "Epoch 14: val_accuracy did not improve from 0.72177\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5053 - accuracy: 0.7876 - val_loss: 0.6388 - val_accuracy: 0.7137\n",
      "Epoch 15/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5422 - accuracy: 0.7732\n",
      "Epoch 15: val_accuracy did not improve from 0.72177\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5419 - accuracy: 0.7723 - val_loss: 0.8798 - val_accuracy: 0.6573\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5398 - accuracy: 0.7741\n",
      "Epoch 16: val_accuracy did not improve from 0.72177\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5457 - accuracy: 0.7719 - val_loss: 0.8760 - val_accuracy: 0.6371\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5195 - accuracy: 0.7902\n",
      "Epoch 17: val_accuracy did not improve from 0.72177\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5161 - accuracy: 0.7907 - val_loss: 0.7359 - val_accuracy: 0.7097\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4872 - accuracy: 0.7948\n",
      "Epoch 18: val_accuracy did not improve from 0.72177\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4872 - accuracy: 0.7948 - val_loss: 0.7613 - val_accuracy: 0.6573\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5189 - accuracy: 0.7808\n",
      "Epoch 19: val_accuracy did not improve from 0.72177\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5108 - accuracy: 0.7854 - val_loss: 0.6946 - val_accuracy: 0.6935\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4830 - accuracy: 0.8101\n",
      "Epoch 20: val_accuracy did not improve from 0.72177\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4871 - accuracy: 0.8078 - val_loss: 0.6892 - val_accuracy: 0.6653\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4582 - accuracy: 0.8083\n",
      "Epoch 21: val_accuracy did not improve from 0.72177\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4582 - accuracy: 0.8083 - val_loss: 0.7026 - val_accuracy: 0.6855\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4836 - accuracy: 0.8011\n",
      "Epoch 22: val_accuracy did not improve from 0.72177\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4989 - accuracy: 0.7943 - val_loss: 0.6744 - val_accuracy: 0.7056\n",
      "Epoch 22: early stopping\n",
      "Epoch 1/120\n",
      " 6/70 [=>............................] - ETA: 0s - loss: 1.6563 - accuracy: 0.2031 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:55:47.419290: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.3185 - accuracy: 0.3776\n",
      "Epoch 1: val_accuracy improved from -inf to 0.55242, saving model to ./ckpt/5_class_lr005_new/val_acc_0.552.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.3185 - accuracy: 0.3776 - val_loss: 0.9952 - val_accuracy: 0.5524\n",
      "Epoch 2/120\n",
      " 6/70 [=>............................] - ETA: 0s - loss: 1.1085 - accuracy: 0.4635"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:55:48.386337: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9903 - accuracy: 0.5445\n",
      "Epoch 2: val_accuracy improved from 0.55242 to 0.57661, saving model to ./ckpt/5_class_lr005_new/val_acc_0.577.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9831 - accuracy: 0.5478 - val_loss: 0.9172 - val_accuracy: 0.5766\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8262 - accuracy: 0.6255\n",
      "Epoch 3: val_accuracy improved from 0.57661 to 0.64113, saving model to ./ckpt/5_class_lr005_new/val_acc_0.641.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8272 - accuracy: 0.6246 - val_loss: 0.8471 - val_accuracy: 0.6411\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7902 - accuracy: 0.6417\n",
      "Epoch 4: val_accuracy improved from 0.64113 to 0.66935, saving model to ./ckpt/5_class_lr005_new/val_acc_0.669.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.7902 - accuracy: 0.6417 - val_loss: 0.7346 - val_accuracy: 0.6694\n",
      "Epoch 5/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.7238 - accuracy: 0.6828\n",
      "Epoch 5: val_accuracy improved from 0.66935 to 0.67339, saving model to ./ckpt/5_class_lr005_new/val_acc_0.673.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.7200 - accuracy: 0.6839 - val_loss: 0.7457 - val_accuracy: 0.6734\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6700 - accuracy: 0.7054\n",
      "Epoch 6: val_accuracy did not improve from 0.67339\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6677 - accuracy: 0.7063 - val_loss: 0.8707 - val_accuracy: 0.6411\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.6631 - accuracy: 0.7008\n",
      "Epoch 7: val_accuracy improved from 0.67339 to 0.72984, saving model to ./ckpt/5_class_lr005_new/val_acc_0.730.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.6598 - accuracy: 0.7027 - val_loss: 0.6769 - val_accuracy: 0.7298\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6405 - accuracy: 0.7198\n",
      "Epoch 8: val_accuracy did not improve from 0.72984\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6405 - accuracy: 0.7198 - val_loss: 0.7320 - val_accuracy: 0.6815\n",
      "Epoch 9/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6437 - accuracy: 0.7211\n",
      "Epoch 9: val_accuracy improved from 0.72984 to 0.73387, saving model to ./ckpt/5_class_lr005_new/val_acc_0.734.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6505 - accuracy: 0.7180 - val_loss: 0.6558 - val_accuracy: 0.7339\n",
      "Epoch 10/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6172 - accuracy: 0.7307\n",
      "Epoch 10: val_accuracy did not improve from 0.73387\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.6180 - accuracy: 0.7306 - val_loss: 0.7875 - val_accuracy: 0.6411\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5734 - accuracy: 0.7405\n",
      "Epoch 11: val_accuracy did not improve from 0.73387\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5735 - accuracy: 0.7405 - val_loss: 0.6011 - val_accuracy: 0.7339\n",
      "Epoch 12/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5718 - accuracy: 0.7457\n",
      "Epoch 12: val_accuracy improved from 0.73387 to 0.75000, saving model to ./ckpt/5_class_lr005_new/val_acc_0.750.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5713 - accuracy: 0.7467 - val_loss: 0.6418 - val_accuracy: 0.7500\n",
      "Epoch 13/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5370 - accuracy: 0.7730\n",
      "Epoch 13: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.5393 - accuracy: 0.7719 - val_loss: 0.6806 - val_accuracy: 0.7097\n",
      "Epoch 14/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5511 - accuracy: 0.7615\n",
      "Epoch 14: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.5513 - accuracy: 0.7625 - val_loss: 0.6148 - val_accuracy: 0.7379\n",
      "Epoch 15/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5664 - accuracy: 0.7530\n",
      "Epoch 15: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.5664 - accuracy: 0.7530 - val_loss: 0.6349 - val_accuracy: 0.7298\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5118 - accuracy: 0.7794\n",
      "Epoch 16: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5183 - accuracy: 0.7768 - val_loss: 0.6441 - val_accuracy: 0.7097\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4964 - accuracy: 0.7917\n",
      "Epoch 17: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4930 - accuracy: 0.7925 - val_loss: 0.7039 - val_accuracy: 0.7218\n",
      "Epoch 18/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4798 - accuracy: 0.7966\n",
      "Epoch 18: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4771 - accuracy: 0.7966 - val_loss: 0.7577 - val_accuracy: 0.6855\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4645 - accuracy: 0.8065\n",
      "Epoch 19: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4645 - accuracy: 0.8065 - val_loss: 0.6172 - val_accuracy: 0.7460\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4741 - accuracy: 0.7984\n",
      "Epoch 20: val_accuracy did not improve from 0.75000\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4741 - accuracy: 0.7984 - val_loss: 0.7536 - val_accuracy: 0.7016\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4397 - accuracy: 0.8186\n",
      "Epoch 21: val_accuracy improved from 0.75000 to 0.76210, saving model to ./ckpt/5_class_lr005_new/val_acc_0.762.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4376 - accuracy: 0.8190 - val_loss: 0.6306 - val_accuracy: 0.7621\n",
      "Epoch 21: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 25s - loss: 1.6230 - accuracy: 0.1562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:56:07.148784: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.2661 - accuracy: 0.4167\n",
      "Epoch 1: val_accuracy improved from -inf to 0.57661, saving model to ./ckpt/5_class_lr005_new/val_acc_0.577.hdf5\n",
      "70/70 [==============================] - 2s 17ms/step - loss: 1.2661 - accuracy: 0.4167 - val_loss: 0.9746 - val_accuracy: 0.5766\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 0.9921 - accuracy: 0.5938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:56:08.291041: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/70 [============================>.] - ETA: 0s - loss: 0.9741 - accuracy: 0.5565\n",
      "Epoch 2: val_accuracy improved from 0.57661 to 0.63710, saving model to ./ckpt/5_class_lr005_new/val_acc_0.637.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9703 - accuracy: 0.5573 - val_loss: 0.8369 - val_accuracy: 0.6371\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8368 - accuracy: 0.6305\n",
      "Epoch 3: val_accuracy improved from 0.63710 to 0.67339, saving model to ./ckpt/5_class_lr005_new/val_acc_0.673.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8381 - accuracy: 0.6300 - val_loss: 0.7675 - val_accuracy: 0.6734\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7651 - accuracy: 0.6517\n",
      "Epoch 4: val_accuracy improved from 0.67339 to 0.73790, saving model to ./ckpt/5_class_lr005_new/val_acc_0.738.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.7654 - accuracy: 0.6520 - val_loss: 0.6757 - val_accuracy: 0.7379\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6895 - accuracy: 0.7045\n",
      "Epoch 5: val_accuracy improved from 0.73790 to 0.75000, saving model to ./ckpt/5_class_lr005_new/val_acc_0.750.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6895 - accuracy: 0.7045 - val_loss: 0.6496 - val_accuracy: 0.7500\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6787 - accuracy: 0.6965\n",
      "Epoch 6: val_accuracy improved from 0.75000 to 0.75403, saving model to ./ckpt/5_class_lr005_new/val_acc_0.754.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.6787 - accuracy: 0.6965 - val_loss: 0.6813 - val_accuracy: 0.7540\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.6374 - accuracy: 0.7164\n",
      "Epoch 7: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6418 - accuracy: 0.7149 - val_loss: 0.6795 - val_accuracy: 0.7460\n",
      "Epoch 8/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6051 - accuracy: 0.7371\n",
      "Epoch 8: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.6170 - accuracy: 0.7319 - val_loss: 0.6954 - val_accuracy: 0.6855\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5862 - accuracy: 0.7562\n",
      "Epoch 9: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5862 - accuracy: 0.7562 - val_loss: 0.7404 - val_accuracy: 0.7097\n",
      "Epoch 10/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5581 - accuracy: 0.7628\n",
      "Epoch 10: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5618 - accuracy: 0.7584 - val_loss: 0.6234 - val_accuracy: 0.7460\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5777 - accuracy: 0.7550\n",
      "Epoch 11: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5762 - accuracy: 0.7553 - val_loss: 0.6957 - val_accuracy: 0.7218\n",
      "Epoch 12/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5437 - accuracy: 0.7595\n",
      "Epoch 12: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5447 - accuracy: 0.7602 - val_loss: 0.6300 - val_accuracy: 0.7258\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5438 - accuracy: 0.7722\n",
      "Epoch 13: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5460 - accuracy: 0.7719 - val_loss: 0.5926 - val_accuracy: 0.7500\n",
      "Epoch 14/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5296 - accuracy: 0.7803\n",
      "Epoch 14: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5312 - accuracy: 0.7782 - val_loss: 0.6784 - val_accuracy: 0.7500\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5298 - accuracy: 0.7681\n",
      "Epoch 15: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5328 - accuracy: 0.7670 - val_loss: 0.6662 - val_accuracy: 0.7298\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5002 - accuracy: 0.7885\n",
      "Epoch 16: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5015 - accuracy: 0.7876 - val_loss: 0.6775 - val_accuracy: 0.7379\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5013 - accuracy: 0.7916\n",
      "Epoch 17: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5013 - accuracy: 0.7916 - val_loss: 0.8624 - val_accuracy: 0.6774\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4876 - accuracy: 0.7912\n",
      "Epoch 18: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4860 - accuracy: 0.7930 - val_loss: 0.7562 - val_accuracy: 0.7056\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5077 - accuracy: 0.7925\n",
      "Epoch 19: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5077 - accuracy: 0.7925 - val_loss: 0.6319 - val_accuracy: 0.7500\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4600 - accuracy: 0.8039\n",
      "Epoch 20: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4621 - accuracy: 0.8029 - val_loss: 0.6643 - val_accuracy: 0.7258\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4681 - accuracy: 0.8130\n",
      "Epoch 21: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4675 - accuracy: 0.8123 - val_loss: 0.6744 - val_accuracy: 0.7460\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4487 - accuracy: 0.8059\n",
      "Epoch 22: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.4536 - accuracy: 0.8024 - val_loss: 0.6622 - val_accuracy: 0.7419\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4163 - accuracy: 0.8211\n",
      "Epoch 23: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4173 - accuracy: 0.8204 - val_loss: 0.6960 - val_accuracy: 0.7419\n",
      "Epoch 23: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 25s - loss: 1.6194 - accuracy: 0.1875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:56:28.011344: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.3299 - accuracy: 0.3669\n",
      "Epoch 1: val_accuracy improved from -inf to 0.44758, saving model to ./ckpt/5_class_lr005_new/val_acc_0.448.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3299 - accuracy: 0.3669 - val_loss: 1.1447 - val_accuracy: 0.4476\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 1s - loss: 1.2400 - accuracy: 0.4062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:56:29.036409: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 0.9579 - accuracy: 0.5734\n",
      "Epoch 2: val_accuracy improved from 0.44758 to 0.66532, saving model to ./ckpt/5_class_lr005_new/val_acc_0.665.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9579 - accuracy: 0.5734 - val_loss: 0.8169 - val_accuracy: 0.6653\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8955 - accuracy: 0.5969\n",
      "Epoch 3: val_accuracy improved from 0.66532 to 0.68145, saving model to ./ckpt/5_class_lr005_new/val_acc_0.681.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8939 - accuracy: 0.5981 - val_loss: 0.7631 - val_accuracy: 0.6815\n",
      "Epoch 4/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.7687 - accuracy: 0.6651\n",
      "Epoch 4: val_accuracy did not improve from 0.68145\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.7692 - accuracy: 0.6628 - val_loss: 0.7182 - val_accuracy: 0.6532\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6948 - accuracy: 0.6988\n",
      "Epoch 5: val_accuracy improved from 0.68145 to 0.72177, saving model to ./ckpt/5_class_lr005_new/val_acc_0.722.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.6936 - accuracy: 0.6982 - val_loss: 0.6453 - val_accuracy: 0.7218\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6723 - accuracy: 0.7036\n",
      "Epoch 6: val_accuracy improved from 0.72177 to 0.73387, saving model to ./ckpt/5_class_lr005_new/val_acc_0.734.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6800 - accuracy: 0.7014 - val_loss: 0.6374 - val_accuracy: 0.7339\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6499 - accuracy: 0.7194\n",
      "Epoch 7: val_accuracy did not improve from 0.73387\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6499 - accuracy: 0.7194 - val_loss: 0.7334 - val_accuracy: 0.6613\n",
      "Epoch 8/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6270 - accuracy: 0.7341\n",
      "Epoch 8: val_accuracy did not improve from 0.73387\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.6241 - accuracy: 0.7360 - val_loss: 0.7936 - val_accuracy: 0.6089\n",
      "Epoch 9/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6150 - accuracy: 0.7472\n",
      "Epoch 9: val_accuracy did not improve from 0.73387\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6165 - accuracy: 0.7463 - val_loss: 0.6725 - val_accuracy: 0.7016\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6109 - accuracy: 0.7351\n",
      "Epoch 10: val_accuracy improved from 0.73387 to 0.75403, saving model to ./ckpt/5_class_lr005_new/val_acc_0.754.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6109 - accuracy: 0.7351 - val_loss: 0.5889 - val_accuracy: 0.7540\n",
      "Epoch 11/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5976 - accuracy: 0.7467\n",
      "Epoch 11: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5977 - accuracy: 0.7463 - val_loss: 0.8388 - val_accuracy: 0.6048\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5663 - accuracy: 0.7542\n",
      "Epoch 12: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5685 - accuracy: 0.7512 - val_loss: 0.6092 - val_accuracy: 0.7097\n",
      "Epoch 13/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5457 - accuracy: 0.7585\n",
      "Epoch 13: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5486 - accuracy: 0.7566 - val_loss: 0.6773 - val_accuracy: 0.7016\n",
      "Epoch 14/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5534 - accuracy: 0.7640\n",
      "Epoch 14: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5537 - accuracy: 0.7634 - val_loss: 0.6755 - val_accuracy: 0.7056\n",
      "Epoch 15/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5372 - accuracy: 0.7705\n",
      "Epoch 15: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5372 - accuracy: 0.7705 - val_loss: 0.8138 - val_accuracy: 0.6331\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5211 - accuracy: 0.7779\n",
      "Epoch 16: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5165 - accuracy: 0.7818 - val_loss: 0.6309 - val_accuracy: 0.7137\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5160 - accuracy: 0.7871\n",
      "Epoch 17: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5146 - accuracy: 0.7881 - val_loss: 0.6241 - val_accuracy: 0.7097\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4957 - accuracy: 0.7893\n",
      "Epoch 18: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4951 - accuracy: 0.7894 - val_loss: 0.6647 - val_accuracy: 0.6895\n",
      "Epoch 19/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4838 - accuracy: 0.7960\n",
      "Epoch 19: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4861 - accuracy: 0.7961 - val_loss: 0.6526 - val_accuracy: 0.7419\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4868 - accuracy: 0.7944\n",
      "Epoch 20: val_accuracy did not improve from 0.75403\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4865 - accuracy: 0.7952 - val_loss: 0.8644 - val_accuracy: 0.6331\n",
      "Epoch 20: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 25s - loss: 1.5824 - accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:56:46.666784: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.2414 - accuracy: 0.4347\n",
      "Epoch 1: val_accuracy improved from -inf to 0.60484, saving model to ./ckpt/5_class_lr005_new/val_acc_0.605.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2414 - accuracy: 0.4347 - val_loss: 0.9424 - val_accuracy: 0.6048\n",
      "Epoch 2/120\n",
      " 6/70 [=>............................] - ETA: 0s - loss: 1.0268 - accuracy: 0.4948"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:56:47.755270: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9362 - accuracy: 0.5843\n",
      "Epoch 2: val_accuracy improved from 0.60484 to 0.63710, saving model to ./ckpt/5_class_lr005_new/val_acc_0.637.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9353 - accuracy: 0.5882 - val_loss: 0.9005 - val_accuracy: 0.6371\n",
      "Epoch 3/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.7881 - accuracy: 0.6728\n",
      "Epoch 3: val_accuracy improved from 0.63710 to 0.67339, saving model to ./ckpt/5_class_lr005_new/val_acc_0.673.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.7905 - accuracy: 0.6704 - val_loss: 0.7783 - val_accuracy: 0.6734\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.7601 - accuracy: 0.6581\n",
      "Epoch 4: val_accuracy did not improve from 0.67339\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.7620 - accuracy: 0.6578 - val_loss: 0.8273 - val_accuracy: 0.6653\n",
      "Epoch 5/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.7397 - accuracy: 0.6771\n",
      "Epoch 5: val_accuracy did not improve from 0.67339\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.7379 - accuracy: 0.6812 - val_loss: 0.8285 - val_accuracy: 0.6613\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6478 - accuracy: 0.7147\n",
      "Epoch 6: val_accuracy improved from 0.67339 to 0.69355, saving model to ./ckpt/5_class_lr005_new/val_acc_0.694.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6464 - accuracy: 0.7153 - val_loss: 0.6860 - val_accuracy: 0.6935\n",
      "Epoch 7/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6092 - accuracy: 0.7371\n",
      "Epoch 7: val_accuracy improved from 0.69355 to 0.72581, saving model to ./ckpt/5_class_lr005_new/val_acc_0.726.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6118 - accuracy: 0.7355 - val_loss: 0.6859 - val_accuracy: 0.7258\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5846 - accuracy: 0.7535\n",
      "Epoch 8: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5846 - accuracy: 0.7535 - val_loss: 0.6430 - val_accuracy: 0.7016\n",
      "Epoch 9/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5898 - accuracy: 0.7454\n",
      "Epoch 9: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.5879 - accuracy: 0.7472 - val_loss: 0.6783 - val_accuracy: 0.6653\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5460 - accuracy: 0.7728\n",
      "Epoch 10: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5460 - accuracy: 0.7728 - val_loss: 0.6888 - val_accuracy: 0.7258\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5504 - accuracy: 0.7678\n",
      "Epoch 11: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.5504 - accuracy: 0.7678 - val_loss: 0.7687 - val_accuracy: 0.6815\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5399 - accuracy: 0.7738\n",
      "Epoch 12: val_accuracy did not improve from 0.72581\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.5421 - accuracy: 0.7737 - val_loss: 0.6713 - val_accuracy: 0.7137\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5134 - accuracy: 0.7890\n",
      "Epoch 13: val_accuracy improved from 0.72581 to 0.73790, saving model to ./ckpt/5_class_lr005_new/val_acc_0.738.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5134 - accuracy: 0.7890 - val_loss: 0.6411 - val_accuracy: 0.7379\n",
      "Epoch 14/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5251 - accuracy: 0.7754\n",
      "Epoch 14: val_accuracy did not improve from 0.73790\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5249 - accuracy: 0.7750 - val_loss: 0.6896 - val_accuracy: 0.7137\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4650 - accuracy: 0.8089\n",
      "Epoch 15: val_accuracy did not improve from 0.73790\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.4651 - accuracy: 0.8087 - val_loss: 0.6163 - val_accuracy: 0.7218\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5204 - accuracy: 0.7715\n",
      "Epoch 16: val_accuracy did not improve from 0.73790\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5203 - accuracy: 0.7714 - val_loss: 0.6013 - val_accuracy: 0.7218\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4528 - accuracy: 0.8045\n",
      "Epoch 17: val_accuracy did not improve from 0.73790\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4531 - accuracy: 0.8033 - val_loss: 0.6116 - val_accuracy: 0.7218\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4852 - accuracy: 0.8045\n",
      "Epoch 18: val_accuracy did not improve from 0.73790\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4855 - accuracy: 0.8020 - val_loss: 0.6900 - val_accuracy: 0.7056\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4561 - accuracy: 0.8132\n",
      "Epoch 19: val_accuracy did not improve from 0.73790\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4561 - accuracy: 0.8132 - val_loss: 0.7095 - val_accuracy: 0.7177\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4553 - accuracy: 0.8116\n",
      "Epoch 20: val_accuracy did not improve from 0.73790\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4541 - accuracy: 0.8119 - val_loss: 0.6851 - val_accuracy: 0.6492\n",
      "Epoch 21/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4216 - accuracy: 0.8365\n",
      "Epoch 21: val_accuracy improved from 0.73790 to 0.76210, saving model to ./ckpt/5_class_lr005_new/val_acc_0.762.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4221 - accuracy: 0.8357 - val_loss: 0.6759 - val_accuracy: 0.7621\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4028 - accuracy: 0.8376\n",
      "Epoch 22: val_accuracy did not improve from 0.76210\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4051 - accuracy: 0.8361 - val_loss: 0.6628 - val_accuracy: 0.7218\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4213 - accuracy: 0.8190\n",
      "Epoch 23: val_accuracy did not improve from 0.76210\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4213 - accuracy: 0.8190 - val_loss: 0.6603 - val_accuracy: 0.7298\n",
      "Epoch 24/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4035 - accuracy: 0.8277\n",
      "Epoch 24: val_accuracy did not improve from 0.76210\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4108 - accuracy: 0.8240 - val_loss: 0.6522 - val_accuracy: 0.7258\n",
      "Epoch 25/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3893 - accuracy: 0.8404\n",
      "Epoch 25: val_accuracy did not improve from 0.76210\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.3983 - accuracy: 0.8357 - val_loss: 0.8023 - val_accuracy: 0.6492\n",
      "Epoch 26/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4346 - accuracy: 0.8111\n",
      "Epoch 26: val_accuracy did not improve from 0.76210\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4374 - accuracy: 0.8083 - val_loss: 0.6257 - val_accuracy: 0.7379\n",
      "Epoch 26: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 49s - loss: 1.6120 - accuracy: 0.1875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:57:11.110626: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.2011 - accuracy: 0.4376"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:57:12.521386: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.59514, saving model to ./ckpt/5_class_lr005_new/val_acc_0.595.hdf5\n",
      "70/70 [==============================] - 2s 22ms/step - loss: 1.2011 - accuracy: 0.4376 - val_loss: 0.9556 - val_accuracy: 0.5951\n",
      "Epoch 2/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0046 - accuracy: 0.5528\n",
      "Epoch 2: val_accuracy improved from 0.59514 to 0.59919, saving model to ./ckpt/5_class_lr005_new/val_acc_0.599.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0021 - accuracy: 0.5530 - val_loss: 1.0075 - val_accuracy: 0.5992\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8881 - accuracy: 0.5979\n",
      "Epoch 3: val_accuracy improved from 0.59919 to 0.68826, saving model to ./ckpt/5_class_lr005_new/val_acc_0.688.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8876 - accuracy: 0.5956 - val_loss: 0.7777 - val_accuracy: 0.6883\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.7823 - accuracy: 0.6494\n",
      "Epoch 4: val_accuracy did not improve from 0.68826\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.7839 - accuracy: 0.6481 - val_loss: 0.7142 - val_accuracy: 0.6883\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7583 - accuracy: 0.6625\n",
      "Epoch 5: val_accuracy improved from 0.68826 to 0.70850, saving model to ./ckpt/5_class_lr005_new/val_acc_0.709.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.7583 - accuracy: 0.6625 - val_loss: 0.7108 - val_accuracy: 0.7085\n",
      "Epoch 6/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.7068 - accuracy: 0.6960\n",
      "Epoch 6: val_accuracy did not improve from 0.70850\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.7109 - accuracy: 0.6939 - val_loss: 0.6470 - val_accuracy: 0.7085\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.6520 - accuracy: 0.7249\n",
      "Epoch 7: val_accuracy did not improve from 0.70850\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6487 - accuracy: 0.7280 - val_loss: 0.7636 - val_accuracy: 0.6964\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6425 - accuracy: 0.7258\n",
      "Epoch 8: val_accuracy improved from 0.70850 to 0.73684, saving model to ./ckpt/5_class_lr005_new/val_acc_0.737.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6425 - accuracy: 0.7258 - val_loss: 0.6138 - val_accuracy: 0.7368\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6213 - accuracy: 0.7289\n",
      "Epoch 9: val_accuracy did not improve from 0.73684\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6213 - accuracy: 0.7289 - val_loss: 0.6998 - val_accuracy: 0.6802\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5995 - accuracy: 0.7388\n",
      "Epoch 10: val_accuracy improved from 0.73684 to 0.74089, saving model to ./ckpt/5_class_lr005_new/val_acc_0.741.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 0.5995 - accuracy: 0.7388 - val_loss: 0.5825 - val_accuracy: 0.7409\n",
      "Epoch 11/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5598 - accuracy: 0.7680\n",
      "Epoch 11: val_accuracy did not improve from 0.74089\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5583 - accuracy: 0.7697 - val_loss: 0.6810 - val_accuracy: 0.7004\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5738 - accuracy: 0.7599\n",
      "Epoch 12: val_accuracy did not improve from 0.74089\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5738 - accuracy: 0.7599 - val_loss: 0.6656 - val_accuracy: 0.7126\n",
      "Epoch 13/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5605 - accuracy: 0.7593\n",
      "Epoch 13: val_accuracy did not improve from 0.74089\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5642 - accuracy: 0.7590 - val_loss: 0.6525 - val_accuracy: 0.7328\n",
      "Epoch 14/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5281 - accuracy: 0.7708\n",
      "Epoch 14: val_accuracy improved from 0.74089 to 0.74899, saving model to ./ckpt/5_class_lr005_new/val_acc_0.749.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5270 - accuracy: 0.7715 - val_loss: 0.6150 - val_accuracy: 0.7490\n",
      "Epoch 15/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5335 - accuracy: 0.7716\n",
      "Epoch 15: val_accuracy did not improve from 0.74899\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5377 - accuracy: 0.7711 - val_loss: 0.6139 - val_accuracy: 0.7409\n",
      "Epoch 16/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5103 - accuracy: 0.7877\n",
      "Epoch 16: val_accuracy did not improve from 0.74899\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.5111 - accuracy: 0.7873 - val_loss: 0.6182 - val_accuracy: 0.7368\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5072 - accuracy: 0.7790\n",
      "Epoch 17: val_accuracy did not improve from 0.74899\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5078 - accuracy: 0.7787 - val_loss: 0.6824 - val_accuracy: 0.7287\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4775 - accuracy: 0.7836\n",
      "Epoch 18: val_accuracy improved from 0.74899 to 0.78947, saving model to ./ckpt/5_class_lr005_new/val_acc_0.789.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4771 - accuracy: 0.7846 - val_loss: 0.6245 - val_accuracy: 0.7895\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4976 - accuracy: 0.7860\n",
      "Epoch 19: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4960 - accuracy: 0.7859 - val_loss: 0.6431 - val_accuracy: 0.7530\n",
      "Epoch 20/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5040 - accuracy: 0.7849\n",
      "Epoch 20: val_accuracy did not improve from 0.78947\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5017 - accuracy: 0.7873 - val_loss: 0.6293 - val_accuracy: 0.7571\n",
      "Epoch 20: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 29s - loss: 1.5982 - accuracy: 0.2812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:57:30.756216: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.2138 - accuracy: 0.4556\n",
      "Epoch 1: val_accuracy improved from -inf to 0.36437, saving model to ./ckpt/5_class_lr005_new/val_acc_0.364.hdf5\n",
      "70/70 [==============================] - 2s 16ms/step - loss: 1.2138 - accuracy: 0.4556 - val_loss: 1.1967 - val_accuracy: 0.3644\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 0.9085 - accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:57:31.919344: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9809 - accuracy: 0.5578\n",
      "Epoch 2: val_accuracy improved from 0.36437 to 0.57085, saving model to ./ckpt/5_class_lr005_new/val_acc_0.571.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9821 - accuracy: 0.5543 - val_loss: 0.9253 - val_accuracy: 0.5709\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9052 - accuracy: 0.5979\n",
      "Epoch 3: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9007 - accuracy: 0.6010 - val_loss: 0.9518 - val_accuracy: 0.5425\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8167 - accuracy: 0.6466\n",
      "Epoch 4: val_accuracy improved from 0.57085 to 0.67611, saving model to ./ckpt/5_class_lr005_new/val_acc_0.676.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.8124 - accuracy: 0.6486 - val_loss: 0.6887 - val_accuracy: 0.6761\n",
      "Epoch 5/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.7493 - accuracy: 0.6714\n",
      "Epoch 5: val_accuracy improved from 0.67611 to 0.70850, saving model to ./ckpt/5_class_lr005_new/val_acc_0.709.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.7422 - accuracy: 0.6768 - val_loss: 0.6897 - val_accuracy: 0.7085\n",
      "Epoch 6/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6923 - accuracy: 0.6982\n",
      "Epoch 6: val_accuracy improved from 0.70850 to 0.71660, saving model to ./ckpt/5_class_lr005_new/val_acc_0.717.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.6897 - accuracy: 0.6979 - val_loss: 0.6403 - val_accuracy: 0.7166\n",
      "Epoch 7/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6711 - accuracy: 0.7183\n",
      "Epoch 7: val_accuracy improved from 0.71660 to 0.74089, saving model to ./ckpt/5_class_lr005_new/val_acc_0.741.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6717 - accuracy: 0.7172 - val_loss: 0.6053 - val_accuracy: 0.7409\n",
      "Epoch 8/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.6686 - accuracy: 0.7064\n",
      "Epoch 8: val_accuracy did not improve from 0.74089\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6721 - accuracy: 0.7060 - val_loss: 0.6413 - val_accuracy: 0.7085\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6259 - accuracy: 0.7244\n",
      "Epoch 9: val_accuracy improved from 0.74089 to 0.75709, saving model to ./ckpt/5_class_lr005_new/val_acc_0.757.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.6259 - accuracy: 0.7244 - val_loss: 0.6117 - val_accuracy: 0.7571\n",
      "Epoch 10/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.6032 - accuracy: 0.7405\n",
      "Epoch 10: val_accuracy did not improve from 0.75709\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6028 - accuracy: 0.7406 - val_loss: 0.7279 - val_accuracy: 0.6559\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6139 - accuracy: 0.7312\n",
      "Epoch 11: val_accuracy did not improve from 0.75709\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6176 - accuracy: 0.7303 - val_loss: 0.5510 - val_accuracy: 0.7368\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5731 - accuracy: 0.7500\n",
      "Epoch 12: val_accuracy did not improve from 0.75709\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5920 - accuracy: 0.7451 - val_loss: 0.6101 - val_accuracy: 0.7409\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6136 - accuracy: 0.7355\n",
      "Epoch 13: val_accuracy did not improve from 0.75709\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6143 - accuracy: 0.7352 - val_loss: 0.5657 - val_accuracy: 0.7328\n",
      "Epoch 14/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5516 - accuracy: 0.7670\n",
      "Epoch 14: val_accuracy improved from 0.75709 to 0.76518, saving model to ./ckpt/5_class_lr005_new/val_acc_0.765.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5474 - accuracy: 0.7680 - val_loss: 0.5303 - val_accuracy: 0.7652\n",
      "Epoch 15/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5733 - accuracy: 0.7453\n",
      "Epoch 15: val_accuracy improved from 0.76518 to 0.76923, saving model to ./ckpt/5_class_lr005_new/val_acc_0.769.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5685 - accuracy: 0.7482 - val_loss: 0.5302 - val_accuracy: 0.7692\n",
      "Epoch 16/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5496 - accuracy: 0.7656\n",
      "Epoch 16: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5536 - accuracy: 0.7626 - val_loss: 0.5465 - val_accuracy: 0.7530\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5382 - accuracy: 0.7699\n",
      "Epoch 17: val_accuracy improved from 0.76923 to 0.77328, saving model to ./ckpt/5_class_lr005_new/val_acc_0.773.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5384 - accuracy: 0.7697 - val_loss: 0.5176 - val_accuracy: 0.7733\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5220 - accuracy: 0.7837\n",
      "Epoch 18: val_accuracy did not improve from 0.77328\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5220 - accuracy: 0.7837 - val_loss: 0.6091 - val_accuracy: 0.7045\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5900 - accuracy: 0.7533\n",
      "Epoch 19: val_accuracy did not improve from 0.77328\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5849 - accuracy: 0.7545 - val_loss: 0.5882 - val_accuracy: 0.7490\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5045 - accuracy: 0.7765\n",
      "Epoch 20: val_accuracy did not improve from 0.77328\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5045 - accuracy: 0.7765 - val_loss: 0.5743 - val_accuracy: 0.7287\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4789 - accuracy: 0.7997\n",
      "Epoch 21: val_accuracy did not improve from 0.77328\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4824 - accuracy: 0.7985 - val_loss: 0.5699 - val_accuracy: 0.7206\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4637 - accuracy: 0.8125\n",
      "Epoch 22: val_accuracy did not improve from 0.77328\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4675 - accuracy: 0.8097 - val_loss: 0.5946 - val_accuracy: 0.6842\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4828 - accuracy: 0.7976\n",
      "Epoch 23: val_accuracy improved from 0.77328 to 0.78543, saving model to ./ckpt/5_class_lr005_new/val_acc_0.785.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4828 - accuracy: 0.7976 - val_loss: 0.5243 - val_accuracy: 0.7854\n",
      "Epoch 24/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4497 - accuracy: 0.8026\n",
      "Epoch 24: val_accuracy did not improve from 0.78543\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4523 - accuracy: 0.8016 - val_loss: 0.7124 - val_accuracy: 0.7085\n",
      "Epoch 25/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4602 - accuracy: 0.8153\n",
      "Epoch 25: val_accuracy did not improve from 0.78543\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4515 - accuracy: 0.8191 - val_loss: 0.6308 - val_accuracy: 0.7409\n",
      "Epoch 26/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4334 - accuracy: 0.8314\n",
      "Epoch 26: val_accuracy did not improve from 0.78543\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.4334 - accuracy: 0.8308 - val_loss: 0.5590 - val_accuracy: 0.7328\n",
      "Epoch 27/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3891 - accuracy: 0.8429\n",
      "Epoch 27: val_accuracy did not improve from 0.78543\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.3891 - accuracy: 0.8429 - val_loss: 0.7022 - val_accuracy: 0.7126\n",
      "Epoch 27: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 25s - loss: 1.6760 - accuracy: 0.0312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:57:55.182833: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.2655 - accuracy: 0.4048\n",
      "Epoch 1: val_accuracy improved from -inf to 0.53036, saving model to ./ckpt/5_class_lr005_new/val_acc_0.530.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2655 - accuracy: 0.4048 - val_loss: 1.0821 - val_accuracy: 0.5304\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.0264 - accuracy: 0.4375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:57:56.243701: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 0.9790 - accuracy: 0.5684\n",
      "Epoch 2: val_accuracy improved from 0.53036 to 0.59919, saving model to ./ckpt/5_class_lr005_new/val_acc_0.599.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9800 - accuracy: 0.5669 - val_loss: 0.9851 - val_accuracy: 0.5992\n",
      "Epoch 3/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8940 - accuracy: 0.5943\n",
      "Epoch 3: val_accuracy improved from 0.59919 to 0.60324, saving model to ./ckpt/5_class_lr005_new/val_acc_0.603.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.8940 - accuracy: 0.5943 - val_loss: 0.8999 - val_accuracy: 0.6032\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.7980 - accuracy: 0.6480\n",
      "Epoch 4: val_accuracy improved from 0.60324 to 0.61134, saving model to ./ckpt/5_class_lr005_new/val_acc_0.611.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.7985 - accuracy: 0.6468 - val_loss: 0.8987 - val_accuracy: 0.6113\n",
      "Epoch 5/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.7414 - accuracy: 0.6629\n",
      "Epoch 5: val_accuracy improved from 0.61134 to 0.65182, saving model to ./ckpt/5_class_lr005_new/val_acc_0.652.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.7364 - accuracy: 0.6638 - val_loss: 0.8222 - val_accuracy: 0.6518\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7068 - accuracy: 0.6857\n",
      "Epoch 6: val_accuracy did not improve from 0.65182\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.7063 - accuracy: 0.6858 - val_loss: 0.9083 - val_accuracy: 0.6356\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.6584 - accuracy: 0.7135\n",
      "Epoch 7: val_accuracy improved from 0.65182 to 0.68421, saving model to ./ckpt/5_class_lr005_new/val_acc_0.684.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.6589 - accuracy: 0.7101 - val_loss: 0.7732 - val_accuracy: 0.6842\n",
      "Epoch 8/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.6414 - accuracy: 0.7145\n",
      "Epoch 8: val_accuracy improved from 0.68421 to 0.71660, saving model to ./ckpt/5_class_lr005_new/val_acc_0.717.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6347 - accuracy: 0.7159 - val_loss: 0.7431 - val_accuracy: 0.7166\n",
      "Epoch 9/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6416 - accuracy: 0.7192\n",
      "Epoch 9: val_accuracy did not improve from 0.71660\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6446 - accuracy: 0.7177 - val_loss: 1.0258 - val_accuracy: 0.5870\n",
      "Epoch 10/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6286 - accuracy: 0.7224\n",
      "Epoch 10: val_accuracy did not improve from 0.71660\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6285 - accuracy: 0.7226 - val_loss: 0.7491 - val_accuracy: 0.6680\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6070 - accuracy: 0.7413\n",
      "Epoch 11: val_accuracy did not improve from 0.71660\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.6042 - accuracy: 0.7415 - val_loss: 0.7839 - val_accuracy: 0.6518\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6087 - accuracy: 0.7550\n",
      "Epoch 12: val_accuracy did not improve from 0.71660\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.6097 - accuracy: 0.7540 - val_loss: 0.7711 - val_accuracy: 0.6559\n",
      "Epoch 13/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5375 - accuracy: 0.7704\n",
      "Epoch 13: val_accuracy did not improve from 0.71660\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5461 - accuracy: 0.7684 - val_loss: 0.8056 - val_accuracy: 0.6599\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5413 - accuracy: 0.7756\n",
      "Epoch 14: val_accuracy did not improve from 0.71660\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5413 - accuracy: 0.7756 - val_loss: 0.7407 - val_accuracy: 0.6802\n",
      "Epoch 15/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5334 - accuracy: 0.7652\n",
      "Epoch 15: val_accuracy did not improve from 0.71660\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5334 - accuracy: 0.7666 - val_loss: 0.8179 - val_accuracy: 0.6802\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.7586\n",
      "Epoch 16: val_accuracy did not improve from 0.71660\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5607 - accuracy: 0.7590 - val_loss: 0.7094 - val_accuracy: 0.7004\n",
      "Epoch 17/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4944 - accuracy: 0.7771\n",
      "Epoch 17: val_accuracy did not improve from 0.71660\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5014 - accuracy: 0.7756 - val_loss: 0.7517 - val_accuracy: 0.6842\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5243 - accuracy: 0.7704\n",
      "Epoch 18: val_accuracy did not improve from 0.71660\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5233 - accuracy: 0.7711 - val_loss: 1.0641 - val_accuracy: 0.6275\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5283 - accuracy: 0.7746\n",
      "Epoch 19: val_accuracy improved from 0.71660 to 0.72065, saving model to ./ckpt/5_class_lr005_new/val_acc_0.721.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5289 - accuracy: 0.7742 - val_loss: 0.7783 - val_accuracy: 0.7206\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5052 - accuracy: 0.7706\n",
      "Epoch 20: val_accuracy did not improve from 0.72065\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5052 - accuracy: 0.7706 - val_loss: 0.7745 - val_accuracy: 0.7085\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4787 - accuracy: 0.7980\n",
      "Epoch 21: val_accuracy did not improve from 0.72065\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.4791 - accuracy: 0.7994 - val_loss: 0.8877 - val_accuracy: 0.6964\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4541 - accuracy: 0.8030\n",
      "Epoch 22: val_accuracy improved from 0.72065 to 0.72874, saving model to ./ckpt/5_class_lr005_new/val_acc_0.729.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4541 - accuracy: 0.8030 - val_loss: 0.7446 - val_accuracy: 0.7287\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4734 - accuracy: 0.7971\n",
      "Epoch 23: val_accuracy did not improve from 0.72874\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4738 - accuracy: 0.7971 - val_loss: 0.8151 - val_accuracy: 0.6964\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4752 - accuracy: 0.7990\n",
      "Epoch 24: val_accuracy did not improve from 0.72874\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.4750 - accuracy: 0.7998 - val_loss: 0.7894 - val_accuracy: 0.6964\n",
      "Epoch 25/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4389 - accuracy: 0.8168\n",
      "Epoch 25: val_accuracy did not improve from 0.72874\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 0.4390 - accuracy: 0.8160 - val_loss: 0.8310 - val_accuracy: 0.7287\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4781 - accuracy: 0.8030\n",
      "Epoch 26: val_accuracy did not improve from 0.72874\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.4781 - accuracy: 0.8030 - val_loss: 0.7972 - val_accuracy: 0.7166\n",
      "Epoch 26: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 26s - loss: 1.6176 - accuracy: 0.2188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:58:19.710400: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.2191 - accuracy: 0.4385\n",
      "Epoch 1: val_accuracy improved from -inf to 0.53441, saving model to ./ckpt/5_class_lr005_new/val_acc_0.534.hdf5\n",
      "70/70 [==============================] - 2s 20ms/step - loss: 1.2191 - accuracy: 0.4385 - val_loss: 1.0256 - val_accuracy: 0.5344\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.1448 - accuracy: 0.3750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:58:21.130291: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 0.9569 - accuracy: 0.5732\n",
      "Epoch 2: val_accuracy improved from 0.53441 to 0.57490, saving model to ./ckpt/5_class_lr005_new/val_acc_0.575.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9569 - accuracy: 0.5732 - val_loss: 0.9318 - val_accuracy: 0.5749\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8593 - accuracy: 0.6199\n",
      "Epoch 3: val_accuracy improved from 0.57490 to 0.64777, saving model to ./ckpt/5_class_lr005_new/val_acc_0.648.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.8586 - accuracy: 0.6176 - val_loss: 0.8075 - val_accuracy: 0.6478\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.7717 - accuracy: 0.6530\n",
      "Epoch 4: val_accuracy improved from 0.64777 to 0.66802, saving model to ./ckpt/5_class_lr005_new/val_acc_0.668.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.7712 - accuracy: 0.6535 - val_loss: 0.7543 - val_accuracy: 0.6680\n",
      "Epoch 5/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.7214 - accuracy: 0.6889\n",
      "Epoch 5: val_accuracy improved from 0.66802 to 0.72874, saving model to ./ckpt/5_class_lr005_new/val_acc_0.729.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.7251 - accuracy: 0.6872 - val_loss: 0.6762 - val_accuracy: 0.7287\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6976 - accuracy: 0.6889\n",
      "Epoch 6: val_accuracy did not improve from 0.72874\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.6991 - accuracy: 0.6885 - val_loss: 0.7123 - val_accuracy: 0.6842\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6825 - accuracy: 0.6911\n",
      "Epoch 7: val_accuracy did not improve from 0.72874\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.6814 - accuracy: 0.6908 - val_loss: 0.7073 - val_accuracy: 0.6964\n",
      "Epoch 8/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6475 - accuracy: 0.7215\n",
      "Epoch 8: val_accuracy improved from 0.72874 to 0.73279, saving model to ./ckpt/5_class_lr005_new/val_acc_0.733.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.6477 - accuracy: 0.7204 - val_loss: 0.6513 - val_accuracy: 0.7328\n",
      "Epoch 9/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6395 - accuracy: 0.7215\n",
      "Epoch 9: val_accuracy did not improve from 0.73279\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6440 - accuracy: 0.7204 - val_loss: 0.6772 - val_accuracy: 0.6923\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6245 - accuracy: 0.7334\n",
      "Epoch 10: val_accuracy improved from 0.73279 to 0.75304, saving model to ./ckpt/5_class_lr005_new/val_acc_0.753.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6245 - accuracy: 0.7334 - val_loss: 0.6294 - val_accuracy: 0.7530\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.5841 - accuracy: 0.7482\n",
      "Epoch 11: val_accuracy did not improve from 0.75304\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 0.5850 - accuracy: 0.7487 - val_loss: 0.6593 - val_accuracy: 0.7045\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5434 - accuracy: 0.7636\n",
      "Epoch 12: val_accuracy did not improve from 0.75304\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5435 - accuracy: 0.7635 - val_loss: 0.6356 - val_accuracy: 0.7287\n",
      "Epoch 13/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5147 - accuracy: 0.7765\n",
      "Epoch 13: val_accuracy did not improve from 0.75304\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5152 - accuracy: 0.7738 - val_loss: 0.7428 - val_accuracy: 0.7085\n",
      "Epoch 14/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5436 - accuracy: 0.7649\n",
      "Epoch 14: val_accuracy did not improve from 0.75304\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5460 - accuracy: 0.7644 - val_loss: 0.8131 - val_accuracy: 0.6599\n",
      "Epoch 15/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5081 - accuracy: 0.7869\n",
      "Epoch 15: val_accuracy improved from 0.75304 to 0.76518, saving model to ./ckpt/5_class_lr005_new/val_acc_0.765.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5067 - accuracy: 0.7877 - val_loss: 0.5793 - val_accuracy: 0.7652\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4855 - accuracy: 0.8007\n",
      "Epoch 16: val_accuracy did not improve from 0.76518\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4997 - accuracy: 0.7949 - val_loss: 0.6024 - val_accuracy: 0.7449\n",
      "Epoch 17/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4850 - accuracy: 0.7983\n",
      "Epoch 17: val_accuracy did not improve from 0.76518\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4919 - accuracy: 0.7944 - val_loss: 0.6941 - val_accuracy: 0.7085\n",
      "Epoch 18/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4907 - accuracy: 0.7889\n",
      "Epoch 18: val_accuracy did not improve from 0.76518\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4927 - accuracy: 0.7882 - val_loss: 0.6098 - val_accuracy: 0.7449\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4626 - accuracy: 0.8049\n",
      "Epoch 19: val_accuracy did not improve from 0.76518\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4629 - accuracy: 0.8039 - val_loss: 0.6175 - val_accuracy: 0.7530\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4804 - accuracy: 0.7893\n",
      "Epoch 20: val_accuracy did not improve from 0.76518\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4781 - accuracy: 0.7913 - val_loss: 0.6435 - val_accuracy: 0.7328\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4392 - accuracy: 0.8168\n",
      "Epoch 21: val_accuracy did not improve from 0.76518\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4502 - accuracy: 0.8133 - val_loss: 0.5929 - val_accuracy: 0.7328\n",
      "Epoch 22/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4261 - accuracy: 0.8199\n",
      "Epoch 22: val_accuracy did not improve from 0.76518\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4224 - accuracy: 0.8218 - val_loss: 0.7878 - val_accuracy: 0.6761\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4463 - accuracy: 0.8130\n",
      "Epoch 23: val_accuracy did not improve from 0.76518\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4468 - accuracy: 0.8119 - val_loss: 0.6120 - val_accuracy: 0.7652\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4144 - accuracy: 0.8365\n",
      "Epoch 24: val_accuracy improved from 0.76518 to 0.78138, saving model to ./ckpt/5_class_lr005_new/val_acc_0.781.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4187 - accuracy: 0.8353 - val_loss: 0.6125 - val_accuracy: 0.7814\n",
      "Epoch 25/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4490 - accuracy: 0.8144\n",
      "Epoch 25: val_accuracy did not improve from 0.78138\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4495 - accuracy: 0.8137 - val_loss: 0.6036 - val_accuracy: 0.7814\n",
      "Epoch 25: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 26s - loss: 1.5449 - accuracy: 0.2188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:58:43.518959: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.3305 - accuracy: 0.3869\n",
      "Epoch 1: val_accuracy improved from -inf to 0.49798, saving model to ./ckpt/5_class_lr005_new/val_acc_0.498.hdf5\n",
      "70/70 [==============================] - 2s 18ms/step - loss: 1.3305 - accuracy: 0.3869 - val_loss: 0.9744 - val_accuracy: 0.4980\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.0521 - accuracy: 0.5625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 11:58:44.801098: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9900 - accuracy: 0.5540\n",
      "Epoch 2: val_accuracy improved from 0.49798 to 0.68016, saving model to ./ckpt/5_class_lr005_new/val_acc_0.680.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9857 - accuracy: 0.5543 - val_loss: 0.7755 - val_accuracy: 0.6802\n",
      "Epoch 3/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8820 - accuracy: 0.6019\n",
      "Epoch 3: val_accuracy did not improve from 0.68016\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.8820 - accuracy: 0.6019 - val_loss: 0.7685 - val_accuracy: 0.6478\n",
      "Epoch 4/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.7757 - accuracy: 0.6719\n",
      "Epoch 4: val_accuracy improved from 0.68016 to 0.68421, saving model to ./ckpt/5_class_lr005_new/val_acc_0.684.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.7737 - accuracy: 0.6741 - val_loss: 0.6989 - val_accuracy: 0.6842\n",
      "Epoch 5/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.7439 - accuracy: 0.6714\n",
      "Epoch 5: val_accuracy did not improve from 0.68421\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.7422 - accuracy: 0.6719 - val_loss: 0.7170 - val_accuracy: 0.6680\n",
      "Epoch 6/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.6839 - accuracy: 0.7088\n",
      "Epoch 6: val_accuracy improved from 0.68421 to 0.74089, saving model to ./ckpt/5_class_lr005_new/val_acc_0.741.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6813 - accuracy: 0.7105 - val_loss: 0.5929 - val_accuracy: 0.7409\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6355 - accuracy: 0.7260\n",
      "Epoch 7: val_accuracy did not improve from 0.74089\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6349 - accuracy: 0.7253 - val_loss: 0.6928 - val_accuracy: 0.7045\n",
      "Epoch 8/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.7031 - accuracy: 0.7063\n",
      "Epoch 8: val_accuracy did not improve from 0.74089\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.7027 - accuracy: 0.7060 - val_loss: 0.7346 - val_accuracy: 0.6599\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6446 - accuracy: 0.7249\n",
      "Epoch 9: val_accuracy did not improve from 0.74089\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.6446 - accuracy: 0.7249 - val_loss: 0.6361 - val_accuracy: 0.7126\n",
      "Epoch 10/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.6045 - accuracy: 0.7424\n",
      "Epoch 10: val_accuracy improved from 0.74089 to 0.76923, saving model to ./ckpt/5_class_lr005_new/val_acc_0.769.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6045 - accuracy: 0.7419 - val_loss: 0.6018 - val_accuracy: 0.7692\n",
      "Epoch 11/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.6133 - accuracy: 0.7410\n",
      "Epoch 11: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.6136 - accuracy: 0.7406 - val_loss: 0.7060 - val_accuracy: 0.7126\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5966 - accuracy: 0.7360\n",
      "Epoch 12: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5958 - accuracy: 0.7365 - val_loss: 0.6478 - val_accuracy: 0.6802\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5689 - accuracy: 0.7621\n",
      "Epoch 13: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5689 - accuracy: 0.7621 - val_loss: 0.5958 - val_accuracy: 0.7085\n",
      "Epoch 14/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5462 - accuracy: 0.7708\n",
      "Epoch 14: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.5483 - accuracy: 0.7702 - val_loss: 0.6405 - val_accuracy: 0.7166\n",
      "Epoch 15/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5174 - accuracy: 0.7868\n",
      "Epoch 15: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.5181 - accuracy: 0.7868 - val_loss: 0.6098 - val_accuracy: 0.7490\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5313 - accuracy: 0.7751\n",
      "Epoch 16: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5342 - accuracy: 0.7724 - val_loss: 0.5892 - val_accuracy: 0.7287\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5347 - accuracy: 0.7640\n",
      "Epoch 17: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5356 - accuracy: 0.7639 - val_loss: 0.5886 - val_accuracy: 0.7611\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4922 - accuracy: 0.7846\n",
      "Epoch 18: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4940 - accuracy: 0.7864 - val_loss: 0.6558 - val_accuracy: 0.7409\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.5051 - accuracy: 0.7907\n",
      "Epoch 19: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4994 - accuracy: 0.7935 - val_loss: 0.6044 - val_accuracy: 0.7449\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4732 - accuracy: 0.8061\n",
      "Epoch 20: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4732 - accuracy: 0.8061 - val_loss: 0.6743 - val_accuracy: 0.6964\n",
      "Epoch 21/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5246 - accuracy: 0.7822\n",
      "Epoch 21: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.5248 - accuracy: 0.7823 - val_loss: 0.5629 - val_accuracy: 0.7449\n",
      "Epoch 22/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4930 - accuracy: 0.7868\n",
      "Epoch 22: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4973 - accuracy: 0.7868 - val_loss: 0.6570 - val_accuracy: 0.7126\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4707 - accuracy: 0.8021\n",
      "Epoch 23: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4707 - accuracy: 0.8021 - val_loss: 0.5974 - val_accuracy: 0.7247\n",
      "Epoch 24/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4646 - accuracy: 0.8139\n",
      "Epoch 24: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4590 - accuracy: 0.8160 - val_loss: 0.5882 - val_accuracy: 0.7409\n",
      "Epoch 25/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4467 - accuracy: 0.8229\n",
      "Epoch 25: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4467 - accuracy: 0.8218 - val_loss: 0.6245 - val_accuracy: 0.7409\n",
      "Epoch 26/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4457 - accuracy: 0.8222\n",
      "Epoch 26: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.4484 - accuracy: 0.8232 - val_loss: 0.5919 - val_accuracy: 0.7328\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4269 - accuracy: 0.8307\n",
      "Epoch 27: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.4287 - accuracy: 0.8294 - val_loss: 0.6739 - val_accuracy: 0.7206\n",
      "Epoch 28/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4264 - accuracy: 0.8201\n",
      "Epoch 28: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4312 - accuracy: 0.8182 - val_loss: 0.6614 - val_accuracy: 0.7409\n",
      "Epoch 29/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4211 - accuracy: 0.8324\n",
      "Epoch 29: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4233 - accuracy: 0.8308 - val_loss: 0.6857 - val_accuracy: 0.7206\n",
      "Epoch 30/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3981 - accuracy: 0.8404\n",
      "Epoch 30: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.4012 - accuracy: 0.8384 - val_loss: 0.6964 - val_accuracy: 0.7449\n",
      "Epoch 31/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3999 - accuracy: 0.8338\n",
      "Epoch 31: val_accuracy did not improve from 0.76923\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.3986 - accuracy: 0.8353 - val_loss: 0.6132 - val_accuracy: 0.7409\n",
      "Epoch 31: early stopping\n",
      "Finish 10-fold cross validation\n",
      "Best performing model has 0.7895 validation accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# modify to save ckpt for each test\n",
    "ckpt = os.path.join(ckpt_path, \"val_acc_{val_accuracy:.3f}.hdf5\")\n",
    "\n",
    "# training params\n",
    "epochs = epochs\n",
    "num_classes = num_classes\n",
    "lr = lr\n",
    "\n",
    "# the k for k fold CV\n",
    "n_split = 10\n",
    "\n",
    "# for recording best performance\n",
    "max_acc = 0\n",
    "best_history = None\n",
    "\n",
    "'''\n",
    "k-fold cross validation\n",
    "Save the best model using validation accuracy as metric\n",
    "Print the global best performace when finished\n",
    "'''\n",
    "for train_index, test_index in KFold(n_split).split(train_examples):\n",
    "\n",
    "    x_train, x_vad = train_examples[train_index], train_examples[test_index]\n",
    "    y_train, y_vad = train_labels[train_index], train_labels[test_index]\n",
    "\n",
    "    model=create_model(num_classes, lr)\n",
    "  \n",
    "    # callbacks\n",
    "    checkpoint_filepath = ckpt\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "        mode='auto',\n",
    "    )\n",
    "\n",
    "    # Model weights are saved at the end of every epoch, if it's the best seen so far.\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_vad, y_vad),\n",
    "                        callbacks=[model_checkpoint_callback, early_stopping_callback])\n",
    "\n",
    "    val_acc = max(history.history['val_accuracy'])\n",
    "    \n",
    "    if val_acc > max_acc:\n",
    "        max_acc = val_acc\n",
    "        best_history = history\n",
    "        # print('Best acc so far. Saving params...\\n')\n",
    "\n",
    "print('Finish {}-fold cross validation'.format(n_split))\n",
    "print('Best performing model has {:.4f} validation accuracy'.format(max_acc))\n",
    "\n",
    "#CPU\n",
    "# with tf.device('/CPU:0'):\n",
    "#     history = model.fit(trainX, trainY, epochs=epochs,validation_data=(testX, testY), batch_size=64)\n",
    "\n",
    "# deafult go with GPU\n",
    "# history = model.fit(trainX, trainY, epochs=epochs,validation_data=(testX, testY), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "history = best_history\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# because of early stopping, can't just use \"epochs\"\n",
    "epochs_range = range(len(history.history['loss']))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - ETA: 0s - loss: 0.8977 - accuracy: 0.6307"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 23:46:38.230611: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 11ms/step - loss: 0.8977 - accuracy: 0.6307\n",
      "\n",
      "evaluate on test set:\n",
      "loss = 0.89768\tacc = 63.073%\n"
     ]
    }
   ],
   "source": [
    "model = create_model(num_classes=num_classes)\n",
    "\n",
    "model.load_weights(os.path.join(ckpt_path, \"val_acc_0.632.hdf5\"))\n",
    "\n",
    "loss, acc = model.evaluate(test_examples, test_labels)\n",
    "\n",
    "print('\\nevaluate on test set:\\nloss = {:.5f}\\tacc = {:.3f}%'.format(loss, acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29cc21816e506614f017a9125cfdb1f5dc655865e499c52ef5f5406a40d25695"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
