{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold CV with 3-layer CNN avg pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and env settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.min_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables/parameters used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../../data/home_sale_data_324_features_3_class.csv'\n",
    "\n",
    "ckpt_path = \"./ckpt/3_class_lr005/\"\n",
    "os.makedirs(ckpt_path, exist_ok=True)\n",
    "\n",
    "num_classes = 3\n",
    "lr = 0.005\n",
    "epochs = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>TotalSF</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>Total_Bathrooms</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>YrBltAndRemod</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>Foundation_PConc</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>hasfireplace</th>\n",
       "      <th>ExterQual_Gd</th>\n",
       "      <th>BsmtQual_Ex</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>HeatingQC_Ex</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>Total_porch_sf</th>\n",
       "      <th>BsmtFinType1_GLQ</th>\n",
       "      <th>KitchenQual_Ex</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>GarageFinish_Fin</th>\n",
       "      <th>...</th>\n",
       "      <th>BsmtExposure_No</th>\n",
       "      <th>Neighborhood_OldTown</th>\n",
       "      <th>Foundation_BrkTil</th>\n",
       "      <th>GarageFinish_None</th>\n",
       "      <th>GarageCond_None</th>\n",
       "      <th>GarageQual_None</th>\n",
       "      <th>GarageType_None</th>\n",
       "      <th>MSSubClass_30</th>\n",
       "      <th>LotShape_Reg</th>\n",
       "      <th>PavedDrive_N</th>\n",
       "      <th>Foundation_CBlock</th>\n",
       "      <th>MSZoning_RM</th>\n",
       "      <th>HeatingQC_TA</th>\n",
       "      <th>CentralAir_N</th>\n",
       "      <th>GarageType_Detchd</th>\n",
       "      <th>MasVnrType_None</th>\n",
       "      <th>GarageFinish_Unf</th>\n",
       "      <th>BsmtQual_TA</th>\n",
       "      <th>FireplaceQu_None</th>\n",
       "      <th>KitchenQual_TA</th>\n",
       "      <th>ExterQual_TA</th>\n",
       "      <th>dummy_1</th>\n",
       "      <th>dummy_2</th>\n",
       "      <th>dummy_3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2681</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.785760</td>\n",
       "      <td>0.556709</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.579170</td>\n",
       "      <td>0.532258</td>\n",
       "      <td>0.957895</td>\n",
       "      <td>0.635059</td>\n",
       "      <td>0.741287</td>\n",
       "      <td>0.971015</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.856754</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166890</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.607283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.492903</td>\n",
       "      <td>0.253387</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.251426</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.390830</td>\n",
       "      <td>0.541894</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400920</td>\n",
       "      <td>0.237635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.545168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.523996</td>\n",
       "      <td>0.272999</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.327743</td>\n",
       "      <td>0.325269</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.421085</td>\n",
       "      <td>0.574935</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.516936</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.367342</td>\n",
       "      <td>0.325200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.180865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.621022</td>\n",
       "      <td>0.385578</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.088659</td>\n",
       "      <td>0.284946</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.413321</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.674300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.485799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.521422</td>\n",
       "      <td>0.272795</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.164976</td>\n",
       "      <td>0.362231</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.420774</td>\n",
       "      <td>0.572204</td>\n",
       "      <td>0.927536</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.516936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.290868</td>\n",
       "      <td>0.277398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OverallQual  GrLivArea   TotalSF  GarageCars  Total_Bathrooms  \\\n",
       "2681     0.888889   0.785760  0.556709         0.6         0.579170   \n",
       "14       0.555556   0.492903  0.253387         0.2         0.251426   \n",
       "1669     0.666667   0.523996  0.272999         0.4         0.327743   \n",
       "986      0.777778   0.621022  0.385578         0.4         0.088659   \n",
       "737      0.666667   0.521422  0.272795         0.4         0.164976   \n",
       "\n",
       "      GarageArea  YrBltAndRemod  TotalBsmtSF  1stFlrSF  YearBuilt  FullBath  \\\n",
       "2681    0.532258       0.957895     0.635059  0.741287   0.971015      0.75   \n",
       "14      0.236559       0.473684     0.390830  0.541894   0.637681      0.25   \n",
       "1669    0.325269       0.973684     0.421085  0.574935   0.978261      0.50   \n",
       "986     0.284946       0.200000     0.285714  0.413321   0.072464      0.25   \n",
       "737     0.362231       0.900000     0.420774  0.572204   0.927536      0.50   \n",
       "\n",
       "      YearRemodAdd  Foundation_PConc  TotRmsAbvGrd  hasfireplace  \\\n",
       "2681      0.933333               1.0      0.856754           1.0   \n",
       "14        0.166667               0.0      0.421336           1.0   \n",
       "1669      0.966667               1.0      0.516936           1.0   \n",
       "986       0.600000               0.0      0.674300           1.0   \n",
       "737       0.850000               1.0      0.516936           0.0   \n",
       "\n",
       "      ExterQual_Gd  BsmtQual_Ex  Fireplaces  HeatingQC_Ex  MasVnrArea  \\\n",
       "2681           1.0          1.0    0.293793           1.0    0.000000   \n",
       "14             0.0          0.0    0.293793           0.0    0.400920   \n",
       "1669           1.0          0.0    0.293793           1.0    0.367342   \n",
       "986            1.0          0.0    0.293793           0.0    0.000000   \n",
       "737            0.0          0.0    0.000000           1.0    0.290868   \n",
       "\n",
       "      Total_porch_sf  BsmtFinType1_GLQ  KitchenQual_Ex  OpenPorchSF  \\\n",
       "2681        0.166890               1.0             1.0     0.607283   \n",
       "14          0.237635               0.0             0.0     0.545168   \n",
       "1669        0.325200               1.0             0.0     0.180865   \n",
       "986         0.133505               0.0             0.0     0.485799   \n",
       "737         0.277398               0.0             0.0     0.272513   \n",
       "\n",
       "      GarageFinish_Fin  ...  BsmtExposure_No  Neighborhood_OldTown  \\\n",
       "2681               0.0  ...              1.0                   0.0   \n",
       "14                 0.0  ...              1.0                   0.0   \n",
       "1669               0.0  ...              1.0                   0.0   \n",
       "986                0.0  ...              1.0                   1.0   \n",
       "737                0.0  ...              1.0                   0.0   \n",
       "\n",
       "      Foundation_BrkTil  GarageFinish_None  GarageCond_None  GarageQual_None  \\\n",
       "2681                0.0                0.0              0.0              0.0   \n",
       "14                  0.0                0.0              0.0              0.0   \n",
       "1669                0.0                0.0              0.0              0.0   \n",
       "986                 1.0                0.0              0.0              0.0   \n",
       "737                 0.0                0.0              0.0              0.0   \n",
       "\n",
       "      GarageType_None  MSSubClass_30  LotShape_Reg  PavedDrive_N  \\\n",
       "2681              0.0            0.0           1.0           0.0   \n",
       "14                0.0            0.0           0.0           0.0   \n",
       "1669              0.0            0.0           0.0           0.0   \n",
       "986               0.0            0.0           1.0           0.0   \n",
       "737               0.0            0.0           1.0           0.0   \n",
       "\n",
       "      Foundation_CBlock  MSZoning_RM  HeatingQC_TA  CentralAir_N  \\\n",
       "2681                0.0          0.0           0.0           0.0   \n",
       "14                  1.0          0.0           1.0           0.0   \n",
       "1669                0.0          0.0           0.0           0.0   \n",
       "986                 0.0          1.0           0.0           0.0   \n",
       "737                 0.0          0.0           0.0           0.0   \n",
       "\n",
       "      GarageType_Detchd  MasVnrType_None  GarageFinish_Unf  BsmtQual_TA  \\\n",
       "2681                0.0              1.0               0.0          0.0   \n",
       "14                  0.0              0.0               0.0          1.0   \n",
       "1669                0.0              0.0               0.0          0.0   \n",
       "986                 0.0              1.0               1.0          1.0   \n",
       "737                 0.0              0.0               1.0          0.0   \n",
       "\n",
       "      FireplaceQu_None  KitchenQual_TA  ExterQual_TA  dummy_1  dummy_2  \\\n",
       "2681               0.0             0.0           0.0      0.0      0.0   \n",
       "14                 0.0             1.0           1.0      0.0      0.0   \n",
       "1669               0.0             0.0           0.0      0.0      0.0   \n",
       "986                0.0             0.0           0.0      0.0      0.0   \n",
       "737                1.0             1.0           1.0      0.0      0.0   \n",
       "\n",
       "      dummy_3  label  \n",
       "2681      0.0     hi  \n",
       "14        0.0    mid  \n",
       "1669      0.0     hi  \n",
       "986       0.0    mid  \n",
       "737       0.0    mid  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(file)\n",
    "\n",
    "'''suffle rows randomly'''\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "labels = data['label']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>TotalSF</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>Total_Bathrooms</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>YrBltAndRemod</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>Foundation_PConc</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>hasfireplace</th>\n",
       "      <th>ExterQual_Gd</th>\n",
       "      <th>BsmtQual_Ex</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>HeatingQC_Ex</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>Total_porch_sf</th>\n",
       "      <th>BsmtFinType1_GLQ</th>\n",
       "      <th>KitchenQual_Ex</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>GarageFinish_Fin</th>\n",
       "      <th>...</th>\n",
       "      <th>Neighborhood_IDOTRR</th>\n",
       "      <th>BsmtExposure_No</th>\n",
       "      <th>Neighborhood_OldTown</th>\n",
       "      <th>Foundation_BrkTil</th>\n",
       "      <th>GarageFinish_None</th>\n",
       "      <th>GarageCond_None</th>\n",
       "      <th>GarageQual_None</th>\n",
       "      <th>GarageType_None</th>\n",
       "      <th>MSSubClass_30</th>\n",
       "      <th>LotShape_Reg</th>\n",
       "      <th>PavedDrive_N</th>\n",
       "      <th>Foundation_CBlock</th>\n",
       "      <th>MSZoning_RM</th>\n",
       "      <th>HeatingQC_TA</th>\n",
       "      <th>CentralAir_N</th>\n",
       "      <th>GarageType_Detchd</th>\n",
       "      <th>MasVnrType_None</th>\n",
       "      <th>GarageFinish_Unf</th>\n",
       "      <th>BsmtQual_TA</th>\n",
       "      <th>FireplaceQu_None</th>\n",
       "      <th>KitchenQual_TA</th>\n",
       "      <th>ExterQual_TA</th>\n",
       "      <th>dummy_1</th>\n",
       "      <th>dummy_2</th>\n",
       "      <th>dummy_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.0</td>\n",
       "      <td>2911.0</td>\n",
       "      <td>2911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.565136</td>\n",
       "      <td>0.542763</td>\n",
       "      <td>0.293431</td>\n",
       "      <td>0.353143</td>\n",
       "      <td>0.200863</td>\n",
       "      <td>0.317285</td>\n",
       "      <td>0.660811</td>\n",
       "      <td>0.326706</td>\n",
       "      <td>0.487996</td>\n",
       "      <td>0.719569</td>\n",
       "      <td>0.391876</td>\n",
       "      <td>0.570892</td>\n",
       "      <td>0.447956</td>\n",
       "      <td>0.542786</td>\n",
       "      <td>0.512882</td>\n",
       "      <td>0.335967</td>\n",
       "      <td>0.087255</td>\n",
       "      <td>0.171718</td>\n",
       "      <td>0.511508</td>\n",
       "      <td>0.159954</td>\n",
       "      <td>0.203170</td>\n",
       "      <td>0.290622</td>\n",
       "      <td>0.069392</td>\n",
       "      <td>0.176542</td>\n",
       "      <td>0.245620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031261</td>\n",
       "      <td>0.652697</td>\n",
       "      <td>0.082102</td>\n",
       "      <td>0.106493</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.053590</td>\n",
       "      <td>0.047750</td>\n",
       "      <td>0.637582</td>\n",
       "      <td>0.073514</td>\n",
       "      <td>0.423222</td>\n",
       "      <td>0.158708</td>\n",
       "      <td>0.293370</td>\n",
       "      <td>0.066644</td>\n",
       "      <td>0.266919</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.421848</td>\n",
       "      <td>0.439368</td>\n",
       "      <td>0.487118</td>\n",
       "      <td>0.511852</td>\n",
       "      <td>0.616627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.155988</td>\n",
       "      <td>0.125193</td>\n",
       "      <td>0.119772</td>\n",
       "      <td>0.152244</td>\n",
       "      <td>0.133043</td>\n",
       "      <td>0.143976</td>\n",
       "      <td>0.242686</td>\n",
       "      <td>0.131930</td>\n",
       "      <td>0.131369</td>\n",
       "      <td>0.219351</td>\n",
       "      <td>0.138140</td>\n",
       "      <td>0.348189</td>\n",
       "      <td>0.497369</td>\n",
       "      <td>0.128554</td>\n",
       "      <td>0.499920</td>\n",
       "      <td>0.472409</td>\n",
       "      <td>0.282257</td>\n",
       "      <td>0.181727</td>\n",
       "      <td>0.499953</td>\n",
       "      <td>0.218350</td>\n",
       "      <td>0.163524</td>\n",
       "      <td>0.454127</td>\n",
       "      <td>0.254163</td>\n",
       "      <td>0.183129</td>\n",
       "      <td>0.430528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174051</td>\n",
       "      <td>0.476195</td>\n",
       "      <td>0.274568</td>\n",
       "      <td>0.308520</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.225245</td>\n",
       "      <td>0.213273</td>\n",
       "      <td>0.480781</td>\n",
       "      <td>0.261024</td>\n",
       "      <td>0.494155</td>\n",
       "      <td>0.365467</td>\n",
       "      <td>0.455385</td>\n",
       "      <td>0.249447</td>\n",
       "      <td>0.442425</td>\n",
       "      <td>0.488798</td>\n",
       "      <td>0.493939</td>\n",
       "      <td>0.496395</td>\n",
       "      <td>0.499920</td>\n",
       "      <td>0.499945</td>\n",
       "      <td>0.486292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.451636</td>\n",
       "      <td>0.213557</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.088659</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.247193</td>\n",
       "      <td>0.395003</td>\n",
       "      <td>0.590580</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.547807</td>\n",
       "      <td>0.282326</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.164976</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.652632</td>\n",
       "      <td>0.308172</td>\n",
       "      <td>0.481664</td>\n",
       "      <td>0.731884</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516936</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.621247</td>\n",
       "      <td>0.356447</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.252530</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.405490</td>\n",
       "      <td>0.582574</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.353218</td>\n",
       "      <td>0.327089</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 324 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       OverallQual    GrLivArea      TotalSF   GarageCars  Total_Bathrooms  \\\n",
       "count  2911.000000  2911.000000  2911.000000  2911.000000      2911.000000   \n",
       "mean      0.565136     0.542763     0.293431     0.353143         0.200863   \n",
       "std       0.155988     0.125193     0.119772     0.152244         0.133043   \n",
       "min       0.000000     0.000000     0.000000     0.000000         0.000000   \n",
       "25%       0.444444     0.451636     0.213557     0.200000         0.088659   \n",
       "50%       0.555556     0.547807     0.282326     0.400000         0.164976   \n",
       "75%       0.666667     0.621247     0.356447     0.400000         0.252530   \n",
       "max       1.000000     1.000000     1.000000     1.000000         1.000000   \n",
       "\n",
       "        GarageArea  YrBltAndRemod  TotalBsmtSF     1stFlrSF    YearBuilt  \\\n",
       "count  2911.000000    2911.000000  2911.000000  2911.000000  2911.000000   \n",
       "mean      0.317285       0.660811     0.326706     0.487996     0.719569   \n",
       "std       0.143976       0.242686     0.131930     0.131369     0.219351   \n",
       "min       0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.215054       0.473684     0.247193     0.395003     0.590580   \n",
       "50%       0.322581       0.652632     0.308172     0.481664     0.731884   \n",
       "75%       0.387097       0.905263     0.405490     0.582574     0.934783   \n",
       "max       1.000000       1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          FullBath  YearRemodAdd  Foundation_PConc  TotRmsAbvGrd  \\\n",
       "count  2911.000000   2911.000000       2911.000000   2911.000000   \n",
       "mean      0.391876      0.570892          0.447956      0.542786   \n",
       "std       0.138140      0.348189          0.497369      0.128554   \n",
       "min       0.000000      0.000000          0.000000      0.000000   \n",
       "25%       0.250000      0.250000          0.000000      0.421336   \n",
       "50%       0.500000      0.716667          0.000000      0.516936   \n",
       "75%       0.500000      0.900000          1.000000      0.600315   \n",
       "max       1.000000      1.000000          1.000000      1.000000   \n",
       "\n",
       "       hasfireplace  ExterQual_Gd  BsmtQual_Ex   Fireplaces  HeatingQC_Ex  \\\n",
       "count   2911.000000   2911.000000  2911.000000  2911.000000   2911.000000   \n",
       "mean       0.512882      0.335967     0.087255     0.171718      0.511508   \n",
       "std        0.499920      0.472409     0.282257     0.181727      0.499953   \n",
       "min        0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "25%        0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "50%        1.000000      0.000000     0.000000     0.293793      1.000000   \n",
       "75%        1.000000      1.000000     0.000000     0.293793      1.000000   \n",
       "max        1.000000      1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "        MasVnrArea  Total_porch_sf  BsmtFinType1_GLQ  KitchenQual_Ex  \\\n",
       "count  2911.000000     2911.000000       2911.000000     2911.000000   \n",
       "mean      0.159954        0.203170          0.290622        0.069392   \n",
       "std       0.218350        0.163524          0.454127        0.254163   \n",
       "min       0.000000        0.000000          0.000000        0.000000   \n",
       "25%       0.000000        0.066964          0.000000        0.000000   \n",
       "50%       0.000000        0.179849          0.000000        0.000000   \n",
       "75%       0.353218        0.327089          1.000000        0.000000   \n",
       "max       1.000000        1.000000          1.000000        1.000000   \n",
       "\n",
       "       OpenPorchSF  GarageFinish_Fin  ...  Neighborhood_IDOTRR  \\\n",
       "count  2911.000000       2911.000000  ...          2911.000000   \n",
       "mean      0.176542          0.245620  ...             0.031261   \n",
       "std       0.183129          0.430528  ...             0.174051   \n",
       "min       0.000000          0.000000  ...             0.000000   \n",
       "25%       0.000000          0.000000  ...             0.000000   \n",
       "50%       0.180865          0.000000  ...             0.000000   \n",
       "75%       0.309510          0.000000  ...             0.000000   \n",
       "max       1.000000          1.000000  ...             1.000000   \n",
       "\n",
       "       BsmtExposure_No  Neighborhood_OldTown  Foundation_BrkTil  \\\n",
       "count      2911.000000           2911.000000        2911.000000   \n",
       "mean          0.652697              0.082102           0.106493   \n",
       "std           0.476195              0.274568           0.308520   \n",
       "min           0.000000              0.000000           0.000000   \n",
       "25%           0.000000              0.000000           0.000000   \n",
       "50%           1.000000              0.000000           0.000000   \n",
       "75%           1.000000              0.000000           0.000000   \n",
       "max           1.000000              1.000000           1.000000   \n",
       "\n",
       "       GarageFinish_None  GarageCond_None  GarageQual_None  GarageType_None  \\\n",
       "count        2911.000000      2911.000000      2911.000000      2911.000000   \n",
       "mean            0.054277         0.054277         0.054277         0.053590   \n",
       "std             0.226602         0.226602         0.226602         0.225245   \n",
       "min             0.000000         0.000000         0.000000         0.000000   \n",
       "25%             0.000000         0.000000         0.000000         0.000000   \n",
       "50%             0.000000         0.000000         0.000000         0.000000   \n",
       "75%             0.000000         0.000000         0.000000         0.000000   \n",
       "max             1.000000         1.000000         1.000000         1.000000   \n",
       "\n",
       "       MSSubClass_30  LotShape_Reg  PavedDrive_N  Foundation_CBlock  \\\n",
       "count    2911.000000   2911.000000   2911.000000        2911.000000   \n",
       "mean        0.047750      0.637582      0.073514           0.423222   \n",
       "std         0.213273      0.480781      0.261024           0.494155   \n",
       "min         0.000000      0.000000      0.000000           0.000000   \n",
       "25%         0.000000      0.000000      0.000000           0.000000   \n",
       "50%         0.000000      1.000000      0.000000           0.000000   \n",
       "75%         0.000000      1.000000      0.000000           1.000000   \n",
       "max         1.000000      1.000000      1.000000           1.000000   \n",
       "\n",
       "       MSZoning_RM  HeatingQC_TA  CentralAir_N  GarageType_Detchd  \\\n",
       "count  2911.000000   2911.000000   2911.000000        2911.000000   \n",
       "mean      0.158708      0.293370      0.066644           0.266919   \n",
       "std       0.365467      0.455385      0.249447           0.442425   \n",
       "min       0.000000      0.000000      0.000000           0.000000   \n",
       "25%       0.000000      0.000000      0.000000           0.000000   \n",
       "50%       0.000000      0.000000      0.000000           0.000000   \n",
       "75%       0.000000      1.000000      0.000000           1.000000   \n",
       "max       1.000000      1.000000      1.000000           1.000000   \n",
       "\n",
       "       MasVnrType_None  GarageFinish_Unf  BsmtQual_TA  FireplaceQu_None  \\\n",
       "count      2911.000000       2911.000000  2911.000000       2911.000000   \n",
       "mean          0.605634          0.421848     0.439368          0.487118   \n",
       "std           0.488798          0.493939     0.496395          0.499920   \n",
       "min           0.000000          0.000000     0.000000          0.000000   \n",
       "25%           0.000000          0.000000     0.000000          0.000000   \n",
       "50%           1.000000          0.000000     0.000000          0.000000   \n",
       "75%           1.000000          1.000000     1.000000          1.000000   \n",
       "max           1.000000          1.000000     1.000000          1.000000   \n",
       "\n",
       "       KitchenQual_TA  ExterQual_TA  dummy_1  dummy_2  dummy_3  \n",
       "count     2911.000000   2911.000000   2911.0   2911.0   2911.0  \n",
       "mean         0.511852      0.616627      0.0      0.0      0.0  \n",
       "std          0.499945      0.486292      0.0      0.0      0.0  \n",
       "min          0.000000      0.000000      0.0      0.0      0.0  \n",
       "25%          0.000000      0.000000      0.0      0.0      0.0  \n",
       "50%          1.000000      1.000000      0.0      0.0      0.0  \n",
       "75%          1.000000      1.000000      0.0      0.0      0.0  \n",
       "max          1.000000      1.000000      0.0      0.0      0.0  \n",
       "\n",
       "[8 rows x 324 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop label column\n",
    "data.drop(['label'], axis=1, inplace=True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hi', 'mid', 'low'], dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  3\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "'''one-hot encode the labels'''\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "# print(list(integer_encoded))\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "labels = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "print('Number of classes: ', len(labels[0]))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 18, 18)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_np = data.to_numpy()\n",
    "data_np.shape\n",
    "data_np = data_np.reshape(len(data), 18, 18)\n",
    "data_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data_np[0].shape\n",
    "\n",
    "nArray = np.array(data_np[99])\n",
    "\n",
    "\n",
    "a11=nArray.reshape(18,18)\n",
    "plt.imshow(a11)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 3)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 18, 18)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_examples = data_np\n",
    "all_examples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train test splitting\n",
    "- hold out 15% for testing\n",
    "- use 85% to train model with K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_samples = all_examples.shape[0] \n",
    "test_ratio = 0.15\n",
    "test_samples = int(test_ratio * all_examples.shape[0])\n",
    "\n",
    "train_examples = all_examples[:-1*test_samples]\n",
    "test_examples = all_examples[-1*test_samples:]\n",
    "\n",
    "train_labels = labels[:-1*test_samples]\n",
    "test_labels = labels[-1*test_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (2475, 18, 18)\n",
      "test:  (436, 18, 18)\n",
      "train label:  (2475, 3)\n",
      "test label:  (436, 3)\n"
     ]
    }
   ],
   "source": [
    "print('train: ', train_examples.shape)\n",
    "print('test: ', test_examples.shape)\n",
    "print('train label: ', train_labels.shape)\n",
    "print('test label: ', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainX = train_examples.reshape(ttl_samples-test_samples, 18,18,1)\n",
    "# trainY = train_labels\n",
    "\n",
    "# testX = test_examples.reshape(test_samples, 18,18,1)\n",
    "# testY = test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "\n",
    "def create_model(num_classes=5, lr=0.005):\n",
    "\n",
    "\t# Working\n",
    "\twith tf.device('/cpu:0'):\n",
    "\t\tdata_augmentation = tf.keras.Sequential([ \n",
    "\t\t\ttf.keras.layers.RandomFlip(\"horizontal\", input_shape=(18, 18, 1)),\n",
    "\t  \t\ttf.keras.layers.RandomRotation(0.1),\n",
    "\t\t    tf.keras.layers.RandomZoom(0.1)\n",
    "\t\t\t])\n",
    "\n",
    "\n",
    "\tmodel = tf.keras.Sequential([\n",
    "\t\t# data_augmentation,\n",
    "\t  \t# tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(18, 18, 1)),\n",
    "\t\ttf.keras.layers.AveragePooling2D((2, 2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.AveragePooling2D((2,2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t  \ttf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.AveragePooling2D(),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Flatten(),\n",
    "\t\ttf.keras.layers.Dense(128, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "\t])\n",
    "\n",
    "\t# opt = tf.keras.optimizers.SGD(lr=0.005, momentum=0.9)\n",
    "\topt = tf.keras.optimizers.Adam(lr=lr)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_51 (Conv2D)          (None, 18, 18, 16)        160       \n",
      "                                                                 \n",
      " average_pooling2d_51 (Avera  (None, 9, 9, 16)         0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 9, 9, 16)          0         \n",
      "                                                                 \n",
      " conv2d_52 (Conv2D)          (None, 9, 9, 32)          4640      \n",
      "                                                                 \n",
      " average_pooling2d_52 (Avera  (None, 4, 4, 32)         0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 4, 4, 32)          0         \n",
      "                                                                 \n",
      " conv2d_53 (Conv2D)          (None, 4, 4, 64)          18496     \n",
      "                                                                 \n",
      " average_pooling2d_53 (Avera  (None, 2, 2, 64)         0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 2, 2, 64)          0         \n",
      "                                                                 \n",
      " flatten_17 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,837\n",
      "Trainable params: 56,837\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evantilu/miniforge3/envs/6998_DL_tf/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:07:54.886371: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 2.0115 - accuracy: 0.2075\n",
      "Epoch 1: val_accuracy improved from -inf to 0.28226, saving model to ./ckpt/10_class_lr005/val_acc_0.282.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 2.0115 - accuracy: 0.2075 - val_loss: 1.7194 - val_accuracy: 0.2823\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.8748 - accuracy: 0.1875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:07:56.032850: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 1.6924 - accuracy: 0.3050\n",
      "Epoch 2: val_accuracy improved from 0.28226 to 0.34274, saving model to ./ckpt/10_class_lr005/val_acc_0.343.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.6933 - accuracy: 0.3026 - val_loss: 1.6846 - val_accuracy: 0.3427\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.6209 - accuracy: 0.3274\n",
      "Epoch 3: val_accuracy improved from 0.34274 to 0.38306, saving model to ./ckpt/10_class_lr005/val_acc_0.383.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.6223 - accuracy: 0.3273 - val_loss: 1.5456 - val_accuracy: 0.3831\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.5429 - accuracy: 0.3458\n",
      "Epoch 4: val_accuracy improved from 0.38306 to 0.39919, saving model to ./ckpt/10_class_lr005/val_acc_0.399.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.5429 - accuracy: 0.3458 - val_loss: 1.4750 - val_accuracy: 0.3992\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4727 - accuracy: 0.3705\n",
      "Epoch 5: val_accuracy improved from 0.39919 to 0.42339, saving model to ./ckpt/10_class_lr005/val_acc_0.423.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.4716 - accuracy: 0.3705 - val_loss: 1.3920 - val_accuracy: 0.4234\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3682 - accuracy: 0.4162\n",
      "Epoch 6: val_accuracy did not improve from 0.42339\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.3700 - accuracy: 0.4149 - val_loss: 1.4477 - val_accuracy: 0.3710\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.3592 - accuracy: 0.4062\n",
      "Epoch 7: val_accuracy improved from 0.42339 to 0.47984, saving model to ./ckpt/10_class_lr005/val_acc_0.480.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.3588 - accuracy: 0.4073 - val_loss: 1.3072 - val_accuracy: 0.4798\n",
      "Epoch 8/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.3151 - accuracy: 0.4413\n",
      "Epoch 8: val_accuracy did not improve from 0.47984\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.3241 - accuracy: 0.4378 - val_loss: 1.2677 - val_accuracy: 0.4435\n",
      "Epoch 9/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.2797 - accuracy: 0.4478\n",
      "Epoch 9: val_accuracy did not improve from 0.47984\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.2814 - accuracy: 0.4472 - val_loss: 1.2782 - val_accuracy: 0.4677\n",
      "Epoch 10/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.1946 - accuracy: 0.4789\n",
      "Epoch 10: val_accuracy did not improve from 0.47984\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1921 - accuracy: 0.4800 - val_loss: 1.2905 - val_accuracy: 0.4798\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2169 - accuracy: 0.4809\n",
      "Epoch 11: val_accuracy did not improve from 0.47984\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.2169 - accuracy: 0.4809 - val_loss: 1.3052 - val_accuracy: 0.4516\n",
      "Epoch 12/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1588 - accuracy: 0.5005\n",
      "Epoch 12: val_accuracy did not improve from 0.47984\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1628 - accuracy: 0.4998 - val_loss: 1.2219 - val_accuracy: 0.4435\n",
      "Epoch 13/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.1420 - accuracy: 0.4954\n",
      "Epoch 13: val_accuracy did not improve from 0.47984\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1416 - accuracy: 0.4953 - val_loss: 1.2560 - val_accuracy: 0.4516\n",
      "Epoch 14/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1487 - accuracy: 0.5114\n",
      "Epoch 14: val_accuracy did not improve from 0.47984\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1503 - accuracy: 0.5092 - val_loss: 1.1602 - val_accuracy: 0.4718\n",
      "Epoch 15/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1123 - accuracy: 0.5271\n",
      "Epoch 15: val_accuracy did not improve from 0.47984\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1128 - accuracy: 0.5249 - val_loss: 1.2335 - val_accuracy: 0.4758\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0775 - accuracy: 0.5412\n",
      "Epoch 16: val_accuracy improved from 0.47984 to 0.53629, saving model to ./ckpt/10_class_lr005/val_acc_0.536.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0776 - accuracy: 0.5415 - val_loss: 1.1085 - val_accuracy: 0.5363\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0820 - accuracy: 0.5303\n",
      "Epoch 17: val_accuracy did not improve from 0.53629\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0783 - accuracy: 0.5330 - val_loss: 1.3360 - val_accuracy: 0.3992\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0381 - accuracy: 0.5599\n",
      "Epoch 18: val_accuracy did not improve from 0.53629\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0381 - accuracy: 0.5599 - val_loss: 1.1599 - val_accuracy: 0.5242\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0752 - accuracy: 0.5330\n",
      "Epoch 19: val_accuracy did not improve from 0.53629\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0752 - accuracy: 0.5330 - val_loss: 1.1842 - val_accuracy: 0.4637\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0734 - accuracy: 0.5339\n",
      "Epoch 20: val_accuracy improved from 0.53629 to 0.56452, saving model to ./ckpt/10_class_lr005/val_acc_0.565.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0734 - accuracy: 0.5339 - val_loss: 1.0940 - val_accuracy: 0.5645\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0238 - accuracy: 0.5698\n",
      "Epoch 21: val_accuracy did not improve from 0.56452\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0238 - accuracy: 0.5698 - val_loss: 1.1692 - val_accuracy: 0.5121\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0275 - accuracy: 0.5617\n",
      "Epoch 22: val_accuracy did not improve from 0.56452\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0275 - accuracy: 0.5617 - val_loss: 1.1091 - val_accuracy: 0.5444\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9930 - accuracy: 0.5599\n",
      "Epoch 23: val_accuracy did not improve from 0.56452\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9930 - accuracy: 0.5599 - val_loss: 1.1635 - val_accuracy: 0.5242\n",
      "Epoch 24/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9895 - accuracy: 0.5838\n",
      "Epoch 24: val_accuracy did not improve from 0.56452\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9834 - accuracy: 0.5828 - val_loss: 1.1062 - val_accuracy: 0.5605\n",
      "Epoch 25/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9642 - accuracy: 0.5850\n",
      "Epoch 25: val_accuracy improved from 0.56452 to 0.58871, saving model to ./ckpt/10_class_lr005/val_acc_0.589.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.9667 - accuracy: 0.5819 - val_loss: 1.0695 - val_accuracy: 0.5887\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9558 - accuracy: 0.5905\n",
      "Epoch 26: val_accuracy did not improve from 0.58871\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.9556 - accuracy: 0.5932 - val_loss: 1.0736 - val_accuracy: 0.5605\n",
      "Epoch 27/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9819 - accuracy: 0.5874\n",
      "Epoch 27: val_accuracy improved from 0.58871 to 0.59274, saving model to ./ckpt/10_class_lr005/val_acc_0.593.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9840 - accuracy: 0.5864 - val_loss: 1.0405 - val_accuracy: 0.5927\n",
      "Epoch 28/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9716 - accuracy: 0.5781\n",
      "Epoch 28: val_accuracy did not improve from 0.59274\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9748 - accuracy: 0.5761 - val_loss: 1.1035 - val_accuracy: 0.5605\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9627 - accuracy: 0.5910\n",
      "Epoch 29: val_accuracy improved from 0.59274 to 0.59677, saving model to ./ckpt/10_class_lr005/val_acc_0.597.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9648 - accuracy: 0.5896 - val_loss: 1.0273 - val_accuracy: 0.5968\n",
      "Epoch 30/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9603 - accuracy: 0.5772\n",
      "Epoch 30: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.9652 - accuracy: 0.5761 - val_loss: 1.0991 - val_accuracy: 0.5484\n",
      "Epoch 31/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8957 - accuracy: 0.6340\n",
      "Epoch 31: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.9038 - accuracy: 0.6345 - val_loss: 1.0428 - val_accuracy: 0.5968\n",
      "Epoch 32/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9595 - accuracy: 0.5882\n",
      "Epoch 32: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9656 - accuracy: 0.5851 - val_loss: 1.1359 - val_accuracy: 0.5565\n",
      "Epoch 33/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9049 - accuracy: 0.6231\n",
      "Epoch 33: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9085 - accuracy: 0.6210 - val_loss: 1.0772 - val_accuracy: 0.5605\n",
      "Epoch 34/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8781 - accuracy: 0.6195\n",
      "Epoch 34: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8809 - accuracy: 0.6197 - val_loss: 1.1412 - val_accuracy: 0.5685\n",
      "Epoch 35/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9392 - accuracy: 0.5999\n",
      "Epoch 35: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9392 - accuracy: 0.5999 - val_loss: 1.0932 - val_accuracy: 0.5766\n",
      "Epoch 36/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9021 - accuracy: 0.6203\n",
      "Epoch 36: val_accuracy improved from 0.59677 to 0.61290, saving model to ./ckpt/10_class_lr005/val_acc_0.613.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9065 - accuracy: 0.6183 - val_loss: 1.0174 - val_accuracy: 0.6129\n",
      "Epoch 37/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8871 - accuracy: 0.6372\n",
      "Epoch 37: val_accuracy improved from 0.61290 to 0.63306, saving model to ./ckpt/10_class_lr005/val_acc_0.633.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8871 - accuracy: 0.6372 - val_loss: 1.0078 - val_accuracy: 0.6331\n",
      "Epoch 38/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8672 - accuracy: 0.6297\n",
      "Epoch 38: val_accuracy did not improve from 0.63306\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 0.8703 - accuracy: 0.6304 - val_loss: 1.0626 - val_accuracy: 0.5887\n",
      "Epoch 39/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8774 - accuracy: 0.6241\n",
      "Epoch 39: val_accuracy did not improve from 0.63306\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8831 - accuracy: 0.6228 - val_loss: 1.0563 - val_accuracy: 0.6008\n",
      "Epoch 40/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8391 - accuracy: 0.6406\n",
      "Epoch 40: val_accuracy did not improve from 0.63306\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.8523 - accuracy: 0.6367 - val_loss: 1.0419 - val_accuracy: 0.5847\n",
      "Epoch 41/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8671 - accuracy: 0.6333\n",
      "Epoch 41: val_accuracy did not improve from 0.63306\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8667 - accuracy: 0.6336 - val_loss: 1.1048 - val_accuracy: 0.5484\n",
      "Epoch 42/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8713 - accuracy: 0.6213\n",
      "Epoch 42: val_accuracy did not improve from 0.63306\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.8752 - accuracy: 0.6201 - val_loss: 1.1283 - val_accuracy: 0.5484\n",
      "Epoch 43/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8522 - accuracy: 0.6378\n",
      "Epoch 43: val_accuracy did not improve from 0.63306\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.8581 - accuracy: 0.6358 - val_loss: 1.0648 - val_accuracy: 0.5726\n",
      "Epoch 44/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8437 - accuracy: 0.6462\n",
      "Epoch 44: val_accuracy did not improve from 0.63306\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8437 - accuracy: 0.6462 - val_loss: 1.0779 - val_accuracy: 0.5645\n",
      "Epoch 45/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8749 - accuracy: 0.6430\n",
      "Epoch 45: val_accuracy did not improve from 0.63306\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8780 - accuracy: 0.6381 - val_loss: 1.1144 - val_accuracy: 0.5685\n",
      "Epoch 46/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8088 - accuracy: 0.6623\n",
      "Epoch 46: val_accuracy did not improve from 0.63306\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.8124 - accuracy: 0.6623 - val_loss: 1.0841 - val_accuracy: 0.5806\n",
      "Epoch 47/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8028 - accuracy: 0.6605\n",
      "Epoch 47: val_accuracy did not improve from 0.63306\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8078 - accuracy: 0.6578 - val_loss: 1.0988 - val_accuracy: 0.5444\n",
      "Epoch 47: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 28s - loss: 2.3059 - accuracy: 0.0312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:08:36.828083: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.9626 - accuracy: 0.2259\n",
      "Epoch 1: val_accuracy improved from -inf to 0.31855, saving model to ./ckpt/10_class_lr005/val_acc_0.319.hdf5\n",
      "70/70 [==============================] - 2s 16ms/step - loss: 1.9626 - accuracy: 0.2259 - val_loss: 1.6893 - val_accuracy: 0.3185\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 1s - loss: 1.5037 - accuracy: 0.4375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:08:37.982081: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.6669 - accuracy: 0.3094\n",
      "Epoch 2: val_accuracy did not improve from 0.31855\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.6669 - accuracy: 0.3094 - val_loss: 1.6301 - val_accuracy: 0.3145\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.6248 - accuracy: 0.3401\n",
      "Epoch 3: val_accuracy improved from 0.31855 to 0.36290, saving model to ./ckpt/10_class_lr005/val_acc_0.363.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.6218 - accuracy: 0.3417 - val_loss: 1.5621 - val_accuracy: 0.3629\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5325 - accuracy: 0.3591\n",
      "Epoch 4: val_accuracy did not improve from 0.36290\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.5332 - accuracy: 0.3597 - val_loss: 1.5075 - val_accuracy: 0.3548\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4850 - accuracy: 0.3646\n",
      "Epoch 5: val_accuracy improved from 0.36290 to 0.39516, saving model to ./ckpt/10_class_lr005/val_acc_0.395.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.4847 - accuracy: 0.3655 - val_loss: 1.4501 - val_accuracy: 0.3952\n",
      "Epoch 6/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.4349 - accuracy: 0.3783\n",
      "Epoch 6: val_accuracy improved from 0.39516 to 0.43952, saving model to ./ckpt/10_class_lr005/val_acc_0.440.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.4342 - accuracy: 0.3767 - val_loss: 1.3959 - val_accuracy: 0.4395\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3915 - accuracy: 0.4099\n",
      "Epoch 7: val_accuracy did not improve from 0.43952\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.3911 - accuracy: 0.4091 - val_loss: 1.3321 - val_accuracy: 0.4234\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3271 - accuracy: 0.4271\n",
      "Epoch 8: val_accuracy improved from 0.43952 to 0.45161, saving model to ./ckpt/10_class_lr005/val_acc_0.452.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.3322 - accuracy: 0.4252 - val_loss: 1.2892 - val_accuracy: 0.4516\n",
      "Epoch 9/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3095 - accuracy: 0.4343\n",
      "Epoch 9: val_accuracy improved from 0.45161 to 0.45968, saving model to ./ckpt/10_class_lr005/val_acc_0.460.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.3110 - accuracy: 0.4333 - val_loss: 1.3228 - val_accuracy: 0.4597\n",
      "Epoch 10/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.2613 - accuracy: 0.4557\n",
      "Epoch 10: val_accuracy did not improve from 0.45968\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.2657 - accuracy: 0.4508 - val_loss: 1.3131 - val_accuracy: 0.4073\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.2647 - accuracy: 0.4761\n",
      "Epoch 11: val_accuracy improved from 0.45968 to 0.47984, saving model to ./ckpt/10_class_lr005/val_acc_0.480.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.2656 - accuracy: 0.4764 - val_loss: 1.2503 - val_accuracy: 0.4798\n",
      "Epoch 12/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1985 - accuracy: 0.4787\n",
      "Epoch 12: val_accuracy did not improve from 0.47984\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.2057 - accuracy: 0.4778 - val_loss: 1.3607 - val_accuracy: 0.3992\n",
      "Epoch 13/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1847 - accuracy: 0.4874\n",
      "Epoch 13: val_accuracy improved from 0.47984 to 0.53629, saving model to ./ckpt/10_class_lr005/val_acc_0.536.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1831 - accuracy: 0.4890 - val_loss: 1.1434 - val_accuracy: 0.5363\n",
      "Epoch 14/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1422 - accuracy: 0.5051\n",
      "Epoch 14: val_accuracy did not improve from 0.53629\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1383 - accuracy: 0.5110 - val_loss: 1.1691 - val_accuracy: 0.5202\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0968 - accuracy: 0.5168\n",
      "Epoch 15: val_accuracy improved from 0.53629 to 0.54032, saving model to ./ckpt/10_class_lr005/val_acc_0.540.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1006 - accuracy: 0.5150 - val_loss: 1.1305 - val_accuracy: 0.5403\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1116 - accuracy: 0.5317\n",
      "Epoch 16: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.1205 - accuracy: 0.5299 - val_loss: 1.1810 - val_accuracy: 0.4556\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0787 - accuracy: 0.5355\n",
      "Epoch 17: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0781 - accuracy: 0.5379 - val_loss: 1.1398 - val_accuracy: 0.5161\n",
      "Epoch 18/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0613 - accuracy: 0.5353\n",
      "Epoch 18: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.0614 - accuracy: 0.5352 - val_loss: 1.1200 - val_accuracy: 0.5242\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0368 - accuracy: 0.5469\n",
      "Epoch 19: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0533 - accuracy: 0.5397 - val_loss: 1.2718 - val_accuracy: 0.4395\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0596 - accuracy: 0.5586\n",
      "Epoch 20: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0596 - accuracy: 0.5586 - val_loss: 1.1389 - val_accuracy: 0.4839\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0299 - accuracy: 0.5625\n",
      "Epoch 21: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0234 - accuracy: 0.5658 - val_loss: 1.0830 - val_accuracy: 0.5242\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0026 - accuracy: 0.5710\n",
      "Epoch 22: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0111 - accuracy: 0.5644 - val_loss: 1.0825 - val_accuracy: 0.5121\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9936 - accuracy: 0.5824\n",
      "Epoch 23: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.9908 - accuracy: 0.5842 - val_loss: 1.1059 - val_accuracy: 0.5363\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0063 - accuracy: 0.5625\n",
      "Epoch 24: val_accuracy improved from 0.54032 to 0.55242, saving model to ./ckpt/10_class_lr005/val_acc_0.552.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.0049 - accuracy: 0.5626 - val_loss: 1.0491 - val_accuracy: 0.5524\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0040 - accuracy: 0.5784\n",
      "Epoch 25: val_accuracy did not improve from 0.55242\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0040 - accuracy: 0.5784 - val_loss: 1.2092 - val_accuracy: 0.4597\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0320 - accuracy: 0.5559\n",
      "Epoch 26: val_accuracy did not improve from 0.55242\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0320 - accuracy: 0.5559 - val_loss: 1.0650 - val_accuracy: 0.5444\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9605 - accuracy: 0.5896\n",
      "Epoch 27: val_accuracy improved from 0.55242 to 0.55645, saving model to ./ckpt/10_class_lr005/val_acc_0.556.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9618 - accuracy: 0.5860 - val_loss: 1.0829 - val_accuracy: 0.5565\n",
      "Epoch 28/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9685 - accuracy: 0.5876\n",
      "Epoch 28: val_accuracy did not improve from 0.55645\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9671 - accuracy: 0.5887 - val_loss: 1.0493 - val_accuracy: 0.5323\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9748 - accuracy: 0.5802\n",
      "Epoch 29: val_accuracy did not improve from 0.55645\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9747 - accuracy: 0.5788 - val_loss: 1.0348 - val_accuracy: 0.5403\n",
      "Epoch 30/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9520 - accuracy: 0.5904\n",
      "Epoch 30: val_accuracy did not improve from 0.55645\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9598 - accuracy: 0.5900 - val_loss: 1.0573 - val_accuracy: 0.5524\n",
      "Epoch 31/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9605 - accuracy: 0.5928\n",
      "Epoch 31: val_accuracy did not improve from 0.55645\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9673 - accuracy: 0.5896 - val_loss: 1.0619 - val_accuracy: 0.5403\n",
      "Epoch 32/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9060 - accuracy: 0.6184\n",
      "Epoch 32: val_accuracy did not improve from 0.55645\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9095 - accuracy: 0.6174 - val_loss: 1.1628 - val_accuracy: 0.5121\n",
      "Epoch 33/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9170 - accuracy: 0.6117\n",
      "Epoch 33: val_accuracy did not improve from 0.55645\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.9229 - accuracy: 0.6066 - val_loss: 1.1574 - val_accuracy: 0.4960\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9342 - accuracy: 0.6134\n",
      "Epoch 34: val_accuracy did not improve from 0.55645\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9342 - accuracy: 0.6134 - val_loss: 1.0715 - val_accuracy: 0.5161\n",
      "Epoch 35/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9245 - accuracy: 0.6111\n",
      "Epoch 35: val_accuracy did not improve from 0.55645\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9245 - accuracy: 0.6111 - val_loss: 1.0728 - val_accuracy: 0.5403\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9197 - accuracy: 0.6161\n",
      "Epoch 36: val_accuracy improved from 0.55645 to 0.58468, saving model to ./ckpt/10_class_lr005/val_acc_0.585.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9197 - accuracy: 0.6161 - val_loss: 1.0326 - val_accuracy: 0.5847\n",
      "Epoch 37/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8988 - accuracy: 0.6195\n",
      "Epoch 37: val_accuracy did not improve from 0.58468\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9030 - accuracy: 0.6174 - val_loss: 1.0026 - val_accuracy: 0.5726\n",
      "Epoch 38/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8902 - accuracy: 0.6188\n",
      "Epoch 38: val_accuracy did not improve from 0.58468\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8902 - accuracy: 0.6188 - val_loss: 1.1968 - val_accuracy: 0.4637\n",
      "Epoch 39/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9203 - accuracy: 0.6181\n",
      "Epoch 39: val_accuracy did not improve from 0.58468\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9196 - accuracy: 0.6183 - val_loss: 1.1221 - val_accuracy: 0.4919\n",
      "Epoch 40/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8566 - accuracy: 0.6535\n",
      "Epoch 40: val_accuracy did not improve from 0.58468\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8587 - accuracy: 0.6511 - val_loss: 1.0928 - val_accuracy: 0.5403\n",
      "Epoch 41/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8762 - accuracy: 0.6179\n",
      "Epoch 41: val_accuracy did not improve from 0.58468\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8710 - accuracy: 0.6170 - val_loss: 1.0261 - val_accuracy: 0.5565\n",
      "Epoch 42/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8362 - accuracy: 0.6427\n",
      "Epoch 42: val_accuracy did not improve from 0.58468\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.8373 - accuracy: 0.6426 - val_loss: 1.2639 - val_accuracy: 0.4839\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8490 - accuracy: 0.6481\n",
      "Epoch 43: val_accuracy did not improve from 0.58468\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8495 - accuracy: 0.6471 - val_loss: 1.0981 - val_accuracy: 0.5403\n",
      "Epoch 44/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8306 - accuracy: 0.6572\n",
      "Epoch 44: val_accuracy did not improve from 0.58468\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8311 - accuracy: 0.6569 - val_loss: 1.0995 - val_accuracy: 0.5323\n",
      "Epoch 45/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8838 - accuracy: 0.6179\n",
      "Epoch 45: val_accuracy did not improve from 0.58468\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.8882 - accuracy: 0.6183 - val_loss: 1.0475 - val_accuracy: 0.5524\n",
      "Epoch 46/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8754 - accuracy: 0.6250\n",
      "Epoch 46: val_accuracy did not improve from 0.58468\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.8847 - accuracy: 0.6237 - val_loss: 1.1446 - val_accuracy: 0.5202\n",
      "Epoch 47/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8193 - accuracy: 0.6615\n",
      "Epoch 47: val_accuracy did not improve from 0.58468\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.8158 - accuracy: 0.6614 - val_loss: 1.1006 - val_accuracy: 0.5242\n",
      "Epoch 47: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 47s - loss: 2.3030 - accuracy: 0.1562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:09:19.342843: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 2.0122 - accuracy: 0.1989\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25806, saving model to ./ckpt/10_class_lr005/val_acc_0.258.hdf5\n",
      "70/70 [==============================] - 2s 14ms/step - loss: 2.0122 - accuracy: 0.1989 - val_loss: 1.7833 - val_accuracy: 0.2581\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.5498 - accuracy: 0.3750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:09:20.355946: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 1.6703 - accuracy: 0.3125\n",
      "Epoch 2: val_accuracy improved from 0.25806 to 0.33871, saving model to ./ckpt/10_class_lr005/val_acc_0.339.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.6716 - accuracy: 0.3089 - val_loss: 1.6272 - val_accuracy: 0.3387\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.6000 - accuracy: 0.3545\n",
      "Epoch 3: val_accuracy did not improve from 0.33871\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.5975 - accuracy: 0.3561 - val_loss: 1.5432 - val_accuracy: 0.3387\n",
      "Epoch 4/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.5493 - accuracy: 0.3689\n",
      "Epoch 4: val_accuracy did not improve from 0.33871\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.5491 - accuracy: 0.3678 - val_loss: 1.5877 - val_accuracy: 0.3185\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4765 - accuracy: 0.3804\n",
      "Epoch 5: val_accuracy improved from 0.33871 to 0.35484, saving model to ./ckpt/10_class_lr005/val_acc_0.355.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.4767 - accuracy: 0.3808 - val_loss: 1.5007 - val_accuracy: 0.3548\n",
      "Epoch 6/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.4133 - accuracy: 0.4118\n",
      "Epoch 6: val_accuracy did not improve from 0.35484\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.4167 - accuracy: 0.4109 - val_loss: 1.5748 - val_accuracy: 0.3105\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3437 - accuracy: 0.4351\n",
      "Epoch 7: val_accuracy improved from 0.35484 to 0.41129, saving model to ./ckpt/10_class_lr005/val_acc_0.411.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.3437 - accuracy: 0.4351 - val_loss: 1.3362 - val_accuracy: 0.4113\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3429 - accuracy: 0.4253\n",
      "Epoch 8: val_accuracy improved from 0.41129 to 0.42339, saving model to ./ckpt/10_class_lr005/val_acc_0.423.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.3451 - accuracy: 0.4239 - val_loss: 1.3289 - val_accuracy: 0.4234\n",
      "Epoch 9/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2547 - accuracy: 0.4543\n",
      "Epoch 9: val_accuracy did not improve from 0.42339\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.2550 - accuracy: 0.4526 - val_loss: 1.2859 - val_accuracy: 0.4194\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2190 - accuracy: 0.4710\n",
      "Epoch 10: val_accuracy improved from 0.42339 to 0.44355, saving model to ./ckpt/10_class_lr005/val_acc_0.444.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.2201 - accuracy: 0.4706 - val_loss: 1.2317 - val_accuracy: 0.4435\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.1900 - accuracy: 0.4839\n",
      "Epoch 11: val_accuracy improved from 0.44355 to 0.45161, saving model to ./ckpt/10_class_lr005/val_acc_0.452.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1921 - accuracy: 0.4841 - val_loss: 1.2470 - val_accuracy: 0.4516\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1705 - accuracy: 0.4860\n",
      "Epoch 12: val_accuracy improved from 0.45161 to 0.47984, saving model to ./ckpt/10_class_lr005/val_acc_0.480.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1746 - accuracy: 0.4854 - val_loss: 1.2660 - val_accuracy: 0.4798\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1667 - accuracy: 0.4968\n",
      "Epoch 13: val_accuracy improved from 0.47984 to 0.49597, saving model to ./ckpt/10_class_lr005/val_acc_0.496.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1667 - accuracy: 0.4966 - val_loss: 1.1799 - val_accuracy: 0.4960\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1325 - accuracy: 0.5101\n",
      "Epoch 14: val_accuracy did not improve from 0.49597\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1325 - accuracy: 0.5101 - val_loss: 1.3291 - val_accuracy: 0.4234\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1141 - accuracy: 0.5177\n",
      "Epoch 15: val_accuracy improved from 0.49597 to 0.52823, saving model to ./ckpt/10_class_lr005/val_acc_0.528.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1127 - accuracy: 0.5177 - val_loss: 1.1357 - val_accuracy: 0.5282\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1044 - accuracy: 0.5289\n",
      "Epoch 16: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0989 - accuracy: 0.5308 - val_loss: 1.1479 - val_accuracy: 0.4798\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0665 - accuracy: 0.5276\n",
      "Epoch 17: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0665 - accuracy: 0.5276 - val_loss: 1.1743 - val_accuracy: 0.5242\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0588 - accuracy: 0.5521\n",
      "Epoch 18: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0600 - accuracy: 0.5501 - val_loss: 1.1141 - val_accuracy: 0.5081\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0698 - accuracy: 0.5275\n",
      "Epoch 19: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.0667 - accuracy: 0.5308 - val_loss: 1.2032 - val_accuracy: 0.5081\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0229 - accuracy: 0.5658\n",
      "Epoch 20: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0229 - accuracy: 0.5658 - val_loss: 1.2195 - val_accuracy: 0.5000\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0821 - accuracy: 0.5254\n",
      "Epoch 21: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0821 - accuracy: 0.5254 - val_loss: 1.1930 - val_accuracy: 0.5121\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0173 - accuracy: 0.5672\n",
      "Epoch 22: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0254 - accuracy: 0.5667 - val_loss: 1.3078 - val_accuracy: 0.4113\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0230 - accuracy: 0.5729\n",
      "Epoch 23: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0263 - accuracy: 0.5739 - val_loss: 1.1740 - val_accuracy: 0.4839\n",
      "Epoch 24/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9892 - accuracy: 0.5532\n",
      "Epoch 24: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.9892 - accuracy: 0.5532 - val_loss: 1.1399 - val_accuracy: 0.5282\n",
      "Epoch 25/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0302 - accuracy: 0.5545\n",
      "Epoch 25: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.0346 - accuracy: 0.5510 - val_loss: 1.2543 - val_accuracy: 0.4435\n",
      "Epoch 26/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9824 - accuracy: 0.5952\n",
      "Epoch 26: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.9862 - accuracy: 0.5923 - val_loss: 1.1375 - val_accuracy: 0.4879\n",
      "Epoch 27/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9862 - accuracy: 0.5793\n",
      "Epoch 27: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9862 - accuracy: 0.5793 - val_loss: 1.1276 - val_accuracy: 0.5040\n",
      "Epoch 28/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9650 - accuracy: 0.5848\n",
      "Epoch 28: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.9687 - accuracy: 0.5833 - val_loss: 1.2675 - val_accuracy: 0.4960\n",
      "Epoch 28: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 27s - loss: 2.3038 - accuracy: 0.0938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:09:44.332267: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 2.0054 - accuracy: 0.2137\n",
      "Epoch 1: val_accuracy improved from -inf to 0.28629, saving model to ./ckpt/10_class_lr005/val_acc_0.286.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 2.0054 - accuracy: 0.2137 - val_loss: 1.6757 - val_accuracy: 0.2863\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 1s - loss: 1.6730 - accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:09:45.428802: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.6703 - accuracy: 0.3247\n",
      "Epoch 2: val_accuracy improved from 0.28629 to 0.34677, saving model to ./ckpt/10_class_lr005/val_acc_0.347.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.6703 - accuracy: 0.3247 - val_loss: 1.5876 - val_accuracy: 0.3468\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5665 - accuracy: 0.3510\n",
      "Epoch 3: val_accuracy improved from 0.34677 to 0.39516, saving model to ./ckpt/10_class_lr005/val_acc_0.395.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.5670 - accuracy: 0.3511 - val_loss: 1.4532 - val_accuracy: 0.3952\n",
      "Epoch 4/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.4817 - accuracy: 0.3806\n",
      "Epoch 4: val_accuracy improved from 0.39516 to 0.43952, saving model to ./ckpt/10_class_lr005/val_acc_0.440.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.4782 - accuracy: 0.3844 - val_loss: 1.3529 - val_accuracy: 0.4395\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.4242 - accuracy: 0.4019\n",
      "Epoch 5: val_accuracy did not improve from 0.43952\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 1.4242 - accuracy: 0.4019 - val_loss: 1.3061 - val_accuracy: 0.4274\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3513 - accuracy: 0.4230\n",
      "Epoch 6: val_accuracy improved from 0.43952 to 0.47984, saving model to ./ckpt/10_class_lr005/val_acc_0.480.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3513 - accuracy: 0.4230 - val_loss: 1.2288 - val_accuracy: 0.4798\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3191 - accuracy: 0.4369\n",
      "Epoch 7: val_accuracy did not improve from 0.47984\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.3191 - accuracy: 0.4369 - val_loss: 1.1941 - val_accuracy: 0.4476\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2738 - accuracy: 0.4352\n",
      "Epoch 8: val_accuracy improved from 0.47984 to 0.50000, saving model to ./ckpt/10_class_lr005/val_acc_0.500.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.2744 - accuracy: 0.4351 - val_loss: 1.1447 - val_accuracy: 0.5000\n",
      "Epoch 9/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.2727 - accuracy: 0.4481\n",
      "Epoch 9: val_accuracy did not improve from 0.50000\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.2757 - accuracy: 0.4477 - val_loss: 1.2905 - val_accuracy: 0.3992\n",
      "Epoch 10/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.2357 - accuracy: 0.4631\n",
      "Epoch 10: val_accuracy did not improve from 0.50000\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.2362 - accuracy: 0.4639 - val_loss: 1.2491 - val_accuracy: 0.4395\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1964 - accuracy: 0.5052\n",
      "Epoch 11: val_accuracy improved from 0.50000 to 0.56452, saving model to ./ckpt/10_class_lr005/val_acc_0.565.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1964 - accuracy: 0.5052 - val_loss: 1.0923 - val_accuracy: 0.5645\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1437 - accuracy: 0.4874\n",
      "Epoch 12: val_accuracy did not improve from 0.56452\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1423 - accuracy: 0.4912 - val_loss: 1.0327 - val_accuracy: 0.5444\n",
      "Epoch 13/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1518 - accuracy: 0.4967\n",
      "Epoch 13: val_accuracy did not improve from 0.56452\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1464 - accuracy: 0.5016 - val_loss: 1.1172 - val_accuracy: 0.5040\n",
      "Epoch 14/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1169 - accuracy: 0.5123\n",
      "Epoch 14: val_accuracy improved from 0.56452 to 0.58065, saving model to ./ckpt/10_class_lr005/val_acc_0.581.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1126 - accuracy: 0.5164 - val_loss: 0.9931 - val_accuracy: 0.5806\n",
      "Epoch 15/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0748 - accuracy: 0.5370\n",
      "Epoch 15: val_accuracy did not improve from 0.58065\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0748 - accuracy: 0.5370 - val_loss: 1.0437 - val_accuracy: 0.5605\n",
      "Epoch 16/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0752 - accuracy: 0.5290\n",
      "Epoch 16: val_accuracy did not improve from 0.58065\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0726 - accuracy: 0.5321 - val_loss: 1.1503 - val_accuracy: 0.5363\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0709 - accuracy: 0.5263\n",
      "Epoch 17: val_accuracy did not improve from 0.58065\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0708 - accuracy: 0.5263 - val_loss: 1.0583 - val_accuracy: 0.5685\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0449 - accuracy: 0.5510\n",
      "Epoch 18: val_accuracy did not improve from 0.58065\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0449 - accuracy: 0.5510 - val_loss: 1.0221 - val_accuracy: 0.5645\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0347 - accuracy: 0.5447\n",
      "Epoch 19: val_accuracy improved from 0.58065 to 0.62500, saving model to ./ckpt/10_class_lr005/val_acc_0.625.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0347 - accuracy: 0.5447 - val_loss: 0.9788 - val_accuracy: 0.6250\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0130 - accuracy: 0.5635\n",
      "Epoch 20: val_accuracy did not improve from 0.62500\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0130 - accuracy: 0.5635 - val_loss: 0.9673 - val_accuracy: 0.6048\n",
      "Epoch 21/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0045 - accuracy: 0.5694\n",
      "Epoch 21: val_accuracy did not improve from 0.62500\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0041 - accuracy: 0.5694 - val_loss: 1.0598 - val_accuracy: 0.5040\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9954 - accuracy: 0.5599\n",
      "Epoch 22: val_accuracy improved from 0.62500 to 0.64113, saving model to ./ckpt/10_class_lr005/val_acc_0.641.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9954 - accuracy: 0.5599 - val_loss: 0.9443 - val_accuracy: 0.6411\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9925 - accuracy: 0.5721\n",
      "Epoch 23: val_accuracy did not improve from 0.64113\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9925 - accuracy: 0.5721 - val_loss: 1.0214 - val_accuracy: 0.6089\n",
      "Epoch 24/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9735 - accuracy: 0.5767\n",
      "Epoch 24: val_accuracy did not improve from 0.64113\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9713 - accuracy: 0.5788 - val_loss: 0.9885 - val_accuracy: 0.5927\n",
      "Epoch 25/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9452 - accuracy: 0.5838\n",
      "Epoch 25: val_accuracy did not improve from 0.64113\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9480 - accuracy: 0.5828 - val_loss: 1.0317 - val_accuracy: 0.5726\n",
      "Epoch 26/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9408 - accuracy: 0.6117\n",
      "Epoch 26: val_accuracy did not improve from 0.64113\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9395 - accuracy: 0.6116 - val_loss: 0.9935 - val_accuracy: 0.6290\n",
      "Epoch 27/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9356 - accuracy: 0.6002\n",
      "Epoch 27: val_accuracy did not improve from 0.64113\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9332 - accuracy: 0.6004 - val_loss: 0.9634 - val_accuracy: 0.6048\n",
      "Epoch 28/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9268 - accuracy: 0.6084\n",
      "Epoch 28: val_accuracy did not improve from 0.64113\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9275 - accuracy: 0.6075 - val_loss: 0.9298 - val_accuracy: 0.6210\n",
      "Epoch 29/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8842 - accuracy: 0.6309\n",
      "Epoch 29: val_accuracy did not improve from 0.64113\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8842 - accuracy: 0.6309 - val_loss: 0.9549 - val_accuracy: 0.6290\n",
      "Epoch 30/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9816 - accuracy: 0.5819\n",
      "Epoch 30: val_accuracy did not improve from 0.64113\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9816 - accuracy: 0.5819 - val_loss: 0.9567 - val_accuracy: 0.6411\n",
      "Epoch 31/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9293 - accuracy: 0.6056\n",
      "Epoch 31: val_accuracy did not improve from 0.64113\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9276 - accuracy: 0.6066 - val_loss: 0.9467 - val_accuracy: 0.6411\n",
      "Epoch 32/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9122 - accuracy: 0.6102\n",
      "Epoch 32: val_accuracy did not improve from 0.64113\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9122 - accuracy: 0.6102 - val_loss: 1.0064 - val_accuracy: 0.5766\n",
      "Epoch 33/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8865 - accuracy: 0.6255\n",
      "Epoch 33: val_accuracy did not improve from 0.64113\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8881 - accuracy: 0.6246 - val_loss: 0.9455 - val_accuracy: 0.6411\n",
      "Epoch 34/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8585 - accuracy: 0.6325\n",
      "Epoch 34: val_accuracy improved from 0.64113 to 0.65323, saving model to ./ckpt/10_class_lr005/val_acc_0.653.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.8646 - accuracy: 0.6295 - val_loss: 0.9325 - val_accuracy: 0.6532\n",
      "Epoch 35/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8536 - accuracy: 0.6372\n",
      "Epoch 35: val_accuracy did not improve from 0.65323\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8536 - accuracy: 0.6372 - val_loss: 1.0023 - val_accuracy: 0.6129\n",
      "Epoch 36/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8600 - accuracy: 0.6288\n",
      "Epoch 36: val_accuracy improved from 0.65323 to 0.66532, saving model to ./ckpt/10_class_lr005/val_acc_0.665.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8647 - accuracy: 0.6291 - val_loss: 1.0042 - val_accuracy: 0.6653\n",
      "Epoch 37/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8765 - accuracy: 0.6268\n",
      "Epoch 37: val_accuracy did not improve from 0.66532\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8790 - accuracy: 0.6251 - val_loss: 1.0326 - val_accuracy: 0.5806\n",
      "Epoch 38/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8323 - accuracy: 0.6383\n",
      "Epoch 38: val_accuracy did not improve from 0.66532\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8377 - accuracy: 0.6372 - val_loss: 0.9795 - val_accuracy: 0.6452\n",
      "Epoch 38: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 28s - loss: 2.3025 - accuracy: 0.0938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:10:19.309681: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.9285 - accuracy: 0.2506\n",
      "Epoch 1: val_accuracy improved from -inf to 0.31855, saving model to ./ckpt/10_class_lr005/val_acc_0.319.hdf5\n",
      "70/70 [==============================] - 2s 16ms/step - loss: 1.9285 - accuracy: 0.2506 - val_loss: 1.6903 - val_accuracy: 0.3185\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.7447 - accuracy: 0.3438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:10:20.445957: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.6568 - accuracy: 0.3107\n",
      "Epoch 2: val_accuracy improved from 0.31855 to 0.35887, saving model to ./ckpt/10_class_lr005/val_acc_0.359.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.6568 - accuracy: 0.3107 - val_loss: 1.6300 - val_accuracy: 0.3589\n",
      "Epoch 3/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.5704 - accuracy: 0.3538\n",
      "Epoch 3: val_accuracy improved from 0.35887 to 0.36290, saving model to ./ckpt/10_class_lr005/val_acc_0.363.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.5704 - accuracy: 0.3538 - val_loss: 1.5105 - val_accuracy: 0.3629\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5227 - accuracy: 0.3542\n",
      "Epoch 4: val_accuracy improved from 0.36290 to 0.38710, saving model to ./ckpt/10_class_lr005/val_acc_0.387.hdf5\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 1.5215 - accuracy: 0.3547 - val_loss: 1.4490 - val_accuracy: 0.3871\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4428 - accuracy: 0.3673\n",
      "Epoch 5: val_accuracy improved from 0.38710 to 0.41935, saving model to ./ckpt/10_class_lr005/val_acc_0.419.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.4428 - accuracy: 0.3664 - val_loss: 1.4134 - val_accuracy: 0.4194\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.3674 - accuracy: 0.4246\n",
      "Epoch 6: val_accuracy did not improve from 0.41935\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3741 - accuracy: 0.4225 - val_loss: 1.5701 - val_accuracy: 0.3508\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3327 - accuracy: 0.4172\n",
      "Epoch 7: val_accuracy improved from 0.41935 to 0.52823, saving model to ./ckpt/10_class_lr005/val_acc_0.528.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.3327 - accuracy: 0.4172 - val_loss: 1.2339 - val_accuracy: 0.5282\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2664 - accuracy: 0.4576\n",
      "Epoch 8: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.2664 - accuracy: 0.4576 - val_loss: 1.7341 - val_accuracy: 0.2581\n",
      "Epoch 9/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3059 - accuracy: 0.4398\n",
      "Epoch 9: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.3050 - accuracy: 0.4387 - val_loss: 1.1883 - val_accuracy: 0.5081\n",
      "Epoch 10/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.2048 - accuracy: 0.4775\n",
      "Epoch 10: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.2050 - accuracy: 0.4782 - val_loss: 1.1812 - val_accuracy: 0.5040\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1918 - accuracy: 0.4728\n",
      "Epoch 11: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1918 - accuracy: 0.4728 - val_loss: 1.1588 - val_accuracy: 0.5081\n",
      "Epoch 12/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1718 - accuracy: 0.4777\n",
      "Epoch 12: val_accuracy did not improve from 0.52823\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1700 - accuracy: 0.4782 - val_loss: 1.1644 - val_accuracy: 0.5081\n",
      "Epoch 13/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1253 - accuracy: 0.5005\n",
      "Epoch 13: val_accuracy improved from 0.52823 to 0.53629, saving model to ./ckpt/10_class_lr005/val_acc_0.536.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1258 - accuracy: 0.4993 - val_loss: 1.1164 - val_accuracy: 0.5363\n",
      "Epoch 14/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0917 - accuracy: 0.5299\n",
      "Epoch 14: val_accuracy improved from 0.53629 to 0.54435, saving model to ./ckpt/10_class_lr005/val_acc_0.544.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0887 - accuracy: 0.5294 - val_loss: 1.1040 - val_accuracy: 0.5444\n",
      "Epoch 15/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1198 - accuracy: 0.5209\n",
      "Epoch 15: val_accuracy did not improve from 0.54435\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1198 - accuracy: 0.5209 - val_loss: 1.2138 - val_accuracy: 0.4677\n",
      "Epoch 16/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0833 - accuracy: 0.5386\n",
      "Epoch 16: val_accuracy did not improve from 0.54435\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0841 - accuracy: 0.5393 - val_loss: 1.1302 - val_accuracy: 0.5282\n",
      "Epoch 17/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0574 - accuracy: 0.5368\n",
      "Epoch 17: val_accuracy improved from 0.54435 to 0.55645, saving model to ./ckpt/10_class_lr005/val_acc_0.556.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.0568 - accuracy: 0.5370 - val_loss: 1.1153 - val_accuracy: 0.5565\n",
      "Epoch 18/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0891 - accuracy: 0.5147\n",
      "Epoch 18: val_accuracy did not improve from 0.55645\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.0885 - accuracy: 0.5155 - val_loss: 1.1901 - val_accuracy: 0.5282\n",
      "Epoch 19/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0393 - accuracy: 0.5476\n",
      "Epoch 19: val_accuracy did not improve from 0.55645\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.0417 - accuracy: 0.5460 - val_loss: 1.2252 - val_accuracy: 0.5081\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0590 - accuracy: 0.5299\n",
      "Epoch 20: val_accuracy did not improve from 0.55645\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0590 - accuracy: 0.5299 - val_loss: 1.1610 - val_accuracy: 0.5000\n",
      "Epoch 21/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0389 - accuracy: 0.5380\n",
      "Epoch 21: val_accuracy did not improve from 0.55645\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0396 - accuracy: 0.5379 - val_loss: 1.1398 - val_accuracy: 0.5282\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0023 - accuracy: 0.5715\n",
      "Epoch 22: val_accuracy did not improve from 0.55645\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.0060 - accuracy: 0.5680 - val_loss: 1.1211 - val_accuracy: 0.5323\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0037 - accuracy: 0.5819\n",
      "Epoch 23: val_accuracy did not improve from 0.55645\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.0121 - accuracy: 0.5761 - val_loss: 1.2218 - val_accuracy: 0.4556\n",
      "Epoch 24/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0437 - accuracy: 0.5386\n",
      "Epoch 24: val_accuracy did not improve from 0.55645\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.0475 - accuracy: 0.5393 - val_loss: 1.1330 - val_accuracy: 0.5484\n",
      "Epoch 24: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 25s - loss: 2.3045 - accuracy: 0.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:10:40.998094: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.9921 - accuracy: 0.2020\n",
      "Epoch 1: val_accuracy improved from -inf to 0.31579, saving model to ./ckpt/10_class_lr005/val_acc_0.316.hdf5\n",
      "70/70 [==============================] - 2s 21ms/step - loss: 1.9921 - accuracy: 0.2020 - val_loss: 1.7034 - val_accuracy: 0.3158\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.7634 - accuracy: 0.2188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:10:42.472285: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 1.6879 - accuracy: 0.3040\n",
      "Epoch 2: val_accuracy improved from 0.31579 to 0.37652, saving model to ./ckpt/10_class_lr005/val_acc_0.377.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.6815 - accuracy: 0.3083 - val_loss: 1.5974 - val_accuracy: 0.3765\n",
      "Epoch 3/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.6033 - accuracy: 0.3357\n",
      "Epoch 3: val_accuracy improved from 0.37652 to 0.39676, saving model to ./ckpt/10_class_lr005/val_acc_0.397.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.5979 - accuracy: 0.3357 - val_loss: 1.5099 - val_accuracy: 0.3968\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.4696 - accuracy: 0.3797\n",
      "Epoch 4: val_accuracy did not improve from 0.39676\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.4696 - accuracy: 0.3797 - val_loss: 1.4182 - val_accuracy: 0.3968\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.4099 - accuracy: 0.3855\n",
      "Epoch 5: val_accuracy improved from 0.39676 to 0.40486, saving model to ./ckpt/10_class_lr005/val_acc_0.405.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.4099 - accuracy: 0.3855 - val_loss: 1.4173 - val_accuracy: 0.4049\n",
      "Epoch 6/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.3686 - accuracy: 0.4044\n",
      "Epoch 6: val_accuracy improved from 0.40486 to 0.45344, saving model to ./ckpt/10_class_lr005/val_acc_0.453.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.3638 - accuracy: 0.4093 - val_loss: 1.4258 - val_accuracy: 0.4534\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.2811 - accuracy: 0.4370\n",
      "Epoch 7: val_accuracy improved from 0.45344 to 0.54251, saving model to ./ckpt/10_class_lr005/val_acc_0.543.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.2888 - accuracy: 0.4304 - val_loss: 1.2017 - val_accuracy: 0.5425\n",
      "Epoch 8/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.2258 - accuracy: 0.4729\n",
      "Epoch 8: val_accuracy did not improve from 0.54251\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.2293 - accuracy: 0.4731 - val_loss: 1.2294 - val_accuracy: 0.5101\n",
      "Epoch 9/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.2229 - accuracy: 0.4877\n",
      "Epoch 9: val_accuracy did not improve from 0.54251\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.2228 - accuracy: 0.4861 - val_loss: 1.2691 - val_accuracy: 0.4453\n",
      "Epoch 10/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.2018 - accuracy: 0.4795\n",
      "Epoch 10: val_accuracy did not improve from 0.54251\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.2049 - accuracy: 0.4762 - val_loss: 1.1657 - val_accuracy: 0.5344\n",
      "Epoch 11/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1607 - accuracy: 0.4867\n",
      "Epoch 11: val_accuracy did not improve from 0.54251\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.1714 - accuracy: 0.4825 - val_loss: 1.2061 - val_accuracy: 0.4980\n",
      "Epoch 12/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1381 - accuracy: 0.5047\n",
      "Epoch 12: val_accuracy improved from 0.54251 to 0.54656, saving model to ./ckpt/10_class_lr005/val_acc_0.547.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.1423 - accuracy: 0.5018 - val_loss: 1.1322 - val_accuracy: 0.5466\n",
      "Epoch 13/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1378 - accuracy: 0.5204\n",
      "Epoch 13: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1461 - accuracy: 0.5162 - val_loss: 1.2051 - val_accuracy: 0.5061\n",
      "Epoch 14/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1293 - accuracy: 0.5084\n",
      "Epoch 14: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1269 - accuracy: 0.5090 - val_loss: 1.1749 - val_accuracy: 0.5263\n",
      "Epoch 15/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1023 - accuracy: 0.5159\n",
      "Epoch 15: val_accuracy improved from 0.54656 to 0.57085, saving model to ./ckpt/10_class_lr005/val_acc_0.571.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1046 - accuracy: 0.5153 - val_loss: 1.0949 - val_accuracy: 0.5709\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0797 - accuracy: 0.5521\n",
      "Epoch 16: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0808 - accuracy: 0.5516 - val_loss: 1.1217 - val_accuracy: 0.5142\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0635 - accuracy: 0.5331\n",
      "Epoch 17: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0626 - accuracy: 0.5341 - val_loss: 1.4677 - val_accuracy: 0.4008\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1069 - accuracy: 0.5223\n",
      "Epoch 18: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1157 - accuracy: 0.5175 - val_loss: 1.1458 - val_accuracy: 0.5709\n",
      "Epoch 19/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0335 - accuracy: 0.5606\n",
      "Epoch 19: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.0342 - accuracy: 0.5592 - val_loss: 1.1450 - val_accuracy: 0.5587\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0481 - accuracy: 0.5582\n",
      "Epoch 20: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.0511 - accuracy: 0.5566 - val_loss: 1.1144 - val_accuracy: 0.5385\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0367 - accuracy: 0.5455\n",
      "Epoch 21: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0406 - accuracy: 0.5471 - val_loss: 1.1544 - val_accuracy: 0.5385\n",
      "Epoch 22/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0093 - accuracy: 0.5560\n",
      "Epoch 22: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.0130 - accuracy: 0.5534 - val_loss: 1.1127 - val_accuracy: 0.5263\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0561 - accuracy: 0.5399\n",
      "Epoch 23: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.0561 - accuracy: 0.5399 - val_loss: 1.1043 - val_accuracy: 0.5425\n",
      "Epoch 24/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0102 - accuracy: 0.5639\n",
      "Epoch 24: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.0085 - accuracy: 0.5646 - val_loss: 1.1135 - val_accuracy: 0.5344\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0024 - accuracy: 0.5660\n",
      "Epoch 25: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0024 - accuracy: 0.5660 - val_loss: 1.1052 - val_accuracy: 0.5547\n",
      "Epoch 25: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 27s - loss: 2.3004 - accuracy: 0.1562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:11:03.103805: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.9894 - accuracy: 0.2105\n",
      "Epoch 1: val_accuracy improved from -inf to 0.31984, saving model to ./ckpt/10_class_lr005/val_acc_0.320.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.9894 - accuracy: 0.2105 - val_loss: 1.6220 - val_accuracy: 0.3198\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.9848 - accuracy: 0.3125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:11:04.231966: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.7053 - accuracy: 0.3070\n",
      "Epoch 2: val_accuracy improved from 0.31984 to 0.34413, saving model to ./ckpt/10_class_lr005/val_acc_0.344.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.7053 - accuracy: 0.3070 - val_loss: 1.6511 - val_accuracy: 0.3441\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.6339 - accuracy: 0.3171\n",
      "Epoch 3: val_accuracy improved from 0.34413 to 0.35628, saving model to ./ckpt/10_class_lr005/val_acc_0.356.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.6315 - accuracy: 0.3160 - val_loss: 1.5448 - val_accuracy: 0.3563\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.5523 - accuracy: 0.3529\n",
      "Epoch 4: val_accuracy did not improve from 0.35628\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.5496 - accuracy: 0.3546 - val_loss: 1.4546 - val_accuracy: 0.3441\n",
      "Epoch 5/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.4737 - accuracy: 0.3741\n",
      "Epoch 5: val_accuracy improved from 0.35628 to 0.45344, saving model to ./ckpt/10_class_lr005/val_acc_0.453.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.4734 - accuracy: 0.3730 - val_loss: 1.3464 - val_accuracy: 0.4534\n",
      "Epoch 6/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.4165 - accuracy: 0.4034\n",
      "Epoch 6: val_accuracy did not improve from 0.45344\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.4144 - accuracy: 0.4031 - val_loss: 1.3356 - val_accuracy: 0.4332\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.3376 - accuracy: 0.4223\n",
      "Epoch 7: val_accuracy improved from 0.45344 to 0.48178, saving model to ./ckpt/10_class_lr005/val_acc_0.482.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.3398 - accuracy: 0.4228 - val_loss: 1.1695 - val_accuracy: 0.4818\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3053 - accuracy: 0.4241\n",
      "Epoch 8: val_accuracy did not improve from 0.48178\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.3053 - accuracy: 0.4241 - val_loss: 1.1807 - val_accuracy: 0.4737\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3044 - accuracy: 0.4399\n",
      "Epoch 9: val_accuracy did not improve from 0.48178\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.3044 - accuracy: 0.4399 - val_loss: 1.2171 - val_accuracy: 0.4494\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2174 - accuracy: 0.4762\n",
      "Epoch 10: val_accuracy improved from 0.48178 to 0.51417, saving model to ./ckpt/10_class_lr005/val_acc_0.514.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.2174 - accuracy: 0.4762 - val_loss: 1.1366 - val_accuracy: 0.5142\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2272 - accuracy: 0.4811\n",
      "Epoch 11: val_accuracy did not improve from 0.51417\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.2272 - accuracy: 0.4811 - val_loss: 1.1909 - val_accuracy: 0.5020\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1752 - accuracy: 0.4780\n",
      "Epoch 12: val_accuracy did not improve from 0.51417\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1752 - accuracy: 0.4780 - val_loss: 1.1609 - val_accuracy: 0.4656\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1515 - accuracy: 0.5126\n",
      "Epoch 13: val_accuracy did not improve from 0.51417\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1515 - accuracy: 0.5126 - val_loss: 1.1074 - val_accuracy: 0.4696\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1207 - accuracy: 0.5099\n",
      "Epoch 14: val_accuracy improved from 0.51417 to 0.55466, saving model to ./ckpt/10_class_lr005/val_acc_0.555.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1207 - accuracy: 0.5099 - val_loss: 1.0289 - val_accuracy: 0.5547\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1189 - accuracy: 0.5213\n",
      "Epoch 15: val_accuracy did not improve from 0.55466\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1178 - accuracy: 0.5211 - val_loss: 1.0437 - val_accuracy: 0.4980\n",
      "Epoch 16/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1006 - accuracy: 0.5180\n",
      "Epoch 16: val_accuracy did not improve from 0.55466\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1006 - accuracy: 0.5180 - val_loss: 1.0029 - val_accuracy: 0.5142\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0721 - accuracy: 0.5260\n",
      "Epoch 17: val_accuracy did not improve from 0.55466\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0701 - accuracy: 0.5283 - val_loss: 1.0809 - val_accuracy: 0.5344\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0637 - accuracy: 0.5284\n",
      "Epoch 18: val_accuracy did not improve from 0.55466\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0631 - accuracy: 0.5292 - val_loss: 1.0271 - val_accuracy: 0.5101\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0567 - accuracy: 0.5462\n",
      "Epoch 19: val_accuracy improved from 0.55466 to 0.55870, saving model to ./ckpt/10_class_lr005/val_acc_0.559.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0567 - accuracy: 0.5462 - val_loss: 0.9877 - val_accuracy: 0.5587\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0302 - accuracy: 0.5620\n",
      "Epoch 20: val_accuracy did not improve from 0.55870\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0395 - accuracy: 0.5566 - val_loss: 1.1021 - val_accuracy: 0.5142\n",
      "Epoch 21/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0555 - accuracy: 0.5487\n",
      "Epoch 21: val_accuracy did not improve from 0.55870\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0548 - accuracy: 0.5480 - val_loss: 1.0244 - val_accuracy: 0.5425\n",
      "Epoch 22/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0052 - accuracy: 0.5760\n",
      "Epoch 22: val_accuracy did not improve from 0.55870\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.0064 - accuracy: 0.5736 - val_loss: 1.0316 - val_accuracy: 0.5425\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0018 - accuracy: 0.5866\n",
      "Epoch 23: val_accuracy did not improve from 0.55870\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0018 - accuracy: 0.5866 - val_loss: 1.0082 - val_accuracy: 0.5385\n",
      "Epoch 24/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9801 - accuracy: 0.5885\n",
      "Epoch 24: val_accuracy did not improve from 0.55870\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9903 - accuracy: 0.5857 - val_loss: 1.2492 - val_accuracy: 0.4696\n",
      "Epoch 25/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0196 - accuracy: 0.5549\n",
      "Epoch 25: val_accuracy improved from 0.55870 to 0.58300, saving model to ./ckpt/10_class_lr005/val_acc_0.583.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0196 - accuracy: 0.5552 - val_loss: 0.9431 - val_accuracy: 0.5830\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9929 - accuracy: 0.5686\n",
      "Epoch 26: val_accuracy did not improve from 0.58300\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0003 - accuracy: 0.5655 - val_loss: 1.2059 - val_accuracy: 0.4737\n",
      "Epoch 27/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9931 - accuracy: 0.5687\n",
      "Epoch 27: val_accuracy did not improve from 0.58300\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9931 - accuracy: 0.5687 - val_loss: 1.0512 - val_accuracy: 0.5263\n",
      "Epoch 28/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9715 - accuracy: 0.5723\n",
      "Epoch 28: val_accuracy did not improve from 0.58300\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9715 - accuracy: 0.5723 - val_loss: 0.9919 - val_accuracy: 0.5587\n",
      "Epoch 29/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9473 - accuracy: 0.6056\n",
      "Epoch 29: val_accuracy did not improve from 0.58300\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9415 - accuracy: 0.6059 - val_loss: 0.9739 - val_accuracy: 0.5789\n",
      "Epoch 30/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9483 - accuracy: 0.5869\n",
      "Epoch 30: val_accuracy did not improve from 0.58300\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9535 - accuracy: 0.5857 - val_loss: 0.9815 - val_accuracy: 0.5709\n",
      "Epoch 31/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9390 - accuracy: 0.5938\n",
      "Epoch 31: val_accuracy did not improve from 0.58300\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9372 - accuracy: 0.5925 - val_loss: 0.9920 - val_accuracy: 0.5587\n",
      "Epoch 32/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9441 - accuracy: 0.6029\n",
      "Epoch 32: val_accuracy did not improve from 0.58300\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9446 - accuracy: 0.6041 - val_loss: 1.0042 - val_accuracy: 0.5668\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9391 - accuracy: 0.5956\n",
      "Epoch 33: val_accuracy did not improve from 0.58300\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9391 - accuracy: 0.5956 - val_loss: 1.0017 - val_accuracy: 0.5668\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8958 - accuracy: 0.6203\n",
      "Epoch 34: val_accuracy did not improve from 0.58300\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8958 - accuracy: 0.6203 - val_loss: 1.0247 - val_accuracy: 0.5789\n",
      "Epoch 35/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8988 - accuracy: 0.6051\n",
      "Epoch 35: val_accuracy did not improve from 0.58300\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9012 - accuracy: 0.6037 - val_loss: 1.0785 - val_accuracy: 0.5587\n",
      "Epoch 35: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:11:35.362852: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.9021 - accuracy: 0.2576\n",
      "Epoch 1: val_accuracy improved from -inf to 0.27530, saving model to ./ckpt/10_class_lr005/val_acc_0.275.hdf5\n",
      "70/70 [==============================] - 2s 17ms/step - loss: 1.9021 - accuracy: 0.2576 - val_loss: 1.6854 - val_accuracy: 0.2753\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.6714 - accuracy: 0.2812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:11:36.591761: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 1.6605 - accuracy: 0.3161\n",
      "Epoch 2: val_accuracy did not improve from 0.27530\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.6620 - accuracy: 0.3151 - val_loss: 1.7536 - val_accuracy: 0.2591\n",
      "Epoch 3/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.5746 - accuracy: 0.3465\n",
      "Epoch 3: val_accuracy improved from 0.27530 to 0.38866, saving model to ./ckpt/10_class_lr005/val_acc_0.389.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5746 - accuracy: 0.3465 - val_loss: 1.5443 - val_accuracy: 0.3887\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4961 - accuracy: 0.3650\n",
      "Epoch 4: val_accuracy improved from 0.38866 to 0.39676, saving model to ./ckpt/10_class_lr005/val_acc_0.397.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.4963 - accuracy: 0.3658 - val_loss: 1.4853 - val_accuracy: 0.3968\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.4525 - accuracy: 0.3954\n",
      "Epoch 5: val_accuracy did not improve from 0.39676\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.4525 - accuracy: 0.3954 - val_loss: 1.5171 - val_accuracy: 0.3360\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3949 - accuracy: 0.4138\n",
      "Epoch 6: val_accuracy did not improve from 0.39676\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.3949 - accuracy: 0.4138 - val_loss: 1.5357 - val_accuracy: 0.3279\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.3344 - accuracy: 0.4337\n",
      "Epoch 7: val_accuracy did not improve from 0.39676\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.3351 - accuracy: 0.4363 - val_loss: 1.4313 - val_accuracy: 0.3968\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3319 - accuracy: 0.4298\n",
      "Epoch 8: val_accuracy improved from 0.39676 to 0.40891, saving model to ./ckpt/10_class_lr005/val_acc_0.409.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.3310 - accuracy: 0.4300 - val_loss: 1.3768 - val_accuracy: 0.4089\n",
      "Epoch 9/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.2516 - accuracy: 0.4735\n",
      "Epoch 9: val_accuracy did not improve from 0.40891\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.2534 - accuracy: 0.4695 - val_loss: 1.3443 - val_accuracy: 0.4049\n",
      "Epoch 10/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.2241 - accuracy: 0.4706\n",
      "Epoch 10: val_accuracy did not improve from 0.40891\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.2275 - accuracy: 0.4686 - val_loss: 1.4669 - val_accuracy: 0.3320\n",
      "Epoch 11/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.2091 - accuracy: 0.4702\n",
      "Epoch 11: val_accuracy improved from 0.40891 to 0.44534, saving model to ./ckpt/10_class_lr005/val_acc_0.445.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.2119 - accuracy: 0.4699 - val_loss: 1.2720 - val_accuracy: 0.4453\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1446 - accuracy: 0.5058\n",
      "Epoch 12: val_accuracy improved from 0.44534 to 0.47368, saving model to ./ckpt/10_class_lr005/val_acc_0.474.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1446 - accuracy: 0.5058 - val_loss: 1.1999 - val_accuracy: 0.4737\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1414 - accuracy: 0.5077\n",
      "Epoch 13: val_accuracy improved from 0.47368 to 0.48583, saving model to ./ckpt/10_class_lr005/val_acc_0.486.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1443 - accuracy: 0.5063 - val_loss: 1.2131 - val_accuracy: 0.4858\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1163 - accuracy: 0.5148\n",
      "Epoch 14: val_accuracy improved from 0.48583 to 0.52227, saving model to ./ckpt/10_class_lr005/val_acc_0.522.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1163 - accuracy: 0.5148 - val_loss: 1.1666 - val_accuracy: 0.5223\n",
      "Epoch 15/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1186 - accuracy: 0.4949\n",
      "Epoch 15: val_accuracy did not improve from 0.52227\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1186 - accuracy: 0.4933 - val_loss: 1.2252 - val_accuracy: 0.4899\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1046 - accuracy: 0.5196\n",
      "Epoch 16: val_accuracy did not improve from 0.52227\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1031 - accuracy: 0.5197 - val_loss: 1.2198 - val_accuracy: 0.4413\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0429 - accuracy: 0.5498\n",
      "Epoch 17: val_accuracy did not improve from 0.52227\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0429 - accuracy: 0.5498 - val_loss: 1.2424 - val_accuracy: 0.4615\n",
      "Epoch 18/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.1035 - accuracy: 0.5152\n",
      "Epoch 18: val_accuracy did not improve from 0.52227\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1083 - accuracy: 0.5126 - val_loss: 1.1442 - val_accuracy: 0.5020\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0481 - accuracy: 0.5413\n",
      "Epoch 19: val_accuracy improved from 0.52227 to 0.53846, saving model to ./ckpt/10_class_lr005/val_acc_0.538.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0481 - accuracy: 0.5413 - val_loss: 1.1187 - val_accuracy: 0.5385\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0126 - accuracy: 0.5763\n",
      "Epoch 20: val_accuracy improved from 0.53846 to 0.54251, saving model to ./ckpt/10_class_lr005/val_acc_0.543.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0126 - accuracy: 0.5763 - val_loss: 1.0804 - val_accuracy: 0.5425\n",
      "Epoch 21/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0027 - accuracy: 0.5597\n",
      "Epoch 21: val_accuracy did not improve from 0.54251\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0024 - accuracy: 0.5597 - val_loss: 1.2389 - val_accuracy: 0.4737\n",
      "Epoch 22/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9828 - accuracy: 0.5905\n",
      "Epoch 22: val_accuracy did not improve from 0.54251\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9869 - accuracy: 0.5889 - val_loss: 1.2142 - val_accuracy: 0.4980\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9889 - accuracy: 0.5691\n",
      "Epoch 23: val_accuracy did not improve from 0.54251\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9889 - accuracy: 0.5691 - val_loss: 1.1959 - val_accuracy: 0.5142\n",
      "Epoch 24/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9803 - accuracy: 0.5819\n",
      "Epoch 24: val_accuracy improved from 0.54251 to 0.56275, saving model to ./ckpt/10_class_lr005/val_acc_0.563.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9739 - accuracy: 0.5862 - val_loss: 1.0747 - val_accuracy: 0.5628\n",
      "Epoch 25/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9968 - accuracy: 0.5593\n",
      "Epoch 25: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9977 - accuracy: 0.5588 - val_loss: 1.1074 - val_accuracy: 0.5182\n",
      "Epoch 26/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9748 - accuracy: 0.5795\n",
      "Epoch 26: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9749 - accuracy: 0.5776 - val_loss: 1.1145 - val_accuracy: 0.5101\n",
      "Epoch 27/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9402 - accuracy: 0.5915\n",
      "Epoch 27: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9550 - accuracy: 0.5875 - val_loss: 1.1277 - val_accuracy: 0.5061\n",
      "Epoch 28/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9492 - accuracy: 0.5871\n",
      "Epoch 28: val_accuracy improved from 0.56275 to 0.56680, saving model to ./ckpt/10_class_lr005/val_acc_0.567.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9487 - accuracy: 0.5848 - val_loss: 1.1080 - val_accuracy: 0.5668\n",
      "Epoch 29/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9101 - accuracy: 0.6136\n",
      "Epoch 29: val_accuracy did not improve from 0.56680\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9111 - accuracy: 0.6113 - val_loss: 1.1015 - val_accuracy: 0.5547\n",
      "Epoch 30/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9312 - accuracy: 0.5919\n",
      "Epoch 30: val_accuracy did not improve from 0.56680\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9308 - accuracy: 0.5911 - val_loss: 1.1069 - val_accuracy: 0.5304\n",
      "Epoch 31/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9008 - accuracy: 0.6194\n",
      "Epoch 31: val_accuracy did not improve from 0.56680\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.8976 - accuracy: 0.6185 - val_loss: 1.1075 - val_accuracy: 0.5425\n",
      "Epoch 32/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9039 - accuracy: 0.5969\n",
      "Epoch 32: val_accuracy did not improve from 0.56680\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9039 - accuracy: 0.5969 - val_loss: 1.1254 - val_accuracy: 0.5668\n",
      "Epoch 33/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8989 - accuracy: 0.6155\n",
      "Epoch 33: val_accuracy did not improve from 0.56680\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8955 - accuracy: 0.6180 - val_loss: 1.1148 - val_accuracy: 0.5385\n",
      "Epoch 34/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9206 - accuracy: 0.6082\n",
      "Epoch 34: val_accuracy improved from 0.56680 to 0.58300, saving model to ./ckpt/10_class_lr005/val_acc_0.583.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9195 - accuracy: 0.6086 - val_loss: 1.0983 - val_accuracy: 0.5830\n",
      "Epoch 34: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 27s - loss: 2.3026 - accuracy: 0.0938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:12:06.456719: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.8966 - accuracy: 0.2451"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:12:07.751913: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.32389, saving model to ./ckpt/10_class_lr005/val_acc_0.324.hdf5\n",
      "70/70 [==============================] - 2s 20ms/step - loss: 1.8966 - accuracy: 0.2451 - val_loss: 1.6358 - val_accuracy: 0.3239\n",
      "Epoch 2/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.6720 - accuracy: 0.3111\n",
      "Epoch 2: val_accuracy improved from 0.32389 to 0.35223, saving model to ./ckpt/10_class_lr005/val_acc_0.352.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.6707 - accuracy: 0.3119 - val_loss: 1.5729 - val_accuracy: 0.3522\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.5700 - accuracy: 0.3535\n",
      "Epoch 3: val_accuracy improved from 0.35223 to 0.36032, saving model to ./ckpt/10_class_lr005/val_acc_0.360.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.5749 - accuracy: 0.3523 - val_loss: 1.5454 - val_accuracy: 0.3603\n",
      "Epoch 4/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.4863 - accuracy: 0.3867\n",
      "Epoch 4: val_accuracy improved from 0.36032 to 0.38462, saving model to ./ckpt/10_class_lr005/val_acc_0.385.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.4867 - accuracy: 0.3869 - val_loss: 1.4706 - val_accuracy: 0.3846\n",
      "Epoch 5/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.4135 - accuracy: 0.4039\n",
      "Epoch 5: val_accuracy did not improve from 0.38462\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.4139 - accuracy: 0.4039 - val_loss: 1.4987 - val_accuracy: 0.3603\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3988 - accuracy: 0.4206\n",
      "Epoch 6: val_accuracy improved from 0.38462 to 0.42105, saving model to ./ckpt/10_class_lr005/val_acc_0.421.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3988 - accuracy: 0.4206 - val_loss: 1.3277 - val_accuracy: 0.4211\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.3169 - accuracy: 0.4276\n",
      "Epoch 7: val_accuracy did not improve from 0.42105\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.3183 - accuracy: 0.4264 - val_loss: 1.3738 - val_accuracy: 0.4089\n",
      "Epoch 8/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.2992 - accuracy: 0.4508\n",
      "Epoch 8: val_accuracy did not improve from 0.42105\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.2990 - accuracy: 0.4520 - val_loss: 1.2450 - val_accuracy: 0.4089\n",
      "Epoch 9/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2557 - accuracy: 0.4556\n",
      "Epoch 9: val_accuracy improved from 0.42105 to 0.42915, saving model to ./ckpt/10_class_lr005/val_acc_0.429.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.2555 - accuracy: 0.4547 - val_loss: 1.2417 - val_accuracy: 0.4291\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2658 - accuracy: 0.4592\n",
      "Epoch 10: val_accuracy improved from 0.42915 to 0.43320, saving model to ./ckpt/10_class_lr005/val_acc_0.433.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.2658 - accuracy: 0.4592 - val_loss: 1.1991 - val_accuracy: 0.4332\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.1992 - accuracy: 0.4674\n",
      "Epoch 11: val_accuracy improved from 0.43320 to 0.49798, saving model to ./ckpt/10_class_lr005/val_acc_0.498.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1988 - accuracy: 0.4681 - val_loss: 1.1403 - val_accuracy: 0.4980\n",
      "Epoch 12/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1465 - accuracy: 0.5009\n",
      "Epoch 12: val_accuracy did not improve from 0.49798\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1500 - accuracy: 0.5000 - val_loss: 1.1805 - val_accuracy: 0.4494\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1320 - accuracy: 0.5077\n",
      "Epoch 13: val_accuracy improved from 0.49798 to 0.52632, saving model to ./ckpt/10_class_lr005/val_acc_0.526.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1326 - accuracy: 0.5058 - val_loss: 1.1316 - val_accuracy: 0.5263\n",
      "Epoch 14/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1289 - accuracy: 0.5128\n",
      "Epoch 14: val_accuracy did not improve from 0.52632\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1345 - accuracy: 0.5126 - val_loss: 1.1225 - val_accuracy: 0.4980\n",
      "Epoch 15/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0801 - accuracy: 0.5401\n",
      "Epoch 15: val_accuracy did not improve from 0.52632\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0836 - accuracy: 0.5390 - val_loss: 1.1080 - val_accuracy: 0.5142\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0708 - accuracy: 0.5430\n",
      "Epoch 16: val_accuracy did not improve from 0.52632\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0706 - accuracy: 0.5431 - val_loss: 1.1021 - val_accuracy: 0.5061\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0682 - accuracy: 0.5265\n",
      "Epoch 17: val_accuracy improved from 0.52632 to 0.54251, saving model to ./ckpt/10_class_lr005/val_acc_0.543.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0682 - accuracy: 0.5265 - val_loss: 1.0765 - val_accuracy: 0.5425\n",
      "Epoch 18/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0765 - accuracy: 0.5380\n",
      "Epoch 18: val_accuracy did not improve from 0.54251\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0742 - accuracy: 0.5390 - val_loss: 1.0508 - val_accuracy: 0.5061\n",
      "Epoch 19/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0368 - accuracy: 0.5675\n",
      "Epoch 19: val_accuracy did not improve from 0.54251\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0378 - accuracy: 0.5660 - val_loss: 1.1118 - val_accuracy: 0.4980\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0707 - accuracy: 0.5436\n",
      "Epoch 20: val_accuracy did not improve from 0.54251\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.0692 - accuracy: 0.5444 - val_loss: 1.3483 - val_accuracy: 0.4494\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0198 - accuracy: 0.5672\n",
      "Epoch 21: val_accuracy improved from 0.54251 to 0.55061, saving model to ./ckpt/10_class_lr005/val_acc_0.551.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0220 - accuracy: 0.5637 - val_loss: 1.0648 - val_accuracy: 0.5506\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0436 - accuracy: 0.5530\n",
      "Epoch 22: val_accuracy did not improve from 0.55061\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0437 - accuracy: 0.5507 - val_loss: 1.0783 - val_accuracy: 0.5506\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0097 - accuracy: 0.5664\n",
      "Epoch 23: val_accuracy did not improve from 0.55061\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0097 - accuracy: 0.5664 - val_loss: 1.0421 - val_accuracy: 0.5425\n",
      "Epoch 24/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0086 - accuracy: 0.5708\n",
      "Epoch 24: val_accuracy did not improve from 0.55061\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0139 - accuracy: 0.5691 - val_loss: 1.1255 - val_accuracy: 0.4858\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9848 - accuracy: 0.5772\n",
      "Epoch 25: val_accuracy improved from 0.55061 to 0.57895, saving model to ./ckpt/10_class_lr005/val_acc_0.579.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9848 - accuracy: 0.5772 - val_loss: 1.0104 - val_accuracy: 0.5789\n",
      "Epoch 26/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9446 - accuracy: 0.6065\n",
      "Epoch 26: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9500 - accuracy: 0.6041 - val_loss: 1.0363 - val_accuracy: 0.5628\n",
      "Epoch 27/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9556 - accuracy: 0.5933\n",
      "Epoch 27: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9584 - accuracy: 0.5925 - val_loss: 1.0542 - val_accuracy: 0.5385\n",
      "Epoch 28/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9538 - accuracy: 0.6018\n",
      "Epoch 28: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9546 - accuracy: 0.6014 - val_loss: 1.0359 - val_accuracy: 0.5668\n",
      "Epoch 29/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9528 - accuracy: 0.6108\n",
      "Epoch 29: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9596 - accuracy: 0.6064 - val_loss: 1.1925 - val_accuracy: 0.4777\n",
      "Epoch 30/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9396 - accuracy: 0.6136\n",
      "Epoch 30: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.9400 - accuracy: 0.6113 - val_loss: 1.0727 - val_accuracy: 0.5587\n",
      "Epoch 31/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9662 - accuracy: 0.5762\n",
      "Epoch 31: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9669 - accuracy: 0.5754 - val_loss: 1.1098 - val_accuracy: 0.5101\n",
      "Epoch 32/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9380 - accuracy: 0.6028\n",
      "Epoch 32: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9415 - accuracy: 0.6010 - val_loss: 1.1040 - val_accuracy: 0.5425\n",
      "Epoch 33/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9641 - accuracy: 0.5910\n",
      "Epoch 33: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.9626 - accuracy: 0.5898 - val_loss: 1.0920 - val_accuracy: 0.5425\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9308 - accuracy: 0.6037\n",
      "Epoch 34: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9308 - accuracy: 0.6037 - val_loss: 0.9722 - val_accuracy: 0.5628\n",
      "Epoch 35/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9075 - accuracy: 0.6261\n",
      "Epoch 35: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9075 - accuracy: 0.6261 - val_loss: 1.0888 - val_accuracy: 0.5385\n",
      "Epoch 36/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9234 - accuracy: 0.6070\n",
      "Epoch 36: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9199 - accuracy: 0.6068 - val_loss: 1.0315 - val_accuracy: 0.5668\n",
      "Epoch 37/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8561 - accuracy: 0.6402\n",
      "Epoch 37: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8630 - accuracy: 0.6364 - val_loss: 1.1309 - val_accuracy: 0.4980\n",
      "Epoch 38/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9020 - accuracy: 0.6241\n",
      "Epoch 38: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9014 - accuracy: 0.6239 - val_loss: 1.0816 - val_accuracy: 0.5142\n",
      "Epoch 39/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8868 - accuracy: 0.6296\n",
      "Epoch 39: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8879 - accuracy: 0.6302 - val_loss: 1.0587 - val_accuracy: 0.5749\n",
      "Epoch 40/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8433 - accuracy: 0.6434\n",
      "Epoch 40: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8469 - accuracy: 0.6418 - val_loss: 0.9992 - val_accuracy: 0.5668\n",
      "Epoch 41/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8861 - accuracy: 0.6387\n",
      "Epoch 41: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8867 - accuracy: 0.6396 - val_loss: 1.1258 - val_accuracy: 0.4737\n",
      "Epoch 42/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8746 - accuracy: 0.6513\n",
      "Epoch 42: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8746 - accuracy: 0.6513 - val_loss: 1.0124 - val_accuracy: 0.5466\n",
      "Epoch 43/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8713 - accuracy: 0.6293\n",
      "Epoch 43: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8720 - accuracy: 0.6284 - val_loss: 1.0163 - val_accuracy: 0.5344\n",
      "Epoch 44/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8565 - accuracy: 0.6458\n",
      "Epoch 44: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.8646 - accuracy: 0.6427 - val_loss: 1.0247 - val_accuracy: 0.5587\n",
      "Epoch 44: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 1:01 - loss: 2.3061 - accuracy: 0.0625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:12:46.834573: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.9547 - accuracy: 0.2217\n",
      "Epoch 1: val_accuracy improved from -inf to 0.27530, saving model to ./ckpt/10_class_lr005/val_acc_0.275.hdf5\n",
      "70/70 [==============================] - 2s 15ms/step - loss: 1.9547 - accuracy: 0.2217 - val_loss: 1.7431 - val_accuracy: 0.2753\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.5955 - accuracy: 0.2812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 00:12:47.908484: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/70 [============================>.] - ETA: 0s - loss: 1.6303 - accuracy: 0.3323\n",
      "Epoch 2: val_accuracy improved from 0.27530 to 0.35223, saving model to ./ckpt/10_class_lr005/val_acc_0.352.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.6314 - accuracy: 0.3317 - val_loss: 1.5952 - val_accuracy: 0.3522\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5561 - accuracy: 0.3537\n",
      "Epoch 3: val_accuracy improved from 0.35223 to 0.40891, saving model to ./ckpt/10_class_lr005/val_acc_0.409.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.5557 - accuracy: 0.3546 - val_loss: 1.4523 - val_accuracy: 0.4089\n",
      "Epoch 4/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.4685 - accuracy: 0.3825\n",
      "Epoch 4: val_accuracy improved from 0.40891 to 0.48178, saving model to ./ckpt/10_class_lr005/val_acc_0.482.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.4690 - accuracy: 0.3793 - val_loss: 1.3567 - val_accuracy: 0.4818\n",
      "Epoch 5/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.4169 - accuracy: 0.3885\n",
      "Epoch 5: val_accuracy did not improve from 0.48178\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4175 - accuracy: 0.3896 - val_loss: 1.3158 - val_accuracy: 0.4494\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3307 - accuracy: 0.4352\n",
      "Epoch 6: val_accuracy did not improve from 0.48178\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.3328 - accuracy: 0.4354 - val_loss: 1.4205 - val_accuracy: 0.3765\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.3305 - accuracy: 0.4309\n",
      "Epoch 7: val_accuracy did not improve from 0.48178\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.3227 - accuracy: 0.4345 - val_loss: 1.2455 - val_accuracy: 0.4494\n",
      "Epoch 8/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.2539 - accuracy: 0.4580\n",
      "Epoch 8: val_accuracy did not improve from 0.48178\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.2564 - accuracy: 0.4565 - val_loss: 1.2023 - val_accuracy: 0.4777\n",
      "Epoch 9/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.2384 - accuracy: 0.4579\n",
      "Epoch 9: val_accuracy improved from 0.48178 to 0.53441, saving model to ./ckpt/10_class_lr005/val_acc_0.534.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.2447 - accuracy: 0.4560 - val_loss: 1.1086 - val_accuracy: 0.5344\n",
      "Epoch 10/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1835 - accuracy: 0.4801\n",
      "Epoch 10: val_accuracy did not improve from 0.53441\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1833 - accuracy: 0.4794 - val_loss: 1.1412 - val_accuracy: 0.4899\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1737 - accuracy: 0.4969\n",
      "Epoch 11: val_accuracy did not improve from 0.53441\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1737 - accuracy: 0.4969 - val_loss: 1.2544 - val_accuracy: 0.4858\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1574 - accuracy: 0.4928\n",
      "Epoch 12: val_accuracy did not improve from 0.53441\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1573 - accuracy: 0.4901 - val_loss: 1.0668 - val_accuracy: 0.5182\n",
      "Epoch 13/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1084 - accuracy: 0.5076\n",
      "Epoch 13: val_accuracy improved from 0.53441 to 0.59919, saving model to ./ckpt/10_class_lr005/val_acc_0.599.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.1167 - accuracy: 0.5027 - val_loss: 1.0526 - val_accuracy: 0.5992\n",
      "Epoch 14/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.1296 - accuracy: 0.5199\n",
      "Epoch 14: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1263 - accuracy: 0.5202 - val_loss: 1.1211 - val_accuracy: 0.5182\n",
      "Epoch 15/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0812 - accuracy: 0.5289\n",
      "Epoch 15: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0838 - accuracy: 0.5265 - val_loss: 1.1243 - val_accuracy: 0.5020\n",
      "Epoch 16/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0868 - accuracy: 0.5332\n",
      "Epoch 16: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0868 - accuracy: 0.5332 - val_loss: 1.0326 - val_accuracy: 0.5506\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0703 - accuracy: 0.5464\n",
      "Epoch 17: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0670 - accuracy: 0.5485 - val_loss: 1.0798 - val_accuracy: 0.5466\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0727 - accuracy: 0.5393\n",
      "Epoch 18: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0751 - accuracy: 0.5368 - val_loss: 1.0401 - val_accuracy: 0.5628\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0976 - accuracy: 0.5421\n",
      "Epoch 19: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.1076 - accuracy: 0.5408 - val_loss: 1.0948 - val_accuracy: 0.4899\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0453 - accuracy: 0.5526\n",
      "Epoch 20: val_accuracy improved from 0.59919 to 0.61538, saving model to ./ckpt/10_class_lr005/val_acc_0.615.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0446 - accuracy: 0.5507 - val_loss: 0.9790 - val_accuracy: 0.6154\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0049 - accuracy: 0.5644\n",
      "Epoch 21: val_accuracy did not improve from 0.61538\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0080 - accuracy: 0.5610 - val_loss: 1.0551 - val_accuracy: 0.5385\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9979 - accuracy: 0.5688\n",
      "Epoch 22: val_accuracy did not improve from 0.61538\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.9970 - accuracy: 0.5691 - val_loss: 1.1031 - val_accuracy: 0.5547\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0191 - accuracy: 0.5705\n",
      "Epoch 23: val_accuracy did not improve from 0.61538\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 1.0086 - accuracy: 0.5736 - val_loss: 1.0181 - val_accuracy: 0.5830\n",
      "Epoch 24/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0329 - accuracy: 0.5620\n",
      "Epoch 24: val_accuracy did not improve from 0.61538\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.0339 - accuracy: 0.5628 - val_loss: 1.2224 - val_accuracy: 0.5223\n",
      "Epoch 25/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9600 - accuracy: 0.5961\n",
      "Epoch 25: val_accuracy did not improve from 0.61538\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9663 - accuracy: 0.5934 - val_loss: 0.9794 - val_accuracy: 0.5992\n",
      "Epoch 26/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9635 - accuracy: 0.5885\n",
      "Epoch 26: val_accuracy did not improve from 0.61538\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9584 - accuracy: 0.5880 - val_loss: 1.0436 - val_accuracy: 0.5223\n",
      "Epoch 27/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9519 - accuracy: 0.6004\n",
      "Epoch 27: val_accuracy did not improve from 0.61538\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9641 - accuracy: 0.5947 - val_loss: 1.0656 - val_accuracy: 0.5263\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9596 - accuracy: 0.5888\n",
      "Epoch 28: val_accuracy did not improve from 0.61538\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9620 - accuracy: 0.5884 - val_loss: 0.9940 - val_accuracy: 0.5749\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9559 - accuracy: 0.5879\n",
      "Epoch 29: val_accuracy did not improve from 0.61538\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9543 - accuracy: 0.5880 - val_loss: 1.0233 - val_accuracy: 0.5951\n",
      "Epoch 30/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.9184 - accuracy: 0.6155\n",
      "Epoch 30: val_accuracy did not improve from 0.61538\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.9200 - accuracy: 0.6131 - val_loss: 1.0148 - val_accuracy: 0.6073\n",
      "Epoch 30: early stopping\n",
      "Finish 10-fold cross validation\n",
      "Best performing model has 0.6653 validation accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# modify to save ckpt for each test\n",
    "ckpt = os.path.join(ckpt_path, \"val_acc_{val_accuracy:.3f}.hdf5\")\n",
    "\n",
    "# training params\n",
    "epochs = epochs\n",
    "num_classes = num_classes\n",
    "lr = lr\n",
    "\n",
    "# the k for k fold CV\n",
    "n_split = 10\n",
    "\n",
    "# for recording best performance\n",
    "max_acc = 0\n",
    "best_history = None\n",
    "\n",
    "'''\n",
    "k-fold cross validation\n",
    "Save the best model using validation accuracy as metric\n",
    "Print the global best performace when finished\n",
    "'''\n",
    "for train_index, test_index in KFold(n_split).split(train_examples):\n",
    "\n",
    "    x_train, x_vad = train_examples[train_index], train_examples[test_index]\n",
    "    y_train, y_vad = train_labels[train_index], train_labels[test_index]\n",
    "\n",
    "    model=create_model(num_classes, lr)\n",
    "  \n",
    "    # callbacks\n",
    "    checkpoint_filepath = ckpt\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "        mode='auto',\n",
    "    )\n",
    "\n",
    "    # Model weights are saved at the end of every epoch, if it's the best seen so far.\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_vad, y_vad),\n",
    "                        callbacks=[model_checkpoint_callback, early_stopping_callback])\n",
    "\n",
    "    val_acc = max(history.history['val_accuracy'])\n",
    "    \n",
    "    if val_acc > max_acc:\n",
    "        max_acc = val_acc\n",
    "        best_history = history\n",
    "        # print('Best acc so far. Saving params...\\n')\n",
    "\n",
    "print('Finish {}-fold cross validation'.format(n_split))\n",
    "print('Best performing model has {:.4f} validation accuracy'.format(max_acc))\n",
    "\n",
    "#CPU\n",
    "# with tf.device('/CPU:0'):\n",
    "#     history = model.fit(trainX, trainY, epochs=epochs,validation_data=(testX, testY), batch_size=64)\n",
    "\n",
    "# deafult go with GPU\n",
    "# history = model.fit(trainX, trainY, epochs=epochs,validation_data=(testX, testY), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "history = best_history\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# because of early stopping, can't just use \"epochs\"\n",
    "epochs_range = range(len(history.history['loss']))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evantilu/miniforge3/envs/6998_DL_tf/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2022-05-09 00:17:29.676630: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 12ms/step - loss: 0.2967 - accuracy: 0.8899\n",
      "\n",
      "evaluate on test set:\n",
      "loss = 0.29670\tacc = 88.991%\n"
     ]
    }
   ],
   "source": [
    "model = create_model(num_classes=num_classes)\n",
    "model.load_weights(os.path.join(ckpt_path, \"val_acc_0.883.hdf5\"))\n",
    "\n",
    "loss, acc = model.evaluate(test_examples, test_labels)\n",
    "\n",
    "print('\\nevaluate on test set:\\nloss = {:.5f}\\tacc = {:.3f}%'.format(loss, acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29cc21816e506614f017a9125cfdb1f5dc655865e499c52ef5f5406a40d25695"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
