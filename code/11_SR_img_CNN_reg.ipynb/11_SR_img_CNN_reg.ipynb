{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold CV with 3-layer CNN max pooling on SR images - Reggresion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and env settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.min_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables/parameters used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"./ckpt/reg_lr005/\"\n",
    "os.makedirs(ckpt_path, exist_ok=True)\n",
    "\n",
    "SR_img_path = '../../data/SR_img/'\n",
    "\n",
    "lr = 0.005\n",
    "epochs = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/images/label_raw_price.csv')\n",
    "df.head()\n",
    "\n",
    "labels = df['SalePrice'].to_numpy()\n",
    "labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 72, 72, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_arr = []\n",
    "\n",
    "for i in range(1, len(labels)+1):\n",
    "# for i in range(1, 10):\n",
    "    img = plt.imread(os.path.join(SR_img_path, 'SR_img_{}.png'.format(i)))\n",
    "    images_arr.append(img)\n",
    "\n",
    "images_arr = np.array(images_arr, dtype='float32')\n",
    "images_arr.shape\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train test splitting\n",
    "- hold out 15% for testing\n",
    "- use 85% to train model with K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_samples = images_arr.shape[0] \n",
    "test_ratio = 0.15\n",
    "test_samples = int(test_ratio * images_arr.shape[0])\n",
    "\n",
    "train_examples = images_arr[:-1*test_samples]\n",
    "test_examples = images_arr[-1*test_samples:]\n",
    "train_labels = labels[:-1*test_samples]\n",
    "test_labels = labels[-1*test_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (2475, 72, 72, 3)\n",
      "test:  (436, 72, 72, 3)\n",
      "train label:  (2475,)\n",
      "test label:  (436,)\n"
     ]
    }
   ],
   "source": [
    "print('train: ', train_examples.shape)\n",
    "print('test: ', test_examples.shape)\n",
    "print('train label: ', train_labels.shape)\n",
    "print('test label: ', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def create_reg_model(lr=0.005):\n",
    "\n",
    "\t# Working\n",
    "\twith tf.device('/cpu:0'):\n",
    "\t\tdata_augmentation = tf.keras.Sequential([ \n",
    "\t\t\ttf.keras.layers.RandomFlip(\"horizontal\", input_shape=(72, 72, 3)),\n",
    "\t  \t\ttf.keras.layers.RandomRotation(0.1),\n",
    "\t\t    tf.keras.layers.RandomZoom(0.1)\n",
    "\t\t\t])\n",
    "\n",
    "\n",
    "\tmodel = tf.keras.Sequential([\n",
    "\t\t# data_augmentation,\n",
    "\t  \t# tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(72, 72,3)),\n",
    "\t\ttf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.MaxPooling2D((2,2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t  \ttf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.MaxPooling2D(),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Flatten(),\n",
    "\t\ttf.keras.layers.Dense(128, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(64, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(32, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "\t])\n",
    "\n",
    "\t# opt = tf.keras.optimizers.SGD(lr=0.005, momentum=0.9)\n",
    "\topt = tf.keras.optimizers.Adam(lr=lr)\n",
    "\tmodel.compile(optimizer=opt, loss=rmse, metrics=[rmse])\n",
    "\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_reg_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:04:27.121476: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - ETA: 0s - loss: 82880.0938 - rmse: 82423.9219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:04:29.890986: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 3s 25ms/step - loss: 82880.0938 - rmse: 82423.9219 - val_loss: 39427.4414 - val_rmse: 39219.7070\n",
      "Epoch 2/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 42884.4258 - rmse: 42924.2578 - val_loss: 32693.8145 - val_rmse: 32545.2969\n",
      "Epoch 3/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 41098.6211 - rmse: 41206.6055 - val_loss: 30623.4688 - val_rmse: 30414.0098\n",
      "Epoch 4/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 38042.6406 - rmse: 38076.0078 - val_loss: 25980.3223 - val_rmse: 25667.4258\n",
      "Epoch 5/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 34230.0547 - rmse: 34231.5195 - val_loss: 24325.1660 - val_rmse: 24074.0488\n",
      "Epoch 6/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 32218.0859 - rmse: 32103.6445 - val_loss: 31000.2695 - val_rmse: 30665.5352\n",
      "Epoch 7/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 29934.8301 - rmse: 29894.6758 - val_loss: 21843.3203 - val_rmse: 21704.7773\n",
      "Epoch 8/120\n",
      "78/78 [==============================] - 1s 14ms/step - loss: 34718.7656 - rmse: 34924.1562 - val_loss: 44031.8789 - val_rmse: 43736.8359\n",
      "Epoch 9/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 34170.4297 - rmse: 34370.7930 - val_loss: 51048.8320 - val_rmse: 50770.6602\n",
      "Epoch 10/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 31581.0508 - rmse: 31496.9805 - val_loss: 23923.2734 - val_rmse: 23701.5059\n",
      "Epoch 11/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 27785.2461 - rmse: 27782.0254 - val_loss: 21430.5312 - val_rmse: 21255.8008\n",
      "Epoch 12/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 27495.5195 - rmse: 27522.9395 - val_loss: 20321.2695 - val_rmse: 20243.1230\n",
      "Epoch 13/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 27129.4707 - rmse: 27095.3086 - val_loss: 22604.8164 - val_rmse: 22407.4980\n",
      "Epoch 14/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 24847.0430 - rmse: 24912.5156 - val_loss: 20234.7422 - val_rmse: 20139.6367\n",
      "Epoch 15/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 25949.6680 - rmse: 25879.1309 - val_loss: 22188.8223 - val_rmse: 22009.0547\n",
      "Epoch 16/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 23455.3125 - rmse: 23467.8633 - val_loss: 22357.4355 - val_rmse: 22177.8164\n",
      "Epoch 17/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 23002.6133 - rmse: 22903.0371 - val_loss: 19125.3242 - val_rmse: 18986.7812\n",
      "Epoch 18/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 25994.4629 - rmse: 25988.1562 - val_loss: 19123.0625 - val_rmse: 19072.2656\n",
      "Epoch 19/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 23511.2969 - rmse: 23615.1992 - val_loss: 17394.1035 - val_rmse: 17305.0664\n",
      "Epoch 20/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 23173.5605 - rmse: 23209.6797 - val_loss: 25310.6602 - val_rmse: 25150.6074\n",
      "Epoch 21/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 22546.6758 - rmse: 22483.7461 - val_loss: 16620.9883 - val_rmse: 16548.8066\n",
      "Epoch 22/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 22201.6230 - rmse: 22133.5977 - val_loss: 18547.2148 - val_rmse: 18415.3047\n",
      "Epoch 23/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 21014.9805 - rmse: 21061.6445 - val_loss: 20395.0176 - val_rmse: 20228.1562\n",
      "Epoch 24/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 22458.5645 - rmse: 22460.4824 - val_loss: 22476.3887 - val_rmse: 22373.4102\n",
      "Epoch 25/120\n",
      "78/78 [==============================] - 1s 13ms/step - loss: 21193.4844 - rmse: 21179.3887 - val_loss: 32684.3438 - val_rmse: 32682.0098\n",
      "Epoch 26/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 25689.1836 - rmse: 25714.9102 - val_loss: 31941.4648 - val_rmse: 31784.5625\n",
      "Epoch 27/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 21742.0215 - rmse: 21653.9023 - val_loss: 22212.6406 - val_rmse: 22061.5098\n",
      "Epoch 28/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 19697.3203 - rmse: 19690.3223 - val_loss: 18098.0645 - val_rmse: 18049.8105\n",
      "Epoch 29/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 21456.0840 - rmse: 21479.0781 - val_loss: 18904.5879 - val_rmse: 18793.8066\n",
      "Epoch 30/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 22142.4102 - rmse: 22052.7305 - val_loss: 15859.4443 - val_rmse: 15791.6016\n",
      "Epoch 31/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 19925.9277 - rmse: 19948.4258 - val_loss: 20656.4844 - val_rmse: 20484.3555\n",
      "Epoch 32/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 20309.1523 - rmse: 20444.3633 - val_loss: 16005.3828 - val_rmse: 15881.7773\n",
      "Epoch 33/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 22034.2188 - rmse: 21969.6660 - val_loss: 17249.6074 - val_rmse: 17116.6465\n",
      "Epoch 34/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 19498.2773 - rmse: 19639.1484 - val_loss: 16196.9395 - val_rmse: 16157.7578\n",
      "Epoch 35/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 18401.1504 - rmse: 18346.5059 - val_loss: 20509.2012 - val_rmse: 20461.3867\n",
      "Epoch 36/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 19569.9844 - rmse: 19522.3887 - val_loss: 15997.6523 - val_rmse: 15911.0527\n",
      "Epoch 37/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 19016.9883 - rmse: 18961.0820 - val_loss: 21948.0449 - val_rmse: 21795.8516\n",
      "Epoch 38/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 19597.1152 - rmse: 19543.5977 - val_loss: 16003.5400 - val_rmse: 15913.2139\n",
      "Epoch 39/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 19201.6211 - rmse: 19311.5684 - val_loss: 18462.1816 - val_rmse: 18308.6758\n",
      "Epoch 40/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 17868.6152 - rmse: 17969.9609 - val_loss: 16623.2227 - val_rmse: 16511.9355\n",
      "Epoch 41/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 20060.2598 - rmse: 20084.5898 - val_loss: 34765.4023 - val_rmse: 34697.2305\n",
      "Epoch 42/120\n",
      "78/78 [==============================] - 1s 13ms/step - loss: 21876.2988 - rmse: 21871.0117 - val_loss: 18188.6133 - val_rmse: 18063.4434\n",
      "Epoch 43/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 18285.4043 - rmse: 18273.6406 - val_loss: 15930.1250 - val_rmse: 15813.0664\n",
      "Epoch 44/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 19819.8262 - rmse: 19860.2715 - val_loss: 22712.4746 - val_rmse: 22600.6445\n",
      "Epoch 45/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 19148.5020 - rmse: 19083.2070 - val_loss: 24031.4141 - val_rmse: 24027.2578\n",
      "Epoch 46/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 20052.9688 - rmse: 20080.9531 - val_loss: 17422.9199 - val_rmse: 17334.4219\n",
      "Epoch 47/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 18872.4980 - rmse: 18938.7930 - val_loss: 17591.4121 - val_rmse: 17444.1289\n",
      "Epoch 48/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 19214.6562 - rmse: 19156.6367 - val_loss: 15760.3447 - val_rmse: 15670.7988\n",
      "Epoch 49/120\n",
      "78/78 [==============================] - 1s 13ms/step - loss: 18371.2500 - rmse: 18461.0840 - val_loss: 15573.9844 - val_rmse: 15507.3018\n",
      "Epoch 50/120\n",
      "78/78 [==============================] - 1s 13ms/step - loss: 19327.3711 - rmse: 19307.8301 - val_loss: 23742.3418 - val_rmse: 23632.7676\n",
      "Epoch 51/120\n",
      "78/78 [==============================] - 1s 13ms/step - loss: 18146.7207 - rmse: 18166.4961 - val_loss: 15219.4160 - val_rmse: 15107.1719\n",
      "Epoch 52/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 19440.8730 - rmse: 19366.0391 - val_loss: 26314.0391 - val_rmse: 26170.1621\n",
      "Epoch 53/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 19832.0742 - rmse: 19807.3828 - val_loss: 15425.1318 - val_rmse: 15386.5078\n",
      "Epoch 54/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 17183.4199 - rmse: 17113.0371 - val_loss: 16357.2207 - val_rmse: 16313.6562\n",
      "Epoch 55/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 16060.5967 - rmse: 16060.8047 - val_loss: 22496.2402 - val_rmse: 22377.0742\n",
      "Epoch 56/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 17005.4902 - rmse: 16994.9805 - val_loss: 17291.5840 - val_rmse: 17263.8848\n",
      "Epoch 57/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 18174.8652 - rmse: 18134.6270 - val_loss: 18949.4805 - val_rmse: 18858.5586\n",
      "Epoch 58/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 20347.0195 - rmse: 20282.2656 - val_loss: 19864.6367 - val_rmse: 19854.8125\n",
      "Epoch 59/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 17388.9727 - rmse: 17396.7812 - val_loss: 19001.0918 - val_rmse: 18979.0820\n",
      "Epoch 60/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 17166.7227 - rmse: 17167.5176 - val_loss: 15852.7559 - val_rmse: 15836.1855\n",
      "Epoch 61/120\n",
      "78/78 [==============================] - 1s 13ms/step - loss: 17596.3203 - rmse: 17583.0469 - val_loss: 17103.9629 - val_rmse: 17026.0723\n",
      "Epoch 62/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 18341.8066 - rmse: 18300.5625 - val_loss: 17263.0391 - val_rmse: 17212.8613\n",
      "Epoch 63/120\n",
      "78/78 [==============================] - 1s 14ms/step - loss: 16588.2129 - rmse: 16701.6562 - val_loss: 17573.5039 - val_rmse: 17522.0898\n",
      "Epoch 64/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 17647.2441 - rmse: 17723.1973 - val_loss: 20230.2930 - val_rmse: 20227.2383\n",
      "Epoch 65/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 16348.6582 - rmse: 16334.9102 - val_loss: 21759.9023 - val_rmse: 21615.9668\n",
      "Epoch 66/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 17137.3281 - rmse: 17111.0312 - val_loss: 15815.8516 - val_rmse: 15785.9980\n",
      "Epoch 67/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 16085.6162 - rmse: 16078.4697 - val_loss: 14698.4756 - val_rmse: 14623.6816\n",
      "Epoch 68/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 15965.4482 - rmse: 15924.9541 - val_loss: 15972.6084 - val_rmse: 15900.6230\n",
      "Epoch 69/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 16188.9561 - rmse: 16201.3574 - val_loss: 16906.7871 - val_rmse: 16854.5508\n",
      "Epoch 70/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 16701.1133 - rmse: 16709.1406 - val_loss: 15887.8018 - val_rmse: 15787.1768\n",
      "Epoch 71/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 15765.5986 - rmse: 15711.2324 - val_loss: 17669.1250 - val_rmse: 17596.9551\n",
      "Epoch 72/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 17125.4473 - rmse: 17150.7344 - val_loss: 15782.0117 - val_rmse: 15790.6045\n",
      "Epoch 73/120\n",
      "78/78 [==============================] - 1s 13ms/step - loss: 15493.7734 - rmse: 15450.7646 - val_loss: 17910.4531 - val_rmse: 17838.9453\n",
      "Epoch 74/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 17199.7188 - rmse: 17133.4551 - val_loss: 19349.4531 - val_rmse: 19254.1602\n",
      "Epoch 75/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 17348.1738 - rmse: 17343.9141 - val_loss: 17492.8164 - val_rmse: 17484.7129\n",
      "Epoch 76/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 16857.8301 - rmse: 16802.3828 - val_loss: 16906.4844 - val_rmse: 16870.8516\n",
      "Epoch 77/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 16608.5039 - rmse: 16619.1602 - val_loss: 15780.7422 - val_rmse: 15730.6025\n",
      "Epoch 78/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 16803.3789 - rmse: 16804.8164 - val_loss: 17905.0762 - val_rmse: 17814.2031\n",
      "Epoch 79/120\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 17720.1582 - rmse: 17776.3496 - val_loss: 16411.9492 - val_rmse: 16385.4141\n",
      "Epoch 80/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 16488.4785 - rmse: 16461.4258 - val_loss: 15497.0459 - val_rmse: 15401.3623\n",
      "Epoch 81/120\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 15969.3545 - rmse: 15993.9824 - val_loss: 15207.8613 - val_rmse: 15120.1543\n",
      "Epoch 82/120\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 17117.5781 - rmse: 17169.9062 - val_loss: 15299.8047 - val_rmse: 15246.9697\n",
      "Epoch 83/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 16142.4404 - rmse: 16101.7500 - val_loss: 15912.4082 - val_rmse: 15861.6680\n",
      "Epoch 84/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 18006.4922 - rmse: 18021.8047 - val_loss: 24679.9648 - val_rmse: 24756.6738\n",
      "Epoch 85/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 16018.2959 - rmse: 16023.6504 - val_loss: 16256.2959 - val_rmse: 16180.1924\n",
      "Epoch 86/120\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 15201.1152 - rmse: 15148.7393 - val_loss: 15252.9492 - val_rmse: 15176.5586\n",
      "Epoch 87/120\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 15390.4131 - rmse: 15314.2725 - val_loss: 16771.2734 - val_rmse: 16728.8633\n",
      "Epoch 88/120\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 15074.2822 - rmse: 15073.0049 - val_loss: 16695.0625 - val_rmse: 16587.4355\n",
      "Epoch 89/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 16798.1055 - rmse: 16918.3438 - val_loss: 17537.4824 - val_rmse: 17567.3711\n",
      "Epoch 90/120\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 17027.9082 - rmse: 17076.6191 - val_loss: 17262.8164 - val_rmse: 17297.5273\n",
      "Epoch 91/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 14765.9463 - rmse: 14792.8623 - val_loss: 15938.6143 - val_rmse: 15928.6221\n",
      "Epoch 92/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 14922.5576 - rmse: 14977.6592 - val_loss: 15156.9248 - val_rmse: 15134.7559\n",
      "Epoch 93/120\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 14291.3037 - rmse: 14310.6123 - val_loss: 15069.5557 - val_rmse: 15015.1934\n",
      "Epoch 94/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 16609.2031 - rmse: 16562.2715 - val_loss: 18634.5898 - val_rmse: 18615.6855\n",
      "Epoch 95/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 15487.4346 - rmse: 15399.4424 - val_loss: 16916.4902 - val_rmse: 16915.2129\n",
      "Epoch 96/120\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 14452.4307 - rmse: 14437.4189 - val_loss: 15275.5938 - val_rmse: 15240.9326\n",
      "Epoch 97/120\n",
      "78/78 [==============================] - 1s 13ms/step - loss: 14879.3008 - rmse: 14876.1699 - val_loss: 19447.3672 - val_rmse: 19410.4570\n",
      "Epoch 98/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 15620.2949 - rmse: 15602.2676 - val_loss: 22636.5879 - val_rmse: 22547.5254\n",
      "Epoch 99/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 15696.7822 - rmse: 15683.8818 - val_loss: 15952.8516 - val_rmse: 15940.8418\n",
      "Epoch 100/120\n",
      "78/78 [==============================] - 1s 15ms/step - loss: 16336.8730 - rmse: 16374.8896 - val_loss: 16767.0566 - val_rmse: 16707.8672\n",
      "Epoch 101/120\n",
      "78/78 [==============================] - 1s 14ms/step - loss: 15446.8320 - rmse: 15458.9141 - val_loss: 23211.5332 - val_rmse: 23154.2949\n",
      "Epoch 102/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 15440.9170 - rmse: 15441.0527 - val_loss: 14621.2012 - val_rmse: 14562.6416\n",
      "Epoch 103/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 15943.3818 - rmse: 15939.3564 - val_loss: 21851.6133 - val_rmse: 21760.2070\n",
      "Epoch 104/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 14505.6035 - rmse: 14501.3867 - val_loss: 19926.3086 - val_rmse: 19875.7637\n",
      "Epoch 105/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 16336.1367 - rmse: 16289.1328 - val_loss: 23690.5527 - val_rmse: 23575.8770\n",
      "Epoch 106/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 15930.7852 - rmse: 15904.1367 - val_loss: 14457.0371 - val_rmse: 14363.8955\n",
      "Epoch 107/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 14077.5088 - rmse: 14073.0918 - val_loss: 17886.2109 - val_rmse: 17799.1562\n",
      "Epoch 108/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 14468.6338 - rmse: 14491.8994 - val_loss: 14480.7695 - val_rmse: 14404.4824\n",
      "Epoch 109/120\n",
      "78/78 [==============================] - 1s 13ms/step - loss: 14270.8594 - rmse: 14210.5371 - val_loss: 15770.3252 - val_rmse: 15716.0137\n",
      "Epoch 110/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 13895.8545 - rmse: 13935.6924 - val_loss: 14586.7451 - val_rmse: 14522.2842\n",
      "Epoch 111/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 14069.9561 - rmse: 14032.1172 - val_loss: 20867.0410 - val_rmse: 20774.2891\n",
      "Epoch 112/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 13292.8818 - rmse: 13295.6514 - val_loss: 14065.7646 - val_rmse: 13976.3164\n",
      "Epoch 113/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 13577.3486 - rmse: 13658.8154 - val_loss: 14832.1621 - val_rmse: 14762.5654\n",
      "Epoch 114/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 13965.9385 - rmse: 13942.0352 - val_loss: 16955.8379 - val_rmse: 16883.2617\n",
      "Epoch 115/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 15707.9199 - rmse: 15718.8643 - val_loss: 15006.0938 - val_rmse: 14920.3311\n",
      "Epoch 116/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 13516.2773 - rmse: 13519.7715 - val_loss: 15012.8711 - val_rmse: 14960.4717\n",
      "Epoch 117/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 14487.3008 - rmse: 14441.0791 - val_loss: 19109.9062 - val_rmse: 19037.4785\n",
      "Epoch 118/120\n",
      "78/78 [==============================] - 1s 13ms/step - loss: 13451.8975 - rmse: 13451.7666 - val_loss: 14813.0537 - val_rmse: 14784.1934\n",
      "Epoch 119/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 14451.5703 - rmse: 14429.3926 - val_loss: 24551.9004 - val_rmse: 24503.5254\n",
      "Epoch 120/120\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 15159.0078 - rmse: 15133.6572 - val_loss: 15621.2002 - val_rmse: 15553.7324\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_examples, train_labels, epochs=epochs, validation_data=(test_examples, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:06:27.211655: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/evantilu/miniforge3/envs/6998_DL_tf/lib/python3.8/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17a14c580>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAHiCAYAAAAH/hLvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABjwUlEQVR4nO29eZxcVZn//35q6arqfc3WnZAEQgJJICEh7AgEFBBBFDSMsggKIjMujDOK4wwuw7gMA375OTCj4oDIGBBlEQWFIILAAIEESEKWzgLprN2d7k7v3dV1fn+cc6tuVdfSne5OOrfO+/Wq1606de+tc2/d8znP85xNlFJYLJb8xHeoM2CxWA4dVgAsljzGCoDFksdYAbBY8hgrABZLHmMFwGLJY6wApCAiT4nI1aO976FERLaJyLljcN7nReSz5v2nRORPQ9n3AH5nmoh0iIj/QPNqSY8nBMA8HM4rJiLdrs+fGs65lFIXKKXuH+19xyMicouIvJAmvVpE+kRk3lDPpZR6UCn1wVHKV5JgKaXeV0oVK6UGRuP8aX5PRGSLiKwbi/OPZzwhAObhKFZKFQPvAx9xpT3o7CcigUOXy3HJA8CpIjIjJX0Z8I5Sas0hyNOh4ExgAjBTRE48mD98qJ9JTwhAJkTkLBFpEJGvichu4H9EpEJEnhSRRhFpMe/rXMe4zdprROSvInK72XeriFxwgPvOEJEXRKRdRJ4Vkf8UkV9myPdQ8vhdEXnJnO9PIlLt+v5KEXlPRJpF5J8y3R+lVAPwHHBlyldXAffnykdKnq8Rkb+6Pp8nIutFpE1EfgyI67sjReQ5k78mEXlQRMrNdw8A04DfGQvuH0Vkuogop7CIyBQReUJE9olIvYh8znXub4nIwyLyC3Nv1orI4kz3wHA18DjwB/PefV1zReQZ81t7ROQbJt0vIt8Qkc3md94QkampeTX7pj4nL4nInSKyD/hWtvthjpkqIr81/0OziPxYREImT/Nd+00Qbf3W5LjeOJ4WAMMkoBI4Argefc3/Yz5PA7qBH2c5/iRgA1AN/BC4V0TkAPb9X+A1oAr4FoMLnZuh5PFvgM+ga64C4KsAInIscI85/xTze2kLreF+d15EZDawAPjVEPMxCCNGvwG+ib4Xm4HT3LsA3zP5OwaYir4nKKWuJNmK+2Gan/gV0GCOvwz4NxFZ6vr+YmA5UA48kS3PIlJozvGgeS0TkQLzXQnwLPC0+a2jgBXm0JuBK4ALgVLgWqAr231xcRKwBf3f3UaW+yE67vEk8B4wHagFliules01ftp13iuAZ5VSjUPMByilPPUCtgHnmvdnAX1AOMv+C4AW1+fngc+a99cA9a7vCgEFTBrOvujCEwUKXd//EvjlEK8pXR6/6fr8BeBp8/5fzAPifFdk7sG5Gc5dCOwHTjWfbwMeP8B79Vfz/irg/1z7CbrAfjbDeT8KrEr3H5rP0829DKALxwBQ4vr+e8B95v230IXA+e5YoDvLvf000GjOHQJagUvNd1e485Vy3AbgkjTp8bxmuU/v5/i/4/cDOMXJX5r9TgK2Az7zeSXwieGUl3ywABqVUj3OBxEpFJH/NibyfuAFoFwyR5h3O2+UUo7CFw9z3ynAPlca6D8uLUPM427X+y5Xnqa4z62U6gSaM/2WydOvgauMtfIptFVwIPfKITUPyv3ZmKrLRWSHOe8v0ZbCUHDuZbsr7T10zeiQem/CktnXvhp4WCkVVbpW/S0JN2Aq2npJR7bvcpH03+e4H1OB95RS0dSTKKVeBTqBD4jIHLSF8sRwMpIPApA63PHvgdnASUqpUnQACFw+6hiwC6g05qbD1Cz7jySPu9znNr9ZleOY+4FPAOcBJWiTcyT5SM2DkHy930P/L8eZ83465ZzZhqjuRN/LElfaNGBHjjwNwsQzzgE+LSK7RceJLgMuNG7MduDIDIdn+q7TbN3/9aSUfVKvL9v92A5MyyJg95v9rwQecVd2QyEfBCCVErQv2yoilcCtY/2DSqn30ObZt0SkQEROAT4yRnl8BLhIRE43vux3yP0/v4g2fX+Cdh/6RpiP3wNzReRj5sH9IsmFoAToMOetBf4h5fg9wMx0J1ZKbQdeBr4nImEROQ64Du2/D5crgY1okVtgXkej3ZUr0EI4SUS+bIJuJSJykjn2Z8B3RWSWaI4TkSql/e8daFHxi8i1ZBYRh2z34zW0oH5fRIrMNbvjKQ8Al6JF4BfDvQH5KAA/AiJAE/B/6ADPweBTaH+uGfhX4CGgN8O+P+IA86iUWgvchA467gJa0A90tmMU+uE5guSH6IDyoZRqAi4Hvo++3lnAS65dvg2cALShxeK3Kaf4HvBNEWkVka+m+Ykr0L72TuBR4Fal1DNDyVsKVwN3K6V2u1/AfwFXGzfjPLRY7wY2AWebY+8AHgb+hI6h3Iu+VwCfQxfiZmAuWrCykfF+KN334SNo8/599H/5Sdf3DcCbaAvixeHeADHBA8tBRkQeAtYrpcbcArF4GxH5ObBTKfXNYR9rBeDgILqDyT5gK/BB4DHgFKXUqkOZL8vhjYhMB1YDC5VSW4d7fD66AIeKSejmoA7gLuBGW/gtI0FEvgusAf79QAo/WAvAYslrrAVgseQxVgAsljzmsB0dV11draZPn36os2GxjHveeOONJqVU2gFCh60ATJ8+nZUrVx7qbFgs4x4ReS/Td9YFsFjyGCsAFkseYwXAYsljDtsYgGXk9Pf309DQQE/PsAaQWcYp4XCYuro6gsHgkI+xApDHNDQ0UFJSwvTp08k8yZHlcEApRXNzMw0NDcyYkTrFY2asC5DH9PT0UFVVZQu/BxARqqqqhm3NWQHIc2zh9w4H8l9aAbAcMpqbm1mwYAELFixg0qRJ1NbWxj/39fVlPXblypV88YtfzPkbp5566qjk9fnnn6esrIyFCxcyZ84cvvrVxDQF9913HyLCihUr4mmPPvooIsIjjzwCwJNPPsnChQs5/vjjOfbYY/nv//5vAL71rW8lXfeCBQtobW0dlTwPBRsDsBwyqqqqWL16NaALQnFxcVLBikajBALpH9HFixezeHGu2b7h5ZdzzcUxdM444wyefPJJuru7WbhwIZdeeimnnaYn55k/fz6/+tWvWLpUT068fPlyjj/+eEAHW6+//npee+016urq6O3tZdu2bfHzfuUrX0m67oOJtQAs44prrrmGm2++mbPPPpuvfe1rvPbaa5x66qksXLiQU089lQ0bNgC6Rr7ooosALR7XXnstZ511FjNnzuSuu+6Kn6+4uDi+/1lnncVll13GnDlz+NSnPuXMrMsf/vAH5syZw+mnn84Xv/jF+HkzEYlEWLBgATt2JKYhPOOMM3jttdfo7++no6OD+vp6FixYAEB7ezvRaJSqKj01YygUYvbs2aNzw0aItQAsAHz7d2tZt3P/qJ7z2Cml3PqRucM+buPGjTz77LP4/X7279/PCy+8QCAQ4Nlnn+Ub3/gGv/nNbwYds379ev785z/T3t7O7NmzufHGGwc1h61atYq1a9cyZcoUTjvtNF566SUWL17MDTfcwAsvvMCMGTO44oorcuavpaWFTZs2ceaZZ8bTRIRzzz2XP/7xj7S1tXHxxRezdaseol9ZWcnFF1/MEUccwdKlS7nooou44oor8Pl0/XvnnXfyy1/qNWIqKir485//POx7dqBYC8Ay7rj88svx+/XM421tbVx++eXMmzePr3zlK6xduzbtMR/+8IcJhUJUV1czYcIE9uzZM2ifJUuWUFdXh8/nY8GCBWzbto3169czc+bMeNNZNgF48cUXOe6445g0aRIXXXQRkyYlT/a7bNkyli9fzvLlywed52c/+xkrVqxgyZIl3H777Vx77bXx777yla+wevVqVq9efVALP1gLwGI4kJp6rCgqKoq//+d//mfOPvtsHn30UbZt28ZZZ52V9phQKBR/7/f7iUYHTaOfdp/hTIjjxAA2btzI6aefzqWXXho380ELzJo1a4hEIhx99NGDjp8/fz7z58/nyiuvZMaMGdx3331D/u2xwloAlnFNW1sbtbV6zY+xKDBz5sxhy5Yt8aDcQw89lPOYo48+mltuuYUf/OAHg7773ve+x7/9278lpXV0dPD888/HP69evZojjjhiRPkeLawFYBnX/OM//iNXX301d9xxB+ecc86onz8SiXD33Xdz/vnnU11dzZIlS4Z03Oc//3luv/32uJ/vcMEFFwzaVynFD3/4Q2644QYikQhFRUVJYuaOAQA89thjHKy5Lg7bOQEXL16s7HwAI+Pdd9/lmGOOOdTZOOR0dHRQXFyMUoqbbrqJWbNm8ZWvfOVQZ+uASPefisgbSqm0baaedQF6+gdo6+oflo9nyU9++tOfsmDBAubOnUtbWxs33HDDoc7SQcOzAnD/y9s4/jt/oqtv4FBnxTLOcaLw69at48EHH6SwsDD3QR7BswLgM/2ibf1vsWTGswLgjIuIWRfAYsmIZwUgbgHEDnFGLJZxjGcFwFoAFktuPCsANgYw/jnrrLP44x//mJT2ox/9iC984QtZj3Gafy+88MK0Q2e/9a1vcfvtt2f97ccee4x169bFP//Lv/wLzz777DByn57DbdiwhwVAb60FMH654oorWL58eVJaun70mfjDH/5AeXn5Af12qgB85zvf4dxzzz2gc6VyxhlnsGrVKlatWsWTTz7JSy+9FP/OGTbskG7Y8O9+9zveeustVq1aldT12T1mYPXq1Qd87W48KwDO7ChWAMYvl112GU8++SS9vb0AbNu2jZ07d3L66adz4403snjxYubOncutt96a9vjp06fT1NQEwG233cbs2bM599xz40OGQbfxn3jiiRx//PF8/OMfp6uri5dffpknnniCf/iHf2DBggVs3ryZa665Jl4Lr1ixgoULFzJ//nyuvfbaeP6mT5/OrbfeygknnMD8+fNZv3591us7HIYNe7YrsBMDsOV/iDz1ddj9zuiec9J8uOD7Gb+uqqpiyZIlPP3001xyySUsX76cT37yk4gIt912G5WVlQwMDLB06VLefvttjjvuuLTneeONN1i+fDmrVq0iGo1ywgknsGjRIgA+9rGP8bnPfQ6Ab37zm9x777383d/9HRdffDEXXXQRl112WdK5enp6uOaaa1ixYgVHH300V111Fffccw9f/vKXAaiurubNN9/k7rvv5vbbb+dnP/tZxus7HIYNe9YCiMcArACMa9xugNv8f/jhhznhhBNYuHAha9euTTLXU3nxxRe59NJLKSwspLS0lIsvvjj+3Zo1azjjjDOYP38+Dz74YMbhxA4bNmxgxowZ8dF8V199NS+88EL8+4997GMALFq0KGlWn9T8HC7Dhj1rAdgYwDDJUlOPJR/96Ee5+eabefPNN+nu7uaEE05g69at3H777bz++utUVFRwzTXX5JztNtOEmNdccw2PPfYYxx9/PPfdd1/SqLx05Oo67gwpzjTkGA6vYcNDsgBE5CsislZE1ojIr0QkLCKVIvKMiGwy2wrX/reISL2IbBCRD7nSF4nIO+a7u8T8ayISEpGHTPqrIjJ9pBdmYwCHB8XFxZx11llce+218dpw//79FBUVUVZWxp49e3jqqaeynuPMM8/k0Ucfpbu7m/b2dn73u9/Fv2tvb2fy5Mn09/fz4IMPxtNLSkpob28fdK45c+awbds26uvrAXjggQf4wAc+cEDXdjgMG84pACJSC3wRWKyUmgf4gWXA14EVSqlZwArzGRE51nw/FzgfuFtE/OZ09wDXA7PM63yTfh3QopQ6CrgTGHzHholTH9jyP/654ooreOutt1i2bBkAxx9/PAsXLmTu3Llce+218Yk3M3HCCSfwyU9+kgULFvDxj3+cM844I/7dd7/7XU466STOO+885syZE09ftmwZ//7v/87ChQvZvHlzPD0cDvM///M/XH755cyfPx+fz8fnP//5A762z3/+87zwwgtphw2fffbZSWnOsOHZs2ezYMECbr311kHDht3NgJlckGGhlMr6AmqB7UAl2mV4EvggsAGYbPaZDGww728BbnEd/0fgFLPPelf6FcB/u/cx7wNAE2aocqbXokWLVDYeWbldHfG1J9V7TZ1Z98tn1q1bd6izYBll0v2nwEqVoRzltACUUjuA24H3gV1Am1LqT8BEpdQus88uYII5xBEMhwaTVmvep6YnHaOUigJtQFWuvGXDBE6tC2CxZGEoLkAFcAkwA5gCFInIp7MdkiZNZUnPdkxqXq4XkZUisrKxsTFrvn02BmCx5GQoQcBzga1KqUalVD/wW+BUYI+ITAYw271m/wZgquv4OmCnSa9Lk550jIgEgDJgX2pGlFI/UUotVkotrqmpGdIFxmz5t1gyMhQBeB84WUQKTdR+KfAu8ARwtdnnauBx8/4JYJmJ7M9AB/teM25Cu4icbM5zVcoxzrkuA54zvsuBX1i8WcgqQDZGeJst44gD+S9z9gNQSr0qIo8AbwJRYBXwE6AYeFhErkOLxOVm/7Ui8jCwzux/k1LKmZbnRuA+IAI8ZV4A9wIPiEg9uuZfNuwrSSHhAoz0TN4lHA7T3NxsVwj2AMosDx4Oh4d13JA6AimlbgVSO2T3oq2BdPvfBtyWJn0lMC9Neg9GQEYL2xEoN3V1dTQ0NJArnmI5PAiHw9TV1eXe0YVnewLG5wOwE4JkJBgMxlfEseQnnh0LYHsCWiy58awA+KxPa7HkxMMCoLfWArBYMuNhAbCtABZLLjwrAFgLwGLJiWcFwE4IYrHkxsMCoLe2p5vFkhkPC4CNAVgsufCsADiNgDYGYLFkxrsCYGMAFktOPCsANgZgseTGuwLgszEAiyUXnhUAGwOwWHLjXQGwi4NaLDnxrADYsQAWS248LABOK4AVAIslE54VADshiMWSG88KgM/GACyWnHhWAMTGACyWnHhWAGwMwGLJjWcFIGEBHNp8WCzjGc8KgJ0PwGLJjYcFQG9tDMBiyYxnBcBOC26x5Ma7AmC2tvxbLJnxrAAk+gFYBbBYMuF5AbA9AS2WzHhWAGxHIIslN54XAFv+LZbMeFYAbAzAYsmN5wXA9gS0WDLjYQHQWxsDsFgy41kBwI4FsFhy4lkB8NkooMWSE88LgLUALJbMeFgA9NbGACyWzHhWAARrAVgsufCuAJgrszMCWSyZ8awA2AlBLJbceFgA9NbGACyWzHhWAGwMwGLJjXcFwFoAFktOPCsA8Y5AFoslIx4WAL2NWR/AYsmIhwXAxgAsllx4VgBsDMBiyY2HBcAuDmqx5CKnAIjIbBFZ7XrtF5Evi0iliDwjIpvMtsJ1zC0iUi8iG0TkQ670RSLyjvnuLjGlVERCIvKQSX9VRKaPysWJ7QlosWQjpwAopTYopRYopRYAi4Au4FHg68AKpdQsYIX5jIgcCywD5gLnA3eLiN+c7h7gemCWeZ1v0q8DWpRSRwF3Aj8YlYsTsS6AxZKF4boAS4HNSqn3gEuA+036/cBHzftLgOVKqV6l1FagHlgiIpOBUqXUK0pXy79IOcY51yPAUsc6GAkiNghosWRjuAKwDPiVeT9RKbULwGwnmPRaYLvrmAaTVmvep6YnHaOUigJtQNUw8zYIEbFjASyWLAxZAESkALgY+HWuXdOkqSzp2Y5JzcP1IrJSRFY2NjbmyIaNAVgsuRiOBXAB8KZSao/5vMeY9ZjtXpPeAEx1HVcH7DTpdWnSk44RkQBQBuxLzYBS6idKqcVKqcU1NTU5M2xjABZLdoYjAFeQMP8BngCuNu+vBh53pS8zkf0Z6GDfa8ZNaBeRk41/f1XKMc65LgOeU6NQdQs2BmCxZCMwlJ1EpBA4D7jBlfx94GERuQ54H7gcQCm1VkQeBtYBUeAmpdSAOeZG4D4gAjxlXgD3Ag+ISD265l82gmuK47MxAIslK0MSAKVUFylBOaVUM7pVIN3+twG3pUlfCcxLk96DEZDRRLcCWAWwWDLh2Z6AAD6f2CCgxZIFTwuAjQFYLNnxtAD4ROzioBZLFjwtACJiLQCLJQueFgDbEchiyY6nBUAEYrFDnQuLZfziaQGwMQCLJTueFwAbA7BYMuNpAbAdgSyW7HheAGz5t1gy42kB0GMBrAJYLJnwvADYGIDFkhlPC4CNAVgs2fG2AGBjABZLNjwtALYfgMWSHc8LgO0JaLFkxtMCYGMAFkt2PC4AthXAYsmGpwVALxFuFcBiyYTHBcBaABZLNjwuADYGYLFkw9MCgLUALJaseFoA7IxAFkt2PC4AdmEQiyUbHhcAGwOwWLLhaQEQ7OKgFks2vC0AYhcGsViy4WkB8InYfkAWSxa8LQA+GwOwWLLhaQGwMQCLJTveFgAbA7BYsuJpAdATglgslkx4XABsT0CLJRseFwAbA7BYsuFpAbCLg1os2fG4ANgYgMWSDU8LgI0BWCzZ8bgA2BiAxZINTwuA7QdgsWTH4wJgFwe1WLLhaQGwE4JYLNnxuADYwUAWSzY8LQCCjQFYLNnwtADYxUEtlux4WgDELg5qsWTF0wJgOwJZLNnxtADYfgAWS3Y8LQA2BmCxZGdIAiAi5SLyiIisF5F3ReQUEakUkWdEZJPZVrj2v0VE6kVkg4h8yJW+SETeMd/dJSJi0kMi8pBJf1VEpo/GxdnlwS2W7AzVAvh/wNNKqTnA8cC7wNeBFUqpWcAK8xkRORZYBswFzgfuFhG/Oc89wPXALPM636RfB7QopY4C7gR+MMLrAmwMwGLJRU4BEJFS4EzgXgClVJ9SqhW4BLjf7HY/8FHz/hJguVKqVym1FagHlojIZKBUKfWK0qXyFynHOOd6BFjqWAcjwcYALJbsDMUCmAk0Av8jIqtE5GciUgRMVErtAjDbCWb/WmC76/gGk1Zr3qemJx2jlIoCbUDVAV2RC58dC2CxZGUoAhAATgDuUUotBDox5n4G0tXcKkt6tmOSTyxyvYisFJGVjY2N2XONMxw4524WS94yFAFoABqUUq+az4+gBWGPMesx272u/ae6jq8Ddpr0ujTpSceISAAoA/alZkQp9ROl1GKl1OKampqcGRc7FsBiyUpOAVBK7Qa2i8hsk7QUWAc8AVxt0q4GHjfvnwCWmcj+DHSw7zXjJrSLyMnGv78q5RjnXJcBz6lRsN0FOxrQYslGYIj7/R3woIgUAFuAz6DF42ERuQ54H7gcQCm1VkQeRotEFLhJKTVgznMjcB8QAZ4yL9ABxgdEpB5d8y8b4XUBthXAYsnFkARAKbUaWJzmq6UZ9r8NuC1N+kpgXpr0HoyAjCY+n40BWCzZ8HRPQBsDsFiy420BsDEAiyUrnhYAn2DHAlgsWfC4ANgYgMWSDY8LgI0BWCzZ8LQAYGYFtk2BFkt6PC0APtPB2JZ/iyU9HhcArQC2/Fss6fG4AOitjQNYLOnxtAA4UwpYAbBY0uNxAdBbW/4tlvR4WgDiMQArABZLWjwuAHprXQCLJT2eFgDBxgAslmx4WwDiFsChzYfFMl7xtAD44lHAQ5sPi2W84nEB0FvrAlgs6fG2APhsDMBiyYanBcCZa9zGACyW9HhbAOJjAawCWCzp8LQA2I5AFkt2PC4AemtjABZLejwtALYfgMWSHY8LgOMCWAWwWNLhaQGwMQCLJTseFwC9tTEAiyU9nhYAGwOwWLLjaQHw2RiAxZIVTwtAYkqwQ5wRi2Wc4mkBSEwLbhXAYkmHpwUgMSHIIc6IxTJO8bQAxC0AOxbAYkmLpwUgHgOIHeKMWCzjFE8LgO0HYLFkx9MCILYnoMWSFU8LgI0BWCzZ8bgA2FYAiyUbnhYAsTEAiyUrHhcA2xXYYsmGpwXAZxcHtViy4nEBsDEAiyUbnhYAGwOwWLLjbQGwi4NaLFnxtADYGIDFkh1vC4DP9gS0WLLhbQGwMQCLJSueFgBsDMBiyYqnBWBYMYBoH9x1Amx4akzzZLGMJ4YkACKyTUTeEZHVIrLSpFWKyDMisslsK1z73yIi9SKyQUQ+5EpfZM5TLyJ3iemqJyIhEXnIpL8qItNH5eKGszhoTxvs2wxNG0fjpy2Ww4LhWABnK6UWKKUWm89fB1YopWYBK8xnRORYYBkwFzgfuFtE/OaYe4DrgVnmdb5Jvw5oUUodBdwJ/ODALymBbzgTgkS79VbZ2UMs+cNIXIBLgPvN+/uBj7rSlyulepVSW4F6YImITAZKlVKvKN05/xcpxzjnegRY6lgHI2FYHYH6jQDEBkb6sxbLYcNQBUABfxKRN0TkepM2USm1C8BsJ5j0WmC769gGk1Zr3qemJx2jlIoCbUDV8C5lMMNaGKS/S2+tBWDJIwJD3O80pdROEZkAPCMi67Psm67mVlnSsx2TfGItPtcDTJs2LXuOSbgAaU41mH7rAljyjyFZAEqpnWa7F3gUWALsMWY9ZrvX7N4ATHUdXgfsNOl1adKTjhGRAFAG7EuTj58opRYrpRbX1NTkvrjhDAZyLADrAljyiJwCICJFIlLivAc+CKwBngCuNrtdDTxu3j8BLDOR/RnoYN9rxk1oF5GTjX9/VcoxzrkuA55TozCI/4BiANYCsOQRQ3EBJgKPmphcAPhfpdTTIvI68LCIXAe8D1wOoJRaKyIPA+uAKHCTUsqpVm8E7gMiwFPmBXAv8ICI1KNr/mWjcG2unoBD2DkuANYCsOQPOQVAKbUFOD5NejOwNMMxtwG3pUlfCcxLk96DEZDRZFgzAtkgoCUP8XhPwGEMBurv0VsbA7DkER4XAL0dWgzAsQDsuAFL/uBpARjW4qA2BmDJQ7wtAMNZHtw2A1ryEE8LwLAmBLHNgJY8xNsCcED9AKwFYMkfPC0Aw4sB2GZAS/7haQEY1uKgUacZ0AqAJX/wtADIgYwFsBaAJY/wtAD4htUKYGMAlvzD0wIQtwCGYgLYCUEseYinBSARAxgC1gWw5CGeFoDhxQCsC2DJPzwtAMOLAVgLwJJ/eFoAEhbAcGIAVgAs+YOnBWDIC4PEBmCgz+xsBcCSP3hcAIYYA3Bqf7AxAEte4WkBGPKcgG4BsM2AljzC2wLAEKcEcwKAYF0AS17haQEYcgzAugCWPMXjAjDUGIDbArBTglnyB08LgI0BWCzZ8bgADDEG4KwMHIjYGIAlr/C0AICOAwy5GTBUbGMAlrwiDwRAck8I4ghAQbG1ACx5RV4IwJCDgAXFNgZgySs8LwDIMIKA1gWw5BmeFwCfDKUfgGMBFNlmQEtekQcCIEPoCdgNCATC1gWw5BV5IQBDagUIFoLPb4OAlrzC8wIgDDEGEAyD+G0MwJJXeF8AhhIDGOgFf8haAJa8w/MC4PMNIQYQi+nCLz4bA7DkFd4XgKHEANSALvxiLQBLfuF5ARhSDCA2kLAArABY8gjPC4DfJ/RFcxRqNaBrf591ASz5hecFYGplIe/v68q+UywKvoC1ACx5h+cF4MiaIjY3dmTfKR4EtM2AlvwiDwSgmKaOPlq7+jLv5AQBbTOgJc/wvAAcNaEYgM2NnZl3cgcB7cIgljzC8wJwZI0RgL1Z3AAnCGhdAEue4XkBmFpZSIHflz0OELcAxLoAlrzC8wLg9wkzqouoz2oBxEwzoN82A1ryCs8LAMCRE3K0BMSitiOQJS/JCwE4qqaY9/d10dOfoXaPuwA2BmDJL/JCAGbUFBFT0NCSoUNQvCegbQa05Bd5IQATSsIANHVk6AvgbgYE2xRoyRvyQgCqigsAaM4kAO5mQLBWgCVvyA8BKAoB0NzZm36HeFdgZzVRGwew5AdDFgAR8YvIKhF50nyuFJFnRGST2Va49r1FROpFZIOIfMiVvkhE3jHf3SVm7S4RCYnIQyb9VRGZPorXSEVhEJEsLoAyLoDPWAC2KdCSJwzHAvgS8K7r89eBFUqpWcAK8xkRORZYBswFzgfuFnFsa+4Brgdmmdf5Jv06oEUpdRRwJ/CDA7qaDAT8PioKC2juyGQBRI0LYG6HdQEsecKQBEBE6oAPAz9zJV8C3G/e3w981JW+XCnVq5TaCtQDS0RkMlCqlHpF6Tm6fpFyjHOuR4CljnUwWlQVFWSOAbibAcG6AJa8YagWwI+AfwTcVeNEpdQuALOdYNJrge2u/RpMWq15n5qedIxSKgq0AVWpmRCR60VkpYisbGxsHGLWNdXFIZoyWQDxIKC1ACz5RU4BEJGLgL1KqTeGeM50NbfKkp7tmOQEpX6ilFqslFpcU1MzxOxoqooLaO7MZAHEUmIAVgAs+UFgCPucBlwsIhcCYaBURH4J7BGRyUqpXca832v2bwCmuo6vA3aa9Lo06e5jGkQkAJQB+w7wmtKS2wLwWQvAknfktACUUrcopeqUUtPRwb3nlFKfBp4Arja7XQ08bt4/ASwzkf0Z6GDfa8ZNaBeRk41/f1XKMc65LjO/MfJF+pSKLwpQVVRAe0+U3mga/z61I5CNAVjyhJH0A/g+cJ6IbALOM59RSq0FHgbWAU8DNykVL1E3ogOJ9cBm4CmTfi9QJSL1wM2YFoUR8fKP4TuVEO0BoKpY9wXYl84NcOYEtM2AljxjKC5AHKXU88Dz5n0zsDTDfrcBt6VJXwnMS5PeA1w+nLzkxB/Upnx/NwQjSb0BJ5dFUjJgg4CW/MS7PQGDhXrbp6cCqzYCkDYO4J4UFKwLYMkbPCwAppbv7wZ0EBAyjAewQUBLnuJdASgo0tt+bQE4MYD0FkBqV2ArAJb8wLsCkGIBFBX4CQV86fsC2BiAJU/xsACYGEC/ngRERDL3BbDNgJY8xfsC0JeYBagy3XiAWAxQthnQkpd4WACSXQCA8sIgbd39yfs5tb11ASx5iHcFICUICFBeWDBYAJza3uezzYCWvMO7ApDOAogEB68RaC0ASx7jYQEYHANwXIBYzDXMIG4B2GZAS/7hXQHwB8EXjLcCAJRFgsQUtPdGE/tZC8CSx3hXAEBbAUlBQN0duK3LFQdwantfwDYDWvIOjwtAJDkIGAkC0NrtigPEjDXg8yVcAGsBWPIEbwtAQaoFoAWgxW0BpHMBbD8AS57gbQEIFg4KAgLJLQHuIKBtBrTkGd4XgKQgoIkBdOewAKwLYMkTPC4AkSQXoMyJASQFAW0zoCV/8bgAFCYFAQsCPooK/MkC4NT24l4azAqAJT/wtgCkBAFBNwWmbwWwMQBL/uFtAQhGkoKAoN2AtlwugLUALHmCxwWgKCkICLoloDVXENA2A1ryBI8LQCS9ANhmQIsF8LwAFMJAHwwk+v4PGhJszP3NTd2uZsCRr0lisRwOeFsACsyIwGjqkOB+4gsPGQvgtqc30twVTUo7LFHq8M6/5aDibQFw5gRI6Q0YjSk6+0whMeZ+7wD86nWzePHhHARc+yjcfjREMyyEarG48LgAOLMCuQTA9AaMxwFMM+AAfh59a5dOO5xjAK3vQVfToNiHxZIOjwuAMyuQqztwYUpvQGMuV5VE6Or3QEegmAfcGMtBw+MC4EwNnogBVBZpC6DRmR7c1PZHVJcQCHhgVmAn77H+7PtZLHhdAAqS1wYAqKvQVsGOFiMKpt9/uKCAypKwTjucXYC4AESz72ex4HUBSBMEnFgSJugXtreYNFPYwwVBKoscATiMmwHjLoAVAEtuPC4Ag4OAPp9QWx6hYZ+2AJQpKJFQARXFRgAOaxfAxgAsQ8fjAjA4CAgwtbKQBmMB9PZpXzkcKqDSEYDDOghoXQDL0PG4AAwOAgLUVRSy3cQAevt0c2A4VEBVsd4/OnAYFx7rAliGgbcFIE0QEHQgcF9nH529UXqMBVAYKqCyRFsMXb2HcScaJ4A5YFsBLLnxtgAEBgcBQbsAAA0t3fT26oISCRVQbQSgs+cwFgAbA7AMA28LgM+nRSA1BmCaAhtauujt14W9MFRAdekYC0B/D7z4H4MEaVSxLoBlGHhbACDtkOC6Cm0BbN/XRW+/cQEiBdQYAegaKwHY/iqs+A6seWRszg82CGgZFt4XgKJq6NiTlFRdXEA46GN7Szd9rhhAlXEBuscqBhA1vQ83PD025wdrAViGhfcFoPwIaH0/KUlEqKvQTYFxAQiHCAUDAHT1jlEAbcAIwJY/D2qZGDXiAmCDgJbc5IEATIOW9wYlT62IsH1fN31RXVCKIiEABvDRM2YCYCyL/i7Y+sLY/EbcBbBBQEtuvC8AFUdATyv0tCUl11UUsqO1m34TAwgF9SjBGH66+8ZIANxj9Dc8NTa/YWMAlmHgfQEon6a3KW5AbUWEtu5+Opwpws18gEqEnr4xigE4LkDNMbDjjbH5DRsDsAyDPBCAI/Q2RQCcUYFN7aaFwJkSXPz09LmmDBtNHAugsCoREBxtrABYhoH3BaBiut6mxAFqy7UAtHb06ARnRmDxoWIxuvvHwId2YgChkoQ1MNrYjkCWYeB9AYhUQEFxWhcAQMWnBdctAIgPH7Hk5cNGC6fQh0rGbs4+ZyCT7QpsGQLeFwAR0xSYbAHUFIcIBXwExLUuAIBPC0BL1xgUUKfQh4oPggVgXQBLbrwvAKADgWn6AtSWR/ARI4bEFwYVXwD/mFkAfdrSCITHzgKwAmAZBvkhABVH6BiAO7D39C38a+xH+ImhXLdBxIcPNXYC4A+Bv8DGACzjgpwCICJhEXlNRN4SkbUi8m2TXikiz4jIJrOtcB1zi4jUi8gGEfmQK32RiLxjvrtLRFe7IhISkYdM+qsiMn1Ur7J8GvS1Q3dLIm3PGmbE3sOPQjkBQEB8/jF0AXohUKAtgIG++HyEo4q1ACzDYCgWQC9wjlLqeGABcL6InAx8HVihlJoFrDCfEZFjgWXAXOB84G6ReAm7B7gemGVe55v064AWpdRRwJ3AD0Z+aS5KJuute0xAfzcRieIjhpLEbfD5/cYCGAMBGOjVFkCgwHweg99wRMUKgGUI5BQApekwH4PmpYBLgPtN+v3AR837S4DlSqlepdRWoB5YIiKTgVKl1CtKN7L/IuUY51yPAEsd62BUCJfqbc/+RFpfFwX0aRfAbQGInwLfWLkA/dr894fM5zFwA+xYAMswGFIMQET8IrIa2As8o5R6FZiolNoFYLYTzO61wHbX4Q0mrda8T01POkYpFQXagKoDuJ70hMr0trc9kdbfRVD142cA5UsIAD4f4YDQMhYCEHcBjACMRSDQxgAsw2BIAqCUGlBKLQDq0LX5vCy7p6u5VZb0bMckn1jkehFZKSIrGxsbc+TaRahEb3td4wH6uwioPqaWhwj4A64f8RH2M0YugCsICGNsAVgXwJKbYbUCKKVagefRvvseY9ZjtnvNbg3AVNdhdcBOk16XJj3pGBEJAGXAvjS//xOl1GKl1OKampqhZzydC9DfjUR7WTq7Cn+SAPgJBRSt3QfDAhgLAbCDgSxDZyitADUiUm7eR4BzgfXAE8DVZrergcfN+yeAZSayPwMd7HvNuAntInKy8e+vSjnGOddlwHNqNDvjh4wAOC6AUtDXaSLx0UQnIADxEfLL2LQCDPSZGMBYBgGtBWAZOoHcuzAZuN9E8n3Aw0qpJ0XkFeBhEbkOeB+4HEAptVZEHgbWAVHgJqXia23dCNwHRICnzAvgXuABEalH1/zLRuPi4hQUgfig11gAA/2J2XP7uxPjAAB8fkJ+RWvHWPUDMM2AANGe0f8NZecDsAydnAKglHobWJgmvRlYmuGY24Db0qSvBAbFD5RSPRgBGRNEdBzAcQH6OxPf9XXpyUPj+/op8OkYQCym8PlGrzGCaC8UFiWaAccyCGjHAliGQH70BATtBjgugHs6rv7OZAtAhJAfYgrae0fZjD4ozYA2BmAZOnkmAMYCcE/L3deVGAkI4PMTNHdl1FsCBnqNC3AwmgGtAFhykz8CEC5NTAvmnia8v2tQELDAp+OPo94XINqrC/9BaQa0MQBLbvJHAEIlLhfAbQGkugB+gj7dnXb0LYC+FAvAugCWQ0seCYDbBXAFAfu7U4KAPgIm7jfq3YHHuhlQKVcrgA0CWnIzlGZAb5BkAbiDgF2DmgGDOC7AKBfQaJ+u/eMWwCg3A7rNfmsBWIZA/lgA4VJXM2CKC5ASAwiIFoDmjrEKAjr9AEb5/O5Cb2MAliGQPwIQKtUFMNqbslagSm4FEB+iYsyvLePev27lpfqm0fl9pbTJP5ZBwCQBsBaAJTf5JQCg3YDU1XlTXABUjJ9fcyLTKgu59r7Xqd/bzohxOub4g2MXBFTWBbAMj/wRgPiAoLZBqwWnugDEBqgpCfHLz55EOOjnG79dQyw2wqEJTm2fNBpwtF0AKwCW4ZE/AhAfErx/sABIcldgpyatKQnxTx8+hte27ePXb2xnRDj+fiCkuyb7C0bfAnAX+gErAJbc5JEAuFyA1JV5Uy0AlZir7/JFdcysKeLpNbtH9vtObe/U/v7QGFgANgZgGR75IwDuOQH6OqGgJPFdagzANVmniHBkTTE7Wke4nHfcBTACECgYg2ZAKwCW4ZE/AhB3AYwFEKlIfJfFAgC9juCOlu6RrRfodgFgbNYGsAJgGSZ5JADOvIAmBhAqSdT8Kc2ASdF09FLinX0DI+sZmGoBjMXaAO5pxm0/AMsQyCMBMBZAjxGAgsJEhxx3ENA0A7pxVhJuaBmBGzCQagGExjYIOBILINoHO1eNPD+WcU/+CICzIEfvft0PIFiYmJgjTTOgm4QApLQeGDbsbqd/IMciH4657w+abcHYBQF9wZGNBVj7KPz0HOhsHp18WcYt+SMAkBgQ1O8IgGMBJI8GHGQBlBcC2gJ4fPUOnnx7Z/y7po5eLrzrRR5btSP7b7v7AcDYWgCB8MgsgJ5WfQ/6RqEDlGVckz+DgSAxLVjcBTCFMUcQsDQSoCQUoKGli5+8uIUJJSEuOm4KALtaexiIKbY2dZIVpydgIMTv3trJkk7FRP8YdQQKhEYWA3AsEzutmOfJLwEoqoaOvboVIBhJbwH4BrsAIkJtRYQV6/fS2N6b1CuwqVPX4jtzNRNGE0HA5a+/z+T9A0wsHOVmQCd4OVILwBGAsZivwDKuyC8XoGY2NL6r+wEEixIR+ZT5AFItANAtAU4QsLmzj55+Xdia2o0AtOUozK5WgPaeKF0x/9g1AwZCIxQAU/OPxbTllnFFngnAMdDVrH1ctwXgS14YJLUZEBKBQIfdpsA3d+pCsqvNWADv/g7272QQ8X4ABezv7qdjIDB2owED4ZGZ707NbwXA8+SXAEw4JvG+oCgRA0gzGjAVRwDm1+r+BDtNgXcsgN1tPcSi/fDwVfDmLwb/drwrcIj2nig9KkBszIKAoxUDsALgdfJXAIKRzEHANIVnZk0RAJ86aRqgg3+QsAD6BxTNrftN9DxNQNAUJuUP0t4TpU8FUP2jLQCjFQMw1sNYzFp8MOlshu6WQ52LcU1+CUDxxEQX4KzNgIO7/J519AQe+fwpXLJAL2i8e78WgKaORCHeva9Vv0lXs5u0XhWkbyBGH0HUmLkAI40BeMQFeOQz8PuvHupcjGvySwBEdBwAjABkagYcbAH4fMLi6ZVECvxUFAbjUf+mjj6mlGkhaWpp1TunG+RjCtP+qJ5xtI8AMupBwFG2AA53AejYC517c+93uLH5OfjDP47KqfJLACDhBmTsCpzeBXAzqSzCrraEBTC/TscFmlvNnIPpLABTmNr79W/1EURiY9UKUACo5LEBw8ErMQBnCjivsekZWPnzUTlV/gpAUjNg6mCg7AVnSlmYna3dxGKKfZ19HDWhmHDQx742s/BIOgsg2gu+APt7tLj0EcAX6z/wQpoOdysADO4OPNTf8ooARHvHZgHWQ020V/+3ozDgK/8EYOpJ2s8vn+pqBkyNAWS/sZPLw+ze30Nbdz8DMUV1cYgpZRHa9uewAEwLAECfMmMCRjMO4HYBINkN2LMObpsETZtynyfqkY5A0Z7D/xrS4Twzo3Bt+ScAk4+DW7brTkHDaAZMOkVZhNaufrabwUFVxSEml4dpa+/QO2SKAQQK4gLQixGA0XxAB1kALgHY9qJ+cPZtzX0er3QF9rIFAKNSeeSfAIDuAwDpLQBfUAtAljn1ppTr497ZoU3+6uICppRFaG83g2cytQL4Q7T36EIVF4DRNLOVaywAJJuIu9/W29T5EB22/RUe+0Ji+nIYm7ULDyaH2gKIDeg+IaM9P2PUWgCjgzMc2B0ELJ6gtx2Z5wCcVKo7Bb35XisA1cUhTp9VTW+PKVyZLAB/AfuNAAQLxmBq8GwWwC5HADKMWdj8HKx+UNf6XogBxAb09R9KC+C9l+CJv4P3Xx7d88bHaoz82vJcANJYAKW6nT9td17DcXVllEWCPPGWHgJcXRzi4uOnsHCyLtR9vWkKWbQ37gKIQFGhsUJGs5DFYwBG2BwBiPbB3nf1+/4Moxb7zcMU7fGGC+AIa/9BEACl0vYdobPR5GGE80mmYi2AUSJdDKBUD/Nlf+bx/UWhAJ85bTr9Awq/TyiPBBERPrmwRh/avn/wQa4gYHEoQDDkLA82hhaAU4CbNiRaBDI9jE5tEu119QQ8jF2AqEvQRjKX41BY9zj8+5GD75fTC3G0rRBrAYwS6QYDlRkLoC37BB/XnDqdogI/lUUF+Hy6c095QNfA0d4euvpS/D4TBNzf3U9pOEhBXABG8eHI5AI45j8MQQA8ZgGgxv46mjeZQWYpwt/lCMBoLzLrWAAjP29+C4A/TU/AcLnuJZjFBQAoLyzg6xfM4WMn1CYSTSEK0scf16bEEKK9JgYQpSQcIBQ2owvHxAUIJ3/e/ba+Jl8w/TgFV9515Hz0osyHDLewjnUcoC9D7Kd739j8frwZ0FoAIyPuArjnAxAdB8jiAjhcecp0brnANcDI/CER6eeRNxqS9o1F+4j5C2jvMRZAuNAccxCCgLvXwMS5uvUjkwWQFAPwQFdg930dzXusFLx4R3IF4bSspP5O1xgJgI0BjBLpgoCg4wBuAVjxXVj9q9znM4UrJP28VN/MCxsb419t3bOPDY29cQsgYgQgbcDwQMkUBOxqhpJJ2grI1AyYFANwfEyXAGx4Ctb/fvTyOta4rZfRLICt78GKb8O7TybSHKsqkwUw2kIatRbA6JAuCAjGAnAp/Os/g3ceTnzubNZ9sVMV2PwhfjXAnAkRvrR8FTtau+kfiOHvbWN7V1BbAJEgkYh2ATq7MhTIA8Ep8P4UAejv0l2fg5EhCEAGC+CvP4K/3jl6eR1rxsoC6HAi+y5XyrGqDnYQ0HYEGiHpRgOCDgS279IdOHra9AxCrWZx0I1/gh8vhie/Apv+lHycy7y+Z9k8eqMxfvTMRur37GcKjWzur2RXWw8l4QCFhdoC6O5OFMgXNzXy1V+/lbQC0fZ9XTR3DPGPjkV1QNMXTHx28hWM6AFQOYOA3emHA/d1Qm/H0PIxHhirGEC6pr3+DDGAuAswVhaAFYCREReAlLlRS6fo3oAdexIFv61B+39//tfEcR17ko9zPQAzygN84OgaXt7czObNmyiQARpUDQMxpV2AYj2CsLdT9yaMxRTf/t06HnmjgRbXCkRX/fw1/vX37w7Oe2cTtGxLTnMEwK+v582tzsNqpkHP5gL0p3EBkgSgQy+rdrgwVhaAIwDuYGqqC+D83pgFAW0z4OiQbkIQgNI6vd2/E1rf1++j3dqXbt4Msy/UaakLZ7hrhWgPpxxZxY7Wbt5Z+w4Au2SiPn04SFFZtT6kU5uJz7y7h/q9uobdvk8X0v09/Wxt6kw/5fizt8LyTyWnqRj4AkTR1/PSpj1atPq7tAUQjCQi1qk4D1NfV2IshFsA+rtGtk7A7nfGvj3ezSGxAHph20vw/Wm64uhu1eljFgOwFsDIqJoFp/wtzPxAcnq8M1BDQgAAdryha8KaObq5sLMx+bhocuDp5JlVAOx9fyMA4ZoZAJSEg1SUV9Cv/EQ79qGU4u7nN1NYoAuuM8ho425d4NJOOd7ROPj3Y1EQP42d2vTv6jG1uYpp8z+YpRXAKSS9ba60VBeg/cAK8baX4L9Ohx1vDv/YAyU6RkHAzia9dVtS8RhAj7bKoj26GzBq9H8/Fkt06rICMEL8AfjQbYn+/w5xAdipo74OW57X26qZUFSTRgDcFkAvsyYUU11cQJ3o/SZPOwqAknCASWUR2iiit6OZxvZe3treymfPmAnA9n36PO8aAWjs6KUvmjJCsb9rcJt+LAo+Pzv26weku7c38aAGC00QMEc/ALeZ79RcSunfUrHMLkQ2Gl7T24M5O8+YCYDLrXKIuwCu+93wuuv3R9ECGBhd1ya/BSATkQooKNbmfuv7ei5BSAhA5ZFGAJqSj+tPNjtFhJNmVlEnTXSHaph7hD5PSTiA3yd0+UqIdbWwuVE/QEumV1JRGIxbABt2655lSsGe/SkPcV+HKZSuGtnEALa36geuu6cnUTs5LkCufgBJAuBubjK/cyCBwF1vmTznWD1pNBlrF8DtSrmDgM79bVg5Nr8/ysJmBSAdIjDtZD2GvvU9mHy8Np/3rtMR9rKpepWhrhQBiHYn4grmjzrtyGqmyl4on8a5x07kqlOOYPH0SgD6gqX4etrY0qQL1YyaIqZWFsZjAOt3tRP0627GO1LdgL5OQCUXaEcA2rQA9Pb2JR7UYKHpCJSmBlcqYb0kCUC/67cY/P1QiQvAQWxFGLMgoOMCuO57nysG4BTKPWsS349mj0p3PMFaAGPIzLOgaSM0boDyI/QMQgAV07XrUFQ92AXo79GxAYg/CJcvruOEsnYiNTMoDQf5ziXzKA7pKH0sXE5BfxtbGjsJB31MLg0z1axApJRiw+52Tpqh4wiD4gBOoXQXzpgOAr7XYqYq7++nv8d8ny0IGIsmAn/u/uzOA5YU8R6mAPS0wb4tg88z1ox5ENBcixNkBS2iznunCTZUlrugbn4u0dqUC/e5bD+AMWTm2Xo70Afl06DMtAxUHam3RTW6ndc92UO0ByLl5r3+c4LECHfuhIojBv2Er7CCEtXBym37mFFdjM8n1FVG2NHSTUNLN+29Uc6arUcY7kpdesypTd21aiwKPl9cAPwM0N5hCnTQBAFj/YMHx7hrs17X/qNhAex+x5XngygAo1xTArqnZVeKBTDQl5iIJdo72MUqnZxbgB6+Bv7vnqHlIem6DoILICJTReTPIvKuiKwVkS+Z9EoReUZENpltheuYW0SkXkQ2iMiHXOmLROQd891dIiImPSQiD5n0V0Vk+oivbKRMnKsLORgBMBZApUsAUIm2XtB/SLgs8R6gfad+QMqnDfqJUEklZdLJWw1t8YVHplYU8k25l40v/RaAhdPKqSwq0C7Amt/qFyRq8iQLIEpM/OwxrQABBujsMAXWCQLC4IfUXUCcAl5QlHjY3G7DcGMAjvmPHGQXYAwsgO6WhKWU7v67YwAAiH5OsgUBldKi25tmCHk6Rtm1GYoFEAX+Xil1DHAycJOIHAt8HVihlJoFrDCfMd8tA+YC5wN3i8Qb2u8Brgdmmdf5Jv06oEUpdRRwJ/CDEV/ZSBHRbgCkWAA6Uk+RbsdPCgS6XQAnqOY0I6YRgKKyGkroxkeMmdWOAET4G/8K2lY9QUk4wDGTS5lSrmch5pX/1K9on2t8v6twxqL0xXwMKH27/RKjq9MRgIhLAFLcAHfrRY9pBiwoTpiY7oJrBCIWU/zw6fU0tORoFdj1FpRMgcLKg+wC9GqLx3k/Gjjmf7g8cQ/d9zLVAoiUa+HNJkD93QyK5WRjlMc45BQApdQupdSb5n078C5QC1wC3G92ux/4qHl/CbBcKdWrlNoK1ANLRGQyUKqUekXpvq6/SDnGOdcjwFLHOjikzPu4VvCqoxIF2LEACh0BcMUBot0uF8D8OU5vvfLBLkBpRTU+UZTQFbcAphUrAhLD19/OF846isKCAFPKIloAetp0TeEukEkuwAC9MV+8I1CQAXo6XSa9mQvxjc0pIx3TWQChYpcL4G7y0t9vaerk7uc389Q7madOA3Tg1BmJeLBjAMGIHhcxWhaA819XHOESgOTOX/R3QfEk/TlSqQdmZesIlO482XBbEwd7PgBjmi8EXgUmKqV2gRYJwGlMrwXcEY0Gk1Zr3qemJx2jlIoCbUDVcPI2Jsy+AP6hHsKlMOuDcPrNcMSp+jvHPXAeioGo9sHjQUBTqJrrdffcNAIQKNKtAWXSyczqYgCmRPSfWhPs4TOnTddp5RG9FmFPmw7SpeuGCqAG6BkQBszf6meAnm79fdtAMG4B3Pn75PEGyTEAxwUoSR8ENN9vb+milkaa2nM8uPt36bEVwaKD3woQCOvXaFsA5UYAnP4R7t+M9uh+JOFybfUEwtkFyDl+qP0rRtm1GbIAiEgx8Bvgy0qpbA5LuppbZUnPdkxqHq4XkZUisrKxsTHNIWNIpBzOvTUxDiAuAMYFcMzoVAuguR4qZsT75ydhxKKMTmYYCyA0oB+EeVVCOKhr8inlYdp7oyjHAkjXCQUgFqV3ACpK9ECjAAP0dutC97XH69kf1YOEOjv209CS3GkpjingqqAI1AD/9eeNyZ2HzPf7dm7hz6GbOWr7I+nuljlvnw6alUw+BBaAnoORQGgULQDzXzsB3f7uFBfAxACChVC3GKqP1pPOZKuph2sBONZEsPDgNQOKSBBd+B9USpkoFHuMWY/ZOt28GoCprsPrgJ0mvS5NetIxIhIAygBXdE2jlPqJUmqxUmpxTU3NULI+dkQq9EQi8WYh85ClWgBN9VA9K/M5gKPLopSGzQg+0wxXJokHYkp5hBB9yIDuabZhW8LAWvfezsQCpbEo3QPCpHJtTYT9ij5jATy/rYPnt2oxCEsfr29zBy9dD5/xMd/r0Jr8Wv3uRMEVXzwIGHnvzxTIAHNbVmS+R07Pv+KJRgBGcehzLqI9xgKIjK4FIL7ExLH93cnX5MQAghFY9r/wkf+XW4DiFsAQxdG5llDpQWsFEOBe4F2l1B2ur54ArjbvrwYed6UvM5H9Gehg32vGTWgXkZPNOa9KOcY512XAc0odzJEjB4DPB4VViWYhpxCFivXgomiPbjbatyXRdJiKsRa+c96URJrTF9/VHj9nUgmlJB60//foX+LvH311E5f8+CU27mmH2ADdUWFKZQkAJUHY16IHG/WoIA+8ocWqsiCaIgD6oVJOJybgjd26JWFfe0fiIS+qiVsAk5peAWB27zuJYa+ptJv4QMlkHVQ86BZAaJQtgEYd+ynQAkt/Z6IG94dcFkBY/64/qLfZYgBxARhmEDBcOiqDjIZiAZwGXAmcIyKrzetC4PvAeSKyCTjPfEYptRZ4GFgHPA3cpFR8ra0bgZ+hA4ObgadM+r1AlYjUAzdjWhTGPe7uwI4yByIJv6+tQf9hVUelP95YC0Uxl2/sFHxXs9DM6mImhRK12ERJrHlfUxClbyDGlfe+Smygn64BmFKhH9CioLC/fT9dKkRpOEhLv3ZD5lUHeH1bC919A9p6MA9fczQhAG1RPalIW3unftB9QR3U6tNCM6vzDd6NTcVPTC9WmY64AEwyFsAwYwBtO+ChTydG1Q2HuAUQHr2pwbuatV9fYKZzc7sAhZWJsQDBwsQxuQToQIOAB8sCUEr9VSklSqnjlFILzOsPSqlmpdRSpdQss93nOuY2pdSRSqnZSqmnXOkrlVLzzHd/69TySqkepdTlSqmjlFJLlFJbRnxlB4Oiati5Ch66MtHe7ah/tFf7/5BZAJx4QU9rIs0JwvW2xxfz9PmEhRMSYZJaf2L/k+pCfPWDR7Nnfy8dXT1ElZ8pVVoAioOKkOqlixCfO2Mm3UrHL2ZXBajf28EHf/QXzv/Ri/T36oewJVYUP28goq2I7u5uoj0dugCHSnS+dq6mRHVwT/Ri9qhyYu7psdy079LbuAAM0wLY8Ad493fw/ivDOw7GxgLobk1MGgv6epxrilTq33FaHxz8xgLIZNAONQi4620ttG4LwHYFPsRUzNBzB777BLxtpgxzWwDNm3VaVYYYQNDs292iI/yxmKvmV0ndbudWJg5bVJGoLY6p8jGvVnc+auvqZQAfdZVFID6KghCRPqK+EFedMp2PLNauyJHl+m9v6eynqaOXF9/VjTNtJASgskL/YFCi9HS1GwEoht4O+jY+C8DmkhN5bmAhbH0+/fW179Y+c1HNgQmAI6pDWdA0FbcFMFoxgJ62RNs+JFsAkYqECxBwCYATNM6Uh6FaAH+9E5682WUBlNjBQIecD/0bfPkd3UvQWXsvyQLYpJvTUocbuwmX60DhHXPhrf9N7ovvjgOUJbocOxZANFROwUA3R08soSDgo6O7hwF8TK3QU4AXBiBML7FAhLLCIF+/eBEA00rgjk8czzM3n0lteYQX1+mgYrevOP4bE6p1K2wBUfq69idZAH3b/o/1sanMOOII3lMT8fXuT1+4O3brAKDPr/3m/s7hLYfu3NPmAxAAsxTbiCyA1GvqadU9PeMC0JUouIUV2tVwJl9xiAtAhjy4pxTPttx39z5dUTjnCVkL4NATKtYdhKqOSpi7SRZAvQ4AZuvTFCmHTX/UtX3jhuQuoa7304sTAlA+oOMOgZIJ0N9F0O/jmMml8a7Ak8vC4AtQGFAU0ovP8VlNkE/6u/nYCXVMLovwicVTCShdqzizFAHUTdKiVUCUaHen6UhUAn0d+JvWs15N5YRpFTRTqg9IHRoN2gJwhlI7C7IOtb17oD+xnFlT/dCOcTNSC2DHG3pmH/dvd7dpwS5wCUBfpxaagmId44hFB8cAIHPALt3kounobtXPiLNPeAiDjIaAFYDRwN3MF/c7TQwgk//vEKlI9C/vbMxoAbhbAYKde/RDFy7XD11nM+dX7CJAjGAwSMDvA1+ASEARoY9g2BQ+n08LlKsQXr64jiKf7vFXWpEQgIlxC6CfWG+HfsBDJdDZSKRrJxtjdSycVk6TMmMfXAJQv7eDfZ190L5HtwBAotAM1Q1oXK8LTbjswCyAkcYAmjbpwrzjDf3Zcc8i5Ykavq8rEfQLhBPBymAimBpffCaXBQDZBcCJEznNzqES3R08m9UwBKwAjAZuH9/x63va9BDPXALg9BsA6Nib0QKQ3jYGREfxJdqta1THr/7rHXx2899SQD8hZ9Vhn5/iIESkl0hRaeKcBckTg04pj3DtyZNRvgA1VYlAgz+k3YGIfwDV36mPCxXHa7KtvmkcM7mUZuVYALrNv6svyqV3v8QPnlqvraISxwIw7sVQWwKc5cyO+Yh+6IfbEjBSC8Bp2mzaoLe9bYAyLoDLmnELgNMUnOQCOPNDZLIA0kwskg7n+jv3alHJFVsYIlYARoNqVyEPmBhA43pAZe4D4BCp0P0Gppyg/9ze/TqiDCnWQBv+oqrEw1dQnBCAlm0EB7qZLM2EC8yaAP4gU0sDzKkKUFiU8O31zMDJNU2pfwAJhCkt1vsN+ILxmmtCxIfPechDJfFjuspmEQ766Q2ZvJqa6ek1u2nvibJhZ1OiFyAkXIChWgDOcmZHX6A/Nw/TDYj2sraxl61tB7hEeJeZ8LXRCIBTAMPlyYOq+rq0ODoFElKCgOb/yGgBuAQxkwUQiyUGaXXsNZaNIywjCwRaARgNqo9OvI9bAK36c2UOATjpBvjo3XrATIdxAZzJR9wTdPa06drHGW5cUJTwO9t0FL9ABqgoMQ9fUQ2+zkbCqifZJ023OIipLcWYrv5AooapKYTAQFfCBQC6KaB2xhy9b3HymAhnSbTWRjPgqGRSIr8wdAHY9ba+JzX6d5JaApSC137K/uZdrN+doVd6tJc3GjpZs6f3wGpJZ5h3k57QNV4Ak4KA3ckWgEM6C2CgVw/lTu0NmeQCZLg3vfuJ94zv2JsIbsKIOwNZARgNSqYkHgrHAnBwhg9nYsoCOH6ZbinobNQPmjP3QIoFoAXAmNzBQl3z9HUmLWM2o8Z8XzYV2rYnuqY6BAsHP4Rucxl0Dza/7ppcHfERHOjRv1WgBWBTrJbFM3S8oLS0lG6JQGcT2/d18fLmZqZXFVIe1QXoP1/v4F8eX5Pcey4TTfWJa27aCBOO0f3ufYFEQQTdu/IPX+XVX/8HH7v75cETpg5EQQ3Q1CM098jQ/O9UHBdg3xYdkHQEPVKux3X4CxL9AAqKkv9zt+A6qzTtWQePfEY3GbtJN7twKu5+Ip2NppdhjtjCELECMBr4fAlT312QCqvi/f1zUjRBTxzS1qDbzX3B5HiAIwAhU8CdGEDP/uQhyc4iJ2V1OgbR351wG0A/nB174O1fJ09nHXQLQCj+4FaFFRG6E82AwEY1lRPNvIY1JWH2UQqdjfzXXzbjE/jqh2Yzx6fnQfjjduG59XtzWwCxGPxsKfzlB7qzUVeTGUQV1Fu3AJgh1v69a+jqG9DdoN2YQtGjgrT2+3Ttm9oRp7sV7jwWHrkueVYnB8cFiEVh39ZkF8C5j/3dicE/SRaA672T7swLkbqYTF9n4j/LJADu+Efv/mQLwMYAxglVs3TB8fkSf04u89+NY0rH+nUtHy7NbgEUFOuXMzGIg/MwlU/VLkRfe4oFEIFdq+G3n4V3zEi+/lQLoCAuANUFUYIM0O+L6CAgsKtgOnUV+pxTysPsHSjlvfff48FX3+czp83gnNId/EvgAeoDR7FuoI6Glm72D5iaMJMA7N+ha7q966DFTMVeMZ32nn5eHZhF38YViYJgpmqfGdUdRt9qaE0+lykUvRTQqzL44Ftf0O3qax6Bx78wOD/dLYn4RdOGZBcAjAB0Jtr9M1kATgzAWWvSERaH/i5dUTjv0+G2ACD5v7IWwDhh/mVw3Cf0e6fA5QoAuilydRYKleiavqdNr0K7/XXTDTU1BuCq2UvMgKK4BeAakOkWgCWfg9O+pMXKiXDHXQDzEPuDCQvArx/KdhVClR9BDwX01p6CM1/L586YSV+4iq6W3UyriPCPtW9TuPzjtEkpV3TcTBSdn3f3aTO9viHDBCImyNe/dxM///3zALSGa7nkxy/x7T2nUxDrpvGFn+p9jUBM9+2hKtjD29sTsZKvPfI2v3tTC0MvQXoIJq7RzeYVUFBC9wnXw9sP6SZLN137YOpJ+n3jhmQXABLrLMZdgBwxgHYjAKmrSfV1JSaXGYoFAGaYc47WhSFiBWC0mPNhuOTH+v0BWQBuATA1fU+brp3++I00LkBhsgA4qxs5C526pyBz10izL4DzvqPFyenkMigGkDAxawJaAHZ1+dg+UMWcnv9h4jGnxk9XXRxi0bGzmBbq5MGT3yf0+A1QPZs7pvwHjZQz33RTfmevtlSeenNz8mQkznsjAIH2Bpq26C7Az+8tYktTJ5+9/BJeU8fif/0n2lx3rYl46eRW3t6hBaC1q4+HVm7nyTf19zFfAb04FoDLVFYK6p9je8WJ3PimFs6+hlXJ33c1ayuqtFa7Hz1turXGiWU4MyzHg4AZWgEcXz2jBdCZmF7ObR25V6RKtQCSmgGtBTD+cApSrgCgG2eCEdCFP1QKO9/UD17D68Y1SHEB3L79jBQByGQBOFQdlehg4x7CCrqGMUHAiUFdK63fN8BfNjUCwilHViedKlAykaJoK1P3/kWbzZ95iso63TJy0XGTqS2P8NCqJmJKUH0dbDJrINLbDv9+FNE3/5f2nesBEBRn+NbQHyjm3VYfBX4fFx8/hYbZ11AZ3cubf36E7sYtbFD6+k4t2sHGPe109w2w6v1WALbt0QG8qvJSAgUppvL21/R9bXuf/2qYTr1Pi3TnNtdCHv1dOm5QWKVbeBo3JCwwp1dnsCjRFTirBZAqACk9Jvu6EgLgWADb/go/mq8Dh5CwAJxelYGQjQGMa5w/J1cnIDeRisSy3qFS/bDFawtTS4bL07sAkUqYNF+/d1yAoppEBNptKThUz9I16UB/8vRZYGIA+hr8va0AvLM3yvPr9zKtspAja1LOV1SjA5ibV2iz2R+ID1A68+ga5tWWsqmxk07CFNHLS/WmEGz5C3Q1sfH5/2XVqpXx+QgW+zawr2AKmxs7mVFdRMDv45yLrqCPAK8//wTde7fwju8YouEqZqttDMQU63a1sfI9XfCDpmtzeWkJJaZvA9FeaNwI954HPzsXgL/EjuPCxbPYHJtMbIfLAjAtABv2B1HVR+smyO6WhPkP2gJr36Vr7VBpbgFwArppYwApArBnbfK2u0U/G05MIikIaC2A8UdpnX4ohuMCiCSsAMcCAOMOlJv0NK0AoM3UqiP1Q+Kk+XyJmYzTWgCzdIS7ZRvxFY2Cg4OAdOu5Bza2xHixvomzZ9cwaL5WpwbradMrKgEfnj+ZP3zxDI6ZXMq8KVoMBgKFTAxHeaneFIJ6PY9AbdubzKSBXZVLdHZlgB0ykfq9HRw1QRfg8tJSAlMXc1nJOiqlgwvPOJlA7fFM6NKtAyu3tbByWwtzJpVQ7NfdY6vKSigvMZ2Xoj2J0YV1J7Jzwpk0qAmcdlQ1a9QMwk2ulXxMH4A7/trEFqnTZvredQnxBW32N20EFF1HnJ0SBEwjAA7uGIAzp2C4TP93ThDQCYI6C6r0tGrxcQTI3RFohIuDWAEYC477JHxlTTxqPmScloBQScLUr1sMR5pFSpKCgMUJf7SsVj901/0RFl+bOJ/jBqQTAGf8QtPGNBZA0LgSEn9gu1SYvmiMs+ekGdnodl+m6kLs8wnHTtHXcNzUcgAC4WKmFsV4dUsz0egA1K+gRyKUSSd10sSKtinsUXrfDb1VvL+viyMnJO6h74hTqOreCkDhxJkweQHBpnc5bWqYn764lbcaWjn1yGrmTTQtGOVlVJRpAYj2dcPetdpCuvpJfjnz3wn4hBOnV7BGzaCoZ3diPIOppfepEt7sMmZ34/rkbtsmrrJVTeKONSkugHm/t72Hq3/xFkn0tiVmW3amBC8oTO6h6SxI26KvNT4PgfPf22bAcY7Pl1xbDBWnJSDksgDqTtQzEoOuaZ101zTf8Tnqahcl9ztwehS6g4AOjnvStMkVA3D1AxDRD9meNShfkB0F0wkHffElz5PzbQQgEIFJxw36+oyjqvn5NYspKiljUmSA9t4oL7/6MrRt57/6L4jv93p7FVtiOii3truCmIJZLgFgWiL4SPkRet2GWJTvLmilqaOXnv4Yi46oYP5ELXgTK0qpqND3Y1/jbj26sPpoCBSwrbmTqZWFFBYE2BE2Yrhztd4aF6CFYp5rdt1PtwtgRPXxgVN5Zv1elNPcFyyMxwle3bKPv2xJ01PRcQPiKzcXJa/c7AjAPiMAjgXgPFOBsO0I5EmclgB3sK/uRJj/CbjiIV24kmIApmCX1Q4+F2S3ACLluuA2b3JZAK5mQDBugEKmn8aHFx3F5YumxmcqTsIRgNpFiWNd+HzCOXMmIgXFVBVEmVoZ4YU//AqAPTMvR1VqMdqqJrM7qK/lfaXvxVFuAZi6hPgE0hXTtbsRLGRm2/9x2aI6RGDx9ApOm6GPmTm5msqZi+hVATo3/kUH1SYcA8AWE18AaCs/Vp9zp4kDGLenVZXwQkMM5bTTu0RdGVH9vTqd95q72N6u4zRRX4gr732V9p5+tjR2AkKvMnGZUuOSOQLgRP0LCpNXbm4xLQCOCxC3AMr154C1ALxJySQ9g06oBCYv0LXV1BN119PZ5+uaZeJcOPp8LQyltTD3Uv05HU4MIlNvxKpZuinQiQG4g4Du7awP8a2L5/Ldj85Lf57CSm2ZzDgz+/UVFBHob+dPX/4AfzNpO02haXz7qvORGWcAsE1NoqdM5/k9NRGfEC+k+jrKYeI8/VuRCl0Ipp8Om5/ju5fM45HPn8rE0jDVXbrmDJVUMW/GFN6S2ZRs+yO0vQ8TjiUWU7zX3BU/d3llDZt8M2Dj0/p3TAFt9xXT2TdAV6m5j6YAKqV4sXAp3+3/FB89V7e+vGRmXG7pD/DipibeeK+FLU0d1JZHGPCZwlpjxox0NsEb9yVaYeIrN3dr8elt0y0QnXt55d33BlsAfvdgICsA3uHEz8EnHtC16Iwz4G9fH+xKhEvhbx7Stb4/CJffp0UhHXM/Clf/TteW6ZiyELb/n56PIMkCSBGAoz+UPd8+P3zhFTj9y9n3q5kNjRuI+KLM7FlP9ZzTKAj44Iy/R33sZ8ydWUfB4ivp/vB/8p6axNTKwsEWx5LPwsIrE81xR54DzfVEOhtYdESF7hjz+r06vayWoN/H3ppTqe4z4yUmzmVPew/d/QNMNwJQVx7h8f6TYcdKHYDr2kc7hZw6Sw9kavCbPhXhMtp7+vngnS9w1VN9/L7o41x3+gzmTCrhuc3a1N8f1bX9ul372dLYqVd8Mu5B1Fg67FoNv/sSvPAf+nOB4wJ0Jdr/jZj+8qm/DI4B2GZAj1I6GY65aPTO5w9mr5XP+SfdK9AXgMoZiQ4sjj/rD0LlzKH1aCyrGxzxTmXqSdpnXf+kHvpcq6coo3wqctzlLL/+FD5+2jwiJ36aKWVhZk0oGXyORdfA+f+W+HzkUr1d95jern1UT0V28k3xXcrnfTD+frNMpd70Q3DWY5xSHuGx6Enx46OdzeyLFbN4eiVTysK83WsCgZFyfv/2Ljbt7eBfLjqWp750BuGgn6XHTOD/3tfmfCwQprY8wtqd+9nS2KF/w9yX5sgMfR5nFuWG1/Q26HIBTAuAMqtTh/ZvS8xF6Fhy/gItuhUz0jfxDoM0y9VY8oaCIt0r8Oxv6sIuonu7OTX/cZ9Mu6jpAWOaCHnlP/W29oSMu/7np06gvLAg9zmrZ2mRe/bbekzD6l9C9Ww4aml8lwVLPkDLc8UUSJRzf76VqmI9fdsMlwA0qAl01iygaM1v6AtW0EIxteURTp5ZxXPrK7gcIFzGIy82cGRNEZ85bXq8OfT6M45kcrEPnoHamirmFZfyUn0TnX0DzKwpxr85DN3wHpOZCIlZjmNmEFJBkRaB7p3xAGBTzcnUADOimyGgBlsAwMsffpbjppYzzLamJKwFYNE1vmNSh0sTzYvn/BOccOXo/U7xBB2X2PGGFpmJGWIKwMJpFcn+fyZE4JMPanfm+X/TAnbRHUnzMBZHQqwqP48X1QKWLTmCfZ19hAI+JpVqP7q2XFs+GydcALvfJrL9BVpUCbUVET4wu4bnuo9i7/zP8375yax8r4WPL6pL6gtRVhjk06fqloTCwiKOnVxGa5du6ptZU0QgpM+/ubtYF+RYVOfTIe4CdGsXIFTGpr4q9qliFosZBZnUChCitauPa+57nf/404bc9ygL1gKwJPM3v06sfTcWTDsZ9m3WLRq5XIahEi6Fqx7TwjL9jER3aBen/u29DMQU54cCXHTcFBrbe/H5dCF2BOATq+ZxmVzHTSUv8nL7XD5THmFmdRF9UsDy8s/SvaYdn8DHFtYNOr9uNg1DsDDe/wHQFoC5zo3tBTq419OqXb11ZmGsYKHpWmxcgIppbG7qJKwmcYrfdAcunpRoBfCHeHz1TvqiMS5blCYvw8AKgCWZqSeO7fmnnQyrH0z4/6NFqET3C8iAO5h42lHJYxlKIwEml4UJ+n081v5BHt5/LgJ8vTSM3yccV1vGY6t2sKO1mwvmTWZSWZi0BEIQjDDXCEA46GNyqW5diSGsazHLye3brPt27H5HN/XFLYBO3TFr4lw27+3gtwNXc9xAPaeefBofOvIc3TrgL4CiGh5+fjtzp5Qyd8oB9DdxYV0Ay8Fl+hk66JiryfAgIiL8/otn8KevnMklC6YwEFNMKtOFH+ADR9ewpamTgE/45kXHZD6RmaVpclmY8sIgM6qLtZURKKDbX8rm5l7a/brAqikLE8ONnSBgd4vu/XfkOWxp6qRv4kKWywWs9M1nf98Ae/ojcNNrrKk4h7U79/PJE6dmzssQsQJgObhUzoCb1+vh0+OIyqICwkE/nzpJuz+OWwCw9BjdCnDzB2czuSxNpyqHi/8/OPWLiAjLTpzGJQvMHA2BMP0F5TR19PLMduhQYe5626e7bS+5wVgOplOX+OHYS3QLQo0ORO5o7eaW377DZf/1MqpiOo+9tSc+SnKkWBfAcvBxxjyMQ+bXlXHesRM5vi5hWh8/tZwVf/+BeLNhRmadF3/79QvmJNKX3MD7lVvhL3B798WsnfZh7l2xhWBwNl+48Id6H6e35syz6CmoYEdrNx8/oY7Wrj62NHayfV8XnX0D1O/t4PmNjZw0s3JorSQ5sAJgsaTw06sWD0o7smYEjW2zzqWkohP+8jznn7qYb3z4GJoeXs0Pn97Aup372binndtqezkRYN7H2dbciVK6BWFvey8vbkrMIfDQ69up39vBJxeP3PwHKwAWy0FhRnURv7nxVI6rK8PvE/7j8uOJKfj92zspDgX4aW8li6efTt/RF/K9h9YjAvNry2ho0eMDQgEfE0pD/OIV3U/gzKNHx4qyMQCL5SCx6IgKgn5d5AJ+H3ctW8Bbt36Qf77oWP7UOoU3znqAmx/byl82NvK9S+czs6Y4PvnqKUdWsXTORPoGYkwqDXP0xJF0/0lgBcBiOUSICCXhIBfMn0wk6OdLy1fz+3d28fUL5rBsie6BWVehg4PnzJnAmUfr5sszZlUPnpTlALEugMVyiCkOBTh/3iQeXbWDc4+ZyA1nJuaSXDi1nB9+/DguXjCFmFKcNKOST4xC85+DFQCLZRzwhbOORIB/+cixSbW7zydJBf6hG04Z1d+1AmCxjANmTSzhjk8uOOi/a2MAFkseYwXAYsljrABYLHmMFQCLJY+xAmCx5DFWACyWPMYKgMWSx1gBsFjyGCsAFkseYwXAYsljrABYLHmMFQCLJY+xAmCx5DE5BUBEfi4ie0VkjSutUkSeEZFNZlvh+u4WEakXkQ0i8iFX+iIRecd8d5eYMY8iEhKRh0z6qyIyfZSv0WKxZGAoFsB9QOr6018HViilZgErzGdE5FhgGTDXHHO3SHwNpHuA64FZ5uWc8zqgRSl1FHAn8IMDvRiLxTI8cgqAUuoFYF9K8iXA/eb9/cBHXenLlVK9SqmtQD2wREQmA6VKqVeUUgr4RcoxzrkeAZbKaM13ZLFYsnKgMYCJSqldAGY7waTXAttd+zWYtFrzPjU96RilVBRoA6oOMF8Wi2UYjHYQMF3NrbKkZztm8MlFrheRlSKysrGx8QCzaLFYHA50SrA9IjJZKbXLmPd7TXoD4J6xsA7YadLr0qS7j2kQkQBQxmCXAwCl1E+AnwCISKOIvJcjn9VAU459DhU2bwfGeM3beM0XQMblng9UAJ4Arga+b7aPu9L/V0TuAKagg32vKaUGRKRdRE4GXgWuAv6/lHO9AlwGPGfiBFlRSuVcGUFEViqlBi/zMg6weTswxmvexmu+cpFTAETkV8BZQLWINAC3ogv+wyJyHfA+cDmAUmqtiDwMrAOiwE1KqQFzqhvRLQoR4CnzArgXeEBE6tE1/7JRuTKLxZITGUJle9gynlXZ5u3AGK95G6/5yoXXewL+5FBnIAs2bwfGeM3beM1XVjxtAVgslux43QKwWCxZ8KwAiMj5ZjxCvYh8/RDmY6qI/FlE3hWRtSLyJZP+LRHZISKrzevCQ5S/bWaMxmoRWWnSMo71OIj5mu26N6tFZL+IfPlQ3bfRGhMz3vCkC2DGH2wEzkP3M3gduEIpte4Q5GUyMFkp9aaIlABvoLtBfwLoUErdfrDzlJK/bcBipVSTK+2HwD6l1PeNeFYopb52CPPoB3YAJwGf4RDcNxE5E+gAfqGUmmfS0t4nMybmV8ASdHP4s8DRrhaxcYNXLYAlQL1SaotSqg9Yjh5zcNBRSu1SSr1p3rcD75LoBj1eyTTW41CxFNislMrV8WvMGI0xMQcjn8PFqwKQaUzCIcUMdV6I7gwF8Lci8rYxLw+6mW1QwJ9E5A0Rud6kZRrrcahYhq5RHcbDfYPhj4kZd3hVAIY8vuBgISLFwG+ALyul9qOHRx8JLAB2Af9xiLJ2mlLqBOAC4CZj6o4bRKQAuBj4tUkaL/ctG+Pu+cuEVwUg05iEQ4KIBNGF/0Gl1G8BlFJ7lFIDSqkY8FMOkYmolNpptnuBR00+9pjYhRPD2Jv5DGPOBcCbSqk9MH7umyHTfRpXz182vCoArwOzRGSGqUGWocccHHTM3Ab3Au8qpe5wpU927XYpsCb12IOQtyITmEREioAPmnw44zMgeazHoeAKXOb/eLhvLjLdpyeAZWa2qxmYMTGHIH+5UUp58gVciG4J2Az80yHMx+lo8+9tYLV5XQg8ALxj0p9AtxQc7LzNBN4yr7XOfULPx7AC2GS2lYfo3hUCzUCZK+2Q3De0CO0C+tE1/HXZ7hPwT+bZ2wBccKiev1wvTzYDWiyWoeFVF8BisQwBKwAWSx5jBcBiyWOsAFgseYwVAIslj7ECYLHkMVYALJY8xgqAxZLH/P/JvSMDsavBgQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_rmse = history.history['rmse']\n",
    "val_rmse = history.history['val_rmse']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_rmse, label='Training RMSE')\n",
    "plt.plot(epochs_range, val_rmse, label='Validation RMSE')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "\n",
    "'''How far are predictions from real values?'''\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def format_tick_labels(x, pos):\n",
    "    return '{:.0f}k'.format(x/1000)\n",
    "\n",
    "predictions = model.predict(test_examples)\n",
    "\n",
    "xlims = [0, max(test_labels)*1.1]\n",
    "ylims = [0, max(predictions)*1.1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(test_labels, predictions)\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "ax.set_xlim(xlims)\n",
    "ax.set_ylim(ylims)\n",
    "ax.set_xlabel('Actual Price')\n",
    "ax.set_ylabel('Predicted Price')\n",
    "\n",
    "ax.plot(xlims, ylims, 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold CV Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:07:47.505405: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 89944.2812 - rmse: 89624.1797\n",
      "Epoch 1: val_loss improved from inf to 43987.60547, saving model to ./ckpt/5_class_lr005/val_rmse_44031.hdf5\n",
      "70/70 [==============================] - 2s 19ms/step - loss: 89944.2812 - rmse: 89624.1797 - val_loss: 43987.6055 - val_rmse: 44030.8906\n",
      "Epoch 2/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:07:48.885436: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 46371.7344 - rmse: 46371.7344\n",
      "Epoch 2: val_loss improved from 43987.60547 to 43963.42578, saving model to ./ckpt/5_class_lr005/val_rmse_44035.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 46539.8008 - rmse: 46653.1523 - val_loss: 43963.4258 - val_rmse: 44035.3867\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 40063.8711 - rmse: 40063.8711\n",
      "Epoch 3: val_loss improved from 43963.42578 to 40522.51172, saving model to ./ckpt/5_class_lr005/val_rmse_40540.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 40102.8359 - rmse: 40223.0039 - val_loss: 40522.5117 - val_rmse: 40540.2891\n",
      "Epoch 4/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 37527.4141 - rmse: 37527.4141\n",
      "Epoch 4: val_loss improved from 40522.51172 to 33671.87500, saving model to ./ckpt/5_class_lr005/val_rmse_33624.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 37071.9258 - rmse: 36973.5664 - val_loss: 33671.8750 - val_rmse: 33623.8633\n",
      "Epoch 5/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 34176.3555 - rmse: 34176.3555\n",
      "Epoch 5: val_loss did not improve from 33671.87500\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 33996.2734 - rmse: 34093.0273 - val_loss: 34400.5664 - val_rmse: 34358.9883\n",
      "Epoch 6/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 33042.1094 - rmse: 33042.1094\n",
      "Epoch 6: val_loss improved from 33671.87500 to 29296.97656, saving model to ./ckpt/5_class_lr005/val_rmse_29245.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 32908.2227 - rmse: 32832.3633 - val_loss: 29296.9766 - val_rmse: 29245.0547\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 30713.3770 - rmse: 30713.3770\n",
      "Epoch 7: val_loss improved from 29296.97656 to 28376.66992, saving model to ./ckpt/5_class_lr005/val_rmse_28255.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 30461.4258 - rmse: 30401.0117 - val_loss: 28376.6699 - val_rmse: 28254.7305\n",
      "Epoch 8/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 30296.0977 - rmse: 30296.0977\n",
      "Epoch 8: val_loss did not improve from 28376.66992\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 30314.2637 - rmse: 30308.6992 - val_loss: 34911.4766 - val_rmse: 34891.8711\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 32652.3086 - rmse: 32664.2969\n",
      "Epoch 9: val_loss improved from 28376.66992 to 27774.98047, saving model to ./ckpt/5_class_lr005/val_rmse_27651.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 32652.3086 - rmse: 32664.2969 - val_loss: 27774.9805 - val_rmse: 27650.9824\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 29509.2891 - rmse: 29509.2891\n",
      "Epoch 10: val_loss did not improve from 27774.98047\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 29364.8633 - rmse: 29267.4551 - val_loss: 28514.6348 - val_rmse: 28528.3301\n",
      "Epoch 11/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 27371.3320 - rmse: 27371.3320\n",
      "Epoch 11: val_loss did not improve from 27774.98047\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 27162.2852 - rmse: 27123.1641 - val_loss: 28527.5449 - val_rmse: 28281.4062\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25827.0449 - rmse: 25827.0449\n",
      "Epoch 12: val_loss improved from 27774.98047 to 25686.05859, saving model to ./ckpt/5_class_lr005/val_rmse_25560.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 25739.4023 - rmse: 25689.7266 - val_loss: 25686.0586 - val_rmse: 25560.3984\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27795.2051 - rmse: 27738.8496\n",
      "Epoch 13: val_loss improved from 25686.05859 to 25436.04297, saving model to ./ckpt/5_class_lr005/val_rmse_25319.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 27795.2051 - rmse: 27738.8496 - val_loss: 25436.0430 - val_rmse: 25319.2637\n",
      "Epoch 14/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24339.2715 - rmse: 24339.2715\n",
      "Epoch 14: val_loss did not improve from 25436.04297\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 24360.1230 - rmse: 24465.0352 - val_loss: 28253.3574 - val_rmse: 28108.7285\n",
      "Epoch 15/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24610.8398 - rmse: 24637.8613\n",
      "Epoch 15: val_loss did not improve from 25436.04297\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 24610.8398 - rmse: 24637.8613 - val_loss: 27777.8730 - val_rmse: 27846.5703\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 24890.5781 - rmse: 24890.5781\n",
      "Epoch 16: val_loss did not improve from 25436.04297\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 24856.9727 - rmse: 24834.3066 - val_loss: 29322.5938 - val_rmse: 29269.7129\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22910.7637 - rmse: 22910.7637\n",
      "Epoch 17: val_loss did not improve from 25436.04297\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22819.7383 - rmse: 22831.6992 - val_loss: 32373.9805 - val_rmse: 32366.5645\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23533.7480 - rmse: 23533.7480\n",
      "Epoch 18: val_loss did not improve from 25436.04297\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23510.2090 - rmse: 23508.3652 - val_loss: 25706.9844 - val_rmse: 25591.2637\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21716.9590 - rmse: 21740.4590\n",
      "Epoch 19: val_loss improved from 25436.04297 to 22822.64453, saving model to ./ckpt/5_class_lr005/val_rmse_22785.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21716.9590 - rmse: 21740.4590 - val_loss: 22822.6445 - val_rmse: 22785.1484\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22055.3711 - rmse: 22055.3711\n",
      "Epoch 20: val_loss did not improve from 22822.64453\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21902.3809 - rmse: 21910.9141 - val_loss: 24140.3262 - val_rmse: 24070.0723\n",
      "Epoch 21/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21076.1621 - rmse: 21076.1621\n",
      "Epoch 21: val_loss did not improve from 22822.64453\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21085.8223 - rmse: 21069.2637 - val_loss: 28397.6055 - val_rmse: 28161.1738\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22148.7148 - rmse: 22148.7148\n",
      "Epoch 22: val_loss did not improve from 22822.64453\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22147.2520 - rmse: 22146.2656 - val_loss: 23430.1562 - val_rmse: 23281.0508\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20872.5703 - rmse: 20873.9434\n",
      "Epoch 23: val_loss did not improve from 22822.64453\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20872.5703 - rmse: 20873.9434 - val_loss: 22971.2090 - val_rmse: 22752.8066\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21423.5527 - rmse: 21423.5527\n",
      "Epoch 24: val_loss improved from 22822.64453 to 21719.33398, saving model to ./ckpt/5_class_lr005/val_rmse_21621.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21257.9453 - rmse: 21194.5352 - val_loss: 21719.3340 - val_rmse: 21620.6191\n",
      "Epoch 25/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22509.5605 - rmse: 22509.5605\n",
      "Epoch 25: val_loss did not improve from 21719.33398\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22533.5996 - rmse: 22549.8086 - val_loss: 35270.3906 - val_rmse: 35179.6211\n",
      "Epoch 26/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23918.0801 - rmse: 23918.0801\n",
      "Epoch 26: val_loss did not improve from 21719.33398\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 23873.8984 - rmse: 23896.3184 - val_loss: 22558.8613 - val_rmse: 22543.2363\n",
      "Epoch 27/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20891.9512 - rmse: 20891.9512\n",
      "Epoch 27: val_loss did not improve from 21719.33398\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20865.0762 - rmse: 20910.0352 - val_loss: 22620.5410 - val_rmse: 22520.4492\n",
      "Epoch 28/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20747.8301 - rmse: 20747.8301\n",
      "Epoch 28: val_loss did not improve from 21719.33398\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20643.5605 - rmse: 20641.6836 - val_loss: 21897.1777 - val_rmse: 21798.2227\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19496.5137 - rmse: 19496.5137\n",
      "Epoch 29: val_loss did not improve from 21719.33398\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19513.6035 - rmse: 19525.1270 - val_loss: 31502.2305 - val_rmse: 31599.2031\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19717.5820 - rmse: 19717.5820\n",
      "Epoch 30: val_loss did not improve from 21719.33398\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19696.4258 - rmse: 19682.1582 - val_loss: 23962.8691 - val_rmse: 24078.2266\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20051.0215 - rmse: 20025.9570\n",
      "Epoch 31: val_loss improved from 21719.33398 to 21538.83594, saving model to ./ckpt/5_class_lr005/val_rmse_21480.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20051.0215 - rmse: 20025.9570 - val_loss: 21538.8359 - val_rmse: 21480.3457\n",
      "Epoch 32/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20435.7852 - rmse: 20435.7852\n",
      "Epoch 32: val_loss did not improve from 21538.83594\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20431.1719 - rmse: 20425.8145 - val_loss: 22314.9219 - val_rmse: 22254.8750\n",
      "Epoch 33/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21736.2344 - rmse: 21736.2344\n",
      "Epoch 33: val_loss did not improve from 21538.83594\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21667.9141 - rmse: 21614.7344 - val_loss: 22403.8574 - val_rmse: 22301.4492\n",
      "Epoch 34/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17566.4688 - rmse: 17566.4688\n",
      "Epoch 34: val_loss did not improve from 21538.83594\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17860.8809 - rmse: 17901.7266 - val_loss: 21556.4512 - val_rmse: 21384.6836\n",
      "Epoch 35/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19560.3789 - rmse: 19560.3789\n",
      "Epoch 35: val_loss did not improve from 21538.83594\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19559.9883 - rmse: 19559.7227 - val_loss: 30721.8770 - val_rmse: 30449.6289\n",
      "Epoch 36/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19155.7617 - rmse: 19155.7617\n",
      "Epoch 36: val_loss did not improve from 21538.83594\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19124.4023 - rmse: 19097.2773 - val_loss: 22001.9551 - val_rmse: 21901.3906\n",
      "Epoch 37/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18276.9258 - rmse: 18276.9258\n",
      "Epoch 37: val_loss did not improve from 21538.83594\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18154.3555 - rmse: 18108.3223 - val_loss: 21763.0918 - val_rmse: 21704.5566\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17879.7227 - rmse: 17879.7227\n",
      "Epoch 38: val_loss did not improve from 21538.83594\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17926.8945 - rmse: 17958.7090 - val_loss: 23073.1543 - val_rmse: 22975.4043\n",
      "Epoch 39/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18274.8203 - rmse: 18274.8203\n",
      "Epoch 39: val_loss did not improve from 21538.83594\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18276.5840 - rmse: 18302.1758 - val_loss: 29860.7148 - val_rmse: 29823.5273\n",
      "Epoch 40/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18098.8477 - rmse: 18098.8477\n",
      "Epoch 40: val_loss did not improve from 21538.83594\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18155.3711 - rmse: 18193.4941 - val_loss: 24198.8711 - val_rmse: 24202.5977\n",
      "Epoch 41/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17814.2793 - rmse: 17814.2793\n",
      "Epoch 41: val_loss improved from 21538.83594 to 21231.58008, saving model to ./ckpt/5_class_lr005/val_rmse_21106.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17812.3164 - rmse: 17800.7207 - val_loss: 21231.5801 - val_rmse: 21105.7539\n",
      "Epoch 42/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18343.5391 - rmse: 18343.5391\n",
      "Epoch 42: val_loss did not improve from 21231.58008\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18511.8047 - rmse: 18583.6074 - val_loss: 24912.6816 - val_rmse: 24827.4316\n",
      "Epoch 43/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21605.3984 - rmse: 21605.3984\n",
      "Epoch 43: val_loss improved from 21231.58008 to 20564.53906, saving model to ./ckpt/5_class_lr005/val_rmse_20467.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 21399.9805 - rmse: 21371.2266 - val_loss: 20564.5391 - val_rmse: 20466.7422\n",
      "Epoch 44/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21105.9355 - rmse: 21105.9355\n",
      "Epoch 44: val_loss did not improve from 20564.53906\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 21101.2812 - rmse: 21098.1406 - val_loss: 26085.0645 - val_rmse: 26120.5039\n",
      "Epoch 45/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18253.1641 - rmse: 18253.1641\n",
      "Epoch 45: val_loss did not improve from 20564.53906\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 18188.5156 - rmse: 18174.3047 - val_loss: 22008.4023 - val_rmse: 21928.8730\n",
      "Epoch 46/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17673.3359 - rmse: 17674.7578\n",
      "Epoch 46: val_loss did not improve from 20564.53906\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 17673.3359 - rmse: 17674.7578 - val_loss: 22278.6621 - val_rmse: 22178.4629\n",
      "Epoch 47/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21926.6660 - rmse: 21926.6660\n",
      "Epoch 47: val_loss did not improve from 20564.53906\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21717.1465 - rmse: 21677.4258 - val_loss: 24031.7656 - val_rmse: 23951.1484\n",
      "Epoch 48/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17540.4492 - rmse: 17540.4492\n",
      "Epoch 48: val_loss did not improve from 20564.53906\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17531.0117 - rmse: 17524.6484 - val_loss: 22103.0469 - val_rmse: 21969.8594\n",
      "Epoch 49/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18018.9707 - rmse: 17978.6250\n",
      "Epoch 49: val_loss did not improve from 20564.53906\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18018.9707 - rmse: 17978.6250 - val_loss: 21931.9004 - val_rmse: 21847.7129\n",
      "Epoch 50/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16307.1416 - rmse: 16307.1416\n",
      "Epoch 50: val_loss did not improve from 20564.53906\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16274.6807 - rmse: 16252.7861 - val_loss: 23653.1934 - val_rmse: 23644.5938\n",
      "Epoch 51/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17326.9727 - rmse: 17326.9727\n",
      "Epoch 51: val_loss did not improve from 20564.53906\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17319.8359 - rmse: 17315.0195 - val_loss: 22916.3242 - val_rmse: 22807.4629\n",
      "Epoch 52/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19293.2266 - rmse: 19263.7070\n",
      "Epoch 52: val_loss did not improve from 20564.53906\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19293.2266 - rmse: 19263.7070 - val_loss: 25995.9805 - val_rmse: 26128.0215\n",
      "Epoch 53/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15839.0664 - rmse: 15839.0664\n",
      "Epoch 53: val_loss did not improve from 20564.53906\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 15848.4473 - rmse: 15867.5127 - val_loss: 23678.3984 - val_rmse: 23759.7344\n",
      "Epoch 54/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16947.2500 - rmse: 16940.5430\n",
      "Epoch 54: val_loss did not improve from 20564.53906\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16947.2500 - rmse: 16940.5430 - val_loss: 22507.2207 - val_rmse: 22531.8633\n",
      "Epoch 55/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17012.8379 - rmse: 17012.8379\n",
      "Epoch 55: val_loss did not improve from 20564.53906\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16969.2695 - rmse: 16964.3496 - val_loss: 26463.3711 - val_rmse: 26456.2363\n",
      "Epoch 56/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18409.2812 - rmse: 18397.7676\n",
      "Epoch 56: val_loss improved from 20564.53906 to 20419.84375, saving model to ./ckpt/5_class_lr005/val_rmse_20311.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18409.2812 - rmse: 18397.7676 - val_loss: 20419.8438 - val_rmse: 20311.0391\n",
      "Epoch 57/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17173.4609 - rmse: 17173.4609\n",
      "Epoch 57: val_loss did not improve from 20419.84375\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17347.1777 - rmse: 17322.2480 - val_loss: 27762.0605 - val_rmse: 27786.3320\n",
      "Epoch 58/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16464.2754 - rmse: 16464.2754\n",
      "Epoch 58: val_loss did not improve from 20419.84375\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 16467.4453 - rmse: 16469.5840 - val_loss: 20478.4980 - val_rmse: 20426.3633\n",
      "Epoch 59/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17234.0840 - rmse: 17234.0840\n",
      "Epoch 59: val_loss did not improve from 20419.84375\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 17208.7559 - rmse: 17191.6719 - val_loss: 27822.1309 - val_rmse: 27846.6484\n",
      "Epoch 60/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16119.3506 - rmse: 16169.8252\n",
      "Epoch 60: val_loss did not improve from 20419.84375\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16119.3506 - rmse: 16169.8252 - val_loss: 24099.9805 - val_rmse: 24073.5879\n",
      "Epoch 61/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16393.8770 - rmse: 16437.9727\n",
      "Epoch 61: val_loss did not improve from 20419.84375\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16393.8770 - rmse: 16437.9727 - val_loss: 21936.4922 - val_rmse: 21896.1348\n",
      "Epoch 62/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17077.1191 - rmse: 17059.9551\n",
      "Epoch 62: val_loss did not improve from 20419.84375\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17077.1191 - rmse: 17059.9551 - val_loss: 21689.2773 - val_rmse: 21604.8203\n",
      "Epoch 63/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17520.2910 - rmse: 17512.5527\n",
      "Epoch 63: val_loss did not improve from 20419.84375\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17520.2910 - rmse: 17512.5527 - val_loss: 20650.7324 - val_rmse: 20591.3809\n",
      "Epoch 64/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17209.7930 - rmse: 17209.7930\n",
      "Epoch 64: val_loss did not improve from 20419.84375\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17162.2578 - rmse: 17165.5215 - val_loss: 25794.2812 - val_rmse: 25846.6602\n",
      "Epoch 65/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16016.1895 - rmse: 16016.1895\n",
      "Epoch 65: val_loss did not improve from 20419.84375\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15865.4785 - rmse: 15875.1396 - val_loss: 20772.5156 - val_rmse: 20757.5020\n",
      "Epoch 66/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16108.3438 - rmse: 16132.7393\n",
      "Epoch 66: val_loss did not improve from 20419.84375\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16108.3438 - rmse: 16132.7393 - val_loss: 27642.6016 - val_rmse: 27664.6641\n",
      "Epoch 67/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16021.8438 - rmse: 16031.0371\n",
      "Epoch 67: val_loss improved from 20419.84375 to 20110.94336, saving model to ./ckpt/5_class_lr005/val_rmse_20099.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16021.8438 - rmse: 16031.0371 - val_loss: 20110.9434 - val_rmse: 20099.4922\n",
      "Epoch 68/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16188.8682 - rmse: 16211.1445\n",
      "Epoch 68: val_loss did not improve from 20110.94336\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16188.8682 - rmse: 16211.1445 - val_loss: 27507.9922 - val_rmse: 27438.7852\n",
      "Epoch 69/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17250.5703 - rmse: 17225.3047\n",
      "Epoch 69: val_loss did not improve from 20110.94336\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17250.5703 - rmse: 17225.3047 - val_loss: 20462.9941 - val_rmse: 20379.7871\n",
      "Epoch 70/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18226.3691 - rmse: 18277.7793\n",
      "Epoch 70: val_loss did not improve from 20110.94336\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 18226.3691 - rmse: 18277.7793 - val_loss: 31156.2559 - val_rmse: 31178.9883\n",
      "Epoch 71/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16180.2119 - rmse: 16180.2119\n",
      "Epoch 71: val_loss did not improve from 20110.94336\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 16184.2539 - rmse: 16207.7266 - val_loss: 21225.9141 - val_rmse: 21229.9062\n",
      "Epoch 72/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14934.1504 - rmse: 14960.7705\n",
      "Epoch 72: val_loss did not improve from 20110.94336\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14934.1504 - rmse: 14960.7705 - val_loss: 21740.0957 - val_rmse: 21765.6270\n",
      "Epoch 73/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17020.5703 - rmse: 17020.5703\n",
      "Epoch 73: val_loss did not improve from 20110.94336\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16975.6328 - rmse: 16952.0508 - val_loss: 25037.5117 - val_rmse: 25014.9766\n",
      "Epoch 74/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15113.4951 - rmse: 15107.2480\n",
      "Epoch 74: val_loss did not improve from 20110.94336\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15113.4951 - rmse: 15107.2480 - val_loss: 20356.6562 - val_rmse: 20301.6055\n",
      "Epoch 75/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16008.7412 - rmse: 16008.7412\n",
      "Epoch 75: val_loss did not improve from 20110.94336\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16133.0615 - rmse: 16150.9736 - val_loss: 20730.8984 - val_rmse: 20700.8711\n",
      "Epoch 76/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 14975.6904 - rmse: 14975.6904\n",
      "Epoch 76: val_loss did not improve from 20110.94336\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14815.5283 - rmse: 14809.1709 - val_loss: 21038.7539 - val_rmse: 20968.0957\n",
      "Epoch 77/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14782.9697 - rmse: 14786.0264\n",
      "Epoch 77: val_loss did not improve from 20110.94336\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14782.9697 - rmse: 14786.0264 - val_loss: 22703.1426 - val_rmse: 22747.5820\n",
      "Epoch 78/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15195.3916 - rmse: 15194.1875\n",
      "Epoch 78: val_loss did not improve from 20110.94336\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15195.3916 - rmse: 15194.1875 - val_loss: 20749.3457 - val_rmse: 20718.9688\n",
      "Epoch 79/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15188.6738 - rmse: 15228.1875\n",
      "Epoch 79: val_loss did not improve from 20110.94336\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15188.6738 - rmse: 15228.1875 - val_loss: 20362.1465 - val_rmse: 20443.2383\n",
      "Epoch 80/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 14623.5928 - rmse: 14623.5928\n",
      "Epoch 80: val_loss did not improve from 20110.94336\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14607.6279 - rmse: 14646.6172 - val_loss: 22667.4961 - val_rmse: 22696.0645\n",
      "Epoch 81/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14074.5176 - rmse: 14055.5527\n",
      "Epoch 81: val_loss did not improve from 20110.94336\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14074.5176 - rmse: 14055.5527 - val_loss: 20371.5566 - val_rmse: 20348.0996\n",
      "Epoch 82/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 13615.8457 - rmse: 13615.8457\n",
      "Epoch 82: val_loss did not improve from 20110.94336\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 13657.1279 - rmse: 13661.7109 - val_loss: 21019.3535 - val_rmse: 21052.2422\n",
      "Epoch 83/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 13997.4229 - rmse: 13997.4229\n",
      "Epoch 83: val_loss improved from 20110.94336 to 20084.54297, saving model to ./ckpt/5_class_lr005/val_rmse_20072.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 13965.0059 - rmse: 13963.0684 - val_loss: 20084.5430 - val_rmse: 20072.1504\n",
      "Epoch 84/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 13711.4043 - rmse: 13740.1855\n",
      "Epoch 84: val_loss did not improve from 20084.54297\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 13711.4043 - rmse: 13740.1855 - val_loss: 22197.0410 - val_rmse: 22262.3711\n",
      "Epoch 85/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 14008.9756 - rmse: 14008.9756\n",
      "Epoch 85: val_loss did not improve from 20084.54297\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14144.4385 - rmse: 14171.5283 - val_loss: 22839.8477 - val_rmse: 22818.9277\n",
      "Epoch 86/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15111.9082 - rmse: 15111.9082\n",
      "Epoch 86: val_loss did not improve from 20084.54297\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 15050.1641 - rmse: 15008.5215 - val_loss: 21428.6777 - val_rmse: 21496.9316\n",
      "Epoch 87/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14652.1904 - rmse: 14660.3662\n",
      "Epoch 87: val_loss did not improve from 20084.54297\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14652.1904 - rmse: 14660.3662 - val_loss: 22243.7363 - val_rmse: 22330.0332\n",
      "Epoch 88/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 14688.6104 - rmse: 14688.6104\n",
      "Epoch 88: val_loss did not improve from 20084.54297\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 14762.4209 - rmse: 14816.0859 - val_loss: 31482.1973 - val_rmse: 31532.3750\n",
      "Epoch 89/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 14972.4736 - rmse: 14972.4736\n",
      "Epoch 89: val_loss did not improve from 20084.54297\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15066.3398 - rmse: 15080.3359 - val_loss: 23357.5723 - val_rmse: 23309.7031\n",
      "Epoch 90/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15400.7451 - rmse: 15400.7451\n",
      "Epoch 90: val_loss did not improve from 20084.54297\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15299.0361 - rmse: 15303.0068 - val_loss: 20609.4785 - val_rmse: 20599.4043\n",
      "Epoch 91/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 14241.1885 - rmse: 14241.1885\n",
      "Epoch 91: val_loss did not improve from 20084.54297\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14481.5088 - rmse: 14533.2393 - val_loss: 20865.1680 - val_rmse: 20857.2500\n",
      "Epoch 92/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 14012.4668 - rmse: 14012.4668\n",
      "Epoch 92: val_loss did not improve from 20084.54297\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14087.0811 - rmse: 14060.1758 - val_loss: 21184.5859 - val_rmse: 21145.5000\n",
      "Epoch 93/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14208.0225 - rmse: 14191.1211\n",
      "Epoch 93: val_loss improved from 20084.54297 to 20076.84570, saving model to ./ckpt/5_class_lr005/val_rmse_20051.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14208.0225 - rmse: 14191.1211 - val_loss: 20076.8457 - val_rmse: 20051.4746\n",
      "Epoch 94/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 14237.3350 - rmse: 14237.3350\n",
      "Epoch 94: val_loss did not improve from 20076.84570\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 14293.6270 - rmse: 14274.1963 - val_loss: 22975.7012 - val_rmse: 23032.1016\n",
      "Epoch 95/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 14310.8682 - rmse: 14310.8682\n",
      "Epoch 95: val_loss did not improve from 20076.84570\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 14334.5186 - rmse: 14372.0654 - val_loss: 20297.2305 - val_rmse: 20250.1855\n",
      "Epoch 96/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 14587.5439 - rmse: 14587.5439\n",
      "Epoch 96: val_loss did not improve from 20076.84570\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 14437.4160 - rmse: 14427.7334 - val_loss: 23293.2051 - val_rmse: 23348.8359\n",
      "Epoch 97/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14202.4531 - rmse: 14205.1426\n",
      "Epoch 97: val_loss did not improve from 20076.84570\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14202.4531 - rmse: 14205.1426 - val_loss: 22008.7188 - val_rmse: 22054.0117\n",
      "Epoch 98/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 14793.6611 - rmse: 14793.6611\n",
      "Epoch 98: val_loss did not improve from 20076.84570\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14885.3809 - rmse: 14888.0400 - val_loss: 20559.5684 - val_rmse: 20635.8242\n",
      "Epoch 99/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15439.5078 - rmse: 15439.5078\n",
      "Epoch 99: val_loss did not improve from 20076.84570\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15390.7539 - rmse: 15415.2129 - val_loss: 27582.9062 - val_rmse: 27699.7383\n",
      "Epoch 100/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 14629.4053 - rmse: 14629.4053\n",
      "Epoch 100: val_loss did not improve from 20076.84570\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14716.7842 - rmse: 14775.7158 - val_loss: 26761.7793 - val_rmse: 26784.4102\n",
      "Epoch 101/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 13816.1543 - rmse: 13816.1543\n",
      "Epoch 101: val_loss did not improve from 20076.84570\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 13820.2061 - rmse: 13819.7646 - val_loss: 21457.4785 - val_rmse: 21454.7168\n",
      "Epoch 102/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 13725.3604 - rmse: 13725.3604\n",
      "Epoch 102: val_loss did not improve from 20076.84570\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 13781.2070 - rmse: 13783.6553 - val_loss: 22317.2773 - val_rmse: 22374.2891\n",
      "Epoch 103/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 14331.0029 - rmse: 14331.0029\n",
      "Epoch 103: val_loss did not improve from 20076.84570\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14173.5947 - rmse: 14184.9336 - val_loss: 20651.1504 - val_rmse: 20646.5039\n",
      "Epoch 104/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 12424.6484 - rmse: 12455.2275\n",
      "Epoch 104: val_loss improved from 20076.84570 to 19931.51172, saving model to ./ckpt/5_class_lr005/val_rmse_19905.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 12424.6484 - rmse: 12455.2275 - val_loss: 19931.5117 - val_rmse: 19904.7344\n",
      "Epoch 105/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 14455.0488 - rmse: 14455.0488\n",
      "Epoch 105: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14432.8369 - rmse: 14444.7676 - val_loss: 20273.4863 - val_rmse: 20215.5234\n",
      "Epoch 106/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 13997.5635 - rmse: 13997.5635\n",
      "Epoch 106: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 14017.8555 - rmse: 14031.5400 - val_loss: 21101.5078 - val_rmse: 21063.1895\n",
      "Epoch 107/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 13397.8037 - rmse: 13397.8037\n",
      "Epoch 107: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 13550.9912 - rmse: 13549.8496 - val_loss: 24971.3184 - val_rmse: 25042.6055\n",
      "Epoch 108/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 13697.0928 - rmse: 13697.0928\n",
      "Epoch 108: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 13607.9434 - rmse: 13606.9639 - val_loss: 20364.8184 - val_rmse: 20309.9512\n",
      "Epoch 109/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 12762.8369 - rmse: 12762.8369\n",
      "Epoch 109: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 12773.2959 - rmse: 12803.9355 - val_loss: 20170.9824 - val_rmse: 20128.2656\n",
      "Epoch 110/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 13663.1250 - rmse: 13642.2295\n",
      "Epoch 110: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 13663.1250 - rmse: 13642.2295 - val_loss: 20295.8711 - val_rmse: 20289.4922\n",
      "Epoch 111/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 12127.4648 - rmse: 12127.4648\n",
      "Epoch 111: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 12149.4561 - rmse: 12134.8770 - val_loss: 19998.6074 - val_rmse: 19986.4824\n",
      "Epoch 112/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 12348.8389 - rmse: 12348.8389\n",
      "Epoch 112: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 12306.6709 - rmse: 12313.1602 - val_loss: 22284.6855 - val_rmse: 22306.6875\n",
      "Epoch 113/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 13684.1377 - rmse: 13684.1377\n",
      "Epoch 113: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 13789.4463 - rmse: 13795.2910 - val_loss: 21418.6602 - val_rmse: 21411.0547\n",
      "Epoch 114/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 13612.3359 - rmse: 13612.3359\n",
      "Epoch 114: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 13583.9414 - rmse: 13659.4502 - val_loss: 29229.1367 - val_rmse: 29253.8672\n",
      "Epoch 115/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15117.7471 - rmse: 15117.7471\n",
      "Epoch 115: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15259.7344 - rmse: 15266.7432 - val_loss: 21520.9395 - val_rmse: 21561.6406\n",
      "Epoch 116/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 14253.4736 - rmse: 14253.4736\n",
      "Epoch 116: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14240.9062 - rmse: 14232.4307 - val_loss: 21545.4375 - val_rmse: 21467.2012\n",
      "Epoch 117/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 12362.4404 - rmse: 12362.4404\n",
      "Epoch 117: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 12355.5859 - rmse: 12348.3975 - val_loss: 21417.6230 - val_rmse: 21408.7363\n",
      "Epoch 118/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 12397.8174 - rmse: 12397.8174\n",
      "Epoch 118: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 12358.8818 - rmse: 12336.5000 - val_loss: 21165.8730 - val_rmse: 21204.9961\n",
      "Epoch 119/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 12330.5029 - rmse: 12330.5029\n",
      "Epoch 119: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 12305.5879 - rmse: 12288.7842 - val_loss: 21520.7109 - val_rmse: 21543.0957\n",
      "Epoch 120/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 12975.1426 - rmse: 12975.1426\n",
      "Epoch 120: val_loss did not improve from 19931.51172\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 13270.5674 - rmse: 13253.2979 - val_loss: 23580.8008 - val_rmse: 23455.4668\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:09:32.644967: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 89149.6328 - rmse: 88864.7578\n",
      "Epoch 1: val_loss improved from inf to 46789.15234, saving model to ./ckpt/5_class_lr005/val_rmse_47113.hdf5\n",
      "70/70 [==============================] - 2s 15ms/step - loss: 89149.6328 - rmse: 88864.7578 - val_loss: 46789.1523 - val_rmse: 47113.0938\n",
      "Epoch 2/120\n",
      " 6/70 [=>............................] - ETA: 0s - loss: 47093.0234 - rmse: 47093.0234"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:09:33.783676: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 43444.5820 - rmse: 43444.5820\n",
      "Epoch 2: val_loss improved from 46789.15234 to 42837.86328, saving model to ./ckpt/5_class_lr005/val_rmse_43061.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 43392.7656 - rmse: 43357.8164 - val_loss: 42837.8633 - val_rmse: 43060.8867\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 39379.3008 - rmse: 39379.3008\n",
      "Epoch 3: val_loss improved from 42837.86328 to 39060.05078, saving model to ./ckpt/5_class_lr005/val_rmse_39254.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 39312.0938 - rmse: 39266.7656 - val_loss: 39060.0508 - val_rmse: 39253.9922\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 35430.6836 - rmse: 35430.6836\n",
      "Epoch 4: val_loss did not improve from 39060.05078\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 35480.5078 - rmse: 35478.2500 - val_loss: 41593.6133 - val_rmse: 41735.6641\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 33794.8906 - rmse: 33734.5898\n",
      "Epoch 5: val_loss improved from 39060.05078 to 36432.19141, saving model to ./ckpt/5_class_lr005/val_rmse_36582.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 33794.8906 - rmse: 33734.5898 - val_loss: 36432.1914 - val_rmse: 36582.3164\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 30169.3105 - rmse: 30169.3105\n",
      "Epoch 6: val_loss improved from 36432.19141 to 34372.18750, saving model to ./ckpt/5_class_lr005/val_rmse_34465.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 30218.3281 - rmse: 30240.5254 - val_loss: 34372.1875 - val_rmse: 34465.3633\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 32785.7031 - rmse: 32785.7031\n",
      "Epoch 7: val_loss improved from 34372.18750 to 34268.16016, saving model to ./ckpt/5_class_lr005/val_rmse_34372.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 32902.6562 - rmse: 32981.5352 - val_loss: 34268.1602 - val_rmse: 34371.5312\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 34332.7695 - rmse: 34433.5352\n",
      "Epoch 8: val_loss improved from 34268.16016 to 31135.99609, saving model to ./ckpt/5_class_lr005/val_rmse_31301.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 34332.7695 - rmse: 34433.5352 - val_loss: 31135.9961 - val_rmse: 31301.4258\n",
      "Epoch 9/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 29670.5352 - rmse: 29670.5352\n",
      "Epoch 9: val_loss did not improve from 31135.99609\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 29989.3457 - rmse: 29989.9785 - val_loss: 33051.0859 - val_rmse: 33245.3477\n",
      "Epoch 10/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 29077.8145 - rmse: 29077.8145\n",
      "Epoch 10: val_loss did not improve from 31135.99609\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 29294.8750 - rmse: 29292.7852 - val_loss: 38341.3008 - val_rmse: 38468.4805\n",
      "Epoch 11/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 36441.6445 - rmse: 36441.6445\n",
      "Epoch 11: val_loss did not improve from 31135.99609\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 35591.9570 - rmse: 35437.6094 - val_loss: 33216.4180 - val_rmse: 33393.5859\n",
      "Epoch 12/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 26796.2051 - rmse: 26796.2051\n",
      "Epoch 12: val_loss improved from 31135.99609 to 27301.50781, saving model to ./ckpt/5_class_lr005/val_rmse_27481.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 26866.1094 - rmse: 26858.9297 - val_loss: 27301.5078 - val_rmse: 27481.2871\n",
      "Epoch 13/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27873.5664 - rmse: 27873.5664\n",
      "Epoch 13: val_loss did not improve from 27301.50781\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 27991.7480 - rmse: 28026.4922 - val_loss: 29698.9766 - val_rmse: 29867.0703\n",
      "Epoch 14/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 26626.7676 - rmse: 26626.7676\n",
      "Epoch 14: val_loss did not improve from 27301.50781\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 26539.9980 - rmse: 26519.9297 - val_loss: 28731.3770 - val_rmse: 28902.5977\n",
      "Epoch 15/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 24267.6426 - rmse: 24267.6426\n",
      "Epoch 15: val_loss improved from 27301.50781 to 26547.85156, saving model to ./ckpt/5_class_lr005/val_rmse_26704.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 24480.4043 - rmse: 24444.9746 - val_loss: 26547.8516 - val_rmse: 26704.2461\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22647.9961 - rmse: 22647.9961\n",
      "Epoch 16: val_loss did not improve from 26547.85156\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22684.3398 - rmse: 22675.9707 - val_loss: 32931.1016 - val_rmse: 33180.9570\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22409.2793 - rmse: 22409.7148\n",
      "Epoch 17: val_loss did not improve from 26547.85156\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 22409.2793 - rmse: 22409.7148 - val_loss: 28485.8750 - val_rmse: 28755.0254\n",
      "Epoch 18/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 24804.7539 - rmse: 24804.7539\n",
      "Epoch 18: val_loss did not improve from 26547.85156\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 25055.6387 - rmse: 25161.4941 - val_loss: 27060.7891 - val_rmse: 27179.1660\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23310.2754 - rmse: 23310.2754\n",
      "Epoch 19: val_loss did not improve from 26547.85156\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23311.8418 - rmse: 23310.4707 - val_loss: 36878.0469 - val_rmse: 37154.8008\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 26433.3594 - rmse: 26433.3594\n",
      "Epoch 20: val_loss did not improve from 26547.85156\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 26393.5527 - rmse: 26412.2949 - val_loss: 34506.4922 - val_rmse: 34579.6016\n",
      "Epoch 21/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 26569.4395 - rmse: 26569.4395\n",
      "Epoch 21: val_loss did not improve from 26547.85156\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 26629.4258 - rmse: 26669.8828 - val_loss: 27640.4746 - val_rmse: 27790.0098\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21045.3730 - rmse: 21055.5762\n",
      "Epoch 22: val_loss did not improve from 26547.85156\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21045.3730 - rmse: 21055.5762 - val_loss: 26630.1641 - val_rmse: 26848.6992\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23122.4336 - rmse: 23122.4336\n",
      "Epoch 23: val_loss did not improve from 26547.85156\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23163.6270 - rmse: 23164.8711 - val_loss: 29521.1328 - val_rmse: 29763.1055\n",
      "Epoch 24/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21832.6875 - rmse: 21832.6875\n",
      "Epoch 24: val_loss did not improve from 26547.85156\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21859.5215 - rmse: 21818.5586 - val_loss: 28581.1348 - val_rmse: 28795.3301\n",
      "Epoch 25/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21753.9160 - rmse: 21753.9160\n",
      "Epoch 25: val_loss improved from 26547.85156 to 25083.44336, saving model to ./ckpt/5_class_lr005/val_rmse_25244.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21566.3223 - rmse: 21523.8438 - val_loss: 25083.4434 - val_rmse: 25243.5176\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21766.3184 - rmse: 21766.3184\n",
      "Epoch 26: val_loss did not improve from 25083.44336\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21706.1094 - rmse: 21665.5000 - val_loss: 28432.3262 - val_rmse: 28590.0078\n",
      "Epoch 27/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20249.8633 - rmse: 20261.5195\n",
      "Epoch 27: val_loss did not improve from 25083.44336\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20249.8633 - rmse: 20261.5195 - val_loss: 25315.1074 - val_rmse: 25359.0938\n",
      "Epoch 28/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20690.2441 - rmse: 20690.2441\n",
      "Epoch 28: val_loss did not improve from 25083.44336\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20634.0000 - rmse: 20612.3301 - val_loss: 34941.2773 - val_rmse: 35178.1875\n",
      "Epoch 29/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20181.3867 - rmse: 20230.1582\n",
      "Epoch 29: val_loss improved from 25083.44336 to 24170.21094, saving model to ./ckpt/5_class_lr005/val_rmse_24200.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20181.3867 - rmse: 20230.1582 - val_loss: 24170.2109 - val_rmse: 24199.9473\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19103.9668 - rmse: 19103.9668\n",
      "Epoch 30: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19177.7500 - rmse: 19227.5117 - val_loss: 26921.2832 - val_rmse: 27103.9043\n",
      "Epoch 31/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20774.1055 - rmse: 20774.1055\n",
      "Epoch 31: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20753.7930 - rmse: 20740.0918 - val_loss: 34609.9102 - val_rmse: 34789.6680\n",
      "Epoch 32/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18393.3359 - rmse: 18393.3359\n",
      "Epoch 32: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18429.4629 - rmse: 18453.6582 - val_loss: 24485.4473 - val_rmse: 24617.2715\n",
      "Epoch 33/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20633.4727 - rmse: 20633.4727\n",
      "Epoch 33: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20564.1289 - rmse: 20517.3594 - val_loss: 29926.1270 - val_rmse: 30097.6172\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20082.7129 - rmse: 20057.9238\n",
      "Epoch 34: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20082.7129 - rmse: 20057.9238 - val_loss: 25398.5566 - val_rmse: 25387.5527\n",
      "Epoch 35/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19077.3164 - rmse: 19077.3164\n",
      "Epoch 35: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19025.9277 - rmse: 19012.1270 - val_loss: 25379.3770 - val_rmse: 25555.8555\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20871.5098 - rmse: 20893.9434\n",
      "Epoch 36: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20871.5098 - rmse: 20893.9434 - val_loss: 25078.5312 - val_rmse: 25169.6309\n",
      "Epoch 37/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20822.3828 - rmse: 20822.3828\n",
      "Epoch 37: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21002.4648 - rmse: 20958.5820 - val_loss: 29799.4785 - val_rmse: 29924.0566\n",
      "Epoch 38/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20616.2852 - rmse: 20616.2852\n",
      "Epoch 38: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20592.4570 - rmse: 20628.6699 - val_loss: 27307.7168 - val_rmse: 27413.8770\n",
      "Epoch 39/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19429.3398 - rmse: 19391.7051\n",
      "Epoch 39: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19429.3398 - rmse: 19391.7051 - val_loss: 24940.3457 - val_rmse: 25022.8926\n",
      "Epoch 40/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20827.4961 - rmse: 20827.4961\n",
      "Epoch 40: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20710.3301 - rmse: 20712.1543 - val_loss: 28909.8828 - val_rmse: 28902.2930\n",
      "Epoch 41/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20894.7129 - rmse: 20894.7129\n",
      "Epoch 41: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20879.2871 - rmse: 20924.7324 - val_loss: 27246.0859 - val_rmse: 27197.4629\n",
      "Epoch 42/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19623.0547 - rmse: 19589.9473\n",
      "Epoch 42: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19623.0547 - rmse: 19589.9473 - val_loss: 26556.7793 - val_rmse: 26729.3320\n",
      "Epoch 43/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17376.1328 - rmse: 17376.1328\n",
      "Epoch 43: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17587.1699 - rmse: 17675.2402 - val_loss: 30567.0977 - val_rmse: 30705.8496\n",
      "Epoch 44/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19115.9023 - rmse: 19115.9023\n",
      "Epoch 44: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19246.4434 - rmse: 19254.2617 - val_loss: 33317.6602 - val_rmse: 33468.0508\n",
      "Epoch 45/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18685.6875 - rmse: 18685.6875\n",
      "Epoch 45: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18498.1836 - rmse: 18452.6797 - val_loss: 25443.4160 - val_rmse: 25572.7500\n",
      "Epoch 46/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19063.2637 - rmse: 19063.2637\n",
      "Epoch 46: val_loss did not improve from 24170.21094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19127.2012 - rmse: 19145.7949 - val_loss: 24569.2871 - val_rmse: 24654.7383\n",
      "Epoch 47/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17570.3750 - rmse: 17570.3750\n",
      "Epoch 47: val_loss improved from 24170.21094 to 24064.35352, saving model to ./ckpt/5_class_lr005/val_rmse_24074.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17518.4629 - rmse: 17499.5410 - val_loss: 24064.3535 - val_rmse: 24073.5293\n",
      "Epoch 48/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18107.3594 - rmse: 18135.0391\n",
      "Epoch 48: val_loss did not improve from 24064.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18107.3594 - rmse: 18135.0391 - val_loss: 27401.4766 - val_rmse: 27530.0977\n",
      "Epoch 49/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17536.4648 - rmse: 17536.4648\n",
      "Epoch 49: val_loss improved from 24064.35352 to 23086.45508, saving model to ./ckpt/5_class_lr005/val_rmse_23118.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17491.8164 - rmse: 17477.5508 - val_loss: 23086.4551 - val_rmse: 23118.3086\n",
      "Epoch 50/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18074.6680 - rmse: 18074.6680\n",
      "Epoch 50: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 18091.9844 - rmse: 18103.6621 - val_loss: 24114.7812 - val_rmse: 24141.6055\n",
      "Epoch 51/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17909.0332 - rmse: 17909.0332\n",
      "Epoch 51: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17946.4141 - rmse: 18026.8203 - val_loss: 23167.2148 - val_rmse: 23200.2012\n",
      "Epoch 52/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17929.4902 - rmse: 17929.4902\n",
      "Epoch 52: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17820.0293 - rmse: 17800.2070 - val_loss: 24771.9199 - val_rmse: 24834.8047\n",
      "Epoch 53/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18174.1855 - rmse: 18166.1309\n",
      "Epoch 53: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18174.1855 - rmse: 18166.1309 - val_loss: 27270.1992 - val_rmse: 27371.2031\n",
      "Epoch 54/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17103.1094 - rmse: 17103.1094\n",
      "Epoch 54: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17409.8398 - rmse: 17428.2168 - val_loss: 24327.4961 - val_rmse: 24292.4961\n",
      "Epoch 55/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18016.3145 - rmse: 18016.3145\n",
      "Epoch 55: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18207.5215 - rmse: 18217.6055 - val_loss: 31154.6367 - val_rmse: 31314.9688\n",
      "Epoch 56/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19450.3770 - rmse: 19450.3770\n",
      "Epoch 56: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19578.5566 - rmse: 19564.5742 - val_loss: 28610.2383 - val_rmse: 28822.0078\n",
      "Epoch 57/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17654.1250 - rmse: 17654.1250\n",
      "Epoch 57: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17665.1523 - rmse: 17672.5898 - val_loss: 24585.3613 - val_rmse: 24657.1035\n",
      "Epoch 58/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16882.1504 - rmse: 16882.1504\n",
      "Epoch 58: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16857.3496 - rmse: 16868.2539 - val_loss: 28380.5391 - val_rmse: 28544.2246\n",
      "Epoch 59/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16086.1201 - rmse: 16086.1201\n",
      "Epoch 59: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16103.9736 - rmse: 16100.4033 - val_loss: 24066.2734 - val_rmse: 24086.9980\n",
      "Epoch 60/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17233.3730 - rmse: 17233.3730\n",
      "Epoch 60: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17104.5723 - rmse: 17087.2129 - val_loss: 24901.6660 - val_rmse: 24971.7441\n",
      "Epoch 61/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16876.3027 - rmse: 16876.3027\n",
      "Epoch 61: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16853.2285 - rmse: 16830.3105 - val_loss: 24719.7051 - val_rmse: 24836.9746\n",
      "Epoch 62/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16404.3809 - rmse: 16404.3809\n",
      "Epoch 62: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16651.1719 - rmse: 16683.2051 - val_loss: 32151.2520 - val_rmse: 32277.4922\n",
      "Epoch 63/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16916.4512 - rmse: 16916.4512\n",
      "Epoch 63: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17286.2305 - rmse: 17351.0996 - val_loss: 24595.3008 - val_rmse: 24518.2480\n",
      "Epoch 64/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18302.4355 - rmse: 18302.4355\n",
      "Epoch 64: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18205.9199 - rmse: 18177.8066 - val_loss: 28955.9902 - val_rmse: 29016.5879\n",
      "Epoch 65/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16048.1982 - rmse: 16080.5430\n",
      "Epoch 65: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16048.1982 - rmse: 16080.5430 - val_loss: 23457.9336 - val_rmse: 23441.1270\n",
      "Epoch 66/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16311.4043 - rmse: 16311.4043\n",
      "Epoch 66: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16468.8848 - rmse: 16531.2598 - val_loss: 26847.0605 - val_rmse: 26772.7930\n",
      "Epoch 67/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17403.4551 - rmse: 17403.4551\n",
      "Epoch 67: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17368.0215 - rmse: 17348.1289 - val_loss: 35189.7422 - val_rmse: 35316.3672\n",
      "Epoch 68/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16993.0977 - rmse: 16993.0977\n",
      "Epoch 68: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17066.3789 - rmse: 17059.3457 - val_loss: 23810.6934 - val_rmse: 23772.4727\n",
      "Epoch 69/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17154.4160 - rmse: 17154.4160\n",
      "Epoch 69: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17161.9727 - rmse: 17183.1289 - val_loss: 25750.1973 - val_rmse: 25896.1230\n",
      "Epoch 70/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15043.0557 - rmse: 15043.0557\n",
      "Epoch 70: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15082.2617 - rmse: 15093.9248 - val_loss: 24587.1777 - val_rmse: 24558.0820\n",
      "Epoch 71/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16367.0918 - rmse: 16367.0918\n",
      "Epoch 71: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16429.5137 - rmse: 16471.5527 - val_loss: 26515.3594 - val_rmse: 26672.1562\n",
      "Epoch 72/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15196.5049 - rmse: 15196.5049\n",
      "Epoch 72: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15455.9375 - rmse: 15497.8643 - val_loss: 27542.0723 - val_rmse: 27701.9551\n",
      "Epoch 73/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16670.1250 - rmse: 16670.1250\n",
      "Epoch 73: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16565.2676 - rmse: 16562.3301 - val_loss: 23653.6309 - val_rmse: 23735.8516\n",
      "Epoch 74/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15133.2744 - rmse: 15133.2744\n",
      "Epoch 74: val_loss did not improve from 23086.45508\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 14983.4639 - rmse: 14973.8164 - val_loss: 25509.4512 - val_rmse: 25646.4883\n",
      "Epoch 74: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 53s - loss: 164029.7031 - rmse: 164029.7031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:10:37.450467: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 82702.7812 - rmse: 82582.6641\n",
      "Epoch 1: val_loss improved from inf to 51466.11719, saving model to ./ckpt/5_class_lr005/val_rmse_51142.hdf5\n",
      "70/70 [==============================] - 2s 16ms/step - loss: 82702.7812 - rmse: 82582.6641 - val_loss: 51466.1172 - val_rmse: 51141.7227\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 40476.0469 - rmse: 40476.0469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:10:38.566858: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 45969.0625 - rmse: 45969.0625\n",
      "Epoch 2: val_loss improved from 51466.11719 to 47407.02734, saving model to ./ckpt/5_class_lr005/val_rmse_47053.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 45290.4531 - rmse: 45197.3203 - val_loss: 47407.0273 - val_rmse: 47053.2148\n",
      "Epoch 3/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 38037.9688 - rmse: 37968.3008\n",
      "Epoch 3: val_loss improved from 47407.02734 to 43408.03906, saving model to ./ckpt/5_class_lr005/val_rmse_43193.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 38037.9688 - rmse: 37968.3008 - val_loss: 43408.0391 - val_rmse: 43193.3125\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 36893.5000 - rmse: 36893.5000\n",
      "Epoch 4: val_loss improved from 43408.03906 to 40963.35547, saving model to ./ckpt/5_class_lr005/val_rmse_40826.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 36690.7656 - rmse: 36732.4688 - val_loss: 40963.3555 - val_rmse: 40825.6641\n",
      "Epoch 5/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 32389.9844 - rmse: 32389.9844\n",
      "Epoch 5: val_loss improved from 40963.35547 to 40524.29297, saving model to ./ckpt/5_class_lr005/val_rmse_40297.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 32034.2285 - rmse: 31965.8750 - val_loss: 40524.2930 - val_rmse: 40297.1992\n",
      "Epoch 6/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 32819.4023 - rmse: 32819.4023\n",
      "Epoch 6: val_loss improved from 40524.29297 to 38805.37109, saving model to ./ckpt/5_class_lr005/val_rmse_38691.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 32877.0625 - rmse: 32893.2383 - val_loss: 38805.3711 - val_rmse: 38691.3750\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 29559.9531 - rmse: 29505.5859\n",
      "Epoch 7: val_loss did not improve from 38805.37109\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 29559.9531 - rmse: 29505.5859 - val_loss: 45060.3164 - val_rmse: 45009.8906\n",
      "Epoch 8/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 31733.3711 - rmse: 31733.3711\n",
      "Epoch 8: val_loss improved from 38805.37109 to 38005.89062, saving model to ./ckpt/5_class_lr005/val_rmse_37821.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 31317.5156 - rmse: 31249.4434 - val_loss: 38005.8906 - val_rmse: 37821.3125\n",
      "Epoch 9/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27746.4492 - rmse: 27746.4492\n",
      "Epoch 9: val_loss improved from 38005.89062 to 36403.66406, saving model to ./ckpt/5_class_lr005/val_rmse_36106.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 27926.0957 - rmse: 27926.8457 - val_loss: 36403.6641 - val_rmse: 36105.6406\n",
      "Epoch 10/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 25747.8301 - rmse: 25747.8301\n",
      "Epoch 10: val_loss improved from 36403.66406 to 32682.49414, saving model to ./ckpt/5_class_lr005/val_rmse_32406.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 25744.6328 - rmse: 25721.6680 - val_loss: 32682.4941 - val_rmse: 32406.0098\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25916.3223 - rmse: 25884.7832\n",
      "Epoch 11: val_loss did not improve from 32682.49414\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 25916.3223 - rmse: 25884.7832 - val_loss: 34216.6797 - val_rmse: 33903.3438\n",
      "Epoch 12/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 24864.3711 - rmse: 24864.3711\n",
      "Epoch 12: val_loss did not improve from 32682.49414\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 24678.3301 - rmse: 24650.9102 - val_loss: 36386.6172 - val_rmse: 36106.2227\n",
      "Epoch 13/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24978.3691 - rmse: 24978.3691\n",
      "Epoch 13: val_loss did not improve from 32682.49414\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 25141.0410 - rmse: 25123.9102 - val_loss: 35062.6367 - val_rmse: 34805.8008\n",
      "Epoch 14/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 24338.1973 - rmse: 24338.1973\n",
      "Epoch 14: val_loss improved from 32682.49414 to 32281.47070, saving model to ./ckpt/5_class_lr005/val_rmse_32000.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 24393.2656 - rmse: 24430.4062 - val_loss: 32281.4707 - val_rmse: 32000.3398\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 26875.4746 - rmse: 26875.4746\n",
      "Epoch 15: val_loss improved from 32281.47070 to 30540.32031, saving model to ./ckpt/5_class_lr005/val_rmse_30297.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 26850.5605 - rmse: 26833.7578 - val_loss: 30540.3203 - val_rmse: 30297.4609\n",
      "Epoch 16/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 28335.6836 - rmse: 28297.4668\n",
      "Epoch 16: val_loss did not improve from 30540.32031\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 28335.6836 - rmse: 28297.4668 - val_loss: 32264.8652 - val_rmse: 32072.3906\n",
      "Epoch 17/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25314.0762 - rmse: 25314.0762\n",
      "Epoch 17: val_loss improved from 30540.32031 to 29666.71094, saving model to ./ckpt/5_class_lr005/val_rmse_29483.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 25450.3789 - rmse: 25446.1992 - val_loss: 29666.7109 - val_rmse: 29482.8184\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21836.4902 - rmse: 21836.4902\n",
      "Epoch 18: val_loss improved from 29666.71094 to 28200.77539, saving model to ./ckpt/5_class_lr005/val_rmse_27919.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22273.7246 - rmse: 22287.9297 - val_loss: 28200.7754 - val_rmse: 27919.2422\n",
      "Epoch 19/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21521.2754 - rmse: 21521.2754\n",
      "Epoch 19: val_loss did not improve from 28200.77539\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 21532.0723 - rmse: 21510.9844 - val_loss: 28525.6328 - val_rmse: 28285.3105\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20774.6172 - rmse: 20774.6172\n",
      "Epoch 20: val_loss did not improve from 28200.77539\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20795.8477 - rmse: 20811.8320 - val_loss: 30694.5508 - val_rmse: 30486.4277\n",
      "Epoch 21/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23365.3750 - rmse: 23365.3750\n",
      "Epoch 21: val_loss did not improve from 28200.77539\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23294.8984 - rmse: 23259.8594 - val_loss: 28632.3027 - val_rmse: 28428.1367\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20755.3770 - rmse: 20755.3770\n",
      "Epoch 22: val_loss did not improve from 28200.77539\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20760.8340 - rmse: 20779.9766 - val_loss: 28409.8418 - val_rmse: 28178.4766\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21170.0176 - rmse: 21155.8398\n",
      "Epoch 23: val_loss improved from 28200.77539 to 27673.58789, saving model to ./ckpt/5_class_lr005/val_rmse_27439.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 21170.0176 - rmse: 21155.8398 - val_loss: 27673.5879 - val_rmse: 27438.5078\n",
      "Epoch 24/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21271.9707 - rmse: 21301.2246\n",
      "Epoch 24: val_loss did not improve from 27673.58789\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21271.9707 - rmse: 21301.2246 - val_loss: 33448.1289 - val_rmse: 33307.6250\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24679.1816 - rmse: 24600.8809\n",
      "Epoch 25: val_loss did not improve from 27673.58789\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 24679.1816 - rmse: 24600.8809 - val_loss: 27822.7422 - val_rmse: 27586.8281\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19548.3379 - rmse: 19532.0430\n",
      "Epoch 26: val_loss improved from 27673.58789 to 26914.38281, saving model to ./ckpt/5_class_lr005/val_rmse_26694.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19548.3379 - rmse: 19532.0430 - val_loss: 26914.3828 - val_rmse: 26694.4902\n",
      "Epoch 27/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22390.3203 - rmse: 22390.3203\n",
      "Epoch 27: val_loss improved from 26914.38281 to 26761.07617, saving model to ./ckpt/5_class_lr005/val_rmse_26547.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22356.1855 - rmse: 22333.1641 - val_loss: 26761.0762 - val_rmse: 26547.1699\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18857.9902 - rmse: 18857.9902\n",
      "Epoch 28: val_loss did not improve from 26761.07617\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18847.7285 - rmse: 18840.8086 - val_loss: 29572.5312 - val_rmse: 29297.5352\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19829.7344 - rmse: 19829.7344\n",
      "Epoch 29: val_loss did not improve from 26761.07617\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19795.2949 - rmse: 19772.0664 - val_loss: 29997.2969 - val_rmse: 29808.2422\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19940.1758 - rmse: 19940.1758\n",
      "Epoch 30: val_loss did not improve from 26761.07617\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19912.2012 - rmse: 19893.3320 - val_loss: 31406.4824 - val_rmse: 31141.6289\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23458.8906 - rmse: 23457.7441\n",
      "Epoch 31: val_loss did not improve from 26761.07617\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23458.8906 - rmse: 23457.7441 - val_loss: 32263.1309 - val_rmse: 32127.5586\n",
      "Epoch 32/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20689.2070 - rmse: 20689.2070\n",
      "Epoch 32: val_loss did not improve from 26761.07617\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20623.9121 - rmse: 20587.9727 - val_loss: 27341.7305 - val_rmse: 27116.5684\n",
      "Epoch 33/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18051.0957 - rmse: 18051.0957\n",
      "Epoch 33: val_loss did not improve from 26761.07617\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18068.4258 - rmse: 18118.4258 - val_loss: 39814.3242 - val_rmse: 39505.1094\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20679.7070 - rmse: 20691.3965\n",
      "Epoch 34: val_loss did not improve from 26761.07617\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20679.7070 - rmse: 20691.3965 - val_loss: 28113.0234 - val_rmse: 27894.1328\n",
      "Epoch 35/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18366.1602 - rmse: 18366.1602\n",
      "Epoch 35: val_loss did not improve from 26761.07617\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 18744.6445 - rmse: 18727.8047 - val_loss: 27126.7461 - val_rmse: 26960.6133\n",
      "Epoch 36/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20730.2617 - rmse: 20730.2617\n",
      "Epoch 36: val_loss did not improve from 26761.07617\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20737.5078 - rmse: 20742.3945 - val_loss: 27795.6328 - val_rmse: 27565.9473\n",
      "Epoch 37/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17244.0391 - rmse: 17244.0391\n",
      "Epoch 37: val_loss did not improve from 26761.07617\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17322.8457 - rmse: 17358.6934 - val_loss: 29063.1152 - val_rmse: 28892.3789\n",
      "Epoch 38/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20170.3047 - rmse: 20170.3047\n",
      "Epoch 38: val_loss did not improve from 26761.07617\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20091.5020 - rmse: 20094.3242 - val_loss: 26926.4473 - val_rmse: 26675.4805\n",
      "Epoch 39/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18215.5918 - rmse: 18215.5918\n",
      "Epoch 39: val_loss did not improve from 26761.07617\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18246.8926 - rmse: 18305.6621 - val_loss: 30975.0859 - val_rmse: 30872.9219\n",
      "Epoch 40/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19787.3809 - rmse: 19787.3809\n",
      "Epoch 40: val_loss improved from 26761.07617 to 26572.92969, saving model to ./ckpt/5_class_lr005/val_rmse_26369.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19716.5137 - rmse: 19678.4531 - val_loss: 26572.9297 - val_rmse: 26369.0996\n",
      "Epoch 41/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18815.8730 - rmse: 18815.8730\n",
      "Epoch 41: val_loss improved from 26572.92969 to 26227.36133, saving model to ./ckpt/5_class_lr005/val_rmse_26006.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18588.8633 - rmse: 18523.0918 - val_loss: 26227.3613 - val_rmse: 26006.1406\n",
      "Epoch 42/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19197.6797 - rmse: 19197.6797\n",
      "Epoch 42: val_loss did not improve from 26227.36133\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19260.5918 - rmse: 19331.4160 - val_loss: 34699.8633 - val_rmse: 34579.7383\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18433.8047 - rmse: 18433.8047\n",
      "Epoch 43: val_loss did not improve from 26227.36133\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18419.0918 - rmse: 18409.1680 - val_loss: 27845.5605 - val_rmse: 27609.5195\n",
      "Epoch 44/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17812.0996 - rmse: 17789.8242\n",
      "Epoch 44: val_loss improved from 26227.36133 to 25993.23633, saving model to ./ckpt/5_class_lr005/val_rmse_25774.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17812.0996 - rmse: 17789.8242 - val_loss: 25993.2363 - val_rmse: 25774.4707\n",
      "Epoch 45/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17798.6230 - rmse: 17798.6230\n",
      "Epoch 45: val_loss did not improve from 25993.23633\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18026.2891 - rmse: 18049.7832 - val_loss: 35720.2734 - val_rmse: 35638.4609\n",
      "Epoch 46/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17878.4453 - rmse: 17870.8457\n",
      "Epoch 46: val_loss improved from 25993.23633 to 25935.12500, saving model to ./ckpt/5_class_lr005/val_rmse_25759.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17878.4453 - rmse: 17870.8457 - val_loss: 25935.1250 - val_rmse: 25758.6621\n",
      "Epoch 47/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17615.6953 - rmse: 17615.6953\n",
      "Epoch 47: val_loss did not improve from 25935.12500\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 17947.5195 - rmse: 18005.8945 - val_loss: 26897.0039 - val_rmse: 26713.5977\n",
      "Epoch 48/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18426.0840 - rmse: 18426.0840\n",
      "Epoch 48: val_loss did not improve from 25935.12500\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18499.8359 - rmse: 18549.5742 - val_loss: 42381.7344 - val_rmse: 42166.1523\n",
      "Epoch 49/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18255.2695 - rmse: 18255.2695\n",
      "Epoch 49: val_loss improved from 25935.12500 to 25839.91797, saving model to ./ckpt/5_class_lr005/val_rmse_25657.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18406.8984 - rmse: 18469.3477 - val_loss: 25839.9180 - val_rmse: 25657.0684\n",
      "Epoch 50/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17290.5195 - rmse: 17290.5195\n",
      "Epoch 50: val_loss improved from 25839.91797 to 25580.64648, saving model to ./ckpt/5_class_lr005/val_rmse_25377.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17330.7461 - rmse: 17345.0410 - val_loss: 25580.6465 - val_rmse: 25377.4062\n",
      "Epoch 51/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17959.1582 - rmse: 17959.1582\n",
      "Epoch 51: val_loss did not improve from 25580.64648\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18097.0312 - rmse: 18145.2598 - val_loss: 35439.3984 - val_rmse: 35242.2148\n",
      "Epoch 52/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17043.1328 - rmse: 17043.1328\n",
      "Epoch 52: val_loss improved from 25580.64648 to 25539.28711, saving model to ./ckpt/5_class_lr005/val_rmse_25333.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17030.3516 - rmse: 17021.7324 - val_loss: 25539.2871 - val_rmse: 25333.2441\n",
      "Epoch 53/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18182.1211 - rmse: 18182.1211\n",
      "Epoch 53: val_loss did not improve from 25539.28711\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18136.4238 - rmse: 18177.4570 - val_loss: 26058.3359 - val_rmse: 25829.5664\n",
      "Epoch 54/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17270.3398 - rmse: 17288.8848\n",
      "Epoch 54: val_loss improved from 25539.28711 to 25216.52734, saving model to ./ckpt/5_class_lr005/val_rmse_24979.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17270.3398 - rmse: 17288.8848 - val_loss: 25216.5273 - val_rmse: 24978.6484\n",
      "Epoch 55/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16541.6289 - rmse: 16541.6289\n",
      "Epoch 55: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 16794.1172 - rmse: 16839.7617 - val_loss: 34000.2305 - val_rmse: 33807.3086\n",
      "Epoch 56/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19581.7090 - rmse: 19581.7090\n",
      "Epoch 56: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19526.1621 - rmse: 19488.6992 - val_loss: 26011.8691 - val_rmse: 25787.3320\n",
      "Epoch 57/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17188.7812 - rmse: 17160.4473\n",
      "Epoch 57: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17188.7812 - rmse: 17160.4473 - val_loss: 25883.7363 - val_rmse: 25666.0469\n",
      "Epoch 58/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16470.5449 - rmse: 16470.5449\n",
      "Epoch 58: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16490.1094 - rmse: 16469.2676 - val_loss: 25293.4258 - val_rmse: 25107.9121\n",
      "Epoch 59/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16749.8789 - rmse: 16749.7988\n",
      "Epoch 59: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16749.8789 - rmse: 16749.7988 - val_loss: 38718.8398 - val_rmse: 38580.5039\n",
      "Epoch 60/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17417.8770 - rmse: 17415.6289\n",
      "Epoch 60: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17417.8770 - rmse: 17415.6289 - val_loss: 25263.8613 - val_rmse: 25054.5098\n",
      "Epoch 61/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17601.0605 - rmse: 17601.0605\n",
      "Epoch 61: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17564.7500 - rmse: 17540.2617 - val_loss: 28325.1035 - val_rmse: 28070.4082\n",
      "Epoch 62/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15902.0674 - rmse: 15920.5215\n",
      "Epoch 62: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15902.0674 - rmse: 15920.5215 - val_loss: 26344.2871 - val_rmse: 26123.0820\n",
      "Epoch 63/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15907.2930 - rmse: 15931.5361\n",
      "Epoch 63: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15907.2930 - rmse: 15931.5361 - val_loss: 26133.9023 - val_rmse: 25912.4473\n",
      "Epoch 64/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15896.7334 - rmse: 15896.7334\n",
      "Epoch 64: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 16033.9111 - rmse: 16038.1719 - val_loss: 26596.9082 - val_rmse: 26417.9688\n",
      "Epoch 65/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16202.4336 - rmse: 16202.4336\n",
      "Epoch 65: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 16283.8643 - rmse: 16338.7842 - val_loss: 25614.4746 - val_rmse: 25388.7363\n",
      "Epoch 66/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17300.4355 - rmse: 17264.6875\n",
      "Epoch 66: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17300.4355 - rmse: 17264.6875 - val_loss: 26309.5645 - val_rmse: 26070.8926\n",
      "Epoch 67/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16428.4199 - rmse: 16428.4199\n",
      "Epoch 67: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16372.3594 - rmse: 16348.6572 - val_loss: 26334.9531 - val_rmse: 26135.6914\n",
      "Epoch 68/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16142.0557 - rmse: 16147.9844\n",
      "Epoch 68: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16142.0557 - rmse: 16147.9844 - val_loss: 25588.7051 - val_rmse: 25346.4004\n",
      "Epoch 69/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15976.1240 - rmse: 15976.1240\n",
      "Epoch 69: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15883.9590 - rmse: 15850.6826 - val_loss: 26171.3535 - val_rmse: 25949.0508\n",
      "Epoch 70/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17575.1113 - rmse: 17575.1113\n",
      "Epoch 70: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17326.2773 - rmse: 17329.2793 - val_loss: 26499.0840 - val_rmse: 26278.5078\n",
      "Epoch 71/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16726.6172 - rmse: 16726.6172\n",
      "Epoch 71: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16617.4180 - rmse: 16596.5098 - val_loss: 25423.9590 - val_rmse: 25185.4453\n",
      "Epoch 72/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17861.4258 - rmse: 17861.4258\n",
      "Epoch 72: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17791.0371 - rmse: 17794.2598 - val_loss: 26814.8965 - val_rmse: 26627.0957\n",
      "Epoch 73/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17589.3438 - rmse: 17589.3438\n",
      "Epoch 73: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17521.2988 - rmse: 17533.3027 - val_loss: 26010.1016 - val_rmse: 25794.7832\n",
      "Epoch 74/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15851.7637 - rmse: 15851.7637\n",
      "Epoch 74: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15884.4834 - rmse: 15892.7998 - val_loss: 26249.9492 - val_rmse: 26048.6094\n",
      "Epoch 75/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16342.9873 - rmse: 16352.5146\n",
      "Epoch 75: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16342.9873 - rmse: 16352.5146 - val_loss: 29715.5742 - val_rmse: 29509.4336\n",
      "Epoch 76/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16824.6621 - rmse: 16824.6621\n",
      "Epoch 76: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16881.6367 - rmse: 16877.4316 - val_loss: 26738.7051 - val_rmse: 26521.6484\n",
      "Epoch 77/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15656.7861 - rmse: 15656.7861\n",
      "Epoch 77: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15656.2822 - rmse: 15643.3164 - val_loss: 25596.8008 - val_rmse: 25359.6328\n",
      "Epoch 78/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 14641.9971 - rmse: 14641.9971\n",
      "Epoch 78: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14651.4814 - rmse: 14657.8779 - val_loss: 26239.7891 - val_rmse: 25973.1211\n",
      "Epoch 79/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17614.5703 - rmse: 17614.5703\n",
      "Epoch 79: val_loss did not improve from 25216.52734\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17568.6562 - rmse: 17546.1406 - val_loss: 30340.4219 - val_rmse: 30107.9844\n",
      "Epoch 79: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:11:48.197104: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 84717.0312 - rmse: 84379.2656\n",
      "Epoch 1: val_loss improved from inf to 46922.94922, saving model to ./ckpt/5_class_lr005/val_rmse_46887.hdf5\n",
      "70/70 [==============================] - 2s 16ms/step - loss: 84717.0312 - rmse: 84379.2656 - val_loss: 46922.9492 - val_rmse: 46887.1953\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 1s - loss: 47420.4922 - rmse: 47420.4922"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:11:49.357559: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 44169.3984 - rmse: 44169.3984\n",
      "Epoch 2: val_loss improved from 46922.94922 to 42493.67969, saving model to ./ckpt/5_class_lr005/val_rmse_42238.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 44243.6992 - rmse: 44183.1289 - val_loss: 42493.6797 - val_rmse: 42237.6289\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 38688.4609 - rmse: 38688.4609\n",
      "Epoch 3: val_loss improved from 42493.67969 to 35180.12109, saving model to ./ckpt/5_class_lr005/val_rmse_34921.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 39003.1641 - rmse: 39215.4102 - val_loss: 35180.1211 - val_rmse: 34920.7305\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 39628.0781 - rmse: 39628.0781\n",
      "Epoch 4: val_loss improved from 35180.12109 to 33616.28906, saving model to ./ckpt/5_class_lr005/val_rmse_33401.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 39560.8008 - rmse: 39515.4297 - val_loss: 33616.2891 - val_rmse: 33401.2969\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 36469.0273 - rmse: 36469.0273\n",
      "Epoch 5: val_loss improved from 33616.28906 to 32338.82227, saving model to ./ckpt/5_class_lr005/val_rmse_32115.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 36399.3594 - rmse: 36352.3672 - val_loss: 32338.8223 - val_rmse: 32115.3789\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 32529.2988 - rmse: 32529.2988\n",
      "Epoch 6: val_loss improved from 32338.82227 to 30365.48438, saving model to ./ckpt/5_class_lr005/val_rmse_30146.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 32556.6484 - rmse: 32489.6816 - val_loss: 30365.4844 - val_rmse: 30146.2637\n",
      "Epoch 7/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 31014.6172 - rmse: 31014.6172\n",
      "Epoch 7: val_loss improved from 30365.48438 to 30282.97656, saving model to ./ckpt/5_class_lr005/val_rmse_30052.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 31343.2246 - rmse: 31418.5332 - val_loss: 30282.9766 - val_rmse: 30051.5000\n",
      "Epoch 8/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 29537.4453 - rmse: 29537.4453\n",
      "Epoch 8: val_loss did not improve from 30282.97656\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 29798.4414 - rmse: 29783.1348 - val_loss: 34750.2227 - val_rmse: 34643.6367\n",
      "Epoch 9/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 28443.9355 - rmse: 28443.9355\n",
      "Epoch 9: val_loss improved from 30282.97656 to 26179.97266, saving model to ./ckpt/5_class_lr005/val_rmse_26083.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 28229.8262 - rmse: 28213.2305 - val_loss: 26179.9727 - val_rmse: 26082.6758\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 29552.3945 - rmse: 29552.3945\n",
      "Epoch 10: val_loss did not improve from 26179.97266\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 29440.9082 - rmse: 29365.7168 - val_loss: 29532.6152 - val_rmse: 29428.2324\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 26970.2637 - rmse: 26970.2637\n",
      "Epoch 11: val_loss did not improve from 26179.97266\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 27098.0723 - rmse: 27010.5332 - val_loss: 30754.5312 - val_rmse: 30618.7090\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 26412.9512 - rmse: 26412.9512\n",
      "Epoch 12: val_loss improved from 26179.97266 to 25202.69922, saving model to ./ckpt/5_class_lr005/val_rmse_25080.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 26152.0078 - rmse: 26125.8516 - val_loss: 25202.6992 - val_rmse: 25080.0000\n",
      "Epoch 13/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 25298.7246 - rmse: 25298.7246\n",
      "Epoch 13: val_loss improved from 25202.69922 to 24400.33008, saving model to ./ckpt/5_class_lr005/val_rmse_24389.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 25425.3340 - rmse: 25410.5254 - val_loss: 24400.3301 - val_rmse: 24388.5996\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27225.3066 - rmse: 27171.8965\n",
      "Epoch 14: val_loss did not improve from 24400.33008\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 27225.3066 - rmse: 27171.8965 - val_loss: 27377.7422 - val_rmse: 27296.7695\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 25361.5000 - rmse: 25361.5000\n",
      "Epoch 15: val_loss improved from 24400.33008 to 23816.34375, saving model to ./ckpt/5_class_lr005/val_rmse_23601.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 25358.6191 - rmse: 25356.6777 - val_loss: 23816.3438 - val_rmse: 23600.8730\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23257.3984 - rmse: 23257.3984\n",
      "Epoch 16: val_loss did not improve from 23816.34375\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 23136.3555 - rmse: 23146.2363 - val_loss: 26301.1777 - val_rmse: 26188.2461\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22322.8496 - rmse: 22303.2227\n",
      "Epoch 17: val_loss did not improve from 23816.34375\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22322.8496 - rmse: 22303.2227 - val_loss: 23821.2422 - val_rmse: 23916.1543\n",
      "Epoch 18/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22032.8535 - rmse: 22032.8535\n",
      "Epoch 18: val_loss did not improve from 23816.34375\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22002.5137 - rmse: 21982.0508 - val_loss: 27841.0859 - val_rmse: 27756.6719\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22960.2207 - rmse: 22960.2207\n",
      "Epoch 19: val_loss did not improve from 23816.34375\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23350.5117 - rmse: 23415.9355 - val_loss: 24293.8730 - val_rmse: 24251.8047\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23503.8535 - rmse: 23503.8535\n",
      "Epoch 20: val_loss improved from 23816.34375 to 23383.00195, saving model to ./ckpt/5_class_lr005/val_rmse_23258.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23278.4785 - rmse: 23287.2070 - val_loss: 23383.0020 - val_rmse: 23257.8262\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20834.9414 - rmse: 20834.9414\n",
      "Epoch 21: val_loss did not improve from 23383.00195\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20813.5684 - rmse: 20796.5410 - val_loss: 26492.0000 - val_rmse: 26401.7773\n",
      "Epoch 22/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22809.3418 - rmse: 22809.3418\n",
      "Epoch 22: val_loss did not improve from 23383.00195\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 22761.2070 - rmse: 22727.2617 - val_loss: 28571.5332 - val_rmse: 28404.9316\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21573.4121 - rmse: 21573.4121\n",
      "Epoch 23: val_loss improved from 23383.00195 to 22213.65430, saving model to ./ckpt/5_class_lr005/val_rmse_22119.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 21620.8242 - rmse: 21652.8008 - val_loss: 22213.6543 - val_rmse: 22119.3809\n",
      "Epoch 24/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21651.4238 - rmse: 21651.4238\n",
      "Epoch 24: val_loss did not improve from 22213.65430\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21832.9219 - rmse: 21832.3184 - val_loss: 26613.8809 - val_rmse: 26480.0977\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22708.4902 - rmse: 22748.0039\n",
      "Epoch 25: val_loss did not improve from 22213.65430\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22708.4902 - rmse: 22748.0039 - val_loss: 24293.4512 - val_rmse: 24150.0410\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21507.0039 - rmse: 21507.0039\n",
      "Epoch 26: val_loss did not improve from 22213.65430\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 21497.6543 - rmse: 21491.3477 - val_loss: 23269.2930 - val_rmse: 23135.7266\n",
      "Epoch 27/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20954.6504 - rmse: 21003.8457\n",
      "Epoch 27: val_loss did not improve from 22213.65430\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20954.6504 - rmse: 21003.8457 - val_loss: 25041.1113 - val_rmse: 24874.8594\n",
      "Epoch 28/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19157.8691 - rmse: 19157.8691\n",
      "Epoch 28: val_loss did not improve from 22213.65430\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19203.0781 - rmse: 19234.0957 - val_loss: 25696.5508 - val_rmse: 25542.6992\n",
      "Epoch 29/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20010.4863 - rmse: 20010.4863\n",
      "Epoch 29: val_loss did not improve from 22213.65430\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20001.3594 - rmse: 20051.1152 - val_loss: 24880.9473 - val_rmse: 24735.5801\n",
      "Epoch 30/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19987.1172 - rmse: 20029.9961\n",
      "Epoch 30: val_loss did not improve from 22213.65430\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19987.1172 - rmse: 20029.9961 - val_loss: 34469.0000 - val_rmse: 34328.6523\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19268.6465 - rmse: 19268.2344\n",
      "Epoch 31: val_loss did not improve from 22213.65430\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 19268.6465 - rmse: 19268.2344 - val_loss: 22881.1133 - val_rmse: 22725.0410\n",
      "Epoch 32/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20175.7402 - rmse: 20175.7402\n",
      "Epoch 32: val_loss improved from 22213.65430 to 22009.26562, saving model to ./ckpt/5_class_lr005/val_rmse_21913.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 19991.8320 - rmse: 19945.5117 - val_loss: 22009.2656 - val_rmse: 21912.6816\n",
      "Epoch 33/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20525.6621 - rmse: 20525.6621\n",
      "Epoch 33: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 20496.9980 - rmse: 20477.6641 - val_loss: 27625.9434 - val_rmse: 27604.7793\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18875.6934 - rmse: 18874.3027\n",
      "Epoch 34: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 18875.6934 - rmse: 18874.3027 - val_loss: 23788.9434 - val_rmse: 23675.0078\n",
      "Epoch 35/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18953.1348 - rmse: 18953.1348\n",
      "Epoch 35: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19031.7891 - rmse: 19012.3047 - val_loss: 26250.0859 - val_rmse: 26059.9629\n",
      "Epoch 36/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18861.4238 - rmse: 18861.4238\n",
      "Epoch 36: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18900.2891 - rmse: 18926.5039 - val_loss: 27037.5059 - val_rmse: 26868.2617\n",
      "Epoch 37/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17169.9883 - rmse: 17169.9883\n",
      "Epoch 37: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17291.1855 - rmse: 17291.1797 - val_loss: 22771.5410 - val_rmse: 22630.7461\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18810.0391 - rmse: 18810.0391\n",
      "Epoch 38: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18788.2500 - rmse: 18773.5527 - val_loss: 23713.0645 - val_rmse: 23526.4297\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20679.1016 - rmse: 20679.1016\n",
      "Epoch 39: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20638.9707 - rmse: 20611.9082 - val_loss: 23778.4707 - val_rmse: 23604.9414\n",
      "Epoch 40/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21570.9277 - rmse: 21570.9277\n",
      "Epoch 40: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21334.3887 - rmse: 21277.1914 - val_loss: 29373.6289 - val_rmse: 29194.2734\n",
      "Epoch 41/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18107.1836 - rmse: 18107.1836\n",
      "Epoch 41: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17987.7988 - rmse: 17961.0684 - val_loss: 22076.8965 - val_rmse: 21901.3867\n",
      "Epoch 42/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18634.8613 - rmse: 18634.8613\n",
      "Epoch 42: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18781.5801 - rmse: 18814.7715 - val_loss: 25309.4062 - val_rmse: 25098.1953\n",
      "Epoch 43/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19707.9492 - rmse: 19707.9492\n",
      "Epoch 43: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19605.4824 - rmse: 19600.1895 - val_loss: 22870.7305 - val_rmse: 22871.4688\n",
      "Epoch 44/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20125.7969 - rmse: 20125.7969\n",
      "Epoch 44: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 19865.9785 - rmse: 19826.5977 - val_loss: 22556.7500 - val_rmse: 22468.1602\n",
      "Epoch 45/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17755.2031 - rmse: 17748.6074\n",
      "Epoch 45: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17755.2031 - rmse: 17748.6074 - val_loss: 32202.1523 - val_rmse: 32258.5527\n",
      "Epoch 46/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18873.3379 - rmse: 18873.3379\n",
      "Epoch 46: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18800.9902 - rmse: 18752.1973 - val_loss: 23650.6621 - val_rmse: 23441.3535\n",
      "Epoch 47/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16760.1445 - rmse: 16806.9180\n",
      "Epoch 47: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16760.1445 - rmse: 16806.9180 - val_loss: 23191.8457 - val_rmse: 23074.1367\n",
      "Epoch 48/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18772.1035 - rmse: 18851.9082\n",
      "Epoch 48: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18772.1035 - rmse: 18851.9082 - val_loss: 27771.1523 - val_rmse: 27601.2969\n",
      "Epoch 49/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19341.7070 - rmse: 19341.7070\n",
      "Epoch 49: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19467.6426 - rmse: 19468.6016 - val_loss: 27566.4746 - val_rmse: 27492.0684\n",
      "Epoch 50/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22038.8691 - rmse: 22038.8691\n",
      "Epoch 50: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22047.7656 - rmse: 22053.7656 - val_loss: 26303.2305 - val_rmse: 26220.4922\n",
      "Epoch 51/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16916.7461 - rmse: 16974.9102\n",
      "Epoch 51: val_loss did not improve from 22009.26562\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16916.7461 - rmse: 16974.9102 - val_loss: 28357.2480 - val_rmse: 28202.2441\n",
      "Epoch 52/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18588.8027 - rmse: 18584.7207\n",
      "Epoch 52: val_loss improved from 22009.26562 to 21628.67383, saving model to ./ckpt/5_class_lr005/val_rmse_21577.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18588.8027 - rmse: 18584.7207 - val_loss: 21628.6738 - val_rmse: 21577.1934\n",
      "Epoch 53/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18866.0977 - rmse: 18866.0977\n",
      "Epoch 53: val_loss did not improve from 21628.67383\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18841.1797 - rmse: 18824.3691 - val_loss: 21899.2949 - val_rmse: 21711.7539\n",
      "Epoch 54/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17552.5039 - rmse: 17552.5039\n",
      "Epoch 54: val_loss did not improve from 21628.67383\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 17498.4336 - rmse: 17461.9668 - val_loss: 23912.9062 - val_rmse: 23798.0586\n",
      "Epoch 55/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16917.1152 - rmse: 16917.1152\n",
      "Epoch 55: val_loss did not improve from 21628.67383\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 16834.4707 - rmse: 16790.8242 - val_loss: 22567.6309 - val_rmse: 22356.5801\n",
      "Epoch 56/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16387.5391 - rmse: 16387.5391\n",
      "Epoch 56: val_loss improved from 21628.67383 to 21573.91602, saving model to ./ckpt/5_class_lr005/val_rmse_21495.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 16350.7734 - rmse: 16325.9785 - val_loss: 21573.9160 - val_rmse: 21494.6992\n",
      "Epoch 57/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16969.6836 - rmse: 16969.6836\n",
      "Epoch 57: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 16945.5703 - rmse: 16944.5391 - val_loss: 21862.2285 - val_rmse: 21697.5078\n",
      "Epoch 58/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18504.0293 - rmse: 18504.0293\n",
      "Epoch 58: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18481.8418 - rmse: 18466.8789 - val_loss: 25334.0840 - val_rmse: 25133.1914\n",
      "Epoch 59/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16308.8555 - rmse: 16340.9658\n",
      "Epoch 59: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16308.8555 - rmse: 16340.9658 - val_loss: 30059.8262 - val_rmse: 29953.9902\n",
      "Epoch 60/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16239.3574 - rmse: 16257.5391\n",
      "Epoch 60: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16239.3574 - rmse: 16257.5391 - val_loss: 22299.0508 - val_rmse: 22120.0039\n",
      "Epoch 61/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15930.5312 - rmse: 15930.5312\n",
      "Epoch 61: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15881.6006 - rmse: 15865.7998 - val_loss: 25784.7812 - val_rmse: 25829.2559\n",
      "Epoch 62/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18366.7988 - rmse: 18366.7988\n",
      "Epoch 62: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 18490.4980 - rmse: 18513.4316 - val_loss: 23004.0508 - val_rmse: 22817.5293\n",
      "Epoch 63/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17646.7891 - rmse: 17659.6875\n",
      "Epoch 63: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17646.7891 - rmse: 17659.6875 - val_loss: 23498.4961 - val_rmse: 23396.6152\n",
      "Epoch 64/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16637.0938 - rmse: 16637.0938\n",
      "Epoch 64: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16690.0840 - rmse: 16707.3652 - val_loss: 25421.6074 - val_rmse: 25234.1758\n",
      "Epoch 65/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19156.0977 - rmse: 19156.0977\n",
      "Epoch 65: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19199.8770 - rmse: 19192.5137 - val_loss: 23067.3613 - val_rmse: 23033.4219\n",
      "Epoch 66/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17265.8203 - rmse: 17265.8203\n",
      "Epoch 66: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17343.9746 - rmse: 17400.1309 - val_loss: 23392.3887 - val_rmse: 23333.7812\n",
      "Epoch 67/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15814.2773 - rmse: 15814.2773\n",
      "Epoch 67: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 15830.4053 - rmse: 15841.2822 - val_loss: 21796.4199 - val_rmse: 21627.8691\n",
      "Epoch 68/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17290.0742 - rmse: 17290.0742\n",
      "Epoch 68: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17224.6895 - rmse: 17236.6895 - val_loss: 23526.8574 - val_rmse: 23336.9551\n",
      "Epoch 69/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16837.0059 - rmse: 16837.0059\n",
      "Epoch 69: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16967.0879 - rmse: 16964.1816 - val_loss: 22202.2480 - val_rmse: 22099.2324\n",
      "Epoch 70/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16167.6963 - rmse: 16167.6963\n",
      "Epoch 70: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16271.3213 - rmse: 16341.2109 - val_loss: 22228.7578 - val_rmse: 22043.9043\n",
      "Epoch 71/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15541.3594 - rmse: 15541.3594\n",
      "Epoch 71: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 15522.7705 - rmse: 15510.2344 - val_loss: 21667.0469 - val_rmse: 21478.1992\n",
      "Epoch 72/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17472.2715 - rmse: 17496.2383\n",
      "Epoch 72: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17472.2715 - rmse: 17496.2383 - val_loss: 23217.5996 - val_rmse: 23001.2227\n",
      "Epoch 73/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17055.7520 - rmse: 17055.7520\n",
      "Epoch 73: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17014.4316 - rmse: 16986.5664 - val_loss: 21628.4902 - val_rmse: 21505.2832\n",
      "Epoch 74/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15998.3662 - rmse: 16066.1289\n",
      "Epoch 74: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 15998.3662 - rmse: 16066.1289 - val_loss: 22541.6836 - val_rmse: 22321.5273\n",
      "Epoch 75/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15147.1738 - rmse: 15140.5479\n",
      "Epoch 75: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 15147.1738 - rmse: 15140.5479 - val_loss: 22555.0215 - val_rmse: 22464.0039\n",
      "Epoch 76/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15475.9375 - rmse: 15475.9375\n",
      "Epoch 76: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 15372.1338 - rmse: 15352.7373 - val_loss: 24008.3223 - val_rmse: 23805.5039\n",
      "Epoch 77/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16984.7500 - rmse: 16965.7207\n",
      "Epoch 77: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16984.7500 - rmse: 16965.7207 - val_loss: 23645.4746 - val_rmse: 23514.0195\n",
      "Epoch 78/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 14746.6602 - rmse: 14746.6602\n",
      "Epoch 78: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 14683.2393 - rmse: 14660.7627 - val_loss: 27349.5371 - val_rmse: 27170.0938\n",
      "Epoch 79/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16138.1729 - rmse: 16138.1729\n",
      "Epoch 79: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16016.1621 - rmse: 16025.4502 - val_loss: 22570.2012 - val_rmse: 22448.5723\n",
      "Epoch 80/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16042.1689 - rmse: 16042.1689\n",
      "Epoch 80: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15927.8691 - rmse: 15914.6592 - val_loss: 21962.1250 - val_rmse: 21786.8125\n",
      "Epoch 81/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15261.8154 - rmse: 15261.8154\n",
      "Epoch 81: val_loss did not improve from 21573.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15199.3594 - rmse: 15187.7627 - val_loss: 23423.0137 - val_rmse: 23330.5039\n",
      "Epoch 81: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:13:01.867599: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 87671.6484 - rmse: 87477.1562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:13:03.049591: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 46865.30469, saving model to ./ckpt/5_class_lr005/val_rmse_47129.hdf5\n",
      "70/70 [==============================] - 2s 17ms/step - loss: 87671.6484 - rmse: 87477.1562 - val_loss: 46865.3047 - val_rmse: 47129.0781\n",
      "Epoch 2/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 42962.9805 - rmse: 42962.9805\n",
      "Epoch 2: val_loss improved from 46865.30469 to 40815.99609, saving model to ./ckpt/5_class_lr005/val_rmse_41034.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 43114.8203 - rmse: 43045.7031 - val_loss: 40815.9961 - val_rmse: 41033.9766\n",
      "Epoch 3/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 39516.5352 - rmse: 39498.8398\n",
      "Epoch 3: val_loss did not improve from 40815.99609\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 39516.5352 - rmse: 39498.8398 - val_loss: 46290.9297 - val_rmse: 46307.8594\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 39845.3789 - rmse: 39834.0469\n",
      "Epoch 4: val_loss improved from 40815.99609 to 37937.32422, saving model to ./ckpt/5_class_lr005/val_rmse_38077.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 39845.3789 - rmse: 39834.0469 - val_loss: 37937.3242 - val_rmse: 38077.0508\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 37353.8906 - rmse: 37353.8906\n",
      "Epoch 5: val_loss improved from 37937.32422 to 34926.02734, saving model to ./ckpt/5_class_lr005/val_rmse_35062.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 37283.4531 - rmse: 37235.9492 - val_loss: 34926.0273 - val_rmse: 35061.8750\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 35476.7539 - rmse: 35476.7539\n",
      "Epoch 6: val_loss did not improve from 34926.02734\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 35571.0430 - rmse: 35545.9062 - val_loss: 37627.9258 - val_rmse: 37855.8242\n",
      "Epoch 7/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 33042.7539 - rmse: 33042.7539\n",
      "Epoch 7: val_loss did not improve from 34926.02734\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 32725.2930 - rmse: 32649.4922 - val_loss: 35045.2109 - val_rmse: 35224.1406\n",
      "Epoch 8/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 31957.2324 - rmse: 31957.2324\n",
      "Epoch 8: val_loss improved from 34926.02734 to 32129.94727, saving model to ./ckpt/5_class_lr005/val_rmse_32239.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 31905.0000 - rmse: 31877.1113 - val_loss: 32129.9473 - val_rmse: 32239.0488\n",
      "Epoch 9/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 29504.4238 - rmse: 29504.4238\n",
      "Epoch 9: val_loss improved from 32129.94727 to 31057.69922, saving model to ./ckpt/5_class_lr005/val_rmse_31088.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 29539.2656 - rmse: 29587.9766 - val_loss: 31057.6992 - val_rmse: 31088.1309\n",
      "Epoch 10/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 30697.8340 - rmse: 30697.8340\n",
      "Epoch 10: val_loss did not improve from 31057.69922\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 30769.0742 - rmse: 30775.9707 - val_loss: 31179.3379 - val_rmse: 31345.8262\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 28841.6191 - rmse: 28841.6191\n",
      "Epoch 11: val_loss improved from 31057.69922 to 28537.19141, saving model to ./ckpt/5_class_lr005/val_rmse_28710.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 28827.6465 - rmse: 28818.2227 - val_loss: 28537.1914 - val_rmse: 28710.0703\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 28545.9258 - rmse: 28545.9258\n",
      "Epoch 12: val_loss did not improve from 28537.19141\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 28455.4512 - rmse: 28394.4316 - val_loss: 30805.4785 - val_rmse: 30997.5078\n",
      "Epoch 13/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 27621.6738 - rmse: 27621.6738\n",
      "Epoch 13: val_loss did not improve from 28537.19141\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 27559.4785 - rmse: 27503.1309 - val_loss: 32353.8965 - val_rmse: 32349.0918\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 26598.5938 - rmse: 26530.0254\n",
      "Epoch 14: val_loss improved from 28537.19141 to 26706.16602, saving model to ./ckpt/5_class_lr005/val_rmse_26902.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 26598.5938 - rmse: 26530.0254 - val_loss: 26706.1660 - val_rmse: 26901.8477\n",
      "Epoch 15/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 25581.3574 - rmse: 25581.3574\n",
      "Epoch 15: val_loss improved from 26706.16602 to 25742.12305, saving model to ./ckpt/5_class_lr005/val_rmse_25715.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 25475.9961 - rmse: 25437.4375 - val_loss: 25742.1230 - val_rmse: 25715.0410\n",
      "Epoch 16/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 25007.2285 - rmse: 25007.2285\n",
      "Epoch 16: val_loss did not improve from 25742.12305\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 24973.2090 - rmse: 25010.9004 - val_loss: 31758.2012 - val_rmse: 31724.9668\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 25058.5859 - rmse: 25058.5859\n",
      "Epoch 17: val_loss improved from 25742.12305 to 25178.17383, saving model to ./ckpt/5_class_lr005/val_rmse_25259.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 24956.4023 - rmse: 25049.0801 - val_loss: 25178.1738 - val_rmse: 25258.9980\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23933.0801 - rmse: 23898.7676\n",
      "Epoch 18: val_loss did not improve from 25178.17383\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23933.0801 - rmse: 23898.7676 - val_loss: 26131.5938 - val_rmse: 26044.5059\n",
      "Epoch 19/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23405.8105 - rmse: 23405.8105\n",
      "Epoch 19: val_loss improved from 25178.17383 to 25156.71680, saving model to ./ckpt/5_class_lr005/val_rmse_25308.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 23504.9375 - rmse: 23592.3223 - val_loss: 25156.7168 - val_rmse: 25307.6113\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22129.4941 - rmse: 22129.4941\n",
      "Epoch 20: val_loss did not improve from 25156.71680\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22112.9102 - rmse: 22101.7246 - val_loss: 26539.8008 - val_rmse: 26644.5449\n",
      "Epoch 21/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22745.3477 - rmse: 22745.3477\n",
      "Epoch 21: val_loss did not improve from 25156.71680\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22956.8105 - rmse: 23039.0566 - val_loss: 26191.5430 - val_rmse: 26315.6113\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22387.1406 - rmse: 22318.2598\n",
      "Epoch 22: val_loss improved from 25156.71680 to 23540.43945, saving model to ./ckpt/5_class_lr005/val_rmse_23591.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22387.1406 - rmse: 22318.2598 - val_loss: 23540.4395 - val_rmse: 23591.1289\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 24291.1992 - rmse: 24291.1992\n",
      "Epoch 23: val_loss did not improve from 23540.43945\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 24324.8496 - rmse: 24347.5449 - val_loss: 24662.5840 - val_rmse: 24509.3418\n",
      "Epoch 24/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23821.2598 - rmse: 23821.2598\n",
      "Epoch 24: val_loss did not improve from 23540.43945\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23759.7910 - rmse: 23759.1582 - val_loss: 24616.3613 - val_rmse: 24668.8594\n",
      "Epoch 25/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21681.9727 - rmse: 21681.9727\n",
      "Epoch 25: val_loss did not improve from 23540.43945\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 21644.5293 - rmse: 21606.9902 - val_loss: 25162.8340 - val_rmse: 25022.2852\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20860.3184 - rmse: 20887.5352\n",
      "Epoch 26: val_loss did not improve from 23540.43945\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20860.3184 - rmse: 20887.5352 - val_loss: 23927.8574 - val_rmse: 23738.1328\n",
      "Epoch 27/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22291.2578 - rmse: 22291.2578\n",
      "Epoch 27: val_loss did not improve from 23540.43945\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22171.1973 - rmse: 22174.9902 - val_loss: 24462.8613 - val_rmse: 24337.5742\n",
      "Epoch 28/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22514.9375 - rmse: 22485.7012\n",
      "Epoch 28: val_loss did not improve from 23540.43945\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22514.9375 - rmse: 22485.7012 - val_loss: 24754.2148 - val_rmse: 24638.5977\n",
      "Epoch 29/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19906.4512 - rmse: 19906.4512\n",
      "Epoch 29: val_loss did not improve from 23540.43945\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20052.6016 - rmse: 20076.6367 - val_loss: 24597.2969 - val_rmse: 24474.8828\n",
      "Epoch 30/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20736.1797 - rmse: 20736.1797\n",
      "Epoch 30: val_loss improved from 23540.43945 to 21612.94141, saving model to ./ckpt/5_class_lr005/val_rmse_21471.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20711.4082 - rmse: 20700.5957 - val_loss: 21612.9414 - val_rmse: 21471.3223\n",
      "Epoch 31/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22651.3281 - rmse: 22651.3281\n",
      "Epoch 31: val_loss did not improve from 21612.94141\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22645.5605 - rmse: 22641.6699 - val_loss: 24866.3223 - val_rmse: 24937.6914\n",
      "Epoch 32/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20465.4258 - rmse: 20465.4258\n",
      "Epoch 32: val_loss did not improve from 21612.94141\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20364.1484 - rmse: 20364.5254 - val_loss: 22316.4707 - val_rmse: 22236.7598\n",
      "Epoch 33/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21440.8379 - rmse: 21440.8379\n",
      "Epoch 33: val_loss did not improve from 21612.94141\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 21318.4453 - rmse: 21268.7246 - val_loss: 22716.2422 - val_rmse: 22600.1465\n",
      "Epoch 34/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20749.5820 - rmse: 20749.5820\n",
      "Epoch 34: val_loss did not improve from 21612.94141\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20945.2305 - rmse: 20978.4766 - val_loss: 26866.1387 - val_rmse: 26895.4219\n",
      "Epoch 35/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19429.9395 - rmse: 19429.9395\n",
      "Epoch 35: val_loss did not improve from 21612.94141\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19378.3086 - rmse: 19343.4863 - val_loss: 22511.5547 - val_rmse: 22498.4004\n",
      "Epoch 36/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20035.6348 - rmse: 20035.6348\n",
      "Epoch 36: val_loss did not improve from 21612.94141\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19897.9180 - rmse: 19850.4238 - val_loss: 24999.6543 - val_rmse: 25056.5840\n",
      "Epoch 37/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19602.7773 - rmse: 19602.7773\n",
      "Epoch 37: val_loss improved from 21612.94141 to 21420.15625, saving model to ./ckpt/5_class_lr005/val_rmse_21435.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19620.3203 - rmse: 19632.1523 - val_loss: 21420.1562 - val_rmse: 21435.4355\n",
      "Epoch 38/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21704.9785 - rmse: 21704.9785\n",
      "Epoch 38: val_loss improved from 21420.15625 to 21035.35352, saving model to ./ckpt/5_class_lr005/val_rmse_20968.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 21555.9922 - rmse: 21550.2754 - val_loss: 21035.3535 - val_rmse: 20967.6250\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18640.2539 - rmse: 18640.2539\n",
      "Epoch 39: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18702.2227 - rmse: 18744.0156 - val_loss: 22953.5625 - val_rmse: 22954.4316\n",
      "Epoch 40/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18736.5176 - rmse: 18736.5176\n",
      "Epoch 40: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18885.2129 - rmse: 18877.2754 - val_loss: 27123.2500 - val_rmse: 26971.9141\n",
      "Epoch 41/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18881.2969 - rmse: 18864.9902\n",
      "Epoch 41: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18881.2969 - rmse: 18864.9902 - val_loss: 21731.7559 - val_rmse: 21670.9062\n",
      "Epoch 42/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18889.6953 - rmse: 18858.8828\n",
      "Epoch 42: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18889.6953 - rmse: 18858.8828 - val_loss: 21265.2012 - val_rmse: 21292.2188\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18988.5332 - rmse: 18988.5332\n",
      "Epoch 43: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18993.7227 - rmse: 18997.2227 - val_loss: 21592.3457 - val_rmse: 21627.4219\n",
      "Epoch 44/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17302.6367 - rmse: 17302.6367\n",
      "Epoch 44: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17283.8770 - rmse: 17271.2227 - val_loss: 21605.8535 - val_rmse: 21633.5293\n",
      "Epoch 45/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19262.2148 - rmse: 19258.9473\n",
      "Epoch 45: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19262.2148 - rmse: 19258.9473 - val_loss: 26744.7773 - val_rmse: 26861.3340\n",
      "Epoch 46/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17487.3008 - rmse: 17549.5586\n",
      "Epoch 46: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17487.3008 - rmse: 17549.5586 - val_loss: 23311.1133 - val_rmse: 23143.2812\n",
      "Epoch 47/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17254.5762 - rmse: 17248.0098\n",
      "Epoch 47: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17254.5762 - rmse: 17248.0098 - val_loss: 21376.1367 - val_rmse: 21335.5449\n",
      "Epoch 48/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19186.8008 - rmse: 19186.8008\n",
      "Epoch 48: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19379.3867 - rmse: 19415.5742 - val_loss: 30107.3984 - val_rmse: 29954.9727\n",
      "Epoch 49/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19724.0547 - rmse: 19758.5664\n",
      "Epoch 49: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19724.0547 - rmse: 19758.5664 - val_loss: 21688.7363 - val_rmse: 21697.8066\n",
      "Epoch 50/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16517.4297 - rmse: 16517.4297\n",
      "Epoch 50: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16663.7383 - rmse: 16739.4668 - val_loss: 29272.4414 - val_rmse: 29390.0234\n",
      "Epoch 51/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16984.7793 - rmse: 16984.7793\n",
      "Epoch 51: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16889.6035 - rmse: 16855.9551 - val_loss: 24067.8574 - val_rmse: 24175.0312\n",
      "Epoch 52/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17834.0059 - rmse: 17834.0059\n",
      "Epoch 52: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17980.0410 - rmse: 17998.1328 - val_loss: 21886.6543 - val_rmse: 21875.7539\n",
      "Epoch 53/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19153.0918 - rmse: 19153.0918\n",
      "Epoch 53: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19174.5000 - rmse: 19202.8457 - val_loss: 22678.6699 - val_rmse: 22753.6133\n",
      "Epoch 54/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20442.4785 - rmse: 20442.4785\n",
      "Epoch 54: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20448.3828 - rmse: 20409.2480 - val_loss: 21832.8984 - val_rmse: 21739.3594\n",
      "Epoch 55/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16544.9688 - rmse: 16544.9688\n",
      "Epoch 55: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16873.3184 - rmse: 16924.0801 - val_loss: 26913.4609 - val_rmse: 26978.2637\n",
      "Epoch 56/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18946.1074 - rmse: 18946.1074\n",
      "Epoch 56: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18920.0645 - rmse: 18891.3945 - val_loss: 24995.3438 - val_rmse: 25060.2520\n",
      "Epoch 57/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18042.9023 - rmse: 18042.9023\n",
      "Epoch 57: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17994.1406 - rmse: 17961.2500 - val_loss: 27646.8711 - val_rmse: 27728.1387\n",
      "Epoch 58/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23327.8672 - rmse: 23327.8672\n",
      "Epoch 58: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23494.4219 - rmse: 23574.8457 - val_loss: 34673.1367 - val_rmse: 34542.1641\n",
      "Epoch 59/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19224.0664 - rmse: 19221.5801\n",
      "Epoch 59: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19224.0664 - rmse: 19221.5801 - val_loss: 21804.5957 - val_rmse: 21761.4258\n",
      "Epoch 60/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16802.7285 - rmse: 16802.7285\n",
      "Epoch 60: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16764.5957 - rmse: 16744.2402 - val_loss: 22902.2480 - val_rmse: 22975.4707\n",
      "Epoch 61/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17230.4766 - rmse: 17230.4766\n",
      "Epoch 61: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17216.5547 - rmse: 17204.3066 - val_loss: 26982.2461 - val_rmse: 27060.8047\n",
      "Epoch 62/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18048.0312 - rmse: 18048.0312\n",
      "Epoch 62: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18040.0234 - rmse: 17999.9648 - val_loss: 22270.5176 - val_rmse: 22192.3262\n",
      "Epoch 63/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15353.0791 - rmse: 15353.0791\n",
      "Epoch 63: val_loss did not improve from 21035.35352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15308.4785 - rmse: 15331.7432 - val_loss: 21830.7520 - val_rmse: 21767.8711\n",
      "Epoch 63: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:13:57.870857: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 85018.0156 - rmse: 84760.3281\n",
      "Epoch 1: val_loss improved from inf to 43313.46484, saving model to ./ckpt/5_class_lr005/val_rmse_43831.hdf5\n",
      "70/70 [==============================] - 2s 20ms/step - loss: 85018.0156 - rmse: 84760.3281 - val_loss: 43313.4648 - val_rmse: 43830.6172\n",
      "Epoch 2/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:13:59.288714: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 45073.0039 - rmse: 44980.1133\n",
      "Epoch 2: val_loss improved from 43313.46484 to 41130.19922, saving model to ./ckpt/5_class_lr005/val_rmse_41728.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 45073.0039 - rmse: 44980.1133 - val_loss: 41130.1992 - val_rmse: 41727.5703\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 47340.9688 - rmse: 47340.9688\n",
      "Epoch 3: val_loss improved from 41130.19922 to 40967.73047, saving model to ./ckpt/5_class_lr005/val_rmse_41439.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 46846.5273 - rmse: 46793.6914 - val_loss: 40967.7305 - val_rmse: 41438.9531\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 36399.1445 - rmse: 36419.0039\n",
      "Epoch 4: val_loss improved from 40967.73047 to 33092.84375, saving model to ./ckpt/5_class_lr005/val_rmse_33337.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 36399.1445 - rmse: 36419.0039 - val_loss: 33092.8438 - val_rmse: 33336.8047\n",
      "Epoch 5/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 35399.8008 - rmse: 35399.8008\n",
      "Epoch 5: val_loss did not improve from 33092.84375\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 35273.7188 - rmse: 35263.3984 - val_loss: 46180.5195 - val_rmse: 46860.9219\n",
      "Epoch 6/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 38479.2266 - rmse: 38479.2266\n",
      "Epoch 6: val_loss did not improve from 33092.84375\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 38010.0156 - rmse: 37934.8867 - val_loss: 33605.7812 - val_rmse: 34075.6992\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 31960.9707 - rmse: 31878.3496\n",
      "Epoch 7: val_loss improved from 33092.84375 to 29602.74414, saving model to ./ckpt/5_class_lr005/val_rmse_29806.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 31960.9707 - rmse: 31878.3496 - val_loss: 29602.7441 - val_rmse: 29805.6191\n",
      "Epoch 8/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 30511.0488 - rmse: 30511.0488\n",
      "Epoch 8: val_loss improved from 29602.74414 to 28006.60547, saving model to ./ckpt/5_class_lr005/val_rmse_27983.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 30356.6328 - rmse: 30266.3359 - val_loss: 28006.6055 - val_rmse: 27983.3066\n",
      "Epoch 9/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 30060.7969 - rmse: 30060.7969\n",
      "Epoch 9: val_loss did not improve from 28006.60547\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 29654.2988 - rmse: 29570.7441 - val_loss: 29871.5039 - val_rmse: 30274.8184\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 30523.6484 - rmse: 30523.6484\n",
      "Epoch 10: val_loss improved from 28006.60547 to 25258.45312, saving model to ./ckpt/5_class_lr005/val_rmse_25346.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 30478.6367 - rmse: 30452.0117 - val_loss: 25258.4531 - val_rmse: 25345.8555\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 28951.2109 - rmse: 28951.2109\n",
      "Epoch 11: val_loss did not improve from 25258.45312\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 29063.7754 - rmse: 29140.3047 - val_loss: 35409.2070 - val_rmse: 35364.4492\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 30267.4727 - rmse: 30253.7852\n",
      "Epoch 12: val_loss improved from 25258.45312 to 24240.77930, saving model to ./ckpt/5_class_lr005/val_rmse_24035.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 30267.4727 - rmse: 30253.7852 - val_loss: 24240.7793 - val_rmse: 24034.6445\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 30117.0703 - rmse: 30083.6172\n",
      "Epoch 13: val_loss did not improve from 24240.77930\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 30117.0703 - rmse: 30083.6172 - val_loss: 25772.8398 - val_rmse: 25543.7207\n",
      "Epoch 14/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 29092.6602 - rmse: 29092.6602\n",
      "Epoch 14: val_loss did not improve from 24240.77930\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 29134.0469 - rmse: 29158.5254 - val_loss: 29408.6504 - val_rmse: 29521.9844\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 27974.0625 - rmse: 27974.0625\n",
      "Epoch 15: val_loss did not improve from 24240.77930\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 27988.4668 - rmse: 27996.9863 - val_loss: 38955.1836 - val_rmse: 39263.0234\n",
      "Epoch 16/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25914.4629 - rmse: 25924.7090\n",
      "Epoch 16: val_loss did not improve from 24240.77930\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 25914.4629 - rmse: 25924.7090 - val_loss: 24464.0273 - val_rmse: 24194.2188\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 26977.5879 - rmse: 26977.5879\n",
      "Epoch 17: val_loss did not improve from 24240.77930\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 27104.7305 - rmse: 27179.9258 - val_loss: 36612.8867 - val_rmse: 36494.7422\n",
      "Epoch 18/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 25961.0645 - rmse: 25961.0645\n",
      "Epoch 18: val_loss improved from 24240.77930 to 21855.15039, saving model to ./ckpt/5_class_lr005/val_rmse_21559.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 25812.2773 - rmse: 25774.0469 - val_loss: 21855.1504 - val_rmse: 21558.8086\n",
      "Epoch 19/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 24816.7207 - rmse: 24816.7207\n",
      "Epoch 19: val_loss did not improve from 21855.15039\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 24895.4336 - rmse: 24941.9863 - val_loss: 25549.9648 - val_rmse: 25632.2188\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23144.7598 - rmse: 23144.7598\n",
      "Epoch 20: val_loss did not improve from 21855.15039\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 23435.1348 - rmse: 23495.7051 - val_loss: 23079.8379 - val_rmse: 22863.9727\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22763.8145 - rmse: 22763.8145\n",
      "Epoch 21: val_loss improved from 21855.15039 to 20662.27930, saving model to ./ckpt/5_class_lr005/val_rmse_20334.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22573.3398 - rmse: 22647.0938 - val_loss: 20662.2793 - val_rmse: 20334.3125\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21608.8828 - rmse: 21635.9102\n",
      "Epoch 22: val_loss improved from 20662.27930 to 20311.89648, saving model to ./ckpt/5_class_lr005/val_rmse_20010.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21608.8828 - rmse: 21635.9102 - val_loss: 20311.8965 - val_rmse: 20010.0879\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21900.3730 - rmse: 21936.1484\n",
      "Epoch 23: val_loss did not improve from 20311.89648\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21900.3730 - rmse: 21936.1484 - val_loss: 20874.7598 - val_rmse: 20567.0469\n",
      "Epoch 24/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21815.4043 - rmse: 21809.1777\n",
      "Epoch 24: val_loss improved from 20311.89648 to 20229.66406, saving model to ./ckpt/5_class_lr005/val_rmse_19859.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 21815.4043 - rmse: 21809.1777 - val_loss: 20229.6641 - val_rmse: 19859.0957\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21643.3359 - rmse: 21638.4902\n",
      "Epoch 25: val_loss improved from 20229.66406 to 19283.44922, saving model to ./ckpt/5_class_lr005/val_rmse_18912.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21643.3359 - rmse: 21638.4902 - val_loss: 19283.4492 - val_rmse: 18912.0000\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20729.8027 - rmse: 20729.8027\n",
      "Epoch 26: val_loss did not improve from 19283.44922\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20721.2637 - rmse: 20716.2129 - val_loss: 23943.1387 - val_rmse: 24070.4805\n",
      "Epoch 27/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21743.7363 - rmse: 21743.7363\n",
      "Epoch 27: val_loss did not improve from 19283.44922\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 21958.2598 - rmse: 22017.7539 - val_loss: 22087.0488 - val_rmse: 21833.5039\n",
      "Epoch 28/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21447.3281 - rmse: 21431.9473\n",
      "Epoch 28: val_loss did not improve from 19283.44922\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21447.3281 - rmse: 21431.9473 - val_loss: 27483.2656 - val_rmse: 27564.6641\n",
      "Epoch 29/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23836.6270 - rmse: 23836.6270\n",
      "Epoch 29: val_loss did not improve from 19283.44922\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23613.5371 - rmse: 23591.5879 - val_loss: 20337.8203 - val_rmse: 20048.3711\n",
      "Epoch 30/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20572.9512 - rmse: 20572.9512\n",
      "Epoch 30: val_loss did not improve from 19283.44922\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20673.5410 - rmse: 20685.6387 - val_loss: 43329.8477 - val_rmse: 43735.8750\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23532.7539 - rmse: 23499.5117\n",
      "Epoch 31: val_loss did not improve from 19283.44922\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23532.7539 - rmse: 23499.5117 - val_loss: 25599.3477 - val_rmse: 25605.2246\n",
      "Epoch 32/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20324.1875 - rmse: 20324.1875\n",
      "Epoch 32: val_loss did not improve from 19283.44922\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20674.1172 - rmse: 20697.2891 - val_loss: 31889.5332 - val_rmse: 31786.0078\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22979.7266 - rmse: 22990.5703\n",
      "Epoch 33: val_loss did not improve from 19283.44922\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22979.7266 - rmse: 22990.5703 - val_loss: 20304.7266 - val_rmse: 20112.0430\n",
      "Epoch 34/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22604.1875 - rmse: 22604.1875\n",
      "Epoch 34: val_loss did not improve from 19283.44922\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22733.9648 - rmse: 22746.4062 - val_loss: 20661.5059 - val_rmse: 20424.5742\n",
      "Epoch 35/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20537.2207 - rmse: 20537.2207\n",
      "Epoch 35: val_loss did not improve from 19283.44922\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20375.8848 - rmse: 20348.6074 - val_loss: 20652.8281 - val_rmse: 20513.1133\n",
      "Epoch 36/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19634.9160 - rmse: 19634.9160\n",
      "Epoch 36: val_loss did not improve from 19283.44922\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19863.5273 - rmse: 19843.6367 - val_loss: 22470.8516 - val_rmse: 22172.5977\n",
      "Epoch 37/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21681.7656 - rmse: 21681.7656\n",
      "Epoch 37: val_loss did not improve from 19283.44922\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 21687.6348 - rmse: 21657.9102 - val_loss: 19300.7227 - val_rmse: 19088.6816\n",
      "Epoch 38/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20181.6914 - rmse: 20181.6914\n",
      "Epoch 38: val_loss did not improve from 19283.44922\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20053.0391 - rmse: 20014.1172 - val_loss: 27470.3809 - val_rmse: 27701.9844\n",
      "Epoch 39/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19530.1367 - rmse: 19530.1367\n",
      "Epoch 39: val_loss did not improve from 19283.44922\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19471.8164 - rmse: 19478.9004 - val_loss: 20897.4590 - val_rmse: 20635.7266\n",
      "Epoch 40/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20447.0625 - rmse: 20447.0625\n",
      "Epoch 40: val_loss improved from 19283.44922 to 18404.63672, saving model to ./ckpt/5_class_lr005/val_rmse_18020.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20446.3730 - rmse: 20451.0723 - val_loss: 18404.6367 - val_rmse: 18019.7852\n",
      "Epoch 41/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20608.8945 - rmse: 20608.8945\n",
      "Epoch 41: val_loss did not improve from 18404.63672\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20768.6348 - rmse: 20778.3320 - val_loss: 23520.3477 - val_rmse: 23314.6816\n",
      "Epoch 42/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18621.8730 - rmse: 18621.8730\n",
      "Epoch 42: val_loss did not improve from 18404.63672\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18745.3516 - rmse: 18791.5527 - val_loss: 21538.1875 - val_rmse: 21271.8145\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17845.9258 - rmse: 17845.9258\n",
      "Epoch 43: val_loss did not improve from 18404.63672\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17813.2402 - rmse: 17793.9082 - val_loss: 22436.7246 - val_rmse: 22771.5645\n",
      "Epoch 44/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19600.6250 - rmse: 19600.6250\n",
      "Epoch 44: val_loss did not improve from 18404.63672\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 19812.3848 - rmse: 19836.3125 - val_loss: 27650.9512 - val_rmse: 27707.1367\n",
      "Epoch 45/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20002.6406 - rmse: 20002.6406\n",
      "Epoch 45: val_loss did not improve from 18404.63672\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20134.4688 - rmse: 20130.3789 - val_loss: 23280.8730 - val_rmse: 23251.4648\n",
      "Epoch 46/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20397.8242 - rmse: 20397.8242\n",
      "Epoch 46: val_loss did not improve from 18404.63672\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20166.7285 - rmse: 20156.1758 - val_loss: 18644.0039 - val_rmse: 18303.7734\n",
      "Epoch 47/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18485.3496 - rmse: 18485.3496\n",
      "Epoch 47: val_loss did not improve from 18404.63672\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18321.0527 - rmse: 18291.9727 - val_loss: 19308.7305 - val_rmse: 18982.8945\n",
      "Epoch 48/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19506.4160 - rmse: 19506.4160\n",
      "Epoch 48: val_loss did not improve from 18404.63672\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19464.1289 - rmse: 19426.3008 - val_loss: 20597.6973 - val_rmse: 20649.0508\n",
      "Epoch 49/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19407.3223 - rmse: 19366.7480\n",
      "Epoch 49: val_loss did not improve from 18404.63672\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19407.3223 - rmse: 19366.7480 - val_loss: 18776.7148 - val_rmse: 18473.8086\n",
      "Epoch 50/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20075.6895 - rmse: 20081.3496\n",
      "Epoch 50: val_loss did not improve from 18404.63672\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20075.6895 - rmse: 20081.3496 - val_loss: 20103.5742 - val_rmse: 20048.9688\n",
      "Epoch 51/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17637.1426 - rmse: 17607.5117\n",
      "Epoch 51: val_loss did not improve from 18404.63672\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17637.1426 - rmse: 17607.5117 - val_loss: 18482.6699 - val_rmse: 18174.6055\n",
      "Epoch 52/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18041.2305 - rmse: 18041.2305\n",
      "Epoch 52: val_loss did not improve from 18404.63672\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18574.0312 - rmse: 18752.5117 - val_loss: 32383.2422 - val_rmse: 32682.4336\n",
      "Epoch 53/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20961.7422 - rmse: 20961.7422\n",
      "Epoch 53: val_loss improved from 18404.63672 to 18136.66406, saving model to ./ckpt/5_class_lr005/val_rmse_17808.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20979.7988 - rmse: 20969.3652 - val_loss: 18136.6641 - val_rmse: 17807.6211\n",
      "Epoch 54/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18070.9102 - rmse: 18070.9102\n",
      "Epoch 54: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18011.3613 - rmse: 18060.0469 - val_loss: 18590.1309 - val_rmse: 18267.3711\n",
      "Epoch 55/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21400.2129 - rmse: 21400.2129\n",
      "Epoch 55: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21214.0117 - rmse: 21173.5547 - val_loss: 20140.4785 - val_rmse: 19886.0078\n",
      "Epoch 56/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17829.1602 - rmse: 17829.1602\n",
      "Epoch 56: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17798.8438 - rmse: 17780.9141 - val_loss: 19134.8945 - val_rmse: 18954.0605\n",
      "Epoch 57/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16381.6846 - rmse: 16381.6846\n",
      "Epoch 57: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16471.2734 - rmse: 16479.8242 - val_loss: 18914.2891 - val_rmse: 18805.5195\n",
      "Epoch 58/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20641.9082 - rmse: 20641.9082\n",
      "Epoch 58: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20774.0195 - rmse: 20852.1543 - val_loss: 20024.9199 - val_rmse: 19730.9531\n",
      "Epoch 59/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19073.2285 - rmse: 19073.2285\n",
      "Epoch 59: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19088.2695 - rmse: 19071.6367 - val_loss: 26307.2969 - val_rmse: 26082.5059\n",
      "Epoch 60/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18829.7051 - rmse: 18829.7051\n",
      "Epoch 60: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18786.8984 - rmse: 18782.6445 - val_loss: 21956.2188 - val_rmse: 22035.2500\n",
      "Epoch 61/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18555.0195 - rmse: 18555.0195\n",
      "Epoch 61: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18606.1719 - rmse: 18626.9961 - val_loss: 20104.6484 - val_rmse: 19917.7227\n",
      "Epoch 62/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17918.3047 - rmse: 17918.3047\n",
      "Epoch 62: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18360.7891 - rmse: 18542.3457 - val_loss: 22868.4453 - val_rmse: 22909.6289\n",
      "Epoch 63/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17022.1641 - rmse: 17022.1641\n",
      "Epoch 63: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16920.5352 - rmse: 16923.0547 - val_loss: 19056.4668 - val_rmse: 18942.3164\n",
      "Epoch 64/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17033.9473 - rmse: 17020.3359\n",
      "Epoch 64: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17033.9473 - rmse: 17020.3359 - val_loss: 22410.6094 - val_rmse: 22458.5195\n",
      "Epoch 65/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16722.1055 - rmse: 16742.8359\n",
      "Epoch 65: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16722.1055 - rmse: 16742.8359 - val_loss: 19346.3027 - val_rmse: 19206.4062\n",
      "Epoch 66/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18175.0234 - rmse: 18175.0234\n",
      "Epoch 66: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18120.8145 - rmse: 18105.6816 - val_loss: 20374.5312 - val_rmse: 20195.2285\n",
      "Epoch 67/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16578.0098 - rmse: 16578.0098\n",
      "Epoch 67: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16709.8496 - rmse: 16787.8242 - val_loss: 26134.1309 - val_rmse: 26231.5000\n",
      "Epoch 68/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16823.1152 - rmse: 16823.1152\n",
      "Epoch 68: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16880.1289 - rmse: 16908.1211 - val_loss: 22841.3906 - val_rmse: 22634.6465\n",
      "Epoch 69/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17764.8828 - rmse: 17798.8242\n",
      "Epoch 69: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17764.8828 - rmse: 17798.8242 - val_loss: 20000.9551 - val_rmse: 19809.0488\n",
      "Epoch 70/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16727.9980 - rmse: 16750.4453\n",
      "Epoch 70: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16727.9980 - rmse: 16750.4453 - val_loss: 19072.7637 - val_rmse: 18786.4473\n",
      "Epoch 71/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16486.1465 - rmse: 16486.1465\n",
      "Epoch 71: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16424.8828 - rmse: 16410.6035 - val_loss: 20196.4629 - val_rmse: 19949.0234\n",
      "Epoch 72/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16455.3496 - rmse: 16455.3496\n",
      "Epoch 72: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16730.9824 - rmse: 16760.2168 - val_loss: 19313.3301 - val_rmse: 19064.5176\n",
      "Epoch 73/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17020.1289 - rmse: 17020.1289\n",
      "Epoch 73: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16877.1523 - rmse: 16879.8691 - val_loss: 23343.3574 - val_rmse: 23178.0254\n",
      "Epoch 74/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16512.6387 - rmse: 16512.6387\n",
      "Epoch 74: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16555.9844 - rmse: 16544.2383 - val_loss: 20968.2656 - val_rmse: 21054.7090\n",
      "Epoch 75/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16880.4707 - rmse: 16887.3145\n",
      "Epoch 75: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16880.4707 - rmse: 16887.3145 - val_loss: 18989.2754 - val_rmse: 18968.1699\n",
      "Epoch 76/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16930.7930 - rmse: 16930.7930\n",
      "Epoch 76: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17108.9062 - rmse: 17132.0625 - val_loss: 21323.6875 - val_rmse: 21218.5156\n",
      "Epoch 77/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16114.6104 - rmse: 16114.6104\n",
      "Epoch 77: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16030.8516 - rmse: 16013.8711 - val_loss: 22235.1816 - val_rmse: 22059.9043\n",
      "Epoch 78/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15701.9893 - rmse: 15701.9893\n",
      "Epoch 78: val_loss did not improve from 18136.66406\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 15865.7148 - rmse: 15899.4326 - val_loss: 19895.7988 - val_rmse: 19697.5918\n",
      "Epoch 78: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:15:06.687922: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 89752.7812 - rmse: 89486.4609\n",
      "Epoch 1: val_loss improved from inf to 40249.63281, saving model to ./ckpt/5_class_lr005/val_rmse_40105.hdf5\n",
      "70/70 [==============================] - 2s 15ms/step - loss: 89752.7812 - rmse: 89486.4609 - val_loss: 40249.6328 - val_rmse: 40104.6719\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 35148.2891 - rmse: 35148.2891"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:15:07.835782: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 47569.0586 - rmse: 47563.4219\n",
      "Epoch 2: val_loss improved from 40249.63281 to 36465.07812, saving model to ./ckpt/5_class_lr005/val_rmse_36130.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 47569.0586 - rmse: 47563.4219 - val_loss: 36465.0781 - val_rmse: 36130.4141\n",
      "Epoch 3/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 44522.7969 - rmse: 44382.0430\n",
      "Epoch 3: val_loss improved from 36465.07812 to 32565.53711, saving model to ./ckpt/5_class_lr005/val_rmse_32281.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 44522.7969 - rmse: 44382.0430 - val_loss: 32565.5371 - val_rmse: 32281.1309\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 41754.0039 - rmse: 41649.4883\n",
      "Epoch 4: val_loss improved from 32565.53711 to 29581.62500, saving model to ./ckpt/5_class_lr005/val_rmse_29376.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 41754.0039 - rmse: 41649.4883 - val_loss: 29581.6250 - val_rmse: 29376.3672\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 36014.2969 - rmse: 36014.2969\n",
      "Epoch 5: val_loss improved from 29581.62500 to 28894.31250, saving model to ./ckpt/5_class_lr005/val_rmse_28654.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 35871.8789 - rmse: 35787.6445 - val_loss: 28894.3125 - val_rmse: 28654.4648\n",
      "Epoch 6/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 35827.9648 - rmse: 35827.9648\n",
      "Epoch 6: val_loss did not improve from 28894.31250\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 36152.1875 - rmse: 36193.4102 - val_loss: 32606.5410 - val_rmse: 32357.3672\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 34149.5547 - rmse: 34086.6836\n",
      "Epoch 7: val_loss did not improve from 28894.31250\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 34149.5547 - rmse: 34086.6836 - val_loss: 33144.5195 - val_rmse: 33167.1211\n",
      "Epoch 8/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 33653.1289 - rmse: 33653.1289\n",
      "Epoch 8: val_loss did not improve from 28894.31250\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 33580.0703 - rmse: 33585.3047 - val_loss: 36389.7383 - val_rmse: 36289.5508\n",
      "Epoch 9/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 38526.3281 - rmse: 38526.3281\n",
      "Epoch 9: val_loss did not improve from 28894.31250\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 37932.6484 - rmse: 37863.0547 - val_loss: 30662.8262 - val_rmse: 30465.1094\n",
      "Epoch 10/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 32819.1758 - rmse: 32819.1758\n",
      "Epoch 10: val_loss improved from 28894.31250 to 25464.58398, saving model to ./ckpt/5_class_lr005/val_rmse_25335.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 32616.0469 - rmse: 32518.7285 - val_loss: 25464.5840 - val_rmse: 25335.3750\n",
      "Epoch 11/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27680.5801 - rmse: 27680.5801\n",
      "Epoch 11: val_loss improved from 25464.58398 to 24943.13672, saving model to ./ckpt/5_class_lr005/val_rmse_24712.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 27979.8809 - rmse: 27954.3945 - val_loss: 24943.1367 - val_rmse: 24712.2852\n",
      "Epoch 12/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 31961.5723 - rmse: 31961.5723\n",
      "Epoch 12: val_loss improved from 24943.13672 to 23101.09180, saving model to ./ckpt/5_class_lr005/val_rmse_22916.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 31618.5039 - rmse: 31586.5508 - val_loss: 23101.0918 - val_rmse: 22915.7188\n",
      "Epoch 13/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 28890.9648 - rmse: 28890.9648\n",
      "Epoch 13: val_loss did not improve from 23101.09180\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 28916.8633 - rmse: 28848.6055 - val_loss: 29522.1289 - val_rmse: 29518.4102\n",
      "Epoch 14/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 27458.3848 - rmse: 27458.3848\n",
      "Epoch 14: val_loss improved from 23101.09180 to 19889.87500, saving model to ./ckpt/5_class_lr005/val_rmse_19720.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 27487.2188 - rmse: 27455.1953 - val_loss: 19889.8750 - val_rmse: 19719.9648\n",
      "Epoch 15/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 27500.8652 - rmse: 27500.8652\n",
      "Epoch 15: val_loss did not improve from 19889.87500\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 27456.9238 - rmse: 27390.1387 - val_loss: 25394.6035 - val_rmse: 25253.5996\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 24665.1992 - rmse: 24665.1992\n",
      "Epoch 16: val_loss did not improve from 19889.87500\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 24776.7852 - rmse: 24761.1250 - val_loss: 36350.3438 - val_rmse: 36175.4648\n",
      "Epoch 17/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 26584.1777 - rmse: 26584.1777\n",
      "Epoch 17: val_loss did not improve from 19889.87500\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 26793.6855 - rmse: 26808.1738 - val_loss: 31542.5371 - val_rmse: 31371.8555\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27510.4844 - rmse: 27510.4844\n",
      "Epoch 18: val_loss did not improve from 19889.87500\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 27345.9238 - rmse: 27365.5605 - val_loss: 24178.4219 - val_rmse: 24076.8047\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27324.5703 - rmse: 27324.5703\n",
      "Epoch 19: val_loss improved from 19889.87500 to 17077.37891, saving model to ./ckpt/5_class_lr005/val_rmse_17019.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 27236.6191 - rmse: 27271.1895 - val_loss: 17077.3789 - val_rmse: 17019.0352\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 24279.0684 - rmse: 24279.0684\n",
      "Epoch 20: val_loss did not improve from 17077.37891\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 24324.7656 - rmse: 24345.0312 - val_loss: 17541.5820 - val_rmse: 17512.1074\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24646.4844 - rmse: 24646.4844\n",
      "Epoch 21: val_loss did not improve from 17077.37891\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 24420.9492 - rmse: 24398.6621 - val_loss: 21494.4180 - val_rmse: 21303.9512\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 24204.7168 - rmse: 24204.7168\n",
      "Epoch 22: val_loss did not improve from 17077.37891\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23835.8359 - rmse: 23810.5781 - val_loss: 17989.0352 - val_rmse: 17911.2246\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23878.4863 - rmse: 23878.4863\n",
      "Epoch 23: val_loss improved from 17077.37891 to 16240.41016, saving model to ./ckpt/5_class_lr005/val_rmse_16123.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23600.0605 - rmse: 23565.2500 - val_loss: 16240.4102 - val_rmse: 16122.7051\n",
      "Epoch 24/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22496.5703 - rmse: 22496.5703\n",
      "Epoch 24: val_loss did not improve from 16240.41016\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22564.0430 - rmse: 22512.4199 - val_loss: 17934.1191 - val_rmse: 17765.8164\n",
      "Epoch 25/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 24314.2598 - rmse: 24314.2598\n",
      "Epoch 25: val_loss improved from 16240.41016 to 16058.34473, saving model to ./ckpt/5_class_lr005/val_rmse_16072.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 24264.8750 - rmse: 24235.6660 - val_loss: 16058.3447 - val_rmse: 16071.7754\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23573.8848 - rmse: 23624.7500\n",
      "Epoch 26: val_loss improved from 16058.34473 to 15385.61621, saving model to ./ckpt/5_class_lr005/val_rmse_15308.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 23573.8848 - rmse: 23624.7500 - val_loss: 15385.6162 - val_rmse: 15308.2373\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22692.3828 - rmse: 22692.3828\n",
      "Epoch 27: val_loss did not improve from 15385.61621\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22603.7051 - rmse: 22580.6934 - val_loss: 19558.8867 - val_rmse: 19454.4922\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23727.6133 - rmse: 23727.6133\n",
      "Epoch 28: val_loss did not improve from 15385.61621\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 23697.6914 - rmse: 23679.9961 - val_loss: 19476.7988 - val_rmse: 19415.7930\n",
      "Epoch 29/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21391.9980 - rmse: 21391.9980\n",
      "Epoch 29: val_loss did not improve from 15385.61621\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21235.0078 - rmse: 21247.5137 - val_loss: 15512.1221 - val_rmse: 15468.5156\n",
      "Epoch 30/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20431.6973 - rmse: 20431.6973\n",
      "Epoch 30: val_loss did not improve from 15385.61621\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20263.1387 - rmse: 20251.1504 - val_loss: 18973.7500 - val_rmse: 18897.4531\n",
      "Epoch 31/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23056.6641 - rmse: 23056.6641\n",
      "Epoch 31: val_loss did not improve from 15385.61621\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23194.4609 - rmse: 23350.0059 - val_loss: 18080.6406 - val_rmse: 18041.5742\n",
      "Epoch 32/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21382.4219 - rmse: 21382.4219\n",
      "Epoch 32: val_loss did not improve from 15385.61621\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21531.7383 - rmse: 21516.7773 - val_loss: 18401.7539 - val_rmse: 18324.7402\n",
      "Epoch 33/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21696.7500 - rmse: 21696.7500\n",
      "Epoch 33: val_loss did not improve from 15385.61621\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21887.6816 - rmse: 21886.0527 - val_loss: 15387.3506 - val_rmse: 15378.6826\n",
      "Epoch 34/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19410.8379 - rmse: 19410.8379\n",
      "Epoch 34: val_loss did not improve from 15385.61621\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19309.7734 - rmse: 19307.2812 - val_loss: 16554.8125 - val_rmse: 16522.7969\n",
      "Epoch 35/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20908.6738 - rmse: 20908.6738\n",
      "Epoch 35: val_loss did not improve from 15385.61621\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20909.6211 - rmse: 20902.9102 - val_loss: 17384.7832 - val_rmse: 17355.1113\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19307.7363 - rmse: 19260.5645\n",
      "Epoch 36: val_loss did not improve from 15385.61621\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19307.7363 - rmse: 19260.5645 - val_loss: 16020.4209 - val_rmse: 16042.5566\n",
      "Epoch 37/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20431.6230 - rmse: 20431.6230\n",
      "Epoch 37: val_loss did not improve from 15385.61621\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20323.7617 - rmse: 20319.3223 - val_loss: 18883.3730 - val_rmse: 18879.1152\n",
      "Epoch 38/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20730.2695 - rmse: 20730.2695\n",
      "Epoch 38: val_loss did not improve from 15385.61621\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20680.5410 - rmse: 20673.5332 - val_loss: 26234.7793 - val_rmse: 26239.3145\n",
      "Epoch 39/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19238.2637 - rmse: 19238.2637\n",
      "Epoch 39: val_loss improved from 15385.61621 to 14137.56055, saving model to ./ckpt/5_class_lr005/val_rmse_14143.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19458.8320 - rmse: 19482.8711 - val_loss: 14137.5605 - val_rmse: 14143.4971\n",
      "Epoch 40/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20375.3027 - rmse: 20375.3027\n",
      "Epoch 40: val_loss improved from 14137.56055 to 14005.20703, saving model to ./ckpt/5_class_lr005/val_rmse_14014.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20297.3750 - rmse: 20275.7754 - val_loss: 14005.2070 - val_rmse: 14014.2959\n",
      "Epoch 41/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21066.2070 - rmse: 21066.2070\n",
      "Epoch 41: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20961.7520 - rmse: 20964.8594 - val_loss: 15503.7178 - val_rmse: 15529.8350\n",
      "Epoch 42/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18745.6016 - rmse: 18765.8828\n",
      "Epoch 42: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 18745.6016 - rmse: 18765.8828 - val_loss: 15206.8721 - val_rmse: 15183.4727\n",
      "Epoch 43/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18912.9883 - rmse: 18912.9883\n",
      "Epoch 43: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19087.4648 - rmse: 19111.9258 - val_loss: 15117.6719 - val_rmse: 15076.7500\n",
      "Epoch 44/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19575.3438 - rmse: 19575.3438\n",
      "Epoch 44: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19367.9434 - rmse: 19345.7188 - val_loss: 17855.2793 - val_rmse: 17769.7949\n",
      "Epoch 45/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18558.7500 - rmse: 18558.7500\n",
      "Epoch 45: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18790.9180 - rmse: 18828.4023 - val_loss: 15837.1875 - val_rmse: 15884.4609\n",
      "Epoch 46/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20371.8047 - rmse: 20371.8047\n",
      "Epoch 46: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20303.1406 - rmse: 20309.5508 - val_loss: 22056.1094 - val_rmse: 22027.2188\n",
      "Epoch 47/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18421.1309 - rmse: 18421.1309\n",
      "Epoch 47: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18570.1719 - rmse: 18585.0957 - val_loss: 28604.9980 - val_rmse: 28591.6172\n",
      "Epoch 48/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18381.8926 - rmse: 18398.6836\n",
      "Epoch 48: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18381.8926 - rmse: 18398.6836 - val_loss: 16898.3906 - val_rmse: 16889.9746\n",
      "Epoch 49/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18368.9297 - rmse: 18368.9297\n",
      "Epoch 49: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18416.8262 - rmse: 18425.2969 - val_loss: 16566.3438 - val_rmse: 16548.0723\n",
      "Epoch 50/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22035.4629 - rmse: 22035.4629\n",
      "Epoch 50: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22353.9297 - rmse: 22341.4219 - val_loss: 23980.5273 - val_rmse: 23885.3242\n",
      "Epoch 51/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19716.1816 - rmse: 19716.1816\n",
      "Epoch 51: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19611.6797 - rmse: 19623.1973 - val_loss: 14966.1006 - val_rmse: 14915.6699\n",
      "Epoch 52/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18083.7051 - rmse: 18083.7051\n",
      "Epoch 52: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18117.7578 - rmse: 18137.8965 - val_loss: 17080.5781 - val_rmse: 17067.6172\n",
      "Epoch 53/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18027.5078 - rmse: 18027.5078\n",
      "Epoch 53: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18148.4531 - rmse: 18165.5039 - val_loss: 14259.7939 - val_rmse: 14237.0605\n",
      "Epoch 54/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18401.4629 - rmse: 18401.4629\n",
      "Epoch 54: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18344.0410 - rmse: 18374.7051 - val_loss: 21149.5703 - val_rmse: 21092.5547\n",
      "Epoch 55/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17928.8789 - rmse: 17928.8789\n",
      "Epoch 55: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17879.8730 - rmse: 17867.5840 - val_loss: 15636.2598 - val_rmse: 15537.3711\n",
      "Epoch 56/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17613.7324 - rmse: 17613.7324\n",
      "Epoch 56: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17533.8125 - rmse: 17522.8574 - val_loss: 14555.0430 - val_rmse: 14501.2070\n",
      "Epoch 57/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19606.6895 - rmse: 19606.6895\n",
      "Epoch 57: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19719.8398 - rmse: 19689.7227 - val_loss: 16264.1113 - val_rmse: 16224.3965\n",
      "Epoch 58/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17142.8145 - rmse: 17142.8145\n",
      "Epoch 58: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17375.6660 - rmse: 17417.2207 - val_loss: 14673.2090 - val_rmse: 14623.5166\n",
      "Epoch 59/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18830.0215 - rmse: 18830.0215\n",
      "Epoch 59: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18820.1465 - rmse: 18842.0938 - val_loss: 15756.9102 - val_rmse: 15720.0957\n",
      "Epoch 60/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18194.8633 - rmse: 18194.8633\n",
      "Epoch 60: val_loss did not improve from 14005.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18058.2031 - rmse: 18041.7852 - val_loss: 17795.6934 - val_rmse: 17721.3398\n",
      "Epoch 61/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17378.0449 - rmse: 17378.0449\n",
      "Epoch 61: val_loss improved from 14005.20703 to 13840.36426, saving model to ./ckpt/5_class_lr005/val_rmse_13775.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17561.7070 - rmse: 17603.7754 - val_loss: 13840.3643 - val_rmse: 13775.2305\n",
      "Epoch 62/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17030.2207 - rmse: 17030.2207\n",
      "Epoch 62: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17113.0117 - rmse: 17124.9902 - val_loss: 15620.0068 - val_rmse: 15485.4766\n",
      "Epoch 63/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18345.4492 - rmse: 18345.4492\n",
      "Epoch 63: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18305.2559 - rmse: 18285.7051 - val_loss: 15054.4746 - val_rmse: 15017.3613\n",
      "Epoch 64/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17694.7617 - rmse: 17694.7617\n",
      "Epoch 64: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17660.6191 - rmse: 17681.4395 - val_loss: 16248.0312 - val_rmse: 16115.1338\n",
      "Epoch 65/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18606.2871 - rmse: 18606.2871\n",
      "Epoch 65: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18711.5996 - rmse: 18735.3906 - val_loss: 23358.9668 - val_rmse: 23307.4688\n",
      "Epoch 66/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17890.3438 - rmse: 17890.3438\n",
      "Epoch 66: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17913.6074 - rmse: 17914.5723 - val_loss: 20537.4473 - val_rmse: 20504.4941\n",
      "Epoch 67/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17198.3379 - rmse: 17198.3379\n",
      "Epoch 67: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17329.6367 - rmse: 17396.1934 - val_loss: 16985.2402 - val_rmse: 17014.2500\n",
      "Epoch 68/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18259.1953 - rmse: 18259.1953\n",
      "Epoch 68: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18121.3555 - rmse: 18102.6895 - val_loss: 14576.6816 - val_rmse: 14522.7129\n",
      "Epoch 69/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16192.1172 - rmse: 16213.8701\n",
      "Epoch 69: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16192.1172 - rmse: 16213.8701 - val_loss: 14007.0801 - val_rmse: 14034.4258\n",
      "Epoch 70/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16420.0527 - rmse: 16420.0527\n",
      "Epoch 70: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16467.4863 - rmse: 16495.5410 - val_loss: 16613.4473 - val_rmse: 16588.7715\n",
      "Epoch 71/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17612.4336 - rmse: 17612.4336\n",
      "Epoch 71: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17667.0410 - rmse: 17699.3379 - val_loss: 17818.7461 - val_rmse: 17798.5742\n",
      "Epoch 72/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17876.4531 - rmse: 17876.4531\n",
      "Epoch 72: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17939.0781 - rmse: 17969.4121 - val_loss: 13930.8467 - val_rmse: 13951.2188\n",
      "Epoch 73/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18078.9258 - rmse: 18078.9258\n",
      "Epoch 73: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17908.1504 - rmse: 17915.5215 - val_loss: 14516.4111 - val_rmse: 14428.0713\n",
      "Epoch 74/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16091.1895 - rmse: 16091.1895\n",
      "Epoch 74: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16062.2227 - rmse: 16044.9326 - val_loss: 17809.9121 - val_rmse: 17786.3965\n",
      "Epoch 75/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17345.3516 - rmse: 17345.3516\n",
      "Epoch 75: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17280.1758 - rmse: 17302.0703 - val_loss: 14703.2295 - val_rmse: 14660.5596\n",
      "Epoch 76/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16648.8594 - rmse: 16648.8594\n",
      "Epoch 76: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16642.6758 - rmse: 16638.2012 - val_loss: 15241.9639 - val_rmse: 15181.7002\n",
      "Epoch 77/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16095.9326 - rmse: 16095.9326\n",
      "Epoch 77: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16134.1025 - rmse: 16124.7051 - val_loss: 19830.2988 - val_rmse: 19766.6816\n",
      "Epoch 78/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16085.9648 - rmse: 16085.9648\n",
      "Epoch 78: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16312.4492 - rmse: 16383.8555 - val_loss: 14567.4629 - val_rmse: 14504.5117\n",
      "Epoch 79/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16183.3057 - rmse: 16183.3057\n",
      "Epoch 79: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16251.7881 - rmse: 16283.3467 - val_loss: 15291.2744 - val_rmse: 15245.6230\n",
      "Epoch 80/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15644.7803 - rmse: 15644.7803\n",
      "Epoch 80: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15710.6016 - rmse: 15697.4824 - val_loss: 20313.6758 - val_rmse: 20234.5039\n",
      "Epoch 81/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17360.4453 - rmse: 17360.4453\n",
      "Epoch 81: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17251.0938 - rmse: 17230.5176 - val_loss: 17582.1211 - val_rmse: 17524.0742\n",
      "Epoch 82/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17270.9766 - rmse: 17270.9766\n",
      "Epoch 82: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17208.9688 - rmse: 17244.6641 - val_loss: 17131.8105 - val_rmse: 17083.7520\n",
      "Epoch 83/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17260.8340 - rmse: 17260.8340\n",
      "Epoch 83: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17353.1211 - rmse: 17350.3398 - val_loss: 15667.0645 - val_rmse: 15663.7959\n",
      "Epoch 84/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15829.7842 - rmse: 15829.7842\n",
      "Epoch 84: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15831.5029 - rmse: 15823.9219 - val_loss: 16259.4717 - val_rmse: 16228.3076\n",
      "Epoch 85/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15168.1992 - rmse: 15168.1992\n",
      "Epoch 85: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15289.2607 - rmse: 15364.7070 - val_loss: 14099.4707 - val_rmse: 14072.7842\n",
      "Epoch 86/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16465.9922 - rmse: 16465.9922\n",
      "Epoch 86: val_loss did not improve from 13840.36426\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16383.4795 - rmse: 16366.6680 - val_loss: 15124.2852 - val_rmse: 15054.7529\n",
      "Epoch 86: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:16:21.099263: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 84020.1094 - rmse: 83846.4766\n",
      "Epoch 1: val_loss improved from inf to 39831.53125, saving model to ./ckpt/5_class_lr005/val_rmse_41426.hdf5\n",
      "70/70 [==============================] - 2s 16ms/step - loss: 84020.1094 - rmse: 83846.4766 - val_loss: 39831.5312 - val_rmse: 41426.1172\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 56872.1484 - rmse: 56872.1484"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:16:22.304125: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 44859.0469 - rmse: 44859.0469\n",
      "Epoch 2: val_loss improved from 39831.53125 to 32548.43555, saving model to ./ckpt/5_class_lr005/val_rmse_34399.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 45250.5039 - rmse: 45241.9805 - val_loss: 32548.4355 - val_rmse: 34398.5195\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 39419.6250 - rmse: 39419.6250\n",
      "Epoch 3: val_loss improved from 32548.43555 to 32130.99219, saving model to ./ckpt/5_class_lr005/val_rmse_33763.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 39167.3125 - rmse: 39147.0469 - val_loss: 32130.9922 - val_rmse: 33763.2227\n",
      "Epoch 4/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 39017.6602 - rmse: 39017.6602\n",
      "Epoch 4: val_loss improved from 32130.99219 to 25505.66211, saving model to ./ckpt/5_class_lr005/val_rmse_26682.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 39100.0781 - rmse: 39065.6211 - val_loss: 25505.6621 - val_rmse: 26682.4980\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 34443.3242 - rmse: 34443.3242\n",
      "Epoch 5: val_loss did not improve from 25505.66211\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 34532.3203 - rmse: 34584.9531 - val_loss: 26302.1543 - val_rmse: 27029.5039\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 35332.8750 - rmse: 35332.8750\n",
      "Epoch 6: val_loss improved from 25505.66211 to 24221.10742, saving model to ./ckpt/5_class_lr005/val_rmse_25314.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 35222.1094 - rmse: 35156.5977 - val_loss: 24221.1074 - val_rmse: 25314.4453\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 34804.0391 - rmse: 34879.4258\n",
      "Epoch 7: val_loss did not improve from 24221.10742\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 34804.0391 - rmse: 34879.4258 - val_loss: 30674.7324 - val_rmse: 31025.0215\n",
      "Epoch 8/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 30535.9863 - rmse: 30535.9863\n",
      "Epoch 8: val_loss improved from 24221.10742 to 22457.31250, saving model to ./ckpt/5_class_lr005/val_rmse_22846.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 30627.6348 - rmse: 30634.0176 - val_loss: 22457.3125 - val_rmse: 22846.2891\n",
      "Epoch 9/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 30319.7754 - rmse: 30319.7754\n",
      "Epoch 9: val_loss improved from 22457.31250 to 20270.21289, saving model to ./ckpt/5_class_lr005/val_rmse_20736.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 30542.1543 - rmse: 30484.1895 - val_loss: 20270.2129 - val_rmse: 20735.6641\n",
      "Epoch 10/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 32769.7383 - rmse: 32769.7383\n",
      "Epoch 10: val_loss did not improve from 20270.21289\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 32855.4023 - rmse: 32924.2305 - val_loss: 26470.3867 - val_rmse: 26751.0508\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 35048.2930 - rmse: 34975.7812\n",
      "Epoch 11: val_loss did not improve from 20270.21289\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 35048.2930 - rmse: 34975.7812 - val_loss: 26007.0273 - val_rmse: 26072.9727\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 30718.1719 - rmse: 30718.1719\n",
      "Epoch 12: val_loss improved from 20270.21289 to 19126.59180, saving model to ./ckpt/5_class_lr005/val_rmse_19404.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 30596.9668 - rmse: 30550.9102 - val_loss: 19126.5918 - val_rmse: 19404.3672\n",
      "Epoch 13/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 26077.8125 - rmse: 26077.8125\n",
      "Epoch 13: val_loss did not improve from 19126.59180\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 26028.5703 - rmse: 26008.3125 - val_loss: 21333.7773 - val_rmse: 21341.6152\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 29392.8281 - rmse: 29409.7578\n",
      "Epoch 14: val_loss did not improve from 19126.59180\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 29392.8281 - rmse: 29409.7578 - val_loss: 32631.2656 - val_rmse: 33240.2734\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 27848.2285 - rmse: 27848.2285\n",
      "Epoch 15: val_loss improved from 19126.59180 to 18427.69141, saving model to ./ckpt/5_class_lr005/val_rmse_18749.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 28052.7090 - rmse: 28173.6445 - val_loss: 18427.6914 - val_rmse: 18748.7539\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 24492.7324 - rmse: 24492.7324\n",
      "Epoch 16: val_loss did not improve from 18427.69141\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 24902.1426 - rmse: 24861.8809 - val_loss: 32992.7109 - val_rmse: 33423.5430\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24895.3887 - rmse: 24885.9277\n",
      "Epoch 17: val_loss improved from 18427.69141 to 16992.95898, saving model to ./ckpt/5_class_lr005/val_rmse_17035.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 24895.3887 - rmse: 24885.9277 - val_loss: 16992.9590 - val_rmse: 17035.0625\n",
      "Epoch 18/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23674.9004 - rmse: 23674.9004\n",
      "Epoch 18: val_loss did not improve from 16992.95898\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 23652.6367 - rmse: 23639.4707 - val_loss: 17867.0215 - val_rmse: 18500.1387\n",
      "Epoch 19/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21737.9316 - rmse: 21737.9316\n",
      "Epoch 19: val_loss did not improve from 16992.95898\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 21648.2129 - rmse: 21604.4277 - val_loss: 17155.6895 - val_rmse: 17152.7910\n",
      "Epoch 20/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 25831.8672 - rmse: 25831.8672\n",
      "Epoch 20: val_loss did not improve from 16992.95898\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 25701.3438 - rmse: 25690.5332 - val_loss: 33828.5977 - val_rmse: 34217.3945\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 25479.9941 - rmse: 25479.9941\n",
      "Epoch 21: val_loss did not improve from 16992.95898\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 25387.8301 - rmse: 25357.4668 - val_loss: 20854.0723 - val_rmse: 21460.9805\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21678.1172 - rmse: 21678.1172\n",
      "Epoch 22: val_loss improved from 16992.95898 to 16794.65820, saving model to ./ckpt/5_class_lr005/val_rmse_16984.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21880.9473 - rmse: 21970.3926 - val_loss: 16794.6582 - val_rmse: 16983.9551\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23094.9102 - rmse: 23094.9102\n",
      "Epoch 23: val_loss did not improve from 16794.65820\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22746.2129 - rmse: 22689.4980 - val_loss: 20558.6035 - val_rmse: 20778.4336\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21019.6523 - rmse: 21019.6523\n",
      "Epoch 24: val_loss did not improve from 16794.65820\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 21016.4141 - rmse: 21014.4980 - val_loss: 21095.7656 - val_rmse: 21766.6758\n",
      "Epoch 25/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20720.1426 - rmse: 20720.1426\n",
      "Epoch 25: val_loss did not improve from 16794.65820\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20809.5195 - rmse: 20862.3789 - val_loss: 17233.6973 - val_rmse: 17259.5879\n",
      "Epoch 26/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20846.6523 - rmse: 20846.6523\n",
      "Epoch 26: val_loss did not improve from 16794.65820\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21073.0195 - rmse: 21143.2129 - val_loss: 17827.8691 - val_rmse: 18369.5684\n",
      "Epoch 27/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 24540.2480 - rmse: 24540.2480\n",
      "Epoch 27: val_loss improved from 16794.65820 to 16563.65820, saving model to ./ckpt/5_class_lr005/val_rmse_16841.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 24271.2480 - rmse: 24221.5859 - val_loss: 16563.6582 - val_rmse: 16841.1895\n",
      "Epoch 28/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21209.3652 - rmse: 21174.3535\n",
      "Epoch 28: val_loss improved from 16563.65820 to 16393.54297, saving model to ./ckpt/5_class_lr005/val_rmse_16441.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21209.3652 - rmse: 21174.3535 - val_loss: 16393.5430 - val_rmse: 16440.6719\n",
      "Epoch 29/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20661.5742 - rmse: 20644.2832\n",
      "Epoch 29: val_loss improved from 16393.54297 to 15681.75391, saving model to ./ckpt/5_class_lr005/val_rmse_15762.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20661.5742 - rmse: 20644.2832 - val_loss: 15681.7539 - val_rmse: 15761.6621\n",
      "Epoch 30/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21255.0527 - rmse: 21234.1582\n",
      "Epoch 30: val_loss did not improve from 15681.75391\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 21255.0527 - rmse: 21234.1582 - val_loss: 23148.6562 - val_rmse: 24009.2578\n",
      "Epoch 31/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20619.5117 - rmse: 20619.5117\n",
      "Epoch 31: val_loss did not improve from 15681.75391\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 20667.8516 - rmse: 20696.4414 - val_loss: 20152.8086 - val_rmse: 20236.7676\n",
      "Epoch 32/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19891.7754 - rmse: 19891.7754\n",
      "Epoch 32: val_loss did not improve from 15681.75391\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19929.1934 - rmse: 19902.1992 - val_loss: 15809.0430 - val_rmse: 16212.4932\n",
      "Epoch 33/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20532.8008 - rmse: 20532.8008\n",
      "Epoch 33: val_loss did not improve from 15681.75391\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20371.1504 - rmse: 20329.3633 - val_loss: 30829.3926 - val_rmse: 31578.9160\n",
      "Epoch 34/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23800.2930 - rmse: 23800.2930\n",
      "Epoch 34: val_loss did not improve from 15681.75391\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23782.3008 - rmse: 23834.5391 - val_loss: 16077.8809 - val_rmse: 16541.0039\n",
      "Epoch 35/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19277.4590 - rmse: 19277.4590\n",
      "Epoch 35: val_loss did not improve from 15681.75391\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19089.4219 - rmse: 19089.3320 - val_loss: 20707.1699 - val_rmse: 20958.1465\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19401.7930 - rmse: 19438.0527\n",
      "Epoch 36: val_loss did not improve from 15681.75391\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19401.7930 - rmse: 19438.0527 - val_loss: 17124.3809 - val_rmse: 17606.4023\n",
      "Epoch 37/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19697.2832 - rmse: 19697.2832\n",
      "Epoch 37: val_loss improved from 15681.75391 to 15336.44043, saving model to ./ckpt/5_class_lr005/val_rmse_15669.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19756.9980 - rmse: 19744.9961 - val_loss: 15336.4404 - val_rmse: 15669.4170\n",
      "Epoch 38/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20405.9043 - rmse: 20405.9043\n",
      "Epoch 38: val_loss did not improve from 15336.44043\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20540.7559 - rmse: 20566.1797 - val_loss: 24194.2559 - val_rmse: 25197.2402\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18441.4531 - rmse: 18441.4531\n",
      "Epoch 39: val_loss did not improve from 15336.44043\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 18447.4824 - rmse: 18451.0508 - val_loss: 15457.8779 - val_rmse: 15563.6543\n",
      "Epoch 40/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19031.9414 - rmse: 19031.9414\n",
      "Epoch 40: val_loss did not improve from 15336.44043\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19037.1230 - rmse: 19025.8672 - val_loss: 17687.0039 - val_rmse: 17791.4473\n",
      "Epoch 41/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17985.3438 - rmse: 17985.3438\n",
      "Epoch 41: val_loss did not improve from 15336.44043\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17984.5879 - rmse: 17954.4727 - val_loss: 16626.0840 - val_rmse: 17132.2383\n",
      "Epoch 42/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18026.0488 - rmse: 18026.0488\n",
      "Epoch 42: val_loss did not improve from 15336.44043\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18034.7051 - rmse: 18039.8223 - val_loss: 16038.9434 - val_rmse: 16217.8613\n",
      "Epoch 43/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20716.6758 - rmse: 20716.6758\n",
      "Epoch 43: val_loss did not improve from 15336.44043\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20822.4297 - rmse: 20857.2344 - val_loss: 20068.5469 - val_rmse: 20535.2598\n",
      "Epoch 44/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19130.1035 - rmse: 19130.1035\n",
      "Epoch 44: val_loss did not improve from 15336.44043\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19247.1504 - rmse: 19208.1543 - val_loss: 15340.6572 - val_rmse: 15515.8809\n",
      "Epoch 45/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17781.9766 - rmse: 17787.5332\n",
      "Epoch 45: val_loss did not improve from 15336.44043\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17781.9766 - rmse: 17787.5332 - val_loss: 16234.9512 - val_rmse: 16478.1172\n",
      "Epoch 46/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19559.3164 - rmse: 19642.4648\n",
      "Epoch 46: val_loss did not improve from 15336.44043\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19559.3164 - rmse: 19642.4648 - val_loss: 19340.3320 - val_rmse: 19683.7949\n",
      "Epoch 47/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19784.3438 - rmse: 19784.3438\n",
      "Epoch 47: val_loss improved from 15336.44043 to 14534.91602, saving model to ./ckpt/5_class_lr005/val_rmse_14719.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19687.2363 - rmse: 19658.2129 - val_loss: 14534.9160 - val_rmse: 14719.0771\n",
      "Epoch 48/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17671.2988 - rmse: 17671.2988\n",
      "Epoch 48: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17868.6133 - rmse: 17884.5957 - val_loss: 23843.1113 - val_rmse: 24234.6133\n",
      "Epoch 49/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20235.6152 - rmse: 20235.6152\n",
      "Epoch 49: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20207.1777 - rmse: 20191.9316 - val_loss: 16911.1934 - val_rmse: 17361.2520\n",
      "Epoch 50/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17076.2949 - rmse: 17076.2949\n",
      "Epoch 50: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17079.3418 - rmse: 17081.1426 - val_loss: 16290.9199 - val_rmse: 16789.3672\n",
      "Epoch 51/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17423.6895 - rmse: 17423.6895\n",
      "Epoch 51: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17422.3418 - rmse: 17421.5449 - val_loss: 14815.4375 - val_rmse: 15006.0547\n",
      "Epoch 52/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17291.3223 - rmse: 17291.3223\n",
      "Epoch 52: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17359.9961 - rmse: 17341.5566 - val_loss: 21574.3047 - val_rmse: 21746.3281\n",
      "Epoch 53/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16597.4219 - rmse: 16597.4219\n",
      "Epoch 53: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16655.9258 - rmse: 16674.1465 - val_loss: 17182.5312 - val_rmse: 17361.4531\n",
      "Epoch 54/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17067.6758 - rmse: 17092.3926\n",
      "Epoch 54: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17067.6758 - rmse: 17092.3926 - val_loss: 16006.1221 - val_rmse: 16205.5312\n",
      "Epoch 55/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17788.5098 - rmse: 17788.5098\n",
      "Epoch 55: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17617.1758 - rmse: 17612.3965 - val_loss: 22262.2480 - val_rmse: 22681.8828\n",
      "Epoch 56/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17900.5996 - rmse: 17900.5996\n",
      "Epoch 56: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17889.2168 - rmse: 17889.4258 - val_loss: 15323.5918 - val_rmse: 15612.3242\n",
      "Epoch 57/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17192.5508 - rmse: 17192.5508\n",
      "Epoch 57: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17195.5371 - rmse: 17197.3027 - val_loss: 22375.9980 - val_rmse: 22809.5547\n",
      "Epoch 58/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17903.5078 - rmse: 17903.5078\n",
      "Epoch 58: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18004.6855 - rmse: 18064.5254 - val_loss: 18630.7656 - val_rmse: 19184.5000\n",
      "Epoch 59/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18129.2637 - rmse: 18129.2637\n",
      "Epoch 59: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17951.5000 - rmse: 17944.3223 - val_loss: 20311.2891 - val_rmse: 20673.8320\n",
      "Epoch 60/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17881.4531 - rmse: 17881.4531\n",
      "Epoch 60: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17735.0156 - rmse: 17706.5742 - val_loss: 15442.9424 - val_rmse: 15875.1055\n",
      "Epoch 61/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18774.2578 - rmse: 18774.2578\n",
      "Epoch 61: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18829.7539 - rmse: 18833.1680 - val_loss: 20584.9883 - val_rmse: 20719.8770\n",
      "Epoch 62/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16569.0547 - rmse: 16569.0547\n",
      "Epoch 62: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 16736.5938 - rmse: 16790.4375 - val_loss: 15552.6562 - val_rmse: 15788.7822\n",
      "Epoch 63/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17442.8945 - rmse: 17442.8945\n",
      "Epoch 63: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17389.1094 - rmse: 17357.2988 - val_loss: 14727.9072 - val_rmse: 15082.6719\n",
      "Epoch 64/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18941.1641 - rmse: 18941.1641\n",
      "Epoch 64: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18960.9941 - rmse: 18931.7090 - val_loss: 17385.9512 - val_rmse: 17655.2754\n",
      "Epoch 65/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16546.7344 - rmse: 16546.7344\n",
      "Epoch 65: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 16442.2676 - rmse: 16414.7031 - val_loss: 15506.0596 - val_rmse: 15913.2393\n",
      "Epoch 66/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17448.8320 - rmse: 17432.1387\n",
      "Epoch 66: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17448.8320 - rmse: 17432.1387 - val_loss: 16833.3516 - val_rmse: 17010.2793\n",
      "Epoch 67/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17771.7090 - rmse: 17771.7090\n",
      "Epoch 67: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17723.0391 - rmse: 17696.1914 - val_loss: 21804.7734 - val_rmse: 22140.5195\n",
      "Epoch 68/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16058.3955 - rmse: 16058.3955\n",
      "Epoch 68: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16034.5322 - rmse: 16020.4199 - val_loss: 16785.0820 - val_rmse: 17026.8379\n",
      "Epoch 69/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16246.5498 - rmse: 16246.5498\n",
      "Epoch 69: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16162.8477 - rmse: 16151.0752 - val_loss: 16511.8145 - val_rmse: 16692.7422\n",
      "Epoch 70/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17540.1934 - rmse: 17540.1934\n",
      "Epoch 70: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17549.2461 - rmse: 17554.5996 - val_loss: 15109.8809 - val_rmse: 15323.7754\n",
      "Epoch 71/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17262.3691 - rmse: 17262.3691\n",
      "Epoch 71: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17295.6484 - rmse: 17326.3281 - val_loss: 17666.4629 - val_rmse: 18169.0898\n",
      "Epoch 72/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15760.8369 - rmse: 15760.8369\n",
      "Epoch 72: val_loss did not improve from 14534.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15969.7979 - rmse: 16006.7930 - val_loss: 25285.2461 - val_rmse: 25900.2266\n",
      "Epoch 72: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:17:25.312435: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 89325.7344 - rmse: 89275.6797\n",
      "Epoch 1: val_loss improved from inf to 40707.83984, saving model to ./ckpt/5_class_lr005/val_rmse_40331.hdf5\n",
      "70/70 [==============================] - 2s 16ms/step - loss: 89325.7344 - rmse: 89275.6797 - val_loss: 40707.8398 - val_rmse: 40330.6680\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 50014.8438 - rmse: 50014.8438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:17:26.514460: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 44932.5117 - rmse: 44932.5117\n",
      "Epoch 2: val_loss improved from 40707.83984 to 29974.71094, saving model to ./ckpt/5_class_lr005/val_rmse_30176.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 44884.5312 - rmse: 44856.1523 - val_loss: 29974.7109 - val_rmse: 30175.5605\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 40557.0703 - rmse: 40557.0703\n",
      "Epoch 3: val_loss did not improve from 29974.71094\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 40583.3945 - rmse: 40567.4219 - val_loss: 30248.2227 - val_rmse: 30401.7363\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 35828.7031 - rmse: 35828.7031\n",
      "Epoch 4: val_loss improved from 29974.71094 to 26493.74414, saving model to ./ckpt/5_class_lr005/val_rmse_26930.hdf5\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 35816.2891 - rmse: 35741.2500 - val_loss: 26493.7441 - val_rmse: 26930.0703\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 35643.2656 - rmse: 35567.0898\n",
      "Epoch 5: val_loss did not improve from 26493.74414\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 35643.2656 - rmse: 35567.0898 - val_loss: 26764.2090 - val_rmse: 27414.9023\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 32910.6836 - rmse: 32910.6836\n",
      "Epoch 6: val_loss did not improve from 26493.74414\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 32872.4336 - rmse: 32871.0117 - val_loss: 30789.0938 - val_rmse: 31386.3281\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 31099.2285 - rmse: 31060.7188\n",
      "Epoch 7: val_loss did not improve from 26493.74414\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 31099.2285 - rmse: 31060.7188 - val_loss: 26505.7773 - val_rmse: 26586.2402\n",
      "Epoch 8/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 30082.7422 - rmse: 30082.7422\n",
      "Epoch 8: val_loss improved from 26493.74414 to 24921.98242, saving model to ./ckpt/5_class_lr005/val_rmse_25703.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 30013.3555 - rmse: 29998.9102 - val_loss: 24921.9824 - val_rmse: 25703.3926\n",
      "Epoch 9/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 30252.2520 - rmse: 30252.2520\n",
      "Epoch 9: val_loss did not improve from 24921.98242\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 30239.7031 - rmse: 30188.1582 - val_loss: 28265.1953 - val_rmse: 28554.2500\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 31169.7070 - rmse: 31143.1211\n",
      "Epoch 10: val_loss improved from 24921.98242 to 24402.10156, saving model to ./ckpt/5_class_lr005/val_rmse_25230.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 31169.7070 - rmse: 31143.1211 - val_loss: 24402.1016 - val_rmse: 25229.5977\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 27512.3809 - rmse: 27512.3809\n",
      "Epoch 11: val_loss improved from 24402.10156 to 22421.38086, saving model to ./ckpt/5_class_lr005/val_rmse_23025.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 27455.9102 - rmse: 27422.5117 - val_loss: 22421.3809 - val_rmse: 23025.1387\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 28239.1094 - rmse: 28239.1094\n",
      "Epoch 12: val_loss improved from 22421.38086 to 21304.85352, saving model to ./ckpt/5_class_lr005/val_rmse_21508.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 28190.6074 - rmse: 28161.9219 - val_loss: 21304.8535 - val_rmse: 21507.9551\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27950.6621 - rmse: 27914.4121\n",
      "Epoch 13: val_loss improved from 21304.85352 to 20203.68555, saving model to ./ckpt/5_class_lr005/val_rmse_20618.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 27950.6621 - rmse: 27914.4121 - val_loss: 20203.6855 - val_rmse: 20617.7422\n",
      "Epoch 14/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 26906.6172 - rmse: 26906.6172\n",
      "Epoch 14: val_loss did not improve from 20203.68555\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 26973.9297 - rmse: 27013.7422 - val_loss: 23411.0527 - val_rmse: 23335.5859\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 25268.8652 - rmse: 25268.8652\n",
      "Epoch 15: val_loss improved from 20203.68555 to 19244.86719, saving model to ./ckpt/5_class_lr005/val_rmse_19536.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 25211.6406 - rmse: 25177.7969 - val_loss: 19244.8672 - val_rmse: 19536.0000\n",
      "Epoch 16/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27698.6055 - rmse: 27737.6230\n",
      "Epoch 16: val_loss did not improve from 19244.86719\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 27698.6055 - rmse: 27737.6230 - val_loss: 23474.2402 - val_rmse: 24125.0469\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 26401.9082 - rmse: 26370.1270\n",
      "Epoch 17: val_loss did not improve from 19244.86719\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 26401.9082 - rmse: 26370.1270 - val_loss: 20346.0000 - val_rmse: 20891.2305\n",
      "Epoch 18/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23687.8809 - rmse: 23687.8809\n",
      "Epoch 18: val_loss did not improve from 19244.86719\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 23742.5625 - rmse: 23741.3633 - val_loss: 34559.6602 - val_rmse: 34197.9102\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23201.5938 - rmse: 23210.0820\n",
      "Epoch 19: val_loss did not improve from 19244.86719\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23201.5938 - rmse: 23210.0820 - val_loss: 20563.5781 - val_rmse: 20472.1289\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21221.0918 - rmse: 21198.0820\n",
      "Epoch 20: val_loss improved from 19244.86719 to 18444.60742, saving model to ./ckpt/5_class_lr005/val_rmse_18862.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21221.0918 - rmse: 21198.0820 - val_loss: 18444.6074 - val_rmse: 18862.0176\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22289.7246 - rmse: 22292.3848\n",
      "Epoch 21: val_loss did not improve from 18444.60742\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22289.7246 - rmse: 22292.3848 - val_loss: 21698.3828 - val_rmse: 22115.2852\n",
      "Epoch 22/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24411.0801 - rmse: 24411.0801\n",
      "Epoch 22: val_loss did not improve from 18444.60742\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 24298.6680 - rmse: 24296.1270 - val_loss: 29697.8867 - val_rmse: 30407.7676\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21619.7402 - rmse: 21619.7402\n",
      "Epoch 23: val_loss did not improve from 18444.60742\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 21644.0586 - rmse: 21658.4395 - val_loss: 22042.8516 - val_rmse: 21837.5801\n",
      "Epoch 24/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23433.3184 - rmse: 23419.3105\n",
      "Epoch 24: val_loss improved from 18444.60742 to 17521.19727, saving model to ./ckpt/5_class_lr005/val_rmse_17906.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23433.3184 - rmse: 23419.3105 - val_loss: 17521.1973 - val_rmse: 17906.3262\n",
      "Epoch 25/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22604.6172 - rmse: 22604.6172\n",
      "Epoch 25: val_loss improved from 17521.19727 to 17001.91602, saving model to ./ckpt/5_class_lr005/val_rmse_17201.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22375.7969 - rmse: 22339.9941 - val_loss: 17001.9160 - val_rmse: 17201.4805\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21212.2012 - rmse: 21271.8281\n",
      "Epoch 26: val_loss did not improve from 17001.91602\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 21212.2012 - rmse: 21271.8281 - val_loss: 20403.3711 - val_rmse: 20984.7402\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23825.1074 - rmse: 23825.1074\n",
      "Epoch 27: val_loss did not improve from 17001.91602\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 23598.7598 - rmse: 23572.0625 - val_loss: 17270.0293 - val_rmse: 17414.7461\n",
      "Epoch 28/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20296.5527 - rmse: 20296.5527\n",
      "Epoch 28: val_loss did not improve from 17001.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20384.5293 - rmse: 20440.0977 - val_loss: 17530.2812 - val_rmse: 17842.0352\n",
      "Epoch 29/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20310.8438 - rmse: 20311.1426\n",
      "Epoch 29: val_loss did not improve from 17001.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20310.8438 - rmse: 20311.1426 - val_loss: 19823.5273 - val_rmse: 19670.4434\n",
      "Epoch 30/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22013.6641 - rmse: 22013.6641\n",
      "Epoch 30: val_loss did not improve from 17001.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21679.4668 - rmse: 21628.4590 - val_loss: 17574.9297 - val_rmse: 17508.2188\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21371.9492 - rmse: 21329.4883\n",
      "Epoch 31: val_loss did not improve from 17001.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21371.9492 - rmse: 21329.4883 - val_loss: 21374.5449 - val_rmse: 21206.1348\n",
      "Epoch 32/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19013.2461 - rmse: 19013.2461\n",
      "Epoch 32: val_loss did not improve from 17001.91602\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19035.3711 - rmse: 19048.4531 - val_loss: 24804.0488 - val_rmse: 25293.7480\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21211.3125 - rmse: 21247.2539\n",
      "Epoch 33: val_loss did not improve from 17001.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21211.3125 - rmse: 21247.2539 - val_loss: 19512.0664 - val_rmse: 19857.7461\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20658.4453 - rmse: 20645.3516\n",
      "Epoch 34: val_loss did not improve from 17001.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20658.4453 - rmse: 20645.3516 - val_loss: 18671.5508 - val_rmse: 19049.8145\n",
      "Epoch 35/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20572.8047 - rmse: 20572.8047\n",
      "Epoch 35: val_loss did not improve from 17001.91602\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20658.1094 - rmse: 20633.9258 - val_loss: 18942.4629 - val_rmse: 19404.4570\n",
      "Epoch 36/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21714.3066 - rmse: 21714.3066\n",
      "Epoch 36: val_loss improved from 17001.91602 to 16516.82617, saving model to ./ckpt/5_class_lr005/val_rmse_16838.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21596.4590 - rmse: 21602.6270 - val_loss: 16516.8262 - val_rmse: 16838.1934\n",
      "Epoch 37/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19112.7617 - rmse: 19086.5371\n",
      "Epoch 37: val_loss did not improve from 16516.82617\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19112.7617 - rmse: 19086.5371 - val_loss: 16899.5000 - val_rmse: 17113.6074\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20221.2656 - rmse: 20221.2656\n",
      "Epoch 38: val_loss did not improve from 16516.82617\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 20205.9258 - rmse: 20196.8516 - val_loss: 18602.4492 - val_rmse: 18471.6504\n",
      "Epoch 39/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18337.3066 - rmse: 18337.3066\n",
      "Epoch 39: val_loss did not improve from 16516.82617\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18263.6230 - rmse: 18248.1270 - val_loss: 18422.7832 - val_rmse: 18348.9902\n",
      "Epoch 40/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19482.5098 - rmse: 19482.5098\n",
      "Epoch 40: val_loss did not improve from 16516.82617\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19642.5938 - rmse: 19673.8457 - val_loss: 17282.5781 - val_rmse: 17411.7344\n",
      "Epoch 41/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19499.0508 - rmse: 19499.0508\n",
      "Epoch 41: val_loss improved from 16516.82617 to 16412.37695, saving model to ./ckpt/5_class_lr005/val_rmse_16369.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19419.3281 - rmse: 19394.6582 - val_loss: 16412.3770 - val_rmse: 16368.6406\n",
      "Epoch 42/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19333.5293 - rmse: 19333.5293\n",
      "Epoch 42: val_loss did not improve from 16412.37695\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19296.6699 - rmse: 19274.8691 - val_loss: 16902.6465 - val_rmse: 17103.9648\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18053.2812 - rmse: 18053.2812\n",
      "Epoch 43: val_loss did not improve from 16412.37695\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18091.4160 - rmse: 18113.9707 - val_loss: 17950.1855 - val_rmse: 18249.2070\n",
      "Epoch 44/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19393.9336 - rmse: 19393.9336\n",
      "Epoch 44: val_loss improved from 16412.37695 to 15471.08496, saving model to ./ckpt/5_class_lr005/val_rmse_15602.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19309.4668 - rmse: 19316.2715 - val_loss: 15471.0850 - val_rmse: 15601.5215\n",
      "Epoch 45/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17738.5020 - rmse: 17738.5020\n",
      "Epoch 45: val_loss did not improve from 15471.08496\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17763.0625 - rmse: 17742.5977 - val_loss: 24705.2520 - val_rmse: 24503.9863\n",
      "Epoch 46/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19912.7168 - rmse: 19912.7168\n",
      "Epoch 46: val_loss did not improve from 15471.08496\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19969.3789 - rmse: 20002.8887 - val_loss: 20673.9668 - val_rmse: 20550.4277\n",
      "Epoch 47/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18543.8066 - rmse: 18543.8066\n",
      "Epoch 47: val_loss did not improve from 15471.08496\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18480.2188 - rmse: 18442.6133 - val_loss: 15764.8750 - val_rmse: 15828.2256\n",
      "Epoch 48/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17484.2891 - rmse: 17484.2891\n",
      "Epoch 48: val_loss did not improve from 15471.08496\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17739.1758 - rmse: 17871.7832 - val_loss: 16145.1670 - val_rmse: 16305.2314\n",
      "Epoch 49/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18932.6836 - rmse: 18932.6836\n",
      "Epoch 49: val_loss did not improve from 15471.08496\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18888.5957 - rmse: 18862.5234 - val_loss: 16575.6875 - val_rmse: 16858.4688\n",
      "Epoch 50/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19624.3711 - rmse: 19624.3711\n",
      "Epoch 50: val_loss did not improve from 15471.08496\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19736.8809 - rmse: 19724.3750 - val_loss: 21458.0508 - val_rmse: 21689.8301\n",
      "Epoch 51/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18369.8672 - rmse: 18369.8672\n",
      "Epoch 51: val_loss did not improve from 15471.08496\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18306.0723 - rmse: 18299.0625 - val_loss: 15779.7637 - val_rmse: 15938.3750\n",
      "Epoch 52/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17539.0488 - rmse: 17539.0488\n",
      "Epoch 52: val_loss did not improve from 15471.08496\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17673.2168 - rmse: 17741.7617 - val_loss: 15556.1748 - val_rmse: 15623.2148\n",
      "Epoch 53/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18217.8457 - rmse: 18230.9492\n",
      "Epoch 53: val_loss did not improve from 15471.08496\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18217.8457 - rmse: 18230.9492 - val_loss: 15837.1133 - val_rmse: 16003.5000\n",
      "Epoch 54/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18256.3262 - rmse: 18238.3242\n",
      "Epoch 54: val_loss improved from 15471.08496 to 15181.32324, saving model to ./ckpt/5_class_lr005/val_rmse_15211.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18256.3262 - rmse: 18238.3242 - val_loss: 15181.3232 - val_rmse: 15211.3682\n",
      "Epoch 55/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17891.6016 - rmse: 17891.6016\n",
      "Epoch 55: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17833.4746 - rmse: 17862.1172 - val_loss: 15244.0049 - val_rmse: 15260.3779\n",
      "Epoch 56/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21591.5801 - rmse: 21591.5801\n",
      "Epoch 56: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 21771.6523 - rmse: 21788.9082 - val_loss: 17817.5586 - val_rmse: 18059.2266\n",
      "Epoch 57/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17501.2207 - rmse: 17501.2207\n",
      "Epoch 57: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 17571.3047 - rmse: 17618.7480 - val_loss: 16296.0850 - val_rmse: 16409.4824\n",
      "Epoch 58/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19259.3770 - rmse: 19259.3770\n",
      "Epoch 58: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19243.6133 - rmse: 19234.2891 - val_loss: 18374.0039 - val_rmse: 18226.1289\n",
      "Epoch 59/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17489.6914 - rmse: 17489.6914\n",
      "Epoch 59: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17439.7461 - rmse: 17407.2969 - val_loss: 15557.3174 - val_rmse: 15748.2334\n",
      "Epoch 60/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17081.7559 - rmse: 17081.7559\n",
      "Epoch 60: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17003.0898 - rmse: 16983.9023 - val_loss: 16456.4453 - val_rmse: 16604.2070\n",
      "Epoch 61/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18785.0586 - rmse: 18785.0586\n",
      "Epoch 61: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18812.6523 - rmse: 18828.9707 - val_loss: 16043.6582 - val_rmse: 16006.3203\n",
      "Epoch 62/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18473.9375 - rmse: 18507.9004\n",
      "Epoch 62: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18473.9375 - rmse: 18507.9004 - val_loss: 17482.6504 - val_rmse: 17708.9395\n",
      "Epoch 63/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17135.9688 - rmse: 17135.9688\n",
      "Epoch 63: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17082.9473 - rmse: 17062.1973 - val_loss: 16075.9678 - val_rmse: 16224.7422\n",
      "Epoch 64/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17754.6582 - rmse: 17754.6582\n",
      "Epoch 64: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17704.3574 - rmse: 17684.9570 - val_loss: 15891.1582 - val_rmse: 16020.1357\n",
      "Epoch 65/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15731.5898 - rmse: 15731.5898\n",
      "Epoch 65: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15787.5049 - rmse: 15834.3496 - val_loss: 24169.2090 - val_rmse: 24061.9043\n",
      "Epoch 66/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19255.4492 - rmse: 19255.4492\n",
      "Epoch 66: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19286.3027 - rmse: 19300.4609 - val_loss: 16641.7852 - val_rmse: 16725.6133\n",
      "Epoch 67/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17495.2324 - rmse: 17495.2324\n",
      "Epoch 67: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17562.1094 - rmse: 17594.4727 - val_loss: 20226.3105 - val_rmse: 20270.1406\n",
      "Epoch 68/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18275.1074 - rmse: 18275.1074\n",
      "Epoch 68: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18455.0410 - rmse: 18425.4512 - val_loss: 18930.0781 - val_rmse: 19099.6504\n",
      "Epoch 69/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17387.9355 - rmse: 17459.4160\n",
      "Epoch 69: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17387.9355 - rmse: 17459.4160 - val_loss: 22171.7480 - val_rmse: 22424.9082\n",
      "Epoch 70/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16364.9893 - rmse: 16357.5449\n",
      "Epoch 70: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16364.9893 - rmse: 16357.5449 - val_loss: 18560.1289 - val_rmse: 18558.1738\n",
      "Epoch 71/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17987.9531 - rmse: 17987.9531\n",
      "Epoch 71: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18007.3242 - rmse: 18018.7793 - val_loss: 16736.6504 - val_rmse: 16663.4844\n",
      "Epoch 72/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17081.5879 - rmse: 17081.5879\n",
      "Epoch 72: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16952.9375 - rmse: 16952.9941 - val_loss: 22599.6113 - val_rmse: 22542.3281\n",
      "Epoch 73/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16592.8164 - rmse: 16592.8164\n",
      "Epoch 73: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16668.9766 - rmse: 16677.0391 - val_loss: 17801.1191 - val_rmse: 18039.9336\n",
      "Epoch 74/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16638.8711 - rmse: 16638.8711\n",
      "Epoch 74: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16434.2988 - rmse: 16432.9316 - val_loss: 16753.0938 - val_rmse: 16858.1191\n",
      "Epoch 75/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17333.6191 - rmse: 17333.6191\n",
      "Epoch 75: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17447.0195 - rmse: 17472.6699 - val_loss: 15538.0703 - val_rmse: 15520.3564\n",
      "Epoch 76/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15698.2236 - rmse: 15698.2236\n",
      "Epoch 76: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 15775.9229 - rmse: 15776.2676 - val_loss: 16177.0361 - val_rmse: 16410.5352\n",
      "Epoch 77/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16712.9570 - rmse: 16712.9570\n",
      "Epoch 77: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16638.9668 - rmse: 16641.0859 - val_loss: 20092.8359 - val_rmse: 20102.0703\n",
      "Epoch 78/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16328.1025 - rmse: 16328.1025\n",
      "Epoch 78: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16376.7812 - rmse: 16392.0312 - val_loss: 16738.8633 - val_rmse: 16904.2324\n",
      "Epoch 79/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16531.3965 - rmse: 16531.3965\n",
      "Epoch 79: val_loss did not improve from 15181.32324\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16443.9961 - rmse: 16444.8262 - val_loss: 16809.7422 - val_rmse: 16818.9902\n",
      "Epoch 79: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:18:35.634414: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 86990.9922 - rmse: 86701.9453\n",
      "Epoch 1: val_loss improved from inf to 45163.82422, saving model to ./ckpt/5_class_lr005/val_rmse_44658.hdf5\n",
      "70/70 [==============================] - 2s 16ms/step - loss: 86990.9922 - rmse: 86701.9453 - val_loss: 45163.8242 - val_rmse: 44657.8438\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 1s - loss: 52398.1055 - rmse: 52398.1055"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:18:36.854131: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 44145.5703 - rmse: 44145.5703\n",
      "Epoch 2: val_loss improved from 45163.82422 to 39543.12891, saving model to ./ckpt/5_class_lr005/val_rmse_39074.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 43863.3867 - rmse: 43809.4453 - val_loss: 39543.1289 - val_rmse: 39074.0664\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 40068.7344 - rmse: 40068.7344\n",
      "Epoch 3: val_loss improved from 39543.12891 to 37821.64062, saving model to ./ckpt/5_class_lr005/val_rmse_37298.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 40211.6133 - rmse: 40296.1133 - val_loss: 37821.6406 - val_rmse: 37297.5586\n",
      "Epoch 4/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 38487.1836 - rmse: 38487.1836\n",
      "Epoch 4: val_loss improved from 37821.64062 to 35364.86328, saving model to ./ckpt/5_class_lr005/val_rmse_35362.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 38193.7773 - rmse: 38143.8555 - val_loss: 35364.8633 - val_rmse: 35362.1602\n",
      "Epoch 5/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 35980.6445 - rmse: 35980.6445\n",
      "Epoch 5: val_loss did not improve from 35364.86328\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 36132.7891 - rmse: 36279.4375 - val_loss: 39858.1797 - val_rmse: 39231.1992\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 34994.1992 - rmse: 34994.1992\n",
      "Epoch 6: val_loss improved from 35364.86328 to 29864.04297, saving model to ./ckpt/5_class_lr005/val_rmse_29785.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 34937.7695 - rmse: 34929.0859 - val_loss: 29864.0430 - val_rmse: 29784.5938\n",
      "Epoch 7/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 34156.0977 - rmse: 34156.0977\n",
      "Epoch 7: val_loss did not improve from 29864.04297\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 34134.0078 - rmse: 34115.3945 - val_loss: 39557.0625 - val_rmse: 39162.8867\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 31173.2949 - rmse: 31173.2949\n",
      "Epoch 8: val_loss improved from 29864.04297 to 25164.22656, saving model to ./ckpt/5_class_lr005/val_rmse_24988.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 31070.1875 - rmse: 31009.2070 - val_loss: 25164.2266 - val_rmse: 24987.5059\n",
      "Epoch 9/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 30362.4277 - rmse: 30362.4277\n",
      "Epoch 9: val_loss did not improve from 25164.22656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 30805.2461 - rmse: 30845.0332 - val_loss: 31884.2051 - val_rmse: 31806.1523\n",
      "Epoch 10/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 29504.0566 - rmse: 29504.0566\n",
      "Epoch 10: val_loss did not improve from 25164.22656\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 29503.6270 - rmse: 29461.8965 - val_loss: 34720.1562 - val_rmse: 34510.3398\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 28075.1387 - rmse: 28075.1387\n",
      "Epoch 11: val_loss improved from 25164.22656 to 21941.85742, saving model to ./ckpt/5_class_lr005/val_rmse_21855.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 28006.7832 - rmse: 27966.3574 - val_loss: 21941.8574 - val_rmse: 21854.7559\n",
      "Epoch 12/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 26025.8984 - rmse: 26025.8984\n",
      "Epoch 12: val_loss improved from 21941.85742 to 20644.54102, saving model to ./ckpt/5_class_lr005/val_rmse_20631.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 26718.8203 - rmse: 26882.5410 - val_loss: 20644.5410 - val_rmse: 20630.7402\n",
      "Epoch 13/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 30116.6016 - rmse: 30116.6016\n",
      "Epoch 13: val_loss did not improve from 20644.54102\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 29750.9316 - rmse: 29731.0938 - val_loss: 24651.4609 - val_rmse: 24635.3887\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27608.3496 - rmse: 27600.5957\n",
      "Epoch 14: val_loss improved from 20644.54102 to 20044.68359, saving model to ./ckpt/5_class_lr005/val_rmse_19968.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 27608.3496 - rmse: 27600.5957 - val_loss: 20044.6836 - val_rmse: 19968.0371\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 26581.5352 - rmse: 26581.5352\n",
      "Epoch 15: val_loss improved from 20044.68359 to 19770.44336, saving model to ./ckpt/5_class_lr005/val_rmse_19779.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 26519.3379 - rmse: 26482.5527 - val_loss: 19770.4434 - val_rmse: 19778.7500\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24927.4082 - rmse: 24927.4082\n",
      "Epoch 16: val_loss did not improve from 19770.44336\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 24783.8145 - rmse: 24762.5996 - val_loss: 19942.4062 - val_rmse: 19860.0918\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 25441.5742 - rmse: 25441.5742\n",
      "Epoch 17: val_loss improved from 19770.44336 to 17998.06445, saving model to ./ckpt/5_class_lr005/val_rmse_17955.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 25405.8242 - rmse: 25384.6797 - val_loss: 17998.0645 - val_rmse: 17955.1055\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24675.0742 - rmse: 24747.6387\n",
      "Epoch 18: val_loss did not improve from 17998.06445\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 24675.0742 - rmse: 24747.6387 - val_loss: 22941.3438 - val_rmse: 22790.8633\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25816.9043 - rmse: 25778.9258\n",
      "Epoch 19: val_loss did not improve from 17998.06445\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 25816.9043 - rmse: 25778.9258 - val_loss: 21391.1387 - val_rmse: 21251.0312\n",
      "Epoch 20/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 26767.0801 - rmse: 26767.0801\n",
      "Epoch 20: val_loss did not improve from 17998.06445\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 26515.9746 - rmse: 26486.2930 - val_loss: 22660.2734 - val_rmse: 22472.9922\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22822.3008 - rmse: 22822.3008\n",
      "Epoch 21: val_loss did not improve from 17998.06445\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22765.7871 - rmse: 22775.3242 - val_loss: 26006.0781 - val_rmse: 25880.7305\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 24294.8945 - rmse: 24294.8945\n",
      "Epoch 22: val_loss did not improve from 17998.06445\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 24397.0840 - rmse: 24457.5195 - val_loss: 23604.7949 - val_rmse: 23362.5195\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22829.4219 - rmse: 22829.4219\n",
      "Epoch 23: val_loss improved from 17998.06445 to 17290.71875, saving model to ./ckpt/5_class_lr005/val_rmse_17277.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22843.3926 - rmse: 22851.6543 - val_loss: 17290.7188 - val_rmse: 17277.1426\n",
      "Epoch 24/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21605.2520 - rmse: 21605.2520\n",
      "Epoch 24: val_loss improved from 17290.71875 to 16943.83594, saving model to ./ckpt/5_class_lr005/val_rmse_16945.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21593.3887 - rmse: 21613.6973 - val_loss: 16943.8359 - val_rmse: 16945.4395\n",
      "Epoch 25/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21736.4102 - rmse: 21736.4102\n",
      "Epoch 25: val_loss improved from 16943.83594 to 16812.15234, saving model to ./ckpt/5_class_lr005/val_rmse_16829.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 21805.3340 - rmse: 21830.5410 - val_loss: 16812.1523 - val_rmse: 16829.0156\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19924.9531 - rmse: 19924.9531\n",
      "Epoch 26: val_loss did not improve from 16812.15234\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20002.2695 - rmse: 20043.6035 - val_loss: 22733.5840 - val_rmse: 22597.4023\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20305.5391 - rmse: 20305.5391\n",
      "Epoch 27: val_loss did not improve from 16812.15234\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20614.0137 - rmse: 20664.1816 - val_loss: 17886.1953 - val_rmse: 17912.2793\n",
      "Epoch 28/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21227.0508 - rmse: 21192.7109\n",
      "Epoch 28: val_loss did not improve from 16812.15234\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21227.0508 - rmse: 21192.7109 - val_loss: 18306.0410 - val_rmse: 18215.1562\n",
      "Epoch 29/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21000.6113 - rmse: 21000.6113\n",
      "Epoch 29: val_loss did not improve from 16812.15234\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20994.3242 - rmse: 21008.1406 - val_loss: 19794.6367 - val_rmse: 19749.6016\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20827.2188 - rmse: 20827.2188\n",
      "Epoch 30: val_loss did not improve from 16812.15234\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20885.9746 - rmse: 20920.7246 - val_loss: 27825.6406 - val_rmse: 27586.9336\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22377.2441 - rmse: 22461.0957\n",
      "Epoch 31: val_loss did not improve from 16812.15234\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22377.2441 - rmse: 22461.0957 - val_loss: 37526.5977 - val_rmse: 37353.6094\n",
      "Epoch 32/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22333.7617 - rmse: 22333.7617\n",
      "Epoch 32: val_loss did not improve from 16812.15234\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22221.1836 - rmse: 22204.9219 - val_loss: 18565.3145 - val_rmse: 18583.3574\n",
      "Epoch 33/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18672.2480 - rmse: 18672.2480\n",
      "Epoch 33: val_loss improved from 16812.15234 to 16472.03320, saving model to ./ckpt/5_class_lr005/val_rmse_16445.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18752.0840 - rmse: 18778.3984 - val_loss: 16472.0332 - val_rmse: 16445.3398\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20529.3711 - rmse: 20609.9648\n",
      "Epoch 34: val_loss did not improve from 16472.03320\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20529.3711 - rmse: 20609.9648 - val_loss: 19020.8379 - val_rmse: 18911.9922\n",
      "Epoch 35/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22446.9844 - rmse: 22446.9844\n",
      "Epoch 35: val_loss did not improve from 16472.03320\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22427.0879 - rmse: 22408.4199 - val_loss: 17710.3438 - val_rmse: 17703.1445\n",
      "Epoch 36/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18688.0996 - rmse: 18688.0996\n",
      "Epoch 36: val_loss improved from 16472.03320 to 15674.92871, saving model to ./ckpt/5_class_lr005/val_rmse_15708.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18640.3633 - rmse: 18612.1289 - val_loss: 15674.9287 - val_rmse: 15707.8252\n",
      "Epoch 37/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20073.7969 - rmse: 20073.7969\n",
      "Epoch 37: val_loss did not improve from 15674.92871\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20032.0449 - rmse: 20021.3516 - val_loss: 17420.4258 - val_rmse: 17424.5469\n",
      "Epoch 38/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21154.0098 - rmse: 21154.0098\n",
      "Epoch 38: val_loss did not improve from 15674.92871\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 21031.3750 - rmse: 21026.8730 - val_loss: 16698.8730 - val_rmse: 16692.9199\n",
      "Epoch 39/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21460.6367 - rmse: 21449.1016\n",
      "Epoch 39: val_loss did not improve from 15674.92871\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 21460.6367 - rmse: 21449.1016 - val_loss: 19385.3379 - val_rmse: 19367.7930\n",
      "Epoch 40/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19242.7695 - rmse: 19242.7695\n",
      "Epoch 40: val_loss did not improve from 15674.92871\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19134.7812 - rmse: 19150.7129 - val_loss: 17308.2324 - val_rmse: 17258.3613\n",
      "Epoch 41/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19393.5781 - rmse: 19421.5957\n",
      "Epoch 41: val_loss did not improve from 15674.92871\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19393.5781 - rmse: 19421.5957 - val_loss: 16287.0654 - val_rmse: 16304.8672\n",
      "Epoch 42/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19673.5859 - rmse: 19673.5859\n",
      "Epoch 42: val_loss did not improve from 15674.92871\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19858.4043 - rmse: 19864.8379 - val_loss: 19784.0957 - val_rmse: 19714.0469\n",
      "Epoch 43/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19695.9141 - rmse: 19695.9141\n",
      "Epoch 43: val_loss did not improve from 15674.92871\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19791.5078 - rmse: 19761.3789 - val_loss: 16555.2969 - val_rmse: 16505.8438\n",
      "Epoch 44/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18804.3281 - rmse: 18857.0527\n",
      "Epoch 44: val_loss improved from 15674.92871 to 15665.27148, saving model to ./ckpt/5_class_lr005/val_rmse_15672.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18804.3281 - rmse: 18857.0527 - val_loss: 15665.2715 - val_rmse: 15671.5039\n",
      "Epoch 45/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18813.4375 - rmse: 18813.4375\n",
      "Epoch 45: val_loss did not improve from 15665.27148\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18895.0449 - rmse: 18917.7305 - val_loss: 17346.4434 - val_rmse: 17351.8535\n",
      "Epoch 46/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18585.2090 - rmse: 18585.2090\n",
      "Epoch 46: val_loss did not improve from 15665.27148\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18620.9004 - rmse: 18642.0098 - val_loss: 20663.0605 - val_rmse: 20502.6719\n",
      "Epoch 47/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19815.6641 - rmse: 19815.6641\n",
      "Epoch 47: val_loss did not improve from 15665.27148\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 19911.7480 - rmse: 19917.4062 - val_loss: 20348.6113 - val_rmse: 20290.1895\n",
      "Epoch 48/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19996.3633 - rmse: 19996.3633\n",
      "Epoch 48: val_loss did not improve from 15665.27148\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20001.5801 - rmse: 20000.0527 - val_loss: 15928.8994 - val_rmse: 15901.1973\n",
      "Epoch 49/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18682.2891 - rmse: 18726.5723\n",
      "Epoch 49: val_loss did not improve from 15665.27148\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18682.2891 - rmse: 18726.5723 - val_loss: 19778.0586 - val_rmse: 19712.6797\n",
      "Epoch 50/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17337.9980 - rmse: 17349.7832\n",
      "Epoch 50: val_loss did not improve from 15665.27148\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17337.9980 - rmse: 17349.7832 - val_loss: 17186.4922 - val_rmse: 17128.0137\n",
      "Epoch 51/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17128.0645 - rmse: 17128.0645\n",
      "Epoch 51: val_loss improved from 15665.27148 to 14990.98242, saving model to ./ckpt/5_class_lr005/val_rmse_14995.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17146.6855 - rmse: 17158.1230 - val_loss: 14990.9824 - val_rmse: 14994.8867\n",
      "Epoch 52/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17270.6895 - rmse: 17270.6895\n",
      "Epoch 52: val_loss did not improve from 14990.98242\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17526.6289 - rmse: 17548.2793 - val_loss: 15723.5732 - val_rmse: 15790.3389\n",
      "Epoch 53/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17686.9629 - rmse: 17686.9629\n",
      "Epoch 53: val_loss did not improve from 14990.98242\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17663.2617 - rmse: 17678.0312 - val_loss: 19453.1289 - val_rmse: 19424.8008\n",
      "Epoch 54/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18531.0449 - rmse: 18531.0449\n",
      "Epoch 54: val_loss improved from 14990.98242 to 14950.56738, saving model to ./ckpt/5_class_lr005/val_rmse_14995.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18396.3125 - rmse: 18373.8457 - val_loss: 14950.5674 - val_rmse: 14995.1562\n",
      "Epoch 55/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18818.5898 - rmse: 18808.4277\n",
      "Epoch 55: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18818.5898 - rmse: 18808.4277 - val_loss: 17888.7422 - val_rmse: 17900.2773\n",
      "Epoch 56/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20036.9863 - rmse: 20036.9863\n",
      "Epoch 56: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20705.6250 - rmse: 20681.3379 - val_loss: 19846.3262 - val_rmse: 19671.1328\n",
      "Epoch 57/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18533.5762 - rmse: 18533.5762\n",
      "Epoch 57: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18531.5684 - rmse: 18530.3789 - val_loss: 18083.1992 - val_rmse: 18033.1875\n",
      "Epoch 58/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17354.2734 - rmse: 17354.4277\n",
      "Epoch 58: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17354.2734 - rmse: 17354.4277 - val_loss: 17072.1855 - val_rmse: 17117.7969\n",
      "Epoch 59/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17756.3320 - rmse: 17756.3320\n",
      "Epoch 59: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17742.0391 - rmse: 17767.4082 - val_loss: 15231.7324 - val_rmse: 15269.0791\n",
      "Epoch 60/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18429.8262 - rmse: 18429.8262\n",
      "Epoch 60: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18394.6484 - rmse: 18373.8418 - val_loss: 22209.5859 - val_rmse: 22195.1230\n",
      "Epoch 61/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17023.2480 - rmse: 17023.2480\n",
      "Epoch 61: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16972.2266 - rmse: 16960.4043 - val_loss: 14965.1367 - val_rmse: 14976.6045\n",
      "Epoch 62/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20348.6621 - rmse: 20348.6621\n",
      "Epoch 62: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20698.6504 - rmse: 20755.3691 - val_loss: 32562.1387 - val_rmse: 32275.2656\n",
      "Epoch 63/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17999.7168 - rmse: 17999.7168\n",
      "Epoch 63: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18180.1133 - rmse: 18192.5566 - val_loss: 21434.4961 - val_rmse: 21303.3457\n",
      "Epoch 64/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17502.0879 - rmse: 17518.7832\n",
      "Epoch 64: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 17502.0879 - rmse: 17518.7832 - val_loss: 18180.1094 - val_rmse: 18262.9062\n",
      "Epoch 65/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17043.9727 - rmse: 17117.6484\n",
      "Epoch 65: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17043.9727 - rmse: 17117.6484 - val_loss: 29811.8340 - val_rmse: 29685.6543\n",
      "Epoch 66/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21123.1348 - rmse: 21123.1348\n",
      "Epoch 66: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21011.3203 - rmse: 20990.9219 - val_loss: 20229.2930 - val_rmse: 20217.8984\n",
      "Epoch 67/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18578.0547 - rmse: 18561.1680\n",
      "Epoch 67: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18578.0547 - rmse: 18561.1680 - val_loss: 15789.4551 - val_rmse: 15806.0215\n",
      "Epoch 68/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17031.1406 - rmse: 17031.1406\n",
      "Epoch 68: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16931.3926 - rmse: 16891.0352 - val_loss: 18915.4453 - val_rmse: 18930.4004\n",
      "Epoch 69/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16555.7090 - rmse: 16555.7090\n",
      "Epoch 69: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16635.2754 - rmse: 16637.4004 - val_loss: 19183.4355 - val_rmse: 19109.4688\n",
      "Epoch 70/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17751.1797 - rmse: 17751.1797\n",
      "Epoch 70: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17865.6191 - rmse: 17892.9980 - val_loss: 20928.6914 - val_rmse: 20907.6445\n",
      "Epoch 71/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16184.5596 - rmse: 16184.5596\n",
      "Epoch 71: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16158.9141 - rmse: 16146.6807 - val_loss: 17269.9824 - val_rmse: 17252.9336\n",
      "Epoch 72/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16142.9229 - rmse: 16142.9229\n",
      "Epoch 72: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16206.3018 - rmse: 16235.0195 - val_loss: 19976.1680 - val_rmse: 19857.1602\n",
      "Epoch 73/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16310.5820 - rmse: 16310.5820\n",
      "Epoch 73: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16570.5938 - rmse: 16615.6875 - val_loss: 23501.6016 - val_rmse: 23375.6641\n",
      "Epoch 74/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16341.8379 - rmse: 16341.8379\n",
      "Epoch 74: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16284.4619 - rmse: 16270.2041 - val_loss: 21925.8887 - val_rmse: 21961.8633\n",
      "Epoch 75/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17678.1445 - rmse: 17678.1445\n",
      "Epoch 75: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17873.6387 - rmse: 17876.2617 - val_loss: 19285.6172 - val_rmse: 19182.9844\n",
      "Epoch 76/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16008.1133 - rmse: 16007.3301\n",
      "Epoch 76: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 16008.1133 - rmse: 16007.3301 - val_loss: 16306.2373 - val_rmse: 16348.0020\n",
      "Epoch 77/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17007.3574 - rmse: 17007.3574\n",
      "Epoch 77: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17404.1504 - rmse: 17422.8418 - val_loss: 15040.3789 - val_rmse: 15098.0928\n",
      "Epoch 78/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16400.6543 - rmse: 16428.7891\n",
      "Epoch 78: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16400.6543 - rmse: 16428.7891 - val_loss: 17971.5977 - val_rmse: 17985.1484\n",
      "Epoch 79/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15913.2217 - rmse: 15913.2217\n",
      "Epoch 79: val_loss did not improve from 14950.56738\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16017.1621 - rmse: 16014.9160 - val_loss: 15692.4629 - val_rmse: 15705.9453\n",
      "Epoch 79: early stopping\n",
      "Finish 10-fold cross validation\n",
      "Best performing model has 39831.53125 validation loss (RMSE)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# modify to save ckpt for each test\n",
    "ckpt = os.path.join(ckpt_path, \"val_rmse_{val_rmse:.0f}.hdf5\")\n",
    "\n",
    "# training params\n",
    "epochs = epochs\n",
    "lr = lr\n",
    "\n",
    "# the k for k fold CV\n",
    "n_split = 10\n",
    "\n",
    "# for recording best performance\n",
    "min_loss = np.inf\n",
    "best_history = None\n",
    "\n",
    "'''\n",
    "k-fold cross validation\n",
    "Save the best model using validation accuracy as metric\n",
    "Print the global best performace when finished\n",
    "'''\n",
    "for train_index, test_index in KFold(n_split).split(train_examples):\n",
    "\n",
    "    x_train, x_vad = train_examples[train_index], train_examples[test_index]\n",
    "    y_train, y_vad = train_labels[train_index], train_labels[test_index]\n",
    "\n",
    "    model=create_reg_model(lr)\n",
    "  \n",
    "    # callbacks\n",
    "    checkpoint_filepath = ckpt\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='auto',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=25,\n",
    "        verbose=1,\n",
    "        mode='auto',\n",
    "    )\n",
    "\n",
    "    # Model weights are saved at the end of every epoch, if it's the best seen so far.\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_vad, y_vad),\n",
    "                        callbacks=[model_checkpoint_callback, early_stopping_callback])\n",
    "\n",
    "    val_loss = max(history.history['val_loss'])\n",
    "    \n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        best_history = history\n",
    "        # print('Best acc so far. Saving params...\\n')\n",
    "\n",
    "print('Finish {}-fold cross validation'.format(n_split))\n",
    "print('Best performing model has {} validation loss (RMSE)'.format(min_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28f9c66d0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = best_history\n",
    "\n",
    "train_rmse = history.history['rmse']\n",
    "val_rmse = history.history['val_rmse']\n",
    "\n",
    "epochs_range = range(len(history.history['loss']))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_rmse, label='Training RMSE')\n",
    "plt.plot(epochs_range, val_rmse, label='Validation RMSE')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "\n",
    "'''How far are predictions from real values?'''\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def format_tick_labels(x, pos):\n",
    "    return '{:.0f}k'.format(x/1000)\n",
    "\n",
    "predictions = model.predict(test_examples)\n",
    "\n",
    "xlims = [0, max(test_labels)*1.1]\n",
    "ylims = [0, max(predictions)*1.1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(test_labels, predictions)\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "ax.set_xlim(xlims)\n",
    "ax.set_ylim(ylims)\n",
    "ax.set_xlabel('Actual Price')\n",
    "ax.set_ylabel('Predicted Price')\n",
    "\n",
    "ax.plot(xlims, ylims, 'r-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/14 [=====>........................] - ETA: 0s - loss: 12921.5547 - rmse: 12921.5547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:21:09.694172: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 20ms/step - loss: 15474.1807 - rmse: 15414.8867\n",
      "\n",
      "evaluation on test set:\n",
      "loss (RMSE) = 15474.18066\n"
     ]
    }
   ],
   "source": [
    "model = create_reg_model()\n",
    "model.load_weights(os.path.join(ckpt_path, \"val_rmse_13775.hdf5\"))\n",
    "\n",
    "loss, acc = model.evaluate(test_examples, test_labels)\n",
    "\n",
    "print('\\nevaluation on test set:\\nloss (RMSE) = {:.5f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29cc21816e506614f017a9125cfdb1f5dc655865e499c52ef5f5406a40d25695"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
