{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.min_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>TotalSF</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>Total_Bathrooms</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>YrBltAndRemod</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>Foundation_PConc</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>hasfireplace</th>\n",
       "      <th>ExterQual_Gd</th>\n",
       "      <th>BsmtQual_Ex</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>HeatingQC_Ex</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>Total_porch_sf</th>\n",
       "      <th>BsmtFinType1_GLQ</th>\n",
       "      <th>KitchenQual_Ex</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>GarageFinish_Fin</th>\n",
       "      <th>...</th>\n",
       "      <th>BsmtExposure_No</th>\n",
       "      <th>Neighborhood_OldTown</th>\n",
       "      <th>Foundation_BrkTil</th>\n",
       "      <th>GarageFinish_None</th>\n",
       "      <th>GarageCond_None</th>\n",
       "      <th>GarageQual_None</th>\n",
       "      <th>GarageType_None</th>\n",
       "      <th>MSSubClass_30</th>\n",
       "      <th>LotShape_Reg</th>\n",
       "      <th>PavedDrive_N</th>\n",
       "      <th>Foundation_CBlock</th>\n",
       "      <th>MSZoning_RM</th>\n",
       "      <th>HeatingQC_TA</th>\n",
       "      <th>CentralAir_N</th>\n",
       "      <th>GarageType_Detchd</th>\n",
       "      <th>MasVnrType_None</th>\n",
       "      <th>GarageFinish_Unf</th>\n",
       "      <th>BsmtQual_TA</th>\n",
       "      <th>FireplaceQu_None</th>\n",
       "      <th>KitchenQual_TA</th>\n",
       "      <th>ExterQual_TA</th>\n",
       "      <th>dummy_1</th>\n",
       "      <th>dummy_2</th>\n",
       "      <th>dummy_3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.534411</td>\n",
       "      <td>0.282095</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.163872</td>\n",
       "      <td>0.295699</td>\n",
       "      <td>0.615789</td>\n",
       "      <td>0.435122</td>\n",
       "      <td>0.585973</td>\n",
       "      <td>0.659420</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600315</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.406188</td>\n",
       "      <td>0.861892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>p3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.549418</td>\n",
       "      <td>0.293213</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.164976</td>\n",
       "      <td>0.529570</td>\n",
       "      <td>0.984211</td>\n",
       "      <td>0.452277</td>\n",
       "      <td>0.601852</td>\n",
       "      <td>0.985507</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.516936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.329526</td>\n",
       "      <td>0.098585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.358734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>p2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.629267</td>\n",
       "      <td>0.356484</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.251426</td>\n",
       "      <td>0.305108</td>\n",
       "      <td>0.521053</td>\n",
       "      <td>0.549906</td>\n",
       "      <td>0.685845</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516936</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.758667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>p4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.555798</td>\n",
       "      <td>0.239177</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.252530</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.119775</td>\n",
       "      <td>0.359823</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.600315</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128562</td>\n",
       "      <td>0.288829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314107</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>p4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.390888</td>\n",
       "      <td>0.194144</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.163872</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.299439</td>\n",
       "      <td>0.432570</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.351144</td>\n",
       "      <td>0.173856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.632630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>p7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OverallQual  GrLivArea   TotalSF  GarageCars  Total_Bathrooms  \\\n",
       "1522     0.555556   0.534411  0.282095         0.4         0.163872   \n",
       "1716     0.666667   0.549418  0.293213         0.6         0.164976   \n",
       "849      0.444444   0.629267  0.356484         0.4         0.251426   \n",
       "83       0.666667   0.555798  0.239177         0.4         0.252530   \n",
       "708      0.333333   0.390888  0.194144         0.4         0.163872   \n",
       "\n",
       "      GarageArea  YrBltAndRemod  TotalBsmtSF  1stFlrSF  YearBuilt  FullBath  \\\n",
       "1522    0.295699       0.615789     0.435122  0.585973   0.659420      0.25   \n",
       "1716    0.529570       0.984211     0.452277  0.601852   0.985507      0.50   \n",
       "849     0.305108       0.521053     0.549906  0.685845   0.601449      0.25   \n",
       "83      0.268817       0.847368     0.119775  0.359823   0.891304      0.50   \n",
       "708     0.387097       0.578947     0.299439  0.432570   0.710145      0.25   \n",
       "\n",
       "      YearRemodAdd  Foundation_PConc  TotRmsAbvGrd  hasfireplace  \\\n",
       "1522      0.566667               0.0      0.600315           1.0   \n",
       "1716      0.983333               1.0      0.516936           0.0   \n",
       "849       0.400000               0.0      0.516936           1.0   \n",
       "83        0.766667               1.0      0.600315           1.0   \n",
       "708       0.333333               0.0      0.516936           0.0   \n",
       "\n",
       "      ExterQual_Gd  BsmtQual_Ex  Fireplaces  HeatingQC_Ex  MasVnrArea  \\\n",
       "1522           0.0          0.0    0.293793           0.0    0.406188   \n",
       "1716           1.0          0.0    0.000000           1.0    0.329526   \n",
       "849            0.0          0.0    0.293793           0.0    0.000000   \n",
       "83             0.0          0.0    0.293793           0.0    0.128562   \n",
       "708            0.0          0.0    0.000000           0.0    0.351144   \n",
       "\n",
       "      Total_porch_sf  BsmtFinType1_GLQ  KitchenQual_Ex  OpenPorchSF  \\\n",
       "1522        0.861892               0.0             0.0     0.000000   \n",
       "1716        0.098585               0.0             0.0     0.358734   \n",
       "849         0.413114               0.0             0.0     0.758667   \n",
       "83          0.288829               0.0             0.0     0.314107   \n",
       "708         0.173856               0.0             0.0     0.632630   \n",
       "\n",
       "      GarageFinish_Fin  ...  BsmtExposure_No  Neighborhood_OldTown  \\\n",
       "1522               0.0  ...              0.0                   0.0   \n",
       "1716               0.0  ...              1.0                   0.0   \n",
       "849                0.0  ...              0.0                   0.0   \n",
       "83                 1.0  ...              1.0                   0.0   \n",
       "708                0.0  ...              0.0                   0.0   \n",
       "\n",
       "      Foundation_BrkTil  GarageFinish_None  GarageCond_None  GarageQual_None  \\\n",
       "1522                0.0                0.0              0.0              0.0   \n",
       "1716                0.0                0.0              0.0              0.0   \n",
       "849                 0.0                0.0              0.0              0.0   \n",
       "83                  0.0                0.0              0.0              0.0   \n",
       "708                 0.0                0.0              0.0              0.0   \n",
       "\n",
       "      GarageType_None  MSSubClass_30  LotShape_Reg  PavedDrive_N  \\\n",
       "1522              0.0            0.0           1.0           0.0   \n",
       "1716              0.0            0.0           1.0           0.0   \n",
       "849               0.0            0.0           1.0           0.0   \n",
       "83                0.0            0.0           0.0           0.0   \n",
       "708               0.0            0.0           1.0           0.0   \n",
       "\n",
       "      Foundation_CBlock  MSZoning_RM  HeatingQC_TA  CentralAir_N  \\\n",
       "1522                1.0          0.0           1.0           0.0   \n",
       "1716                0.0          0.0           0.0           0.0   \n",
       "849                 1.0          0.0           1.0           0.0   \n",
       "83                  0.0          0.0           0.0           0.0   \n",
       "708                 1.0          0.0           1.0           1.0   \n",
       "\n",
       "      GarageType_Detchd  MasVnrType_None  GarageFinish_Unf  BsmtQual_TA  \\\n",
       "1522                0.0              0.0               0.0          1.0   \n",
       "1716                0.0              0.0               0.0          0.0   \n",
       "849                 0.0              1.0               1.0          1.0   \n",
       "83                  0.0              0.0               0.0          0.0   \n",
       "708                 1.0              0.0               1.0          1.0   \n",
       "\n",
       "      FireplaceQu_None  KitchenQual_TA  ExterQual_TA  dummy_1  dummy_2  \\\n",
       "1522               0.0             1.0           1.0      0.0      0.0   \n",
       "1716               1.0             0.0           0.0      0.0      0.0   \n",
       "849                0.0             1.0           1.0      0.0      0.0   \n",
       "83                 0.0             1.0           1.0      0.0      0.0   \n",
       "708                1.0             1.0           1.0      0.0      0.0   \n",
       "\n",
       "      dummy_3  label  \n",
       "1522      0.0     p3  \n",
       "1716      0.0     p2  \n",
       "849       0.0     p4  \n",
       "83        0.0     p4  \n",
       "708       0.0     p7  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '../data/home_sale_data_324_features_5_class.csv'\n",
    "\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "'''suffle rows randomly'''\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "labels = data['label']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>TotalSF</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>Total_Bathrooms</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>YrBltAndRemod</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>Foundation_PConc</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>hasfireplace</th>\n",
       "      <th>ExterQual_Gd</th>\n",
       "      <th>BsmtQual_Ex</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>HeatingQC_Ex</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>Total_porch_sf</th>\n",
       "      <th>BsmtFinType1_GLQ</th>\n",
       "      <th>KitchenQual_Ex</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>GarageFinish_Fin</th>\n",
       "      <th>...</th>\n",
       "      <th>Neighborhood_IDOTRR</th>\n",
       "      <th>BsmtExposure_No</th>\n",
       "      <th>Neighborhood_OldTown</th>\n",
       "      <th>Foundation_BrkTil</th>\n",
       "      <th>GarageFinish_None</th>\n",
       "      <th>GarageCond_None</th>\n",
       "      <th>GarageQual_None</th>\n",
       "      <th>GarageType_None</th>\n",
       "      <th>MSSubClass_30</th>\n",
       "      <th>LotShape_Reg</th>\n",
       "      <th>PavedDrive_N</th>\n",
       "      <th>Foundation_CBlock</th>\n",
       "      <th>MSZoning_RM</th>\n",
       "      <th>HeatingQC_TA</th>\n",
       "      <th>CentralAir_N</th>\n",
       "      <th>GarageType_Detchd</th>\n",
       "      <th>MasVnrType_None</th>\n",
       "      <th>GarageFinish_Unf</th>\n",
       "      <th>BsmtQual_TA</th>\n",
       "      <th>FireplaceQu_None</th>\n",
       "      <th>KitchenQual_TA</th>\n",
       "      <th>ExterQual_TA</th>\n",
       "      <th>dummy_1</th>\n",
       "      <th>dummy_2</th>\n",
       "      <th>dummy_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.0</td>\n",
       "      <td>2911.0</td>\n",
       "      <td>2911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.565136</td>\n",
       "      <td>0.542763</td>\n",
       "      <td>0.293431</td>\n",
       "      <td>0.353143</td>\n",
       "      <td>0.200863</td>\n",
       "      <td>0.317285</td>\n",
       "      <td>0.660811</td>\n",
       "      <td>0.326706</td>\n",
       "      <td>0.487996</td>\n",
       "      <td>0.719569</td>\n",
       "      <td>0.391876</td>\n",
       "      <td>0.570892</td>\n",
       "      <td>0.447956</td>\n",
       "      <td>0.542786</td>\n",
       "      <td>0.512882</td>\n",
       "      <td>0.335967</td>\n",
       "      <td>0.087255</td>\n",
       "      <td>0.171718</td>\n",
       "      <td>0.511508</td>\n",
       "      <td>0.159954</td>\n",
       "      <td>0.203170</td>\n",
       "      <td>0.290622</td>\n",
       "      <td>0.069392</td>\n",
       "      <td>0.176542</td>\n",
       "      <td>0.245620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031261</td>\n",
       "      <td>0.652697</td>\n",
       "      <td>0.082102</td>\n",
       "      <td>0.106493</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.053590</td>\n",
       "      <td>0.047750</td>\n",
       "      <td>0.637582</td>\n",
       "      <td>0.073514</td>\n",
       "      <td>0.423222</td>\n",
       "      <td>0.158708</td>\n",
       "      <td>0.293370</td>\n",
       "      <td>0.066644</td>\n",
       "      <td>0.266919</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.421848</td>\n",
       "      <td>0.439368</td>\n",
       "      <td>0.487118</td>\n",
       "      <td>0.511852</td>\n",
       "      <td>0.616627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.155988</td>\n",
       "      <td>0.125193</td>\n",
       "      <td>0.119772</td>\n",
       "      <td>0.152244</td>\n",
       "      <td>0.133043</td>\n",
       "      <td>0.143976</td>\n",
       "      <td>0.242686</td>\n",
       "      <td>0.131930</td>\n",
       "      <td>0.131369</td>\n",
       "      <td>0.219351</td>\n",
       "      <td>0.138140</td>\n",
       "      <td>0.348189</td>\n",
       "      <td>0.497369</td>\n",
       "      <td>0.128554</td>\n",
       "      <td>0.499920</td>\n",
       "      <td>0.472409</td>\n",
       "      <td>0.282257</td>\n",
       "      <td>0.181727</td>\n",
       "      <td>0.499953</td>\n",
       "      <td>0.218350</td>\n",
       "      <td>0.163524</td>\n",
       "      <td>0.454127</td>\n",
       "      <td>0.254163</td>\n",
       "      <td>0.183129</td>\n",
       "      <td>0.430528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174051</td>\n",
       "      <td>0.476195</td>\n",
       "      <td>0.274568</td>\n",
       "      <td>0.308520</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.225245</td>\n",
       "      <td>0.213273</td>\n",
       "      <td>0.480781</td>\n",
       "      <td>0.261024</td>\n",
       "      <td>0.494155</td>\n",
       "      <td>0.365467</td>\n",
       "      <td>0.455385</td>\n",
       "      <td>0.249447</td>\n",
       "      <td>0.442425</td>\n",
       "      <td>0.488798</td>\n",
       "      <td>0.493939</td>\n",
       "      <td>0.496395</td>\n",
       "      <td>0.499920</td>\n",
       "      <td>0.499945</td>\n",
       "      <td>0.486292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.451636</td>\n",
       "      <td>0.213557</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.088659</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.247193</td>\n",
       "      <td>0.395003</td>\n",
       "      <td>0.590580</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.547807</td>\n",
       "      <td>0.282326</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.164976</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.652632</td>\n",
       "      <td>0.308172</td>\n",
       "      <td>0.481664</td>\n",
       "      <td>0.731884</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516936</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.621247</td>\n",
       "      <td>0.356447</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.252530</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.405490</td>\n",
       "      <td>0.582574</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.353218</td>\n",
       "      <td>0.327089</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 324 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       OverallQual    GrLivArea      TotalSF   GarageCars  Total_Bathrooms  \\\n",
       "count  2911.000000  2911.000000  2911.000000  2911.000000      2911.000000   \n",
       "mean      0.565136     0.542763     0.293431     0.353143         0.200863   \n",
       "std       0.155988     0.125193     0.119772     0.152244         0.133043   \n",
       "min       0.000000     0.000000     0.000000     0.000000         0.000000   \n",
       "25%       0.444444     0.451636     0.213557     0.200000         0.088659   \n",
       "50%       0.555556     0.547807     0.282326     0.400000         0.164976   \n",
       "75%       0.666667     0.621247     0.356447     0.400000         0.252530   \n",
       "max       1.000000     1.000000     1.000000     1.000000         1.000000   \n",
       "\n",
       "        GarageArea  YrBltAndRemod  TotalBsmtSF     1stFlrSF    YearBuilt  \\\n",
       "count  2911.000000    2911.000000  2911.000000  2911.000000  2911.000000   \n",
       "mean      0.317285       0.660811     0.326706     0.487996     0.719569   \n",
       "std       0.143976       0.242686     0.131930     0.131369     0.219351   \n",
       "min       0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.215054       0.473684     0.247193     0.395003     0.590580   \n",
       "50%       0.322581       0.652632     0.308172     0.481664     0.731884   \n",
       "75%       0.387097       0.905263     0.405490     0.582574     0.934783   \n",
       "max       1.000000       1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          FullBath  YearRemodAdd  Foundation_PConc  TotRmsAbvGrd  \\\n",
       "count  2911.000000   2911.000000       2911.000000   2911.000000   \n",
       "mean      0.391876      0.570892          0.447956      0.542786   \n",
       "std       0.138140      0.348189          0.497369      0.128554   \n",
       "min       0.000000      0.000000          0.000000      0.000000   \n",
       "25%       0.250000      0.250000          0.000000      0.421336   \n",
       "50%       0.500000      0.716667          0.000000      0.516936   \n",
       "75%       0.500000      0.900000          1.000000      0.600315   \n",
       "max       1.000000      1.000000          1.000000      1.000000   \n",
       "\n",
       "       hasfireplace  ExterQual_Gd  BsmtQual_Ex   Fireplaces  HeatingQC_Ex  \\\n",
       "count   2911.000000   2911.000000  2911.000000  2911.000000   2911.000000   \n",
       "mean       0.512882      0.335967     0.087255     0.171718      0.511508   \n",
       "std        0.499920      0.472409     0.282257     0.181727      0.499953   \n",
       "min        0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "25%        0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "50%        1.000000      0.000000     0.000000     0.293793      1.000000   \n",
       "75%        1.000000      1.000000     0.000000     0.293793      1.000000   \n",
       "max        1.000000      1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "        MasVnrArea  Total_porch_sf  BsmtFinType1_GLQ  KitchenQual_Ex  \\\n",
       "count  2911.000000     2911.000000       2911.000000     2911.000000   \n",
       "mean      0.159954        0.203170          0.290622        0.069392   \n",
       "std       0.218350        0.163524          0.454127        0.254163   \n",
       "min       0.000000        0.000000          0.000000        0.000000   \n",
       "25%       0.000000        0.066964          0.000000        0.000000   \n",
       "50%       0.000000        0.179849          0.000000        0.000000   \n",
       "75%       0.353218        0.327089          1.000000        0.000000   \n",
       "max       1.000000        1.000000          1.000000        1.000000   \n",
       "\n",
       "       OpenPorchSF  GarageFinish_Fin  ...  Neighborhood_IDOTRR  \\\n",
       "count  2911.000000       2911.000000  ...          2911.000000   \n",
       "mean      0.176542          0.245620  ...             0.031261   \n",
       "std       0.183129          0.430528  ...             0.174051   \n",
       "min       0.000000          0.000000  ...             0.000000   \n",
       "25%       0.000000          0.000000  ...             0.000000   \n",
       "50%       0.180865          0.000000  ...             0.000000   \n",
       "75%       0.309510          0.000000  ...             0.000000   \n",
       "max       1.000000          1.000000  ...             1.000000   \n",
       "\n",
       "       BsmtExposure_No  Neighborhood_OldTown  Foundation_BrkTil  \\\n",
       "count      2911.000000           2911.000000        2911.000000   \n",
       "mean          0.652697              0.082102           0.106493   \n",
       "std           0.476195              0.274568           0.308520   \n",
       "min           0.000000              0.000000           0.000000   \n",
       "25%           0.000000              0.000000           0.000000   \n",
       "50%           1.000000              0.000000           0.000000   \n",
       "75%           1.000000              0.000000           0.000000   \n",
       "max           1.000000              1.000000           1.000000   \n",
       "\n",
       "       GarageFinish_None  GarageCond_None  GarageQual_None  GarageType_None  \\\n",
       "count        2911.000000      2911.000000      2911.000000      2911.000000   \n",
       "mean            0.054277         0.054277         0.054277         0.053590   \n",
       "std             0.226602         0.226602         0.226602         0.225245   \n",
       "min             0.000000         0.000000         0.000000         0.000000   \n",
       "25%             0.000000         0.000000         0.000000         0.000000   \n",
       "50%             0.000000         0.000000         0.000000         0.000000   \n",
       "75%             0.000000         0.000000         0.000000         0.000000   \n",
       "max             1.000000         1.000000         1.000000         1.000000   \n",
       "\n",
       "       MSSubClass_30  LotShape_Reg  PavedDrive_N  Foundation_CBlock  \\\n",
       "count    2911.000000   2911.000000   2911.000000        2911.000000   \n",
       "mean        0.047750      0.637582      0.073514           0.423222   \n",
       "std         0.213273      0.480781      0.261024           0.494155   \n",
       "min         0.000000      0.000000      0.000000           0.000000   \n",
       "25%         0.000000      0.000000      0.000000           0.000000   \n",
       "50%         0.000000      1.000000      0.000000           0.000000   \n",
       "75%         0.000000      1.000000      0.000000           1.000000   \n",
       "max         1.000000      1.000000      1.000000           1.000000   \n",
       "\n",
       "       MSZoning_RM  HeatingQC_TA  CentralAir_N  GarageType_Detchd  \\\n",
       "count  2911.000000   2911.000000   2911.000000        2911.000000   \n",
       "mean      0.158708      0.293370      0.066644           0.266919   \n",
       "std       0.365467      0.455385      0.249447           0.442425   \n",
       "min       0.000000      0.000000      0.000000           0.000000   \n",
       "25%       0.000000      0.000000      0.000000           0.000000   \n",
       "50%       0.000000      0.000000      0.000000           0.000000   \n",
       "75%       0.000000      1.000000      0.000000           1.000000   \n",
       "max       1.000000      1.000000      1.000000           1.000000   \n",
       "\n",
       "       MasVnrType_None  GarageFinish_Unf  BsmtQual_TA  FireplaceQu_None  \\\n",
       "count      2911.000000       2911.000000  2911.000000       2911.000000   \n",
       "mean          0.605634          0.421848     0.439368          0.487118   \n",
       "std           0.488798          0.493939     0.496395          0.499920   \n",
       "min           0.000000          0.000000     0.000000          0.000000   \n",
       "25%           0.000000          0.000000     0.000000          0.000000   \n",
       "50%           1.000000          0.000000     0.000000          0.000000   \n",
       "75%           1.000000          1.000000     1.000000          1.000000   \n",
       "max           1.000000          1.000000     1.000000          1.000000   \n",
       "\n",
       "       KitchenQual_TA  ExterQual_TA  dummy_1  dummy_2  dummy_3  \n",
       "count     2911.000000   2911.000000   2911.0   2911.0   2911.0  \n",
       "mean         0.511852      0.616627      0.0      0.0      0.0  \n",
       "std          0.499945      0.486292      0.0      0.0      0.0  \n",
       "min          0.000000      0.000000      0.0      0.0      0.0  \n",
       "25%          0.000000      0.000000      0.0      0.0      0.0  \n",
       "50%          1.000000      1.000000      0.0      0.0      0.0  \n",
       "75%          1.000000      1.000000      0.0      0.0      0.0  \n",
       "max          1.000000      1.000000      0.0      0.0      0.0  \n",
       "\n",
       "[8 rows x 324 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop label column\n",
    "data.drop(['label'], axis=1, inplace=True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['p3', 'p2', 'p4', 'p7', 'p9', 'p1', 'p6', 'p5', 'p8', 'p0'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  10\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "'''one-hot encode the labels'''\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "# print(list(integer_encoded))\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "labels = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "print('Number of classes: ', len(labels[0]))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 18, 18)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_np = data.to_numpy()\n",
    "data_np.shape\n",
    "data_np = data_np.reshape(len(data), 18, 18)\n",
    "data_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data_np[0].shape\n",
    "\n",
    "nArray = np.array(data_np[99])\n",
    "\n",
    "\n",
    "a11=nArray.reshape(18,18)\n",
    "plt.imshow(a11)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 10)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 18, 18)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples = data_np\n",
    "train_examples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train test splitting\n",
    "- hold out 15% for testing\n",
    "- use 85% to train model with K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_samples = train_examples.shape[0] \n",
    "test_ratio = 0.15\n",
    "test_samples = int(test_ratio * train_examples.shape[0])\n",
    "\n",
    "test_examples = train_examples[-1*test_samples:]\n",
    "train_examples = train_examples[:-1*test_samples]\n",
    "test_labels = labels[-1*test_samples:]\n",
    "train_labels = labels[:-1*test_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (2475, 18, 18)\n",
      "test:  (436, 18, 18)\n"
     ]
    }
   ],
   "source": [
    "print('train: ', train_examples.shape)\n",
    "print('test: ', test_examples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = train_examples.reshape(ttl_samples-test_samples, 18,18,1)\n",
    "trainY = train_labels\n",
    "\n",
    "testX = test_examples.reshape(test_samples, 18,18,1)\n",
    "testY = test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "\n",
    "def create_model(num_classes=5, lr=0.005):\n",
    "\n",
    "\t# Working\n",
    "\twith tf.device('/cpu:0'):\n",
    "\t\tdata_augmentation = tf.keras.Sequential([ \n",
    "\t\t\ttf.keras.layers.RandomFlip(\"horizontal\", input_shape=(18, 18, 1)),\n",
    "\t  \t\ttf.keras.layers.RandomRotation(0.1),\n",
    "\t\t    tf.keras.layers.RandomZoom(0.1)\n",
    "\t\t\t])\n",
    "\n",
    "\n",
    "\tmodel = tf.keras.Sequential([\n",
    "\t\t# data_augmentation,\n",
    "\t  \t# tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(18, 18, 1)),\n",
    "\t\ttf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.MaxPooling2D((2,2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t  \ttf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.MaxPooling2D(),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Flatten(),\n",
    "\t\ttf.keras.layers.Dense(128, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "\t])\n",
    "\n",
    "\t# opt = tf.keras.optimizers.SGD(lr=0.005, momentum=0.9)\n",
    "\topt = tf.keras.optimizers.Adam(lr=lr)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evantilu/miniforge3/envs/6998_DL_tf/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2022-04-23 21:55:21.696838: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 2.2677 - accuracy: 0.1370\n",
      "Epoch 1: val_accuracy improved from -inf to 0.22581, saving model to ./ckpt/10_class_lr0005/val_acc_0.226.hdf5\n",
      "70/70 [==============================] - 2s 21ms/step - loss: 2.2677 - accuracy: 0.1370 - val_loss: 2.1577 - val_accuracy: 0.2258\n",
      "Epoch 2/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 21:55:23.260058: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/70 [============================>.] - ETA: 0s - loss: 1.8920 - accuracy: 0.2698\n",
      "Epoch 2: val_accuracy improved from 0.22581 to 0.26613, saving model to ./ckpt/10_class_lr0005/val_acc_0.266.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.8901 - accuracy: 0.2699 - val_loss: 1.7392 - val_accuracy: 0.2661\n",
      "Epoch 3/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.6651 - accuracy: 0.3368\n",
      "Epoch 3: val_accuracy improved from 0.26613 to 0.27016, saving model to ./ckpt/10_class_lr0005/val_acc_0.270.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.6651 - accuracy: 0.3368 - val_loss: 1.7149 - val_accuracy: 0.2702\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5935 - accuracy: 0.3428\n",
      "Epoch 4: val_accuracy improved from 0.27016 to 0.34274, saving model to ./ckpt/10_class_lr0005/val_acc_0.343.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5924 - accuracy: 0.3444 - val_loss: 1.6257 - val_accuracy: 0.3427\n",
      "Epoch 5/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.5445 - accuracy: 0.3598\n",
      "Epoch 5: val_accuracy did not improve from 0.34274\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.5466 - accuracy: 0.3565 - val_loss: 1.6051 - val_accuracy: 0.3065\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4917 - accuracy: 0.3895\n",
      "Epoch 6: val_accuracy improved from 0.34274 to 0.35484, saving model to ./ckpt/10_class_lr0005/val_acc_0.355.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4922 - accuracy: 0.3893 - val_loss: 1.5764 - val_accuracy: 0.3548\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4812 - accuracy: 0.3791\n",
      "Epoch 7: val_accuracy did not improve from 0.35484\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4813 - accuracy: 0.3790 - val_loss: 1.5442 - val_accuracy: 0.3266\n",
      "Epoch 8/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.4156 - accuracy: 0.4164\n",
      "Epoch 8: val_accuracy improved from 0.35484 to 0.38710, saving model to ./ckpt/10_class_lr0005/val_acc_0.387.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4163 - accuracy: 0.4154 - val_loss: 1.4995 - val_accuracy: 0.3871\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3849 - accuracy: 0.4185\n",
      "Epoch 9: val_accuracy improved from 0.38710 to 0.39516, saving model to ./ckpt/10_class_lr0005/val_acc_0.395.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3849 - accuracy: 0.4185 - val_loss: 1.4584 - val_accuracy: 0.3952\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3478 - accuracy: 0.4198\n",
      "Epoch 10: val_accuracy improved from 0.39516 to 0.43145, saving model to ./ckpt/10_class_lr0005/val_acc_0.431.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3478 - accuracy: 0.4198 - val_loss: 1.4190 - val_accuracy: 0.4315\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3209 - accuracy: 0.4436\n",
      "Epoch 11: val_accuracy improved from 0.43145 to 0.43548, saving model to ./ckpt/10_class_lr0005/val_acc_0.435.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3209 - accuracy: 0.4436 - val_loss: 1.3637 - val_accuracy: 0.4355\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3045 - accuracy: 0.4475\n",
      "Epoch 12: val_accuracy did not improve from 0.43548\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3056 - accuracy: 0.4463 - val_loss: 1.3882 - val_accuracy: 0.4153\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2693 - accuracy: 0.4612\n",
      "Epoch 13: val_accuracy did not improve from 0.43548\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2693 - accuracy: 0.4612 - val_loss: 1.3715 - val_accuracy: 0.4032\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2422 - accuracy: 0.4710\n",
      "Epoch 14: val_accuracy improved from 0.43548 to 0.46774, saving model to ./ckpt/10_class_lr0005/val_acc_0.468.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2422 - accuracy: 0.4710 - val_loss: 1.2926 - val_accuracy: 0.4677\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1985 - accuracy: 0.5045\n",
      "Epoch 15: val_accuracy did not improve from 0.46774\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1994 - accuracy: 0.5034 - val_loss: 1.3290 - val_accuracy: 0.4395\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1952 - accuracy: 0.5054\n",
      "Epoch 16: val_accuracy improved from 0.46774 to 0.47984, saving model to ./ckpt/10_class_lr0005/val_acc_0.480.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1953 - accuracy: 0.5047 - val_loss: 1.2419 - val_accuracy: 0.4798\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1732 - accuracy: 0.4950\n",
      "Epoch 17: val_accuracy did not improve from 0.47984\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1736 - accuracy: 0.4948 - val_loss: 1.2390 - val_accuracy: 0.4637\n",
      "Epoch 18/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.1451 - accuracy: 0.5161\n",
      "Epoch 18: val_accuracy improved from 0.47984 to 0.48790, saving model to ./ckpt/10_class_lr0005/val_acc_0.488.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1473 - accuracy: 0.5177 - val_loss: 1.2275 - val_accuracy: 0.4879\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1367 - accuracy: 0.5123\n",
      "Epoch 19: val_accuracy improved from 0.48790 to 0.50000, saving model to ./ckpt/10_class_lr0005/val_acc_0.500.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1367 - accuracy: 0.5123 - val_loss: 1.2067 - val_accuracy: 0.5000\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1086 - accuracy: 0.5362\n",
      "Epoch 20: val_accuracy improved from 0.50000 to 0.50403, saving model to ./ckpt/10_class_lr0005/val_acc_0.504.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1088 - accuracy: 0.5361 - val_loss: 1.1790 - val_accuracy: 0.5040\n",
      "Epoch 21/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0938 - accuracy: 0.5439\n",
      "Epoch 21: val_accuracy improved from 0.50403 to 0.50806, saving model to ./ckpt/10_class_lr0005/val_acc_0.508.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0972 - accuracy: 0.5442 - val_loss: 1.1767 - val_accuracy: 0.5081\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0910 - accuracy: 0.5322\n",
      "Epoch 22: val_accuracy did not improve from 0.50806\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0914 - accuracy: 0.5326 - val_loss: 1.2345 - val_accuracy: 0.4960\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0708 - accuracy: 0.5308\n",
      "Epoch 23: val_accuracy did not improve from 0.50806\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0721 - accuracy: 0.5299 - val_loss: 1.1768 - val_accuracy: 0.5081\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0759 - accuracy: 0.5394\n",
      "Epoch 24: val_accuracy did not improve from 0.50806\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0784 - accuracy: 0.5388 - val_loss: 1.1611 - val_accuracy: 0.4960\n",
      "Epoch 25/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0448 - accuracy: 0.5553\n",
      "Epoch 25: val_accuracy did not improve from 0.50806\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0468 - accuracy: 0.5555 - val_loss: 1.2219 - val_accuracy: 0.4677\n",
      "Epoch 26/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0653 - accuracy: 0.5446\n",
      "Epoch 26: val_accuracy improved from 0.50806 to 0.52419, saving model to ./ckpt/10_class_lr0005/val_acc_0.524.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0673 - accuracy: 0.5451 - val_loss: 1.1431 - val_accuracy: 0.5242\n",
      "Epoch 27/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0216 - accuracy: 0.5666\n",
      "Epoch 27: val_accuracy did not improve from 0.52419\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0214 - accuracy: 0.5658 - val_loss: 1.1985 - val_accuracy: 0.4718\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0011 - accuracy: 0.5670\n",
      "Epoch 28: val_accuracy did not improve from 0.52419\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.0027 - accuracy: 0.5671 - val_loss: 1.1463 - val_accuracy: 0.5040\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0131 - accuracy: 0.5707\n",
      "Epoch 29: val_accuracy did not improve from 0.52419\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0149 - accuracy: 0.5694 - val_loss: 1.1708 - val_accuracy: 0.4516\n",
      "Epoch 30/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.5648\n",
      "Epoch 30: val_accuracy did not improve from 0.52419\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0371 - accuracy: 0.5644 - val_loss: 1.1306 - val_accuracy: 0.5242\n",
      "Epoch 31/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9649 - accuracy: 0.5793\n",
      "Epoch 31: val_accuracy did not improve from 0.52419\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9695 - accuracy: 0.5784 - val_loss: 1.1182 - val_accuracy: 0.5121\n",
      "Epoch 32/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0284 - accuracy: 0.5578\n",
      "Epoch 32: val_accuracy did not improve from 0.52419\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0244 - accuracy: 0.5581 - val_loss: 1.1487 - val_accuracy: 0.5121\n",
      "Epoch 33/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9667 - accuracy: 0.5942\n",
      "Epoch 33: val_accuracy did not improve from 0.52419\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9672 - accuracy: 0.5941 - val_loss: 1.1784 - val_accuracy: 0.5040\n",
      "Epoch 34/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9535 - accuracy: 0.6082\n",
      "Epoch 34: val_accuracy did not improve from 0.52419\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9523 - accuracy: 0.6080 - val_loss: 1.1333 - val_accuracy: 0.5121\n",
      "Epoch 35/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9417 - accuracy: 0.6112\n",
      "Epoch 35: val_accuracy did not improve from 0.52419\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9412 - accuracy: 0.6111 - val_loss: 1.0858 - val_accuracy: 0.5202\n",
      "Epoch 36/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9508 - accuracy: 0.6033\n",
      "Epoch 36: val_accuracy did not improve from 0.52419\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9510 - accuracy: 0.6040 - val_loss: 1.1910 - val_accuracy: 0.4839\n",
      "Epoch 37/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9557 - accuracy: 0.6011\n",
      "Epoch 37: val_accuracy improved from 0.52419 to 0.53226, saving model to ./ckpt/10_class_lr0005/val_acc_0.532.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9568 - accuracy: 0.6004 - val_loss: 1.0981 - val_accuracy: 0.5323\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9091 - accuracy: 0.6332\n",
      "Epoch 38: val_accuracy improved from 0.53226 to 0.53629, saving model to ./ckpt/10_class_lr0005/val_acc_0.536.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9094 - accuracy: 0.6331 - val_loss: 1.0999 - val_accuracy: 0.5363\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9291 - accuracy: 0.6178\n",
      "Epoch 39: val_accuracy did not improve from 0.53629\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9314 - accuracy: 0.6174 - val_loss: 1.2717 - val_accuracy: 0.4597\n",
      "Epoch 40/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9387 - accuracy: 0.5968\n",
      "Epoch 40: val_accuracy did not improve from 0.53629\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9387 - accuracy: 0.5968 - val_loss: 1.1353 - val_accuracy: 0.5040\n",
      "Epoch 41/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9137 - accuracy: 0.6236\n",
      "Epoch 41: val_accuracy did not improve from 0.53629\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9185 - accuracy: 0.6233 - val_loss: 1.0932 - val_accuracy: 0.5363\n",
      "Epoch 42/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8938 - accuracy: 0.6328\n",
      "Epoch 42: val_accuracy did not improve from 0.53629\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8928 - accuracy: 0.6318 - val_loss: 1.1194 - val_accuracy: 0.5323\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8927 - accuracy: 0.6327\n",
      "Epoch 43: val_accuracy did not improve from 0.53629\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8928 - accuracy: 0.6318 - val_loss: 1.1506 - val_accuracy: 0.5000\n",
      "Epoch 44/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9335 - accuracy: 0.6105\n",
      "Epoch 44: val_accuracy did not improve from 0.53629\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9335 - accuracy: 0.6111 - val_loss: 1.1251 - val_accuracy: 0.5000\n",
      "Epoch 45/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8845 - accuracy: 0.6336\n",
      "Epoch 45: val_accuracy did not improve from 0.53629\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8841 - accuracy: 0.6336 - val_loss: 1.1296 - val_accuracy: 0.5081\n",
      "Epoch 45: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 21:56:09.731797: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 2.2612 - accuracy: 0.1486\n",
      "Epoch 1: val_accuracy improved from -inf to 0.20968, saving model to ./ckpt/10_class_lr0005/val_acc_0.210.hdf5\n",
      "70/70 [==============================] - 2s 19ms/step - loss: 2.2612 - accuracy: 0.1486 - val_loss: 2.1470 - val_accuracy: 0.2097\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 1s - loss: 2.1944 - accuracy: 0.1875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 21:56:11.050394: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 1.8849 - accuracy: 0.2649\n",
      "Epoch 2: val_accuracy improved from 0.20968 to 0.33871, saving model to ./ckpt/10_class_lr0005/val_acc_0.339.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.8828 - accuracy: 0.2667 - val_loss: 1.7181 - val_accuracy: 0.3387\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.6659 - accuracy: 0.3320\n",
      "Epoch 3: val_accuracy did not improve from 0.33871\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.6668 - accuracy: 0.3309 - val_loss: 1.6563 - val_accuracy: 0.3185\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5950 - accuracy: 0.3483\n",
      "Epoch 4: val_accuracy improved from 0.33871 to 0.36694, saving model to ./ckpt/10_class_lr0005/val_acc_0.367.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5961 - accuracy: 0.3476 - val_loss: 1.5775 - val_accuracy: 0.3669\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.5270 - accuracy: 0.3785\n",
      "Epoch 5: val_accuracy did not improve from 0.36694\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5270 - accuracy: 0.3785 - val_loss: 1.5382 - val_accuracy: 0.3629\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4800 - accuracy: 0.3841\n",
      "Epoch 6: val_accuracy did not improve from 0.36694\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4828 - accuracy: 0.3844 - val_loss: 1.4726 - val_accuracy: 0.3629\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4481 - accuracy: 0.3967\n",
      "Epoch 7: val_accuracy improved from 0.36694 to 0.37903, saving model to ./ckpt/10_class_lr0005/val_acc_0.379.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4484 - accuracy: 0.3952 - val_loss: 1.5042 - val_accuracy: 0.3790\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.4020 - accuracy: 0.4140\n",
      "Epoch 8: val_accuracy improved from 0.37903 to 0.39113, saving model to ./ckpt/10_class_lr0005/val_acc_0.391.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4020 - accuracy: 0.4140 - val_loss: 1.4425 - val_accuracy: 0.3911\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3857 - accuracy: 0.4234\n",
      "Epoch 9: val_accuracy improved from 0.39113 to 0.40323, saving model to ./ckpt/10_class_lr0005/val_acc_0.403.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3857 - accuracy: 0.4234 - val_loss: 1.4038 - val_accuracy: 0.4032\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3309 - accuracy: 0.4561\n",
      "Epoch 10: val_accuracy improved from 0.40323 to 0.41935, saving model to ./ckpt/10_class_lr0005/val_acc_0.419.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3324 - accuracy: 0.4553 - val_loss: 1.3867 - val_accuracy: 0.4194\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3141 - accuracy: 0.4497\n",
      "Epoch 11: val_accuracy improved from 0.41935 to 0.47177, saving model to ./ckpt/10_class_lr0005/val_acc_0.472.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3140 - accuracy: 0.4504 - val_loss: 1.3191 - val_accuracy: 0.4718\n",
      "Epoch 12/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.2571 - accuracy: 0.4779\n",
      "Epoch 12: val_accuracy did not improve from 0.47177\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2585 - accuracy: 0.4760 - val_loss: 1.3384 - val_accuracy: 0.4194\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2305 - accuracy: 0.4755\n",
      "Epoch 13: val_accuracy improved from 0.47177 to 0.49597, saving model to ./ckpt/10_class_lr0005/val_acc_0.496.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2325 - accuracy: 0.4746 - val_loss: 1.2483 - val_accuracy: 0.4960\n",
      "Epoch 14/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2264 - accuracy: 0.4846\n",
      "Epoch 14: val_accuracy did not improve from 0.49597\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.2298 - accuracy: 0.4854 - val_loss: 1.2468 - val_accuracy: 0.4879\n",
      "Epoch 15/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.2097 - accuracy: 0.4935\n",
      "Epoch 15: val_accuracy did not improve from 0.49597\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2097 - accuracy: 0.4899 - val_loss: 1.5085 - val_accuracy: 0.3508\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1832 - accuracy: 0.4828\n",
      "Epoch 16: val_accuracy did not improve from 0.49597\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1844 - accuracy: 0.4818 - val_loss: 1.2365 - val_accuracy: 0.4556\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1709 - accuracy: 0.4964\n",
      "Epoch 17: val_accuracy did not improve from 0.49597\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1724 - accuracy: 0.4957 - val_loss: 1.2573 - val_accuracy: 0.4637\n",
      "Epoch 18/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1697 - accuracy: 0.5082\n",
      "Epoch 18: val_accuracy did not improve from 0.49597\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1674 - accuracy: 0.5101 - val_loss: 1.2084 - val_accuracy: 0.4879\n",
      "Epoch 19/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1203 - accuracy: 0.5326\n",
      "Epoch 19: val_accuracy did not improve from 0.49597\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1240 - accuracy: 0.5312 - val_loss: 1.1883 - val_accuracy: 0.4637\n",
      "Epoch 20/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.1184 - accuracy: 0.5188\n",
      "Epoch 20: val_accuracy improved from 0.49597 to 0.53629, saving model to ./ckpt/10_class_lr0005/val_acc_0.536.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1173 - accuracy: 0.5200 - val_loss: 1.1449 - val_accuracy: 0.5363\n",
      "Epoch 21/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0895 - accuracy: 0.5354\n",
      "Epoch 21: val_accuracy improved from 0.53629 to 0.54435, saving model to ./ckpt/10_class_lr0005/val_acc_0.544.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0924 - accuracy: 0.5344 - val_loss: 1.1329 - val_accuracy: 0.5444\n",
      "Epoch 22/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0954 - accuracy: 0.5473\n",
      "Epoch 22: val_accuracy did not improve from 0.54435\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0944 - accuracy: 0.5478 - val_loss: 1.1261 - val_accuracy: 0.5202\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0800 - accuracy: 0.5380\n",
      "Epoch 23: val_accuracy did not improve from 0.54435\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0807 - accuracy: 0.5361 - val_loss: 1.1553 - val_accuracy: 0.5161\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0784 - accuracy: 0.5516\n",
      "Epoch 24: val_accuracy did not improve from 0.54435\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0776 - accuracy: 0.5519 - val_loss: 1.1149 - val_accuracy: 0.5000\n",
      "Epoch 25/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0602 - accuracy: 0.5483\n",
      "Epoch 25: val_accuracy did not improve from 0.54435\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0576 - accuracy: 0.5505 - val_loss: 1.1104 - val_accuracy: 0.5242\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0307 - accuracy: 0.5652\n",
      "Epoch 26: val_accuracy did not improve from 0.54435\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0322 - accuracy: 0.5640 - val_loss: 1.1192 - val_accuracy: 0.5323\n",
      "Epoch 27/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0389 - accuracy: 0.5657\n",
      "Epoch 27: val_accuracy improved from 0.54435 to 0.58871, saving model to ./ckpt/10_class_lr0005/val_acc_0.589.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0396 - accuracy: 0.5640 - val_loss: 1.0919 - val_accuracy: 0.5887\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0246 - accuracy: 0.5670\n",
      "Epoch 28: val_accuracy did not improve from 0.58871\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0280 - accuracy: 0.5658 - val_loss: 1.0622 - val_accuracy: 0.5605\n",
      "Epoch 29/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0005 - accuracy: 0.5923\n",
      "Epoch 29: val_accuracy did not improve from 0.58871\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0005 - accuracy: 0.5923 - val_loss: 1.0891 - val_accuracy: 0.5605\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0100 - accuracy: 0.5856\n",
      "Epoch 30: val_accuracy did not improve from 0.58871\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0097 - accuracy: 0.5855 - val_loss: 1.0473 - val_accuracy: 0.5444\n",
      "Epoch 31/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0062 - accuracy: 0.5734\n",
      "Epoch 31: val_accuracy did not improve from 0.58871\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0092 - accuracy: 0.5716 - val_loss: 1.0736 - val_accuracy: 0.5645\n",
      "Epoch 32/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9824 - accuracy: 0.5900\n",
      "Epoch 32: val_accuracy did not improve from 0.58871\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9824 - accuracy: 0.5900 - val_loss: 1.0832 - val_accuracy: 0.5444\n",
      "Epoch 33/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9642 - accuracy: 0.5960\n",
      "Epoch 33: val_accuracy did not improve from 0.58871\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9648 - accuracy: 0.5941 - val_loss: 1.0982 - val_accuracy: 0.5242\n",
      "Epoch 34/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9462 - accuracy: 0.6064\n",
      "Epoch 34: val_accuracy did not improve from 0.58871\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9469 - accuracy: 0.6071 - val_loss: 1.0873 - val_accuracy: 0.5282\n",
      "Epoch 35/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9604 - accuracy: 0.6073\n",
      "Epoch 35: val_accuracy did not improve from 0.58871\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9604 - accuracy: 0.6071 - val_loss: 1.0544 - val_accuracy: 0.5242\n",
      "Epoch 36/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9418 - accuracy: 0.6105\n",
      "Epoch 36: val_accuracy did not improve from 0.58871\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9447 - accuracy: 0.6107 - val_loss: 1.0632 - val_accuracy: 0.5363\n",
      "Epoch 37/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9237 - accuracy: 0.6105\n",
      "Epoch 37: val_accuracy did not improve from 0.58871\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9326 - accuracy: 0.6075 - val_loss: 1.0476 - val_accuracy: 0.5565\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9563 - accuracy: 0.6014\n",
      "Epoch 38: val_accuracy did not improve from 0.58871\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9617 - accuracy: 0.5995 - val_loss: 1.0733 - val_accuracy: 0.5645\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9247 - accuracy: 0.6105\n",
      "Epoch 39: val_accuracy improved from 0.58871 to 0.59677, saving model to ./ckpt/10_class_lr0005/val_acc_0.597.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9278 - accuracy: 0.6098 - val_loss: 1.0214 - val_accuracy: 0.5968\n",
      "Epoch 40/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9542 - accuracy: 0.6060\n",
      "Epoch 40: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9568 - accuracy: 0.6057 - val_loss: 1.0343 - val_accuracy: 0.5565\n",
      "Epoch 41/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9138 - accuracy: 0.6200\n",
      "Epoch 41: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9145 - accuracy: 0.6201 - val_loss: 1.0490 - val_accuracy: 0.5484\n",
      "Epoch 42/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9014 - accuracy: 0.6273\n",
      "Epoch 42: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9043 - accuracy: 0.6255 - val_loss: 1.0572 - val_accuracy: 0.5363\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8911 - accuracy: 0.6286\n",
      "Epoch 43: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8899 - accuracy: 0.6291 - val_loss: 1.0306 - val_accuracy: 0.5685\n",
      "Epoch 44/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8824 - accuracy: 0.6386\n",
      "Epoch 44: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8813 - accuracy: 0.6385 - val_loss: 1.0627 - val_accuracy: 0.5403\n",
      "Epoch 45/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8747 - accuracy: 0.6350\n",
      "Epoch 45: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8746 - accuracy: 0.6363 - val_loss: 1.0001 - val_accuracy: 0.5927\n",
      "Epoch 46/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8944 - accuracy: 0.6155\n",
      "Epoch 46: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8951 - accuracy: 0.6161 - val_loss: 1.0452 - val_accuracy: 0.5645\n",
      "Epoch 47/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8920 - accuracy: 0.6319\n",
      "Epoch 47: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9018 - accuracy: 0.6273 - val_loss: 1.0145 - val_accuracy: 0.5645\n",
      "Epoch 48/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8631 - accuracy: 0.6427\n",
      "Epoch 48: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8658 - accuracy: 0.6412 - val_loss: 1.0162 - val_accuracy: 0.5887\n",
      "Epoch 49/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8496 - accuracy: 0.6517\n",
      "Epoch 49: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8513 - accuracy: 0.6507 - val_loss: 1.0782 - val_accuracy: 0.5524\n",
      "Epoch 50/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8391 - accuracy: 0.6551\n",
      "Epoch 50: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8391 - accuracy: 0.6551 - val_loss: 1.1124 - val_accuracy: 0.5565\n",
      "Epoch 51/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8547 - accuracy: 0.6504\n",
      "Epoch 51: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8572 - accuracy: 0.6498 - val_loss: 1.0221 - val_accuracy: 0.5605\n",
      "Epoch 52/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8348 - accuracy: 0.6581\n",
      "Epoch 52: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8348 - accuracy: 0.6583 - val_loss: 1.0290 - val_accuracy: 0.5847\n",
      "Epoch 53/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8547 - accuracy: 0.6390\n",
      "Epoch 53: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8580 - accuracy: 0.6367 - val_loss: 1.0905 - val_accuracy: 0.5565\n",
      "Epoch 54/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8617 - accuracy: 0.6431\n",
      "Epoch 54: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8627 - accuracy: 0.6435 - val_loss: 1.0055 - val_accuracy: 0.5887\n",
      "Epoch 55/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8312 - accuracy: 0.6609\n",
      "Epoch 55: val_accuracy did not improve from 0.59677\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8321 - accuracy: 0.6601 - val_loss: 1.0318 - val_accuracy: 0.5726\n",
      "Epoch 55: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 21:57:06.850321: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 2.2568 - accuracy: 0.1522\n",
      "Epoch 1: val_accuracy improved from -inf to 0.28226, saving model to ./ckpt/10_class_lr0005/val_acc_0.282.hdf5\n",
      "70/70 [==============================] - 2s 19ms/step - loss: 2.2568 - accuracy: 0.1522 - val_loss: 2.0799 - val_accuracy: 0.2823\n",
      "Epoch 2/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 21:57:08.168480: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 1.8929 - accuracy: 0.2708\n",
      "Epoch 2: val_accuracy improved from 0.28226 to 0.33871, saving model to ./ckpt/10_class_lr0005/val_acc_0.339.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.8906 - accuracy: 0.2721 - val_loss: 1.6425 - val_accuracy: 0.3387\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.6962 - accuracy: 0.3065\n",
      "Epoch 3: val_accuracy improved from 0.33871 to 0.37903, saving model to ./ckpt/10_class_lr0005/val_acc_0.379.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.6957 - accuracy: 0.3053 - val_loss: 1.5513 - val_accuracy: 0.3790\n",
      "Epoch 4/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.6269 - accuracy: 0.3307\n",
      "Epoch 4: val_accuracy did not improve from 0.37903\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.6217 - accuracy: 0.3336 - val_loss: 1.5693 - val_accuracy: 0.3508\n",
      "Epoch 5/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.5560 - accuracy: 0.3647\n",
      "Epoch 5: val_accuracy did not improve from 0.37903\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.5645 - accuracy: 0.3601 - val_loss: 1.5056 - val_accuracy: 0.3629\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.5337 - accuracy: 0.3655\n",
      "Epoch 6: val_accuracy improved from 0.37903 to 0.40726, saving model to ./ckpt/10_class_lr0005/val_acc_0.407.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.5337 - accuracy: 0.3655 - val_loss: 1.4486 - val_accuracy: 0.4073\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4764 - accuracy: 0.3822\n",
      "Epoch 7: val_accuracy improved from 0.40726 to 0.44355, saving model to ./ckpt/10_class_lr0005/val_acc_0.444.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4768 - accuracy: 0.3830 - val_loss: 1.3878 - val_accuracy: 0.4435\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.4364 - accuracy: 0.4055\n",
      "Epoch 8: val_accuracy did not improve from 0.44355\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.4364 - accuracy: 0.4055 - val_loss: 1.4132 - val_accuracy: 0.4153\n",
      "Epoch 9/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.4052 - accuracy: 0.4118\n",
      "Epoch 9: val_accuracy improved from 0.44355 to 0.46371, saving model to ./ckpt/10_class_lr0005/val_acc_0.464.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4056 - accuracy: 0.4122 - val_loss: 1.3687 - val_accuracy: 0.4637\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3597 - accuracy: 0.4321\n",
      "Epoch 10: val_accuracy improved from 0.46371 to 0.47581, saving model to ./ckpt/10_class_lr0005/val_acc_0.476.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3580 - accuracy: 0.4329 - val_loss: 1.2919 - val_accuracy: 0.4758\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3243 - accuracy: 0.4553\n",
      "Epoch 11: val_accuracy improved from 0.47581 to 0.47984, saving model to ./ckpt/10_class_lr0005/val_acc_0.480.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3243 - accuracy: 0.4553 - val_loss: 1.3001 - val_accuracy: 0.4798\n",
      "Epoch 12/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.2972 - accuracy: 0.4554\n",
      "Epoch 12: val_accuracy did not improve from 0.47984\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2982 - accuracy: 0.4558 - val_loss: 1.3850 - val_accuracy: 0.3790\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2705 - accuracy: 0.4715\n",
      "Epoch 13: val_accuracy did not improve from 0.47984\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.2686 - accuracy: 0.4719 - val_loss: 1.3603 - val_accuracy: 0.4435\n",
      "Epoch 14/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.2384 - accuracy: 0.4798\n",
      "Epoch 14: val_accuracy improved from 0.47984 to 0.52016, saving model to ./ckpt/10_class_lr0005/val_acc_0.520.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2384 - accuracy: 0.4805 - val_loss: 1.2294 - val_accuracy: 0.5202\n",
      "Epoch 15/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.2058 - accuracy: 0.5051\n",
      "Epoch 15: val_accuracy did not improve from 0.52016\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2038 - accuracy: 0.5092 - val_loss: 1.3117 - val_accuracy: 0.4315\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1751 - accuracy: 0.5005\n",
      "Epoch 16: val_accuracy did not improve from 0.52016\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1745 - accuracy: 0.5007 - val_loss: 1.1731 - val_accuracy: 0.5121\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1654 - accuracy: 0.4968\n",
      "Epoch 17: val_accuracy did not improve from 0.52016\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1644 - accuracy: 0.4984 - val_loss: 1.1957 - val_accuracy: 0.4637\n",
      "Epoch 18/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1312 - accuracy: 0.5335\n",
      "Epoch 18: val_accuracy did not improve from 0.52016\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1326 - accuracy: 0.5317 - val_loss: 1.2144 - val_accuracy: 0.4839\n",
      "Epoch 19/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1197 - accuracy: 0.5303\n",
      "Epoch 19: val_accuracy did not improve from 0.52016\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1211 - accuracy: 0.5303 - val_loss: 1.1592 - val_accuracy: 0.5081\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0826 - accuracy: 0.5340\n",
      "Epoch 20: val_accuracy improved from 0.52016 to 0.54032, saving model to ./ckpt/10_class_lr0005/val_acc_0.540.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0826 - accuracy: 0.5344 - val_loss: 1.1630 - val_accuracy: 0.5403\n",
      "Epoch 21/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0814 - accuracy: 0.5473\n",
      "Epoch 21: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0858 - accuracy: 0.5451 - val_loss: 1.2189 - val_accuracy: 0.4758\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0686 - accuracy: 0.5498\n",
      "Epoch 22: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0689 - accuracy: 0.5496 - val_loss: 1.1946 - val_accuracy: 0.4718\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0339 - accuracy: 0.5639\n",
      "Epoch 23: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0365 - accuracy: 0.5622 - val_loss: 1.1964 - val_accuracy: 0.5282\n",
      "Epoch 24/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0643 - accuracy: 0.5411\n",
      "Epoch 24: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0643 - accuracy: 0.5411 - val_loss: 1.1612 - val_accuracy: 0.5242\n",
      "Epoch 25/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0202 - accuracy: 0.5714\n",
      "Epoch 25: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0182 - accuracy: 0.5725 - val_loss: 1.1680 - val_accuracy: 0.5040\n",
      "Epoch 26/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0008 - accuracy: 0.5901\n",
      "Epoch 26: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0025 - accuracy: 0.5891 - val_loss: 1.1168 - val_accuracy: 0.5202\n",
      "Epoch 27/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9986 - accuracy: 0.5855\n",
      "Epoch 27: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9962 - accuracy: 0.5864 - val_loss: 1.1820 - val_accuracy: 0.5081\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0114 - accuracy: 0.5865\n",
      "Epoch 28: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0135 - accuracy: 0.5851 - val_loss: 1.1653 - val_accuracy: 0.5282\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0003 - accuracy: 0.5756\n",
      "Epoch 29: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0012 - accuracy: 0.5743 - val_loss: 1.3007 - val_accuracy: 0.4919\n",
      "Epoch 30/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9772 - accuracy: 0.5896\n",
      "Epoch 30: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9794 - accuracy: 0.5900 - val_loss: 1.1427 - val_accuracy: 0.4879\n",
      "Epoch 31/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9555 - accuracy: 0.5861\n",
      "Epoch 31: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9538 - accuracy: 0.5873 - val_loss: 1.1881 - val_accuracy: 0.5000\n",
      "Epoch 32/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9441 - accuracy: 0.6119\n",
      "Epoch 32: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9487 - accuracy: 0.6084 - val_loss: 1.1367 - val_accuracy: 0.5040\n",
      "Epoch 33/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9441 - accuracy: 0.6037\n",
      "Epoch 33: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9459 - accuracy: 0.6017 - val_loss: 1.1693 - val_accuracy: 0.5202\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9276 - accuracy: 0.6170\n",
      "Epoch 34: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9276 - accuracy: 0.6170 - val_loss: 1.1769 - val_accuracy: 0.5000\n",
      "Epoch 35/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9399 - accuracy: 0.6138\n",
      "Epoch 35: val_accuracy improved from 0.54032 to 0.56048, saving model to ./ckpt/10_class_lr0005/val_acc_0.560.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9399 - accuracy: 0.6138 - val_loss: 1.1446 - val_accuracy: 0.5605\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9121 - accuracy: 0.6300\n",
      "Epoch 36: val_accuracy did not improve from 0.56048\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9121 - accuracy: 0.6300 - val_loss: 1.1399 - val_accuracy: 0.4960\n",
      "Epoch 36: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 58s - loss: 2.3073 - accuracy: 0.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 21:57:45.031829: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 2.2750 - accuracy: 0.1545\n",
      "Epoch 1: val_accuracy improved from -inf to 0.18952, saving model to ./ckpt/10_class_lr0005/val_acc_0.190.hdf5\n",
      "70/70 [==============================] - 2s 19ms/step - loss: 2.2750 - accuracy: 0.1545 - val_loss: 2.1816 - val_accuracy: 0.1895\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 1s - loss: 2.1870 - accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 21:57:46.354500: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 1.8985 - accuracy: 0.2749\n",
      "Epoch 2: val_accuracy improved from 0.18952 to 0.31048, saving model to ./ckpt/10_class_lr0005/val_acc_0.310.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.8985 - accuracy: 0.2739 - val_loss: 1.7479 - val_accuracy: 0.3105\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.6780 - accuracy: 0.3193\n",
      "Epoch 3: val_accuracy did not improve from 0.31048\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.6778 - accuracy: 0.3188 - val_loss: 1.6722 - val_accuracy: 0.2782\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.6099 - accuracy: 0.3415\n",
      "Epoch 4: val_accuracy improved from 0.31048 to 0.38306, saving model to ./ckpt/10_class_lr0005/val_acc_0.383.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.6101 - accuracy: 0.3426 - val_loss: 1.5796 - val_accuracy: 0.3831\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.5630 - accuracy: 0.3570\n",
      "Epoch 5: val_accuracy did not improve from 0.38306\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5630 - accuracy: 0.3570 - val_loss: 1.6105 - val_accuracy: 0.3548\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.5263 - accuracy: 0.3667\n",
      "Epoch 6: val_accuracy did not improve from 0.38306\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5286 - accuracy: 0.3651 - val_loss: 1.5398 - val_accuracy: 0.3548\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4705 - accuracy: 0.3936\n",
      "Epoch 7: val_accuracy did not improve from 0.38306\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4705 - accuracy: 0.3925 - val_loss: 1.5321 - val_accuracy: 0.3548\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.4499 - accuracy: 0.3992\n",
      "Epoch 8: val_accuracy did not improve from 0.38306\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4499 - accuracy: 0.3992 - val_loss: 1.5166 - val_accuracy: 0.3468\n",
      "Epoch 9/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3947 - accuracy: 0.4149\n",
      "Epoch 9: val_accuracy did not improve from 0.38306\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3988 - accuracy: 0.4149 - val_loss: 1.3872 - val_accuracy: 0.3790\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3879 - accuracy: 0.4212\n",
      "Epoch 10: val_accuracy improved from 0.38306 to 0.38710, saving model to ./ckpt/10_class_lr0005/val_acc_0.387.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3870 - accuracy: 0.4225 - val_loss: 1.3622 - val_accuracy: 0.3871\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3461 - accuracy: 0.4425\n",
      "Epoch 11: val_accuracy did not improve from 0.38710\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3468 - accuracy: 0.4423 - val_loss: 1.4755 - val_accuracy: 0.3347\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3212 - accuracy: 0.4398\n",
      "Epoch 12: val_accuracy did not improve from 0.38710\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3206 - accuracy: 0.4401 - val_loss: 1.3792 - val_accuracy: 0.3508\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2842 - accuracy: 0.4607\n",
      "Epoch 13: val_accuracy improved from 0.38710 to 0.41532, saving model to ./ckpt/10_class_lr0005/val_acc_0.415.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2842 - accuracy: 0.4607 - val_loss: 1.3083 - val_accuracy: 0.4153\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2691 - accuracy: 0.4836\n",
      "Epoch 14: val_accuracy improved from 0.41532 to 0.42742, saving model to ./ckpt/10_class_lr0005/val_acc_0.427.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2691 - accuracy: 0.4836 - val_loss: 1.2800 - val_accuracy: 0.4274\n",
      "Epoch 15/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.2419 - accuracy: 0.4770\n",
      "Epoch 15: val_accuracy did not improve from 0.42742\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.2499 - accuracy: 0.4737 - val_loss: 1.2688 - val_accuracy: 0.4194\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2096 - accuracy: 0.5000\n",
      "Epoch 16: val_accuracy did not improve from 0.42742\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2076 - accuracy: 0.5016 - val_loss: 1.2595 - val_accuracy: 0.4113\n",
      "Epoch 17/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1967 - accuracy: 0.4986\n",
      "Epoch 17: val_accuracy did not improve from 0.42742\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1951 - accuracy: 0.4993 - val_loss: 1.2654 - val_accuracy: 0.4274\n",
      "Epoch 18/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1763 - accuracy: 0.5100\n",
      "Epoch 18: val_accuracy improved from 0.42742 to 0.46371, saving model to ./ckpt/10_class_lr0005/val_acc_0.464.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1783 - accuracy: 0.5083 - val_loss: 1.2054 - val_accuracy: 0.4637\n",
      "Epoch 19/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1603 - accuracy: 0.5059\n",
      "Epoch 19: val_accuracy did not improve from 0.46371\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1609 - accuracy: 0.5061 - val_loss: 1.1790 - val_accuracy: 0.4435\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1358 - accuracy: 0.5222\n",
      "Epoch 20: val_accuracy improved from 0.46371 to 0.50403, saving model to ./ckpt/10_class_lr0005/val_acc_0.504.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1387 - accuracy: 0.5209 - val_loss: 1.1540 - val_accuracy: 0.5040\n",
      "Epoch 21/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1420 - accuracy: 0.5254\n",
      "Epoch 21: val_accuracy did not improve from 0.50403\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1409 - accuracy: 0.5249 - val_loss: 1.1807 - val_accuracy: 0.4556\n",
      "Epoch 22/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.1158 - accuracy: 0.5290\n",
      "Epoch 22: val_accuracy improved from 0.50403 to 0.50806, saving model to ./ckpt/10_class_lr0005/val_acc_0.508.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1177 - accuracy: 0.5285 - val_loss: 1.1171 - val_accuracy: 0.5081\n",
      "Epoch 23/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1019 - accuracy: 0.5462\n",
      "Epoch 23: val_accuracy did not improve from 0.50806\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1039 - accuracy: 0.5456 - val_loss: 1.1826 - val_accuracy: 0.4355\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0870 - accuracy: 0.5385\n",
      "Epoch 24: val_accuracy did not improve from 0.50806\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0866 - accuracy: 0.5379 - val_loss: 1.1406 - val_accuracy: 0.4718\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0585 - accuracy: 0.5595\n",
      "Epoch 25: val_accuracy did not improve from 0.50806\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0585 - accuracy: 0.5595 - val_loss: 1.2209 - val_accuracy: 0.4355\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0594 - accuracy: 0.5613\n",
      "Epoch 26: val_accuracy did not improve from 0.50806\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0594 - accuracy: 0.5613 - val_loss: 1.1752 - val_accuracy: 0.4758\n",
      "Epoch 27/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0388 - accuracy: 0.5707\n",
      "Epoch 27: val_accuracy did not improve from 0.50806\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0399 - accuracy: 0.5689 - val_loss: 1.1306 - val_accuracy: 0.4839\n",
      "Epoch 28/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0365 - accuracy: 0.5592\n",
      "Epoch 28: val_accuracy did not improve from 0.50806\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0373 - accuracy: 0.5622 - val_loss: 1.1389 - val_accuracy: 0.4556\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0329 - accuracy: 0.5616\n",
      "Epoch 29: val_accuracy did not improve from 0.50806\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0389 - accuracy: 0.5604 - val_loss: 1.1267 - val_accuracy: 0.4758\n",
      "Epoch 30/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9997 - accuracy: 0.5918\n",
      "Epoch 30: val_accuracy did not improve from 0.50806\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9997 - accuracy: 0.5918 - val_loss: 1.1834 - val_accuracy: 0.4718\n",
      "Epoch 31/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0078 - accuracy: 0.5765\n",
      "Epoch 31: val_accuracy did not improve from 0.50806\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0111 - accuracy: 0.5761 - val_loss: 1.1479 - val_accuracy: 0.4758\n",
      "Epoch 32/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0126 - accuracy: 0.5766\n",
      "Epoch 32: val_accuracy did not improve from 0.50806\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0126 - accuracy: 0.5766 - val_loss: 1.1110 - val_accuracy: 0.4960\n",
      "Epoch 33/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9807 - accuracy: 0.5802\n",
      "Epoch 33: val_accuracy did not improve from 0.50806\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9795 - accuracy: 0.5815 - val_loss: 1.1047 - val_accuracy: 0.5081\n",
      "Epoch 34/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9854 - accuracy: 0.5806\n",
      "Epoch 34: val_accuracy improved from 0.50806 to 0.54032, saving model to ./ckpt/10_class_lr0005/val_acc_0.540.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9871 - accuracy: 0.5802 - val_loss: 1.1485 - val_accuracy: 0.5403\n",
      "Epoch 35/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9936 - accuracy: 0.5905\n",
      "Epoch 35: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9936 - accuracy: 0.5905 - val_loss: 1.1163 - val_accuracy: 0.5081\n",
      "Epoch 36/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9636 - accuracy: 0.5856\n",
      "Epoch 36: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9626 - accuracy: 0.5864 - val_loss: 1.0882 - val_accuracy: 0.5121\n",
      "Epoch 37/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9584 - accuracy: 0.6004\n",
      "Epoch 37: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9584 - accuracy: 0.6004 - val_loss: 1.0970 - val_accuracy: 0.4960\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9705 - accuracy: 0.5888\n",
      "Epoch 38: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9707 - accuracy: 0.5891 - val_loss: 1.0978 - val_accuracy: 0.4919\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9270 - accuracy: 0.6123\n",
      "Epoch 39: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9285 - accuracy: 0.6125 - val_loss: 1.1544 - val_accuracy: 0.4597\n",
      "Epoch 40/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9174 - accuracy: 0.6119\n",
      "Epoch 40: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9186 - accuracy: 0.6125 - val_loss: 1.1004 - val_accuracy: 0.5121\n",
      "Epoch 41/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9408 - accuracy: 0.6094\n",
      "Epoch 41: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9416 - accuracy: 0.6089 - val_loss: 1.0975 - val_accuracy: 0.5121\n",
      "Epoch 42/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9182 - accuracy: 0.6164\n",
      "Epoch 42: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9193 - accuracy: 0.6161 - val_loss: 1.0951 - val_accuracy: 0.5081\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9345 - accuracy: 0.6168\n",
      "Epoch 43: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9338 - accuracy: 0.6161 - val_loss: 1.0723 - val_accuracy: 0.4879\n",
      "Epoch 44/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8838 - accuracy: 0.6441\n",
      "Epoch 44: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8863 - accuracy: 0.6394 - val_loss: 1.0864 - val_accuracy: 0.5202\n",
      "Epoch 45/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8948 - accuracy: 0.6245\n",
      "Epoch 45: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8984 - accuracy: 0.6228 - val_loss: 1.0988 - val_accuracy: 0.5121\n",
      "Epoch 46/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8780 - accuracy: 0.6472\n",
      "Epoch 46: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8783 - accuracy: 0.6457 - val_loss: 1.1040 - val_accuracy: 0.5323\n",
      "Epoch 47/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8772 - accuracy: 0.6486\n",
      "Epoch 47: val_accuracy did not improve from 0.54032\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8791 - accuracy: 0.6480 - val_loss: 1.0821 - val_accuracy: 0.5242\n",
      "Epoch 48/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8718 - accuracy: 0.6418\n",
      "Epoch 48: val_accuracy improved from 0.54032 to 0.54435, saving model to ./ckpt/10_class_lr0005/val_acc_0.544.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8736 - accuracy: 0.6408 - val_loss: 1.1423 - val_accuracy: 0.5444\n",
      "Epoch 49/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8686 - accuracy: 0.6399\n",
      "Epoch 49: val_accuracy did not improve from 0.54435\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8677 - accuracy: 0.6390 - val_loss: 1.0889 - val_accuracy: 0.5040\n",
      "Epoch 50/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8761 - accuracy: 0.6386\n",
      "Epoch 50: val_accuracy did not improve from 0.54435\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8772 - accuracy: 0.6372 - val_loss: 1.0908 - val_accuracy: 0.4919\n",
      "Epoch 51/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8496 - accuracy: 0.6639\n",
      "Epoch 51: val_accuracy did not improve from 0.54435\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8510 - accuracy: 0.6632 - val_loss: 1.0844 - val_accuracy: 0.5161\n",
      "Epoch 52/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8528 - accuracy: 0.6490\n",
      "Epoch 52: val_accuracy improved from 0.54435 to 0.54839, saving model to ./ckpt/10_class_lr0005/val_acc_0.548.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8526 - accuracy: 0.6489 - val_loss: 1.0852 - val_accuracy: 0.5484\n",
      "Epoch 53/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8284 - accuracy: 0.6713\n",
      "Epoch 53: val_accuracy did not improve from 0.54839\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8284 - accuracy: 0.6713 - val_loss: 1.1585 - val_accuracy: 0.5282\n",
      "Epoch 53: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 27s - loss: 2.2794 - accuracy: 0.1562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 21:58:40.187793: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 2.2633 - accuracy: 0.1482\n",
      "Epoch 1: val_accuracy improved from -inf to 0.21774, saving model to ./ckpt/10_class_lr0005/val_acc_0.218.hdf5\n",
      "70/70 [==============================] - 2s 19ms/step - loss: 2.2633 - accuracy: 0.1482 - val_loss: 2.1545 - val_accuracy: 0.2177\n",
      "Epoch 2/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 21:58:41.488679: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/70 [============================>.] - ETA: 0s - loss: 1.8769 - accuracy: 0.2647\n",
      "Epoch 2: val_accuracy improved from 0.21774 to 0.27016, saving model to ./ckpt/10_class_lr0005/val_acc_0.270.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.8723 - accuracy: 0.2649 - val_loss: 1.7051 - val_accuracy: 0.2702\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.6700 - accuracy: 0.3202\n",
      "Epoch 3: val_accuracy improved from 0.27016 to 0.31048, saving model to ./ckpt/10_class_lr0005/val_acc_0.310.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.6687 - accuracy: 0.3206 - val_loss: 1.6898 - val_accuracy: 0.3105\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.6007 - accuracy: 0.3320\n",
      "Epoch 4: val_accuracy improved from 0.31048 to 0.32661, saving model to ./ckpt/10_class_lr0005/val_acc_0.327.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5992 - accuracy: 0.3323 - val_loss: 1.5712 - val_accuracy: 0.3266\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5430 - accuracy: 0.3555\n",
      "Epoch 5: val_accuracy improved from 0.32661 to 0.33468, saving model to ./ckpt/10_class_lr0005/val_acc_0.335.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5450 - accuracy: 0.3543 - val_loss: 1.5437 - val_accuracy: 0.3347\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5126 - accuracy: 0.3809\n",
      "Epoch 6: val_accuracy improved from 0.33468 to 0.33871, saving model to ./ckpt/10_class_lr0005/val_acc_0.339.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5110 - accuracy: 0.3808 - val_loss: 1.4840 - val_accuracy: 0.3387\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.4523 - accuracy: 0.3947\n",
      "Epoch 7: val_accuracy improved from 0.33871 to 0.35484, saving model to ./ckpt/10_class_lr0005/val_acc_0.355.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4523 - accuracy: 0.3947 - val_loss: 1.4665 - val_accuracy: 0.3548\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.4206 - accuracy: 0.4149\n",
      "Epoch 8: val_accuracy improved from 0.35484 to 0.37500, saving model to ./ckpt/10_class_lr0005/val_acc_0.375.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4206 - accuracy: 0.4149 - val_loss: 1.4208 - val_accuracy: 0.3750\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3882 - accuracy: 0.4172\n",
      "Epoch 9: val_accuracy did not improve from 0.37500\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3882 - accuracy: 0.4172 - val_loss: 1.3832 - val_accuracy: 0.3427\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3565 - accuracy: 0.4275\n",
      "Epoch 10: val_accuracy did not improve from 0.37500\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3567 - accuracy: 0.4266 - val_loss: 1.4312 - val_accuracy: 0.3508\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3422 - accuracy: 0.4298\n",
      "Epoch 11: val_accuracy improved from 0.37500 to 0.42339, saving model to ./ckpt/10_class_lr0005/val_acc_0.423.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3430 - accuracy: 0.4306 - val_loss: 1.3283 - val_accuracy: 0.4234\n",
      "Epoch 12/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.3001 - accuracy: 0.4481\n",
      "Epoch 12: val_accuracy improved from 0.42339 to 0.44355, saving model to ./ckpt/10_class_lr0005/val_acc_0.444.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2998 - accuracy: 0.4481 - val_loss: 1.3186 - val_accuracy: 0.4435\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2789 - accuracy: 0.4683\n",
      "Epoch 13: val_accuracy improved from 0.44355 to 0.45968, saving model to ./ckpt/10_class_lr0005/val_acc_0.460.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2775 - accuracy: 0.4692 - val_loss: 1.3292 - val_accuracy: 0.4597\n",
      "Epoch 14/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2427 - accuracy: 0.4737\n",
      "Epoch 14: val_accuracy improved from 0.45968 to 0.49194, saving model to ./ckpt/10_class_lr0005/val_acc_0.492.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2438 - accuracy: 0.4728 - val_loss: 1.2675 - val_accuracy: 0.4919\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1956 - accuracy: 0.4973\n",
      "Epoch 15: val_accuracy did not improve from 0.49194\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1987 - accuracy: 0.4962 - val_loss: 1.2470 - val_accuracy: 0.4637\n",
      "Epoch 16/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.2012 - accuracy: 0.4899\n",
      "Epoch 16: val_accuracy improved from 0.49194 to 0.52016, saving model to ./ckpt/10_class_lr0005/val_acc_0.520.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1995 - accuracy: 0.4912 - val_loss: 1.2250 - val_accuracy: 0.5202\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1600 - accuracy: 0.5077\n",
      "Epoch 17: val_accuracy did not improve from 0.52016\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1615 - accuracy: 0.5061 - val_loss: 1.1969 - val_accuracy: 0.5081\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1572 - accuracy: 0.5061\n",
      "Epoch 18: val_accuracy did not improve from 0.52016\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1572 - accuracy: 0.5061 - val_loss: 1.1667 - val_accuracy: 0.5121\n",
      "Epoch 19/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1561 - accuracy: 0.5113\n",
      "Epoch 19: val_accuracy did not improve from 0.52016\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1574 - accuracy: 0.5115 - val_loss: 1.3338 - val_accuracy: 0.4194\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1552 - accuracy: 0.5154\n",
      "Epoch 20: val_accuracy did not improve from 0.52016\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1548 - accuracy: 0.5150 - val_loss: 1.1777 - val_accuracy: 0.5161\n",
      "Epoch 21/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1047 - accuracy: 0.5349\n",
      "Epoch 21: val_accuracy did not improve from 0.52016\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1059 - accuracy: 0.5339 - val_loss: 1.1695 - val_accuracy: 0.5121\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0793 - accuracy: 0.5380\n",
      "Epoch 22: val_accuracy did not improve from 0.52016\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0819 - accuracy: 0.5370 - val_loss: 1.1604 - val_accuracy: 0.5081\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0845 - accuracy: 0.5444\n",
      "Epoch 23: val_accuracy did not improve from 0.52016\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0846 - accuracy: 0.5442 - val_loss: 1.2320 - val_accuracy: 0.4556\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0597 - accuracy: 0.5588\n",
      "Epoch 24: val_accuracy improved from 0.52016 to 0.53226, saving model to ./ckpt/10_class_lr0005/val_acc_0.532.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0626 - accuracy: 0.5577 - val_loss: 1.1184 - val_accuracy: 0.5323\n",
      "Epoch 25/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0544 - accuracy: 0.5598\n",
      "Epoch 25: val_accuracy improved from 0.53226 to 0.54839, saving model to ./ckpt/10_class_lr0005/val_acc_0.548.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0524 - accuracy: 0.5613 - val_loss: 1.1007 - val_accuracy: 0.5484\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0535 - accuracy: 0.5430\n",
      "Epoch 26: val_accuracy did not improve from 0.54839\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0537 - accuracy: 0.5433 - val_loss: 1.1814 - val_accuracy: 0.4960\n",
      "Epoch 27/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0357 - accuracy: 0.5617\n",
      "Epoch 27: val_accuracy did not improve from 0.54839\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0357 - accuracy: 0.5617 - val_loss: 1.1603 - val_accuracy: 0.4879\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0103 - accuracy: 0.5765\n",
      "Epoch 28: val_accuracy did not improve from 0.54839\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0090 - accuracy: 0.5770 - val_loss: 1.1138 - val_accuracy: 0.5282\n",
      "Epoch 29/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9941 - accuracy: 0.5793\n",
      "Epoch 29: val_accuracy did not improve from 0.54839\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9941 - accuracy: 0.5793 - val_loss: 1.1231 - val_accuracy: 0.5403\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9758 - accuracy: 0.5987\n",
      "Epoch 30: val_accuracy improved from 0.54839 to 0.58065, saving model to ./ckpt/10_class_lr0005/val_acc_0.581.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9792 - accuracy: 0.5972 - val_loss: 1.0663 - val_accuracy: 0.5806\n",
      "Epoch 31/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9765 - accuracy: 0.6039\n",
      "Epoch 31: val_accuracy did not improve from 0.58065\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9805 - accuracy: 0.6031 - val_loss: 1.1672 - val_accuracy: 0.4879\n",
      "Epoch 32/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9982 - accuracy: 0.5870\n",
      "Epoch 32: val_accuracy did not improve from 0.58065\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0005 - accuracy: 0.5851 - val_loss: 1.0811 - val_accuracy: 0.5605\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9766 - accuracy: 0.5941\n",
      "Epoch 33: val_accuracy did not improve from 0.58065\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9766 - accuracy: 0.5941 - val_loss: 1.0933 - val_accuracy: 0.5282\n",
      "Epoch 34/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9416 - accuracy: 0.6028\n",
      "Epoch 34: val_accuracy did not improve from 0.58065\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9444 - accuracy: 0.6017 - val_loss: 1.1613 - val_accuracy: 0.4879\n",
      "Epoch 35/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9648 - accuracy: 0.5886\n",
      "Epoch 35: val_accuracy did not improve from 0.58065\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9731 - accuracy: 0.5882 - val_loss: 1.2045 - val_accuracy: 0.4637\n",
      "Epoch 36/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9599 - accuracy: 0.5983\n",
      "Epoch 36: val_accuracy did not improve from 0.58065\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9600 - accuracy: 0.5981 - val_loss: 1.2649 - val_accuracy: 0.4476\n",
      "Epoch 37/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9610 - accuracy: 0.5969\n",
      "Epoch 37: val_accuracy did not improve from 0.58065\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9629 - accuracy: 0.5954 - val_loss: 1.0941 - val_accuracy: 0.5484\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9272 - accuracy: 0.6123\n",
      "Epoch 38: val_accuracy did not improve from 0.58065\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9308 - accuracy: 0.6102 - val_loss: 1.1005 - val_accuracy: 0.5363\n",
      "Epoch 39/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9276 - accuracy: 0.6152\n",
      "Epoch 39: val_accuracy did not improve from 0.58065\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9276 - accuracy: 0.6152 - val_loss: 1.0717 - val_accuracy: 0.5484\n",
      "Epoch 40/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8997 - accuracy: 0.6320\n",
      "Epoch 40: val_accuracy did not improve from 0.58065\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9036 - accuracy: 0.6336 - val_loss: 1.1218 - val_accuracy: 0.5323\n",
      "Epoch 40: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 21:59:22.160300: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 2.2671 - accuracy: 0.1477\n",
      "Epoch 1: val_accuracy improved from -inf to 0.27530, saving model to ./ckpt/10_class_lr0005/val_acc_0.275.hdf5\n",
      "70/70 [==============================] - 2s 19ms/step - loss: 2.2671 - accuracy: 0.1477 - val_loss: 2.1460 - val_accuracy: 0.2753\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 1s - loss: 2.1638 - accuracy: 0.1562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 21:59:23.469827: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/70 [============================>.] - ETA: 0s - loss: 1.9124 - accuracy: 0.2523\n",
      "Epoch 2: val_accuracy improved from 0.27530 to 0.34413, saving model to ./ckpt/10_class_lr0005/val_acc_0.344.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.9114 - accuracy: 0.2518 - val_loss: 1.6702 - val_accuracy: 0.3441\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.6867 - accuracy: 0.3116\n",
      "Epoch 3: val_accuracy improved from 0.34413 to 0.38057, saving model to ./ckpt/10_class_lr0005/val_acc_0.381.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.6884 - accuracy: 0.3106 - val_loss: 1.5777 - val_accuracy: 0.3806\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.6219 - accuracy: 0.3460\n",
      "Epoch 4: val_accuracy did not improve from 0.38057\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.6205 - accuracy: 0.3461 - val_loss: 1.5732 - val_accuracy: 0.3806\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5854 - accuracy: 0.3401\n",
      "Epoch 5: val_accuracy did not improve from 0.38057\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5842 - accuracy: 0.3411 - val_loss: 1.4936 - val_accuracy: 0.3765\n",
      "Epoch 6/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.5203 - accuracy: 0.3666\n",
      "Epoch 6: val_accuracy improved from 0.38057 to 0.41700, saving model to ./ckpt/10_class_lr0005/val_acc_0.417.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.5182 - accuracy: 0.3640 - val_loss: 1.4486 - val_accuracy: 0.4170\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4759 - accuracy: 0.3940\n",
      "Epoch 7: val_accuracy did not improve from 0.41700\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4758 - accuracy: 0.3945 - val_loss: 1.4120 - val_accuracy: 0.4089\n",
      "Epoch 8/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.4435 - accuracy: 0.3993\n",
      "Epoch 8: val_accuracy improved from 0.41700 to 0.45749, saving model to ./ckpt/10_class_lr0005/val_acc_0.457.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4412 - accuracy: 0.4013 - val_loss: 1.3587 - val_accuracy: 0.4575\n",
      "Epoch 9/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3818 - accuracy: 0.4253\n",
      "Epoch 9: val_accuracy did not improve from 0.45749\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3826 - accuracy: 0.4241 - val_loss: 1.4060 - val_accuracy: 0.3887\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3977 - accuracy: 0.4162\n",
      "Epoch 10: val_accuracy did not improve from 0.45749\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.3970 - accuracy: 0.4156 - val_loss: 1.2976 - val_accuracy: 0.4453\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3307 - accuracy: 0.4325\n",
      "Epoch 11: val_accuracy did not improve from 0.45749\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3320 - accuracy: 0.4318 - val_loss: 1.3066 - val_accuracy: 0.4494\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.2838 - accuracy: 0.4552\n",
      "Epoch 12: val_accuracy improved from 0.45749 to 0.46964, saving model to ./ckpt/10_class_lr0005/val_acc_0.470.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.2885 - accuracy: 0.4565 - val_loss: 1.2834 - val_accuracy: 0.4696\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2662 - accuracy: 0.4701\n",
      "Epoch 13: val_accuracy did not improve from 0.46964\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2664 - accuracy: 0.4677 - val_loss: 1.2501 - val_accuracy: 0.4413\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2565 - accuracy: 0.4636\n",
      "Epoch 14: val_accuracy did not improve from 0.46964\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.2565 - accuracy: 0.4636 - val_loss: 1.3416 - val_accuracy: 0.4211\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2203 - accuracy: 0.4909\n",
      "Epoch 15: val_accuracy improved from 0.46964 to 0.55061, saving model to ./ckpt/10_class_lr0005/val_acc_0.551.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2234 - accuracy: 0.4906 - val_loss: 1.1814 - val_accuracy: 0.5506\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1895 - accuracy: 0.5113\n",
      "Epoch 16: val_accuracy did not improve from 0.55061\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1879 - accuracy: 0.5126 - val_loss: 1.2271 - val_accuracy: 0.4696\n",
      "Epoch 17/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1720 - accuracy: 0.5079\n",
      "Epoch 17: val_accuracy did not improve from 0.55061\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1742 - accuracy: 0.5094 - val_loss: 1.1407 - val_accuracy: 0.5506\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1662 - accuracy: 0.5076\n",
      "Epoch 18: val_accuracy did not improve from 0.55061\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1662 - accuracy: 0.5076 - val_loss: 1.1374 - val_accuracy: 0.5506\n",
      "Epoch 19/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1472 - accuracy: 0.5263\n",
      "Epoch 19: val_accuracy improved from 0.55061 to 0.57085, saving model to ./ckpt/10_class_lr0005/val_acc_0.571.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1454 - accuracy: 0.5274 - val_loss: 1.1468 - val_accuracy: 0.5709\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1388 - accuracy: 0.5168\n",
      "Epoch 20: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1409 - accuracy: 0.5153 - val_loss: 1.1519 - val_accuracy: 0.5466\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1170 - accuracy: 0.5274\n",
      "Epoch 21: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1170 - accuracy: 0.5274 - val_loss: 1.1436 - val_accuracy: 0.5466\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0843 - accuracy: 0.5494\n",
      "Epoch 22: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0833 - accuracy: 0.5498 - val_loss: 1.2011 - val_accuracy: 0.5182\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0630 - accuracy: 0.5593\n",
      "Epoch 23: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0671 - accuracy: 0.5592 - val_loss: 1.1043 - val_accuracy: 0.5547\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0860 - accuracy: 0.5430\n",
      "Epoch 24: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0844 - accuracy: 0.5435 - val_loss: 1.1637 - val_accuracy: 0.5223\n",
      "Epoch 25/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0701 - accuracy: 0.5532\n",
      "Epoch 25: val_accuracy did not improve from 0.57085\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0729 - accuracy: 0.5494 - val_loss: 1.1176 - val_accuracy: 0.5182\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0481 - accuracy: 0.5557\n",
      "Epoch 26: val_accuracy improved from 0.57085 to 0.60729, saving model to ./ckpt/10_class_lr0005/val_acc_0.607.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0483 - accuracy: 0.5557 - val_loss: 1.1125 - val_accuracy: 0.6073\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0234 - accuracy: 0.5616\n",
      "Epoch 27: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0306 - accuracy: 0.5579 - val_loss: 1.1055 - val_accuracy: 0.5506\n",
      "Epoch 28/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0227 - accuracy: 0.5785\n",
      "Epoch 28: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.0227 - accuracy: 0.5785 - val_loss: 1.2222 - val_accuracy: 0.4615\n",
      "Epoch 29/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0052 - accuracy: 0.5790\n",
      "Epoch 29: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0052 - accuracy: 0.5790 - val_loss: 1.0864 - val_accuracy: 0.5992\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0232 - accuracy: 0.5657\n",
      "Epoch 30: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0229 - accuracy: 0.5664 - val_loss: 1.1192 - val_accuracy: 0.5911\n",
      "Epoch 31/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0386 - accuracy: 0.5738\n",
      "Epoch 31: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0412 - accuracy: 0.5727 - val_loss: 1.0751 - val_accuracy: 0.5830\n",
      "Epoch 32/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9844 - accuracy: 0.5910\n",
      "Epoch 32: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9820 - accuracy: 0.5925 - val_loss: 1.0566 - val_accuracy: 0.5911\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9777 - accuracy: 0.5880\n",
      "Epoch 33: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9777 - accuracy: 0.5880 - val_loss: 1.0833 - val_accuracy: 0.5668\n",
      "Epoch 34/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9571 - accuracy: 0.5979\n",
      "Epoch 34: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9629 - accuracy: 0.5956 - val_loss: 1.0892 - val_accuracy: 0.5547\n",
      "Epoch 35/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9589 - accuracy: 0.6069\n",
      "Epoch 35: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9574 - accuracy: 0.6073 - val_loss: 1.1158 - val_accuracy: 0.5506\n",
      "Epoch 36/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9331 - accuracy: 0.6232\n",
      "Epoch 36: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9329 - accuracy: 0.6234 - val_loss: 1.0696 - val_accuracy: 0.5830\n",
      "Epoch 37/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9370 - accuracy: 0.6105\n",
      "Epoch 37: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9401 - accuracy: 0.6095 - val_loss: 1.1345 - val_accuracy: 0.5142\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9108 - accuracy: 0.6114\n",
      "Epoch 38: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9103 - accuracy: 0.6109 - val_loss: 1.0565 - val_accuracy: 0.5951\n",
      "Epoch 39/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9151 - accuracy: 0.6275\n",
      "Epoch 39: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9151 - accuracy: 0.6275 - val_loss: 1.0741 - val_accuracy: 0.5587\n",
      "Epoch 40/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9093 - accuracy: 0.6082\n",
      "Epoch 40: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9117 - accuracy: 0.6082 - val_loss: 1.0669 - val_accuracy: 0.5911\n",
      "Epoch 41/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8839 - accuracy: 0.6422\n",
      "Epoch 41: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8841 - accuracy: 0.6409 - val_loss: 1.0737 - val_accuracy: 0.5992\n",
      "Epoch 42/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9084 - accuracy: 0.6288\n",
      "Epoch 42: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9084 - accuracy: 0.6288 - val_loss: 1.0709 - val_accuracy: 0.5870\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9058 - accuracy: 0.6359\n",
      "Epoch 43: val_accuracy did not improve from 0.60729\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9053 - accuracy: 0.6369 - val_loss: 1.0156 - val_accuracy: 0.6032\n",
      "Epoch 44/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8830 - accuracy: 0.6295\n",
      "Epoch 44: val_accuracy improved from 0.60729 to 0.62348, saving model to ./ckpt/10_class_lr0005/val_acc_0.623.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8843 - accuracy: 0.6288 - val_loss: 1.0295 - val_accuracy: 0.6235\n",
      "Epoch 45/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8795 - accuracy: 0.6459\n",
      "Epoch 45: val_accuracy did not improve from 0.62348\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8795 - accuracy: 0.6459 - val_loss: 1.0472 - val_accuracy: 0.5709\n",
      "Epoch 46/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8729 - accuracy: 0.6301\n",
      "Epoch 46: val_accuracy did not improve from 0.62348\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8731 - accuracy: 0.6288 - val_loss: 1.1693 - val_accuracy: 0.5344\n",
      "Epoch 47/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8957 - accuracy: 0.6273\n",
      "Epoch 47: val_accuracy did not improve from 0.62348\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8933 - accuracy: 0.6284 - val_loss: 1.0577 - val_accuracy: 0.5749\n",
      "Epoch 48/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8557 - accuracy: 0.6381\n",
      "Epoch 48: val_accuracy did not improve from 0.62348\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8580 - accuracy: 0.6369 - val_loss: 1.0546 - val_accuracy: 0.5668\n",
      "Epoch 49/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8456 - accuracy: 0.6599\n",
      "Epoch 49: val_accuracy did not improve from 0.62348\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8466 - accuracy: 0.6602 - val_loss: 1.0850 - val_accuracy: 0.5668\n",
      "Epoch 50/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8575 - accuracy: 0.6413\n",
      "Epoch 50: val_accuracy did not improve from 0.62348\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8625 - accuracy: 0.6400 - val_loss: 1.0623 - val_accuracy: 0.5992\n",
      "Epoch 51/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8620 - accuracy: 0.6336\n",
      "Epoch 51: val_accuracy did not improve from 0.62348\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8694 - accuracy: 0.6311 - val_loss: 1.0225 - val_accuracy: 0.6032\n",
      "Epoch 52/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8423 - accuracy: 0.6522\n",
      "Epoch 52: val_accuracy did not improve from 0.62348\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8423 - accuracy: 0.6522 - val_loss: 1.0636 - val_accuracy: 0.5951\n",
      "Epoch 53/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8225 - accuracy: 0.6656\n",
      "Epoch 53: val_accuracy did not improve from 0.62348\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8230 - accuracy: 0.6638 - val_loss: 1.0592 - val_accuracy: 0.6073\n",
      "Epoch 53: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 22:00:17.190879: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 2.2679 - accuracy: 0.1405\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25101, saving model to ./ckpt/10_class_lr0005/val_acc_0.251.hdf5\n",
      "70/70 [==============================] - 2s 19ms/step - loss: 2.2679 - accuracy: 0.1405 - val_loss: 2.1462 - val_accuracy: 0.2510\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 2.1836 - accuracy: 0.2188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 22:00:18.507828: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 1.9079 - accuracy: 0.2554\n",
      "Epoch 2: val_accuracy improved from 0.25101 to 0.30769, saving model to ./ckpt/10_class_lr0005/val_acc_0.308.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.9076 - accuracy: 0.2558 - val_loss: 1.7317 - val_accuracy: 0.3077\n",
      "Epoch 3/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.6848 - accuracy: 0.3137\n",
      "Epoch 3: val_accuracy improved from 0.30769 to 0.34008, saving model to ./ckpt/10_class_lr0005/val_acc_0.340.hdf5\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 1.6848 - accuracy: 0.3137 - val_loss: 1.6414 - val_accuracy: 0.3401\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.6099 - accuracy: 0.3428\n",
      "Epoch 4: val_accuracy improved from 0.34008 to 0.38462, saving model to ./ckpt/10_class_lr0005/val_acc_0.385.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.6084 - accuracy: 0.3407 - val_loss: 1.5378 - val_accuracy: 0.3846\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5541 - accuracy: 0.3723\n",
      "Epoch 5: val_accuracy did not improve from 0.38462\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.5542 - accuracy: 0.3712 - val_loss: 1.5182 - val_accuracy: 0.3603\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5078 - accuracy: 0.3818\n",
      "Epoch 6: val_accuracy improved from 0.38462 to 0.40081, saving model to ./ckpt/10_class_lr0005/val_acc_0.401.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.5088 - accuracy: 0.3829 - val_loss: 1.5478 - val_accuracy: 0.4008\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4893 - accuracy: 0.3854\n",
      "Epoch 7: val_accuracy improved from 0.40081 to 0.40891, saving model to ./ckpt/10_class_lr0005/val_acc_0.409.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4882 - accuracy: 0.3860 - val_loss: 1.4359 - val_accuracy: 0.4089\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.4457 - accuracy: 0.4031\n",
      "Epoch 8: val_accuracy did not improve from 0.40891\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4457 - accuracy: 0.4031 - val_loss: 1.4304 - val_accuracy: 0.4008\n",
      "Epoch 9/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3940 - accuracy: 0.4198\n",
      "Epoch 9: val_accuracy improved from 0.40891 to 0.48178, saving model to ./ckpt/10_class_lr0005/val_acc_0.482.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3941 - accuracy: 0.4201 - val_loss: 1.3405 - val_accuracy: 0.4818\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3568 - accuracy: 0.4303\n",
      "Epoch 10: val_accuracy did not improve from 0.48178\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3564 - accuracy: 0.4295 - val_loss: 1.3671 - val_accuracy: 0.4291\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3093 - accuracy: 0.4529\n",
      "Epoch 11: val_accuracy did not improve from 0.48178\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3109 - accuracy: 0.4529 - val_loss: 1.3371 - val_accuracy: 0.4494\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2991 - accuracy: 0.4583\n",
      "Epoch 12: val_accuracy did not improve from 0.48178\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2964 - accuracy: 0.4596 - val_loss: 1.2968 - val_accuracy: 0.4453\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2593 - accuracy: 0.4746\n",
      "Epoch 13: val_accuracy did not improve from 0.48178\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.2579 - accuracy: 0.4767 - val_loss: 1.2463 - val_accuracy: 0.4696\n",
      "Epoch 14/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2162 - accuracy: 0.4878\n",
      "Epoch 14: val_accuracy improved from 0.48178 to 0.53036, saving model to ./ckpt/10_class_lr0005/val_acc_0.530.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2169 - accuracy: 0.4879 - val_loss: 1.1917 - val_accuracy: 0.5304\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2049 - accuracy: 0.4792\n",
      "Epoch 15: val_accuracy did not improve from 0.53036\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.2090 - accuracy: 0.4785 - val_loss: 1.1933 - val_accuracy: 0.5304\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1791 - accuracy: 0.5023\n",
      "Epoch 16: val_accuracy improved from 0.53036 to 0.56275, saving model to ./ckpt/10_class_lr0005/val_acc_0.563.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1785 - accuracy: 0.5027 - val_loss: 1.1446 - val_accuracy: 0.5628\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1697 - accuracy: 0.5086\n",
      "Epoch 17: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1728 - accuracy: 0.5076 - val_loss: 1.1567 - val_accuracy: 0.5385\n",
      "Epoch 18/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1535 - accuracy: 0.5000\n",
      "Epoch 18: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1545 - accuracy: 0.4991 - val_loss: 1.1649 - val_accuracy: 0.5020\n",
      "Epoch 19/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1193 - accuracy: 0.5275\n",
      "Epoch 19: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1229 - accuracy: 0.5256 - val_loss: 1.1460 - val_accuracy: 0.5304\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1109 - accuracy: 0.5394\n",
      "Epoch 20: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1125 - accuracy: 0.5390 - val_loss: 1.1101 - val_accuracy: 0.5385\n",
      "Epoch 21/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0920 - accuracy: 0.5389\n",
      "Epoch 21: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0933 - accuracy: 0.5377 - val_loss: 1.1052 - val_accuracy: 0.5547\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1150 - accuracy: 0.5204\n",
      "Epoch 22: val_accuracy improved from 0.56275 to 0.56680, saving model to ./ckpt/10_class_lr0005/val_acc_0.567.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1181 - accuracy: 0.5193 - val_loss: 1.1234 - val_accuracy: 0.5668\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0746 - accuracy: 0.5607\n",
      "Epoch 23: val_accuracy did not improve from 0.56680\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0766 - accuracy: 0.5583 - val_loss: 1.0951 - val_accuracy: 0.5587\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0748 - accuracy: 0.5344\n",
      "Epoch 24: val_accuracy did not improve from 0.56680\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0797 - accuracy: 0.5346 - val_loss: 1.0879 - val_accuracy: 0.5466\n",
      "Epoch 25/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0658 - accuracy: 0.5439\n",
      "Epoch 25: val_accuracy did not improve from 0.56680\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0660 - accuracy: 0.5435 - val_loss: 1.0831 - val_accuracy: 0.5668\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0314 - accuracy: 0.5785\n",
      "Epoch 26: val_accuracy did not improve from 0.56680\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0314 - accuracy: 0.5785 - val_loss: 1.0747 - val_accuracy: 0.5628\n",
      "Epoch 27/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0629 - accuracy: 0.5476\n",
      "Epoch 27: val_accuracy improved from 0.56680 to 0.57895, saving model to ./ckpt/10_class_lr0005/val_acc_0.579.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0629 - accuracy: 0.5476 - val_loss: 1.0565 - val_accuracy: 0.5789\n",
      "Epoch 28/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0043 - accuracy: 0.5862\n",
      "Epoch 28: val_accuracy improved from 0.57895 to 0.59109, saving model to ./ckpt/10_class_lr0005/val_acc_0.591.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0043 - accuracy: 0.5862 - val_loss: 1.0791 - val_accuracy: 0.5911\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9995 - accuracy: 0.5906\n",
      "Epoch 29: val_accuracy did not improve from 0.59109\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0003 - accuracy: 0.5898 - val_loss: 1.0595 - val_accuracy: 0.5668\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9905 - accuracy: 0.5738\n",
      "Epoch 30: val_accuracy did not improve from 0.59109\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9899 - accuracy: 0.5750 - val_loss: 1.0882 - val_accuracy: 0.5587\n",
      "Epoch 31/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9932 - accuracy: 0.5833\n",
      "Epoch 31: val_accuracy did not improve from 0.59109\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9933 - accuracy: 0.5835 - val_loss: 1.1641 - val_accuracy: 0.5101\n",
      "Epoch 32/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9849 - accuracy: 0.5906\n",
      "Epoch 32: val_accuracy did not improve from 0.59109\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9870 - accuracy: 0.5902 - val_loss: 1.0789 - val_accuracy: 0.5547\n",
      "Epoch 33/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0077 - accuracy: 0.5820\n",
      "Epoch 33: val_accuracy did not improve from 0.59109\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0083 - accuracy: 0.5812 - val_loss: 1.0912 - val_accuracy: 0.5628\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9608 - accuracy: 0.5934\n",
      "Epoch 34: val_accuracy did not improve from 0.59109\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9608 - accuracy: 0.5934 - val_loss: 1.0860 - val_accuracy: 0.5628\n",
      "Epoch 35/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9556 - accuracy: 0.5838\n",
      "Epoch 35: val_accuracy did not improve from 0.59109\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9568 - accuracy: 0.5826 - val_loss: 1.0626 - val_accuracy: 0.5547\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9578 - accuracy: 0.5920\n",
      "Epoch 36: val_accuracy did not improve from 0.59109\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9578 - accuracy: 0.5920 - val_loss: 1.0179 - val_accuracy: 0.5870\n",
      "Epoch 37/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9404 - accuracy: 0.6123\n",
      "Epoch 37: val_accuracy did not improve from 0.59109\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9420 - accuracy: 0.6113 - val_loss: 1.0393 - val_accuracy: 0.5789\n",
      "Epoch 38/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9255 - accuracy: 0.6234\n",
      "Epoch 38: val_accuracy did not improve from 0.59109\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9255 - accuracy: 0.6234 - val_loss: 1.2075 - val_accuracy: 0.4737\n",
      "Epoch 39/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9468 - accuracy: 0.5938\n",
      "Epoch 39: val_accuracy did not improve from 0.59109\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9518 - accuracy: 0.5925 - val_loss: 1.1900 - val_accuracy: 0.4818\n",
      "Epoch 40/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9360 - accuracy: 0.6087\n",
      "Epoch 40: val_accuracy did not improve from 0.59109\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9372 - accuracy: 0.6100 - val_loss: 1.0578 - val_accuracy: 0.5749\n",
      "Epoch 41/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9603 - accuracy: 0.5983\n",
      "Epoch 41: val_accuracy improved from 0.59109 to 0.59919, saving model to ./ckpt/10_class_lr0005/val_acc_0.599.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9603 - accuracy: 0.5983 - val_loss: 1.0176 - val_accuracy: 0.5992\n",
      "Epoch 42/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9010 - accuracy: 0.6222\n",
      "Epoch 42: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8981 - accuracy: 0.6225 - val_loss: 1.0433 - val_accuracy: 0.5587\n",
      "Epoch 43/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8821 - accuracy: 0.6273\n",
      "Epoch 43: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8854 - accuracy: 0.6248 - val_loss: 1.0500 - val_accuracy: 0.5709\n",
      "Epoch 44/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8882 - accuracy: 0.6324\n",
      "Epoch 44: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8882 - accuracy: 0.6324 - val_loss: 1.0519 - val_accuracy: 0.5466\n",
      "Epoch 45/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9092 - accuracy: 0.6360\n",
      "Epoch 45: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9092 - accuracy: 0.6360 - val_loss: 1.0289 - val_accuracy: 0.5992\n",
      "Epoch 46/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8778 - accuracy: 0.6283\n",
      "Epoch 46: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8772 - accuracy: 0.6297 - val_loss: 1.1913 - val_accuracy: 0.5263\n",
      "Epoch 47/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8773 - accuracy: 0.6288\n",
      "Epoch 47: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8773 - accuracy: 0.6288 - val_loss: 1.0746 - val_accuracy: 0.5506\n",
      "Epoch 48/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8782 - accuracy: 0.6404\n",
      "Epoch 48: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8782 - accuracy: 0.6405 - val_loss: 1.0540 - val_accuracy: 0.5628\n",
      "Epoch 49/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8873 - accuracy: 0.6306\n",
      "Epoch 49: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8873 - accuracy: 0.6306 - val_loss: 1.0688 - val_accuracy: 0.5789\n",
      "Epoch 50/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8646 - accuracy: 0.6432\n",
      "Epoch 50: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8640 - accuracy: 0.6427 - val_loss: 1.0705 - val_accuracy: 0.5709\n",
      "Epoch 51/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.8690 - accuracy: 0.6311\n",
      "Epoch 51: val_accuracy did not improve from 0.59919\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8703 - accuracy: 0.6320 - val_loss: 1.0308 - val_accuracy: 0.5709\n",
      "Epoch 51: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 50s - loss: 2.2933 - accuracy: 0.0625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 22:01:10.269449: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 2.2729 - accuracy: 0.1342\n",
      "Epoch 1: val_accuracy improved from -inf to 0.22672, saving model to ./ckpt/10_class_lr0005/val_acc_0.227.hdf5\n",
      "70/70 [==============================] - 2s 19ms/step - loss: 2.2729 - accuracy: 0.1342 - val_loss: 2.1507 - val_accuracy: 0.2267\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 2.1660 - accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 22:01:11.619941: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 1.9028 - accuracy: 0.2704\n",
      "Epoch 2: val_accuracy improved from 0.22672 to 0.29960, saving model to ./ckpt/10_class_lr0005/val_acc_0.300.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.9035 - accuracy: 0.2684 - val_loss: 1.6613 - val_accuracy: 0.2996\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.6792 - accuracy: 0.3130\n",
      "Epoch 3: val_accuracy improved from 0.29960 to 0.31579, saving model to ./ckpt/10_class_lr0005/val_acc_0.316.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.6803 - accuracy: 0.3119 - val_loss: 1.6034 - val_accuracy: 0.3158\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5798 - accuracy: 0.3596\n",
      "Epoch 4: val_accuracy improved from 0.31579 to 0.38866, saving model to ./ckpt/10_class_lr0005/val_acc_0.389.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5794 - accuracy: 0.3582 - val_loss: 1.5071 - val_accuracy: 0.3887\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5452 - accuracy: 0.3614\n",
      "Epoch 5: val_accuracy did not improve from 0.38866\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5448 - accuracy: 0.3622 - val_loss: 1.4958 - val_accuracy: 0.3806\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.4974 - accuracy: 0.3802\n",
      "Epoch 6: val_accuracy improved from 0.38866 to 0.39676, saving model to ./ckpt/10_class_lr0005/val_acc_0.397.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4974 - accuracy: 0.3802 - val_loss: 1.4883 - val_accuracy: 0.3968\n",
      "Epoch 7/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.4485 - accuracy: 0.4058\n",
      "Epoch 7: val_accuracy improved from 0.39676 to 0.40486, saving model to ./ckpt/10_class_lr0005/val_acc_0.405.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4501 - accuracy: 0.4048 - val_loss: 1.4380 - val_accuracy: 0.4049\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4129 - accuracy: 0.4094\n",
      "Epoch 8: val_accuracy did not improve from 0.40486\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4139 - accuracy: 0.4075 - val_loss: 1.4146 - val_accuracy: 0.3684\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3841 - accuracy: 0.4250\n",
      "Epoch 9: val_accuracy did not improve from 0.40486\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.3841 - accuracy: 0.4250 - val_loss: 1.3965 - val_accuracy: 0.3846\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3439 - accuracy: 0.4271\n",
      "Epoch 10: val_accuracy improved from 0.40486 to 0.40891, saving model to ./ckpt/10_class_lr0005/val_acc_0.409.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3437 - accuracy: 0.4273 - val_loss: 1.3594 - val_accuracy: 0.4089\n",
      "Epoch 11/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.2978 - accuracy: 0.4492\n",
      "Epoch 11: val_accuracy improved from 0.40891 to 0.46559, saving model to ./ckpt/10_class_lr0005/val_acc_0.466.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3006 - accuracy: 0.4475 - val_loss: 1.3131 - val_accuracy: 0.4656\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2801 - accuracy: 0.4533\n",
      "Epoch 12: val_accuracy did not improve from 0.46559\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2801 - accuracy: 0.4533 - val_loss: 1.3250 - val_accuracy: 0.4453\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2239 - accuracy: 0.5009\n",
      "Epoch 13: val_accuracy did not improve from 0.46559\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2278 - accuracy: 0.4991 - val_loss: 1.2764 - val_accuracy: 0.4372\n",
      "Epoch 14/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2356 - accuracy: 0.4746\n",
      "Epoch 14: val_accuracy did not improve from 0.46559\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.2359 - accuracy: 0.4744 - val_loss: 1.3032 - val_accuracy: 0.4453\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2039 - accuracy: 0.4909\n",
      "Epoch 15: val_accuracy improved from 0.46559 to 0.48178, saving model to ./ckpt/10_class_lr0005/val_acc_0.482.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2064 - accuracy: 0.4888 - val_loss: 1.2361 - val_accuracy: 0.4818\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1733 - accuracy: 0.5023\n",
      "Epoch 16: val_accuracy did not improve from 0.48178\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1745 - accuracy: 0.5022 - val_loss: 1.2474 - val_accuracy: 0.4696\n",
      "Epoch 17/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.1437 - accuracy: 0.5234\n",
      "Epoch 17: val_accuracy did not improve from 0.48178\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1441 - accuracy: 0.5247 - val_loss: 1.2502 - val_accuracy: 0.4494\n",
      "Epoch 18/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.1413 - accuracy: 0.5331\n",
      "Epoch 18: val_accuracy improved from 0.48178 to 0.48583, saving model to ./ckpt/10_class_lr0005/val_acc_0.486.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1415 - accuracy: 0.5310 - val_loss: 1.2297 - val_accuracy: 0.4858\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0950 - accuracy: 0.5305\n",
      "Epoch 19: val_accuracy did not improve from 0.48583\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0950 - accuracy: 0.5305 - val_loss: 1.1824 - val_accuracy: 0.4818\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1208 - accuracy: 0.5353\n",
      "Epoch 20: val_accuracy did not improve from 0.48583\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1215 - accuracy: 0.5337 - val_loss: 1.2504 - val_accuracy: 0.4818\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0829 - accuracy: 0.5429\n",
      "Epoch 21: val_accuracy improved from 0.48583 to 0.53441, saving model to ./ckpt/10_class_lr0005/val_acc_0.534.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0860 - accuracy: 0.5382 - val_loss: 1.1688 - val_accuracy: 0.5344\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0963 - accuracy: 0.5258\n",
      "Epoch 22: val_accuracy did not improve from 0.53441\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0972 - accuracy: 0.5242 - val_loss: 1.1813 - val_accuracy: 0.5061\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0547 - accuracy: 0.5494\n",
      "Epoch 23: val_accuracy did not improve from 0.53441\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0552 - accuracy: 0.5480 - val_loss: 1.1801 - val_accuracy: 0.5142\n",
      "Epoch 24/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0780 - accuracy: 0.5310\n",
      "Epoch 24: val_accuracy did not improve from 0.53441\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0780 - accuracy: 0.5310 - val_loss: 1.1370 - val_accuracy: 0.5061\n",
      "Epoch 25/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.5802\n",
      "Epoch 25: val_accuracy did not improve from 0.53441\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0394 - accuracy: 0.5776 - val_loss: 1.1504 - val_accuracy: 0.5020\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0379 - accuracy: 0.5530\n",
      "Epoch 26: val_accuracy did not improve from 0.53441\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0429 - accuracy: 0.5512 - val_loss: 1.1226 - val_accuracy: 0.5263\n",
      "Epoch 27/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0297 - accuracy: 0.5684\n",
      "Epoch 27: val_accuracy did not improve from 0.53441\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0304 - accuracy: 0.5655 - val_loss: 1.1585 - val_accuracy: 0.4453\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0222 - accuracy: 0.5802\n",
      "Epoch 28: val_accuracy did not improve from 0.53441\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0259 - accuracy: 0.5781 - val_loss: 1.1132 - val_accuracy: 0.5263\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0030 - accuracy: 0.5820\n",
      "Epoch 29: val_accuracy did not improve from 0.53441\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0072 - accuracy: 0.5799 - val_loss: 1.1630 - val_accuracy: 0.5061\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9820 - accuracy: 0.5752\n",
      "Epoch 30: val_accuracy did not improve from 0.53441\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9857 - accuracy: 0.5750 - val_loss: 1.1181 - val_accuracy: 0.4980\n",
      "Epoch 31/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9968 - accuracy: 0.5924\n",
      "Epoch 31: val_accuracy did not improve from 0.53441\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0002 - accuracy: 0.5884 - val_loss: 1.1611 - val_accuracy: 0.4939\n",
      "Epoch 32/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9721 - accuracy: 0.5851\n",
      "Epoch 32: val_accuracy did not improve from 0.53441\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9767 - accuracy: 0.5835 - val_loss: 1.0935 - val_accuracy: 0.5263\n",
      "Epoch 33/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9570 - accuracy: 0.6055\n",
      "Epoch 33: val_accuracy did not improve from 0.53441\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9584 - accuracy: 0.6041 - val_loss: 1.1148 - val_accuracy: 0.5101\n",
      "Epoch 34/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9581 - accuracy: 0.6003\n",
      "Epoch 34: val_accuracy improved from 0.53441 to 0.54656, saving model to ./ckpt/10_class_lr0005/val_acc_0.547.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9611 - accuracy: 0.5978 - val_loss: 1.0949 - val_accuracy: 0.5466\n",
      "Epoch 35/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9580 - accuracy: 0.6055\n",
      "Epoch 35: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9609 - accuracy: 0.6041 - val_loss: 1.1174 - val_accuracy: 0.5344\n",
      "Epoch 36/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9418 - accuracy: 0.6014\n",
      "Epoch 36: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9456 - accuracy: 0.5992 - val_loss: 1.1365 - val_accuracy: 0.4777\n",
      "Epoch 37/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9367 - accuracy: 0.6001\n",
      "Epoch 37: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9367 - accuracy: 0.6001 - val_loss: 1.1528 - val_accuracy: 0.4737\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9276 - accuracy: 0.6241\n",
      "Epoch 38: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9277 - accuracy: 0.6243 - val_loss: 1.0740 - val_accuracy: 0.5466\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9316 - accuracy: 0.6087\n",
      "Epoch 39: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9310 - accuracy: 0.6095 - val_loss: 1.1747 - val_accuracy: 0.5223\n",
      "Epoch 40/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9168 - accuracy: 0.6191\n",
      "Epoch 40: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9181 - accuracy: 0.6185 - val_loss: 1.0819 - val_accuracy: 0.5466\n",
      "Epoch 41/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8996 - accuracy: 0.6363\n",
      "Epoch 41: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 0.9021 - accuracy: 0.6355 - val_loss: 1.0954 - val_accuracy: 0.5101\n",
      "Epoch 42/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8878 - accuracy: 0.6388\n",
      "Epoch 42: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8961 - accuracy: 0.6351 - val_loss: 1.0854 - val_accuracy: 0.5223\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9094 - accuracy: 0.6250\n",
      "Epoch 43: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9090 - accuracy: 0.6243 - val_loss: 1.1130 - val_accuracy: 0.4939\n",
      "Epoch 44/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8853 - accuracy: 0.6351\n",
      "Epoch 44: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8853 - accuracy: 0.6351 - val_loss: 1.0964 - val_accuracy: 0.5304\n",
      "Epoch 45/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8871 - accuracy: 0.6225\n",
      "Epoch 45: val_accuracy improved from 0.54656 to 0.59514, saving model to ./ckpt/10_class_lr0005/val_acc_0.595.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8871 - accuracy: 0.6225 - val_loss: 1.0424 - val_accuracy: 0.5951\n",
      "Epoch 46/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8693 - accuracy: 0.6392\n",
      "Epoch 46: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8660 - accuracy: 0.6400 - val_loss: 1.1148 - val_accuracy: 0.4939\n",
      "Epoch 47/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8562 - accuracy: 0.6481\n",
      "Epoch 47: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8570 - accuracy: 0.6481 - val_loss: 1.0673 - val_accuracy: 0.5425\n",
      "Epoch 48/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8568 - accuracy: 0.6507\n",
      "Epoch 48: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8572 - accuracy: 0.6513 - val_loss: 1.1024 - val_accuracy: 0.5547\n",
      "Epoch 49/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8639 - accuracy: 0.6324\n",
      "Epoch 49: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8639 - accuracy: 0.6324 - val_loss: 1.0907 - val_accuracy: 0.5223\n",
      "Epoch 50/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8566 - accuracy: 0.6427\n",
      "Epoch 50: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8566 - accuracy: 0.6427 - val_loss: 1.1493 - val_accuracy: 0.5587\n",
      "Epoch 51/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8422 - accuracy: 0.6647\n",
      "Epoch 51: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8422 - accuracy: 0.6647 - val_loss: 1.0758 - val_accuracy: 0.5789\n",
      "Epoch 52/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8592 - accuracy: 0.6454\n",
      "Epoch 52: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8584 - accuracy: 0.6459 - val_loss: 1.0448 - val_accuracy: 0.5547\n",
      "Epoch 53/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8301 - accuracy: 0.6471\n",
      "Epoch 53: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8328 - accuracy: 0.6463 - val_loss: 1.0528 - val_accuracy: 0.5709\n",
      "Epoch 54/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8244 - accuracy: 0.6710\n",
      "Epoch 54: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8244 - accuracy: 0.6710 - val_loss: 1.0203 - val_accuracy: 0.5870\n",
      "Epoch 55/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8197 - accuracy: 0.6622\n",
      "Epoch 55: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8172 - accuracy: 0.6652 - val_loss: 1.0600 - val_accuracy: 0.5668\n",
      "Epoch 56/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8045 - accuracy: 0.6753\n",
      "Epoch 56: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 0.8060 - accuracy: 0.6746 - val_loss: 1.0615 - val_accuracy: 0.5547\n",
      "Epoch 57/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8047 - accuracy: 0.6647\n",
      "Epoch 57: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8047 - accuracy: 0.6647 - val_loss: 1.0400 - val_accuracy: 0.5628\n",
      "Epoch 58/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8348 - accuracy: 0.6595\n",
      "Epoch 58: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8390 - accuracy: 0.6584 - val_loss: 1.0416 - val_accuracy: 0.5830\n",
      "Epoch 59/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8033 - accuracy: 0.6671\n",
      "Epoch 59: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8023 - accuracy: 0.6679 - val_loss: 1.0503 - val_accuracy: 0.5749\n",
      "Epoch 60/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8094 - accuracy: 0.6667\n",
      "Epoch 60: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8085 - accuracy: 0.6670 - val_loss: 1.0555 - val_accuracy: 0.5425\n",
      "Epoch 61/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.7861 - accuracy: 0.6774\n",
      "Epoch 61: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.7901 - accuracy: 0.6750 - val_loss: 1.0418 - val_accuracy: 0.5709\n",
      "Epoch 62/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7765 - accuracy: 0.6762\n",
      "Epoch 62: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.7774 - accuracy: 0.6759 - val_loss: 1.0346 - val_accuracy: 0.5506\n",
      "Epoch 63/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7722 - accuracy: 0.6908\n",
      "Epoch 63: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.7722 - accuracy: 0.6908 - val_loss: 1.0837 - val_accuracy: 0.5182\n",
      "Epoch 64/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7641 - accuracy: 0.6866\n",
      "Epoch 64: val_accuracy did not improve from 0.59514\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.7659 - accuracy: 0.6854 - val_loss: 1.0828 - val_accuracy: 0.5344\n",
      "Epoch 64: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 22:02:17.499306: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 2.2645 - accuracy: 0.1454\n",
      "Epoch 1: val_accuracy improved from -inf to 0.21457, saving model to ./ckpt/10_class_lr0005/val_acc_0.215.hdf5\n",
      "70/70 [==============================] - 2s 19ms/step - loss: 2.2645 - accuracy: 0.1454 - val_loss: 2.1277 - val_accuracy: 0.2146\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 2.1116 - accuracy: 0.2812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 22:02:18.821359: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 1.8852 - accuracy: 0.2767\n",
      "Epoch 2: val_accuracy improved from 0.21457 to 0.29150, saving model to ./ckpt/10_class_lr0005/val_acc_0.291.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.8835 - accuracy: 0.2760 - val_loss: 1.6917 - val_accuracy: 0.2915\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.6820 - accuracy: 0.3074\n",
      "Epoch 3: val_accuracy did not improve from 0.29150\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.6795 - accuracy: 0.3110 - val_loss: 1.6853 - val_accuracy: 0.2632\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.6053 - accuracy: 0.3405\n",
      "Epoch 4: val_accuracy improved from 0.29150 to 0.31579, saving model to ./ckpt/10_class_lr0005/val_acc_0.316.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.6063 - accuracy: 0.3389 - val_loss: 1.5888 - val_accuracy: 0.3158\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5452 - accuracy: 0.3664\n",
      "Epoch 5: val_accuracy did not improve from 0.31579\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5458 - accuracy: 0.3649 - val_loss: 1.5918 - val_accuracy: 0.2955\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5145 - accuracy: 0.3786\n",
      "Epoch 6: val_accuracy improved from 0.31579 to 0.40486, saving model to ./ckpt/10_class_lr0005/val_acc_0.405.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5147 - accuracy: 0.3788 - val_loss: 1.4546 - val_accuracy: 0.4049\n",
      "Epoch 7/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.4655 - accuracy: 0.3860\n",
      "Epoch 7: val_accuracy did not improve from 0.40486\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4629 - accuracy: 0.3869 - val_loss: 1.5522 - val_accuracy: 0.3279\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.4218 - accuracy: 0.4174\n",
      "Epoch 8: val_accuracy improved from 0.40486 to 0.43320, saving model to ./ckpt/10_class_lr0005/val_acc_0.433.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4218 - accuracy: 0.4174 - val_loss: 1.3947 - val_accuracy: 0.4332\n",
      "Epoch 9/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.3715 - accuracy: 0.4343\n",
      "Epoch 9: val_accuracy improved from 0.43320 to 0.46154, saving model to ./ckpt/10_class_lr0005/val_acc_0.462.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.3743 - accuracy: 0.4313 - val_loss: 1.3655 - val_accuracy: 0.4615\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3642 - accuracy: 0.4363\n",
      "Epoch 10: val_accuracy did not improve from 0.46154\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.3642 - accuracy: 0.4363 - val_loss: 1.3788 - val_accuracy: 0.4089\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3115 - accuracy: 0.4543\n",
      "Epoch 11: val_accuracy did not improve from 0.46154\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3125 - accuracy: 0.4547 - val_loss: 1.3165 - val_accuracy: 0.4170\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2800 - accuracy: 0.4665\n",
      "Epoch 12: val_accuracy did not improve from 0.46154\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2827 - accuracy: 0.4663 - val_loss: 1.3672 - val_accuracy: 0.4251\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2775 - accuracy: 0.4642\n",
      "Epoch 13: val_accuracy did not improve from 0.46154\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2769 - accuracy: 0.4659 - val_loss: 1.3330 - val_accuracy: 0.4211\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2211 - accuracy: 0.4852\n",
      "Epoch 14: val_accuracy did not improve from 0.46154\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2211 - accuracy: 0.4852 - val_loss: 1.2907 - val_accuracy: 0.4494\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2109 - accuracy: 0.4918\n",
      "Epoch 15: val_accuracy improved from 0.46154 to 0.47368, saving model to ./ckpt/10_class_lr0005/val_acc_0.474.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2102 - accuracy: 0.4919 - val_loss: 1.2296 - val_accuracy: 0.4737\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1853 - accuracy: 0.4896\n",
      "Epoch 16: val_accuracy did not improve from 0.47368\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1871 - accuracy: 0.4888 - val_loss: 1.2161 - val_accuracy: 0.4575\n",
      "Epoch 17/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.1790 - accuracy: 0.5101\n",
      "Epoch 17: val_accuracy improved from 0.47368 to 0.50607, saving model to ./ckpt/10_class_lr0005/val_acc_0.506.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.1842 - accuracy: 0.5108 - val_loss: 1.1826 - val_accuracy: 0.5061\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1796 - accuracy: 0.4919\n",
      "Epoch 18: val_accuracy did not improve from 0.50607\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.1796 - accuracy: 0.4919 - val_loss: 1.2329 - val_accuracy: 0.4615\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1229 - accuracy: 0.5319\n",
      "Epoch 19: val_accuracy did not improve from 0.50607\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1229 - accuracy: 0.5319 - val_loss: 1.1562 - val_accuracy: 0.4980\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1346 - accuracy: 0.5292\n",
      "Epoch 20: val_accuracy improved from 0.50607 to 0.51012, saving model to ./ckpt/10_class_lr0005/val_acc_0.510.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1346 - accuracy: 0.5292 - val_loss: 1.1829 - val_accuracy: 0.5101\n",
      "Epoch 21/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0936 - accuracy: 0.5480\n",
      "Epoch 21: val_accuracy did not improve from 0.51012\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0983 - accuracy: 0.5476 - val_loss: 1.1489 - val_accuracy: 0.4980\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0832 - accuracy: 0.5471\n",
      "Epoch 22: val_accuracy did not improve from 0.51012\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0801 - accuracy: 0.5480 - val_loss: 1.1346 - val_accuracy: 0.5101\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0766 - accuracy: 0.5435\n",
      "Epoch 23: val_accuracy did not improve from 0.51012\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0780 - accuracy: 0.5431 - val_loss: 1.1119 - val_accuracy: 0.5061\n",
      "Epoch 24/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0707 - accuracy: 0.5534\n",
      "Epoch 24: val_accuracy did not improve from 0.51012\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0707 - accuracy: 0.5534 - val_loss: 1.1084 - val_accuracy: 0.4980\n",
      "Epoch 25/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0499 - accuracy: 0.5639\n",
      "Epoch 25: val_accuracy did not improve from 0.51012\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0499 - accuracy: 0.5637 - val_loss: 1.1286 - val_accuracy: 0.4899\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0302 - accuracy: 0.5842\n",
      "Epoch 26: val_accuracy improved from 0.51012 to 0.54656, saving model to ./ckpt/10_class_lr0005/val_acc_0.547.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0322 - accuracy: 0.5830 - val_loss: 1.1019 - val_accuracy: 0.5466\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0400 - accuracy: 0.5639\n",
      "Epoch 27: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0401 - accuracy: 0.5628 - val_loss: 1.1009 - val_accuracy: 0.4899\n",
      "Epoch 28/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0117 - accuracy: 0.5826\n",
      "Epoch 28: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0117 - accuracy: 0.5826 - val_loss: 1.0784 - val_accuracy: 0.5425\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0207 - accuracy: 0.5747\n",
      "Epoch 29: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0233 - accuracy: 0.5736 - val_loss: 1.1035 - val_accuracy: 0.4737\n",
      "Epoch 30/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0052 - accuracy: 0.5875\n",
      "Epoch 30: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0052 - accuracy: 0.5875 - val_loss: 1.1068 - val_accuracy: 0.5182\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9951 - accuracy: 0.5844\n",
      "Epoch 31: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9951 - accuracy: 0.5844 - val_loss: 1.0998 - val_accuracy: 0.4737\n",
      "Epoch 32/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0018 - accuracy: 0.5716\n",
      "Epoch 32: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0014 - accuracy: 0.5723 - val_loss: 1.1465 - val_accuracy: 0.4939\n",
      "Epoch 33/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9599 - accuracy: 0.5951\n",
      "Epoch 33: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9615 - accuracy: 0.5943 - val_loss: 1.1131 - val_accuracy: 0.4858\n",
      "Epoch 34/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9573 - accuracy: 0.5992\n",
      "Epoch 34: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9632 - accuracy: 0.5983 - val_loss: 1.1251 - val_accuracy: 0.5020\n",
      "Epoch 35/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9754 - accuracy: 0.5824\n",
      "Epoch 35: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9797 - accuracy: 0.5803 - val_loss: 1.0350 - val_accuracy: 0.5101\n",
      "Epoch 36/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9492 - accuracy: 0.6110\n",
      "Epoch 36: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9500 - accuracy: 0.6095 - val_loss: 1.0563 - val_accuracy: 0.5263\n",
      "Epoch 37/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9462 - accuracy: 0.6171\n",
      "Epoch 37: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9481 - accuracy: 0.6180 - val_loss: 1.0687 - val_accuracy: 0.5344\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9315 - accuracy: 0.6209\n",
      "Epoch 38: val_accuracy did not improve from 0.54656\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9321 - accuracy: 0.6212 - val_loss: 1.0896 - val_accuracy: 0.5182\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9274 - accuracy: 0.6164\n",
      "Epoch 39: val_accuracy improved from 0.54656 to 0.56275, saving model to ./ckpt/10_class_lr0005/val_acc_0.563.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9285 - accuracy: 0.6158 - val_loss: 1.0177 - val_accuracy: 0.5628\n",
      "Epoch 40/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9325 - accuracy: 0.6154\n",
      "Epoch 40: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9325 - accuracy: 0.6154 - val_loss: 1.0643 - val_accuracy: 0.5182\n",
      "Epoch 41/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9349 - accuracy: 0.6150\n",
      "Epoch 41: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9432 - accuracy: 0.6122 - val_loss: 1.0378 - val_accuracy: 0.5547\n",
      "Epoch 42/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.9243 - accuracy: 0.6167\n",
      "Epoch 42: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9281 - accuracy: 0.6167 - val_loss: 1.0695 - val_accuracy: 0.5101\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8970 - accuracy: 0.6386\n",
      "Epoch 43: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8996 - accuracy: 0.6364 - val_loss: 1.0452 - val_accuracy: 0.5263\n",
      "Epoch 44/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8885 - accuracy: 0.6315\n",
      "Epoch 44: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8885 - accuracy: 0.6315 - val_loss: 1.0763 - val_accuracy: 0.5182\n",
      "Epoch 45/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8886 - accuracy: 0.6273\n",
      "Epoch 45: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8895 - accuracy: 0.6266 - val_loss: 1.0405 - val_accuracy: 0.5466\n",
      "Epoch 46/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9018 - accuracy: 0.6355\n",
      "Epoch 46: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9018 - accuracy: 0.6355 - val_loss: 1.0656 - val_accuracy: 0.5506\n",
      "Epoch 47/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8808 - accuracy: 0.6386\n",
      "Epoch 47: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8815 - accuracy: 0.6382 - val_loss: 1.1048 - val_accuracy: 0.5182\n",
      "Epoch 48/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8691 - accuracy: 0.6472\n",
      "Epoch 48: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.8691 - accuracy: 0.6472 - val_loss: 1.1250 - val_accuracy: 0.5101\n",
      "Epoch 49/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8610 - accuracy: 0.6418\n",
      "Epoch 49: val_accuracy did not improve from 0.56275\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.8620 - accuracy: 0.6405 - val_loss: 1.0338 - val_accuracy: 0.5061\n",
      "Epoch 49: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 22:03:09.299016: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 2.2710 - accuracy: 0.1387\n",
      "Epoch 1: val_accuracy improved from -inf to 0.28340, saving model to ./ckpt/10_class_lr0005/val_acc_0.283.hdf5\n",
      "70/70 [==============================] - 2s 19ms/step - loss: 2.2710 - accuracy: 0.1387 - val_loss: 2.1218 - val_accuracy: 0.2834\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 2.1884 - accuracy: 0.0938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 22:03:10.632289: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 1.9022 - accuracy: 0.2715\n",
      "Epoch 2: val_accuracy improved from 0.28340 to 0.36437, saving model to ./ckpt/10_class_lr0005/val_acc_0.364.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.8950 - accuracy: 0.2715 - val_loss: 1.5840 - val_accuracy: 0.3644\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.6851 - accuracy: 0.3188\n",
      "Epoch 3: val_accuracy did not improve from 0.36437\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.6843 - accuracy: 0.3196 - val_loss: 1.5297 - val_accuracy: 0.3482\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.6139 - accuracy: 0.3389\n",
      "Epoch 4: val_accuracy improved from 0.36437 to 0.40081, saving model to ./ckpt/10_class_lr0005/val_acc_0.401.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.6139 - accuracy: 0.3389 - val_loss: 1.4690 - val_accuracy: 0.4008\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.5570 - accuracy: 0.3609\n",
      "Epoch 5: val_accuracy improved from 0.40081 to 0.44534, saving model to ./ckpt/10_class_lr0005/val_acc_0.445.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5570 - accuracy: 0.3609 - val_loss: 1.4394 - val_accuracy: 0.4453\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.5099 - accuracy: 0.3748\n",
      "Epoch 6: val_accuracy did not improve from 0.44534\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.5099 - accuracy: 0.3748 - val_loss: 1.3811 - val_accuracy: 0.4332\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4647 - accuracy: 0.3936\n",
      "Epoch 7: val_accuracy did not improve from 0.44534\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4647 - accuracy: 0.3932 - val_loss: 1.3521 - val_accuracy: 0.4049\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4365 - accuracy: 0.3963\n",
      "Epoch 8: val_accuracy did not improve from 0.44534\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.4359 - accuracy: 0.3954 - val_loss: 1.3314 - val_accuracy: 0.3927\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3848 - accuracy: 0.4183\n",
      "Epoch 9: val_accuracy improved from 0.44534 to 0.48583, saving model to ./ckpt/10_class_lr0005/val_acc_0.486.hdf5\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 1.3848 - accuracy: 0.4183 - val_loss: 1.2553 - val_accuracy: 0.4858\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3348 - accuracy: 0.4438\n",
      "Epoch 10: val_accuracy did not improve from 0.48583\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3360 - accuracy: 0.4439 - val_loss: 1.2541 - val_accuracy: 0.4696\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3001 - accuracy: 0.4574\n",
      "Epoch 11: val_accuracy improved from 0.48583 to 0.49393, saving model to ./ckpt/10_class_lr0005/val_acc_0.494.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.3001 - accuracy: 0.4569 - val_loss: 1.2208 - val_accuracy: 0.4939\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2700 - accuracy: 0.4636\n",
      "Epoch 12: val_accuracy did not improve from 0.49393\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2700 - accuracy: 0.4636 - val_loss: 1.1797 - val_accuracy: 0.4818\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2589 - accuracy: 0.4650\n",
      "Epoch 13: val_accuracy improved from 0.49393 to 0.50607, saving model to ./ckpt/10_class_lr0005/val_acc_0.506.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2589 - accuracy: 0.4650 - val_loss: 1.2150 - val_accuracy: 0.5061\n",
      "Epoch 14/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2317 - accuracy: 0.4774\n",
      "Epoch 14: val_accuracy improved from 0.50607 to 0.52632, saving model to ./ckpt/10_class_lr0005/val_acc_0.526.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2344 - accuracy: 0.4767 - val_loss: 1.1563 - val_accuracy: 0.5263\n",
      "Epoch 15/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.2215 - accuracy: 0.4823\n",
      "Epoch 15: val_accuracy did not improve from 0.52632\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.2227 - accuracy: 0.4838 - val_loss: 1.1704 - val_accuracy: 0.4777\n",
      "Epoch 16/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.1943 - accuracy: 0.4835\n",
      "Epoch 16: val_accuracy improved from 0.52632 to 0.55466, saving model to ./ckpt/10_class_lr0005/val_acc_0.555.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1935 - accuracy: 0.4838 - val_loss: 1.0997 - val_accuracy: 0.5547\n",
      "Epoch 17/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.1610 - accuracy: 0.5152\n",
      "Epoch 17: val_accuracy did not improve from 0.55466\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 1.1577 - accuracy: 0.5157 - val_loss: 1.1125 - val_accuracy: 0.5101\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1506 - accuracy: 0.5135\n",
      "Epoch 18: val_accuracy did not improve from 0.55466\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1506 - accuracy: 0.5135 - val_loss: 1.1202 - val_accuracy: 0.5142\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1092 - accuracy: 0.5296\n",
      "Epoch 19: val_accuracy did not improve from 0.55466\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1092 - accuracy: 0.5296 - val_loss: 1.1426 - val_accuracy: 0.5061\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1392 - accuracy: 0.5204\n",
      "Epoch 20: val_accuracy did not improve from 0.55466\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1388 - accuracy: 0.5206 - val_loss: 1.1074 - val_accuracy: 0.5385\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1001 - accuracy: 0.5350\n",
      "Epoch 21: val_accuracy did not improve from 0.55466\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.1001 - accuracy: 0.5350 - val_loss: 1.1424 - val_accuracy: 0.4777\n",
      "Epoch 22/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0761 - accuracy: 0.5550\n",
      "Epoch 22: val_accuracy improved from 0.55466 to 0.56680, saving model to ./ckpt/10_class_lr0005/val_acc_0.567.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0776 - accuracy: 0.5548 - val_loss: 1.0749 - val_accuracy: 0.5668\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0618 - accuracy: 0.5619\n",
      "Epoch 23: val_accuracy did not improve from 0.56680\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0618 - accuracy: 0.5619 - val_loss: 1.0961 - val_accuracy: 0.5304\n",
      "Epoch 24/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0567 - accuracy: 0.5579\n",
      "Epoch 24: val_accuracy did not improve from 0.56680\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0567 - accuracy: 0.5579 - val_loss: 1.1168 - val_accuracy: 0.5182\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0517 - accuracy: 0.5660\n",
      "Epoch 25: val_accuracy did not improve from 0.56680\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0517 - accuracy: 0.5660 - val_loss: 1.0797 - val_accuracy: 0.5547\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0495 - accuracy: 0.5453\n",
      "Epoch 26: val_accuracy improved from 0.56680 to 0.57895, saving model to ./ckpt/10_class_lr0005/val_acc_0.579.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0495 - accuracy: 0.5453 - val_loss: 1.0645 - val_accuracy: 0.5789\n",
      "Epoch 27/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0292 - accuracy: 0.5682\n",
      "Epoch 27: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0292 - accuracy: 0.5682 - val_loss: 1.0939 - val_accuracy: 0.5709\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0092 - accuracy: 0.5707\n",
      "Epoch 28: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0086 - accuracy: 0.5709 - val_loss: 1.0717 - val_accuracy: 0.5344\n",
      "Epoch 29/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0074 - accuracy: 0.5788\n",
      "Epoch 29: val_accuracy did not improve from 0.57895\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 1.0085 - accuracy: 0.5772 - val_loss: 1.1094 - val_accuracy: 0.5263\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0058 - accuracy: 0.5856\n",
      "Epoch 30: val_accuracy improved from 0.57895 to 0.58704, saving model to ./ckpt/10_class_lr0005/val_acc_0.587.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0097 - accuracy: 0.5839 - val_loss: 1.0383 - val_accuracy: 0.5870\n",
      "Epoch 31/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9825 - accuracy: 0.5984\n",
      "Epoch 31: val_accuracy did not improve from 0.58704\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9806 - accuracy: 0.5978 - val_loss: 1.0791 - val_accuracy: 0.5830\n",
      "Epoch 32/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9848 - accuracy: 0.5815\n",
      "Epoch 32: val_accuracy did not improve from 0.58704\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9857 - accuracy: 0.5817 - val_loss: 1.0570 - val_accuracy: 0.5547\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9796 - accuracy: 0.5978\n",
      "Epoch 33: val_accuracy did not improve from 0.58704\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9796 - accuracy: 0.5978 - val_loss: 1.0756 - val_accuracy: 0.5385\n",
      "Epoch 34/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9572 - accuracy: 0.5984\n",
      "Epoch 34: val_accuracy did not improve from 0.58704\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9634 - accuracy: 0.5987 - val_loss: 1.0494 - val_accuracy: 0.5789\n",
      "Epoch 35/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9601 - accuracy: 0.5842\n",
      "Epoch 35: val_accuracy did not improve from 0.58704\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9606 - accuracy: 0.5853 - val_loss: 1.0579 - val_accuracy: 0.5466\n",
      "Epoch 36/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9538 - accuracy: 0.5956\n",
      "Epoch 36: val_accuracy did not improve from 0.58704\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9562 - accuracy: 0.5934 - val_loss: 1.0657 - val_accuracy: 0.5547\n",
      "Epoch 37/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9696 - accuracy: 0.5842\n",
      "Epoch 37: val_accuracy did not improve from 0.58704\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9702 - accuracy: 0.5835 - val_loss: 1.0776 - val_accuracy: 0.5142\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9219 - accuracy: 0.6205\n",
      "Epoch 38: val_accuracy did not improve from 0.58704\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9274 - accuracy: 0.6185 - val_loss: 1.1013 - val_accuracy: 0.5709\n",
      "Epoch 39/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9235 - accuracy: 0.6222\n",
      "Epoch 39: val_accuracy did not improve from 0.58704\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.9253 - accuracy: 0.6207 - val_loss: 1.0867 - val_accuracy: 0.5628\n",
      "Epoch 40/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9515 - accuracy: 0.5951\n",
      "Epoch 40: val_accuracy did not improve from 0.58704\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.9529 - accuracy: 0.5943 - val_loss: 1.0632 - val_accuracy: 0.5628\n",
      "Epoch 40: early stopping\n",
      "Finish 10-fold cross validation\n",
      "Best performing model has 0.6235 validation accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# modify to save ckpt for each test\n",
    "ckpt_path = \"./ckpt/10_class_lr0005/val_acc_{val_accuracy:.3f}.hdf5\"\n",
    "\n",
    "# training params\n",
    "epochs = 120\n",
    "num_classes = 10\n",
    "lr = 0.0005\n",
    "\n",
    "# the k for k fold CV\n",
    "n_split = 10\n",
    "\n",
    "# for recording best performance\n",
    "max_acc = 0\n",
    "best_history = None\n",
    "\n",
    "'''\n",
    "k-fold cross validation\n",
    "Save the best model using validation accuracy as metric\n",
    "Print the global best performace when finished\n",
    "'''\n",
    "for train_index,test_index in KFold(n_split).split(train_examples):\n",
    "\n",
    "    x_train, x_vad = train_examples[train_index], train_examples[test_index]\n",
    "    y_train, y_vad = labels[train_index], labels[test_index]\n",
    "\n",
    "    model=create_model(num_classes, lr)\n",
    "  \n",
    "    # callbacks\n",
    "    checkpoint_filepath = ckpt_path\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "        mode='auto',\n",
    "    )\n",
    "\n",
    "    # Model weights are saved at the end of every epoch, if it's the best seen so far.\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_vad, y_vad),\n",
    "                        callbacks=[model_checkpoint_callback, early_stopping_callback])\n",
    "\n",
    "    val_acc = max(history.history['val_accuracy'])\n",
    "    \n",
    "    if val_acc > max_acc:\n",
    "        max_acc = val_acc\n",
    "        best_history = history\n",
    "        # print('Best acc so far. Saving params...\\n')\n",
    "\n",
    "print('Finish {}-fold cross validation'.format(n_split))\n",
    "print('Best performing model has {:.4f} validation accuracy'.format(max_acc))\n",
    "\n",
    "#CPU\n",
    "# with tf.device('/CPU:0'):\n",
    "#     history = model.fit(trainX, trainY, epochs=epochs,validation_data=(testX, testY), batch_size=64)\n",
    "\n",
    "# deafult go with GPU\n",
    "# history = model.fit(trainX, trainY, epochs=epochs,validation_data=(testX, testY), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "history = best_history\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# because of early stopping, can't just use \"epochs\"\n",
    "epochs_range = range(len(history.history['loss']))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/14 [========================>.....] - ETA: 0s - loss: 1.0669 - accuracy: 0.5677"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 22:12:53.128250: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 12ms/step - loss: 1.0712 - accuracy: 0.5596\n",
      "\n",
      "evaluate on test set:\n",
      "loss = 1.07115\tacc = 55.963%\n"
     ]
    }
   ],
   "source": [
    "model = create_model(num_classes=num_classes)\n",
    "model.load_weights(\"./ckpt/10_class_lr0005/val_acc_0.599.hdf5\")\n",
    "\n",
    "loss, acc = model.evaluate(test_examples, test_labels)\n",
    "\n",
    "print('\\nevaluate on test set:\\nloss = {:.5f}\\tacc = {:.3f}%'.format(loss, acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29cc21816e506614f017a9125cfdb1f5dc655865e499c52ef5f5406a40d25695"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
