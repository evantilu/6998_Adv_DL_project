{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold CV with 3-layer CNN avg pooling on real image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and env settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.min_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables/parameters used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"./ckpt/5_class_lr005/\"\n",
    "os.makedirs(ckpt_path, exist_ok=True)\n",
    "\n",
    "lr = 0.005\n",
    "epochs = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/images/label.csv')\n",
    "df.head()\n",
    "\n",
    "labels = df['SalePrice'].to_numpy()\n",
    "labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 18, 18, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_arr = []\n",
    "\n",
    "for i in range(1, len(labels)+1):\n",
    "# for i in range(1, 10):\n",
    "    img = plt.imread('../../data/images/img_{}.png'.format(i))\n",
    "    images_arr.append(img)\n",
    "\n",
    "images_arr = np.array(images_arr, dtype='float32')\n",
    "images_arr.shape\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train test splitting\n",
    "- hold out 15% for testing\n",
    "- use 85% to train model with K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_samples = images_arr.shape[0] \n",
    "test_ratio = 0.15\n",
    "test_samples = int(test_ratio * images_arr.shape[0])\n",
    "\n",
    "train_examples = images_arr[:-1*test_samples]\n",
    "test_examples = images_arr[-1*test_samples:]\n",
    "train_labels = labels[:-1*test_samples]\n",
    "test_labels = labels[-1*test_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (2475, 18, 18, 3)\n",
      "test:  (436, 18, 18, 3)\n",
      "train label:  (2475,)\n",
      "test label:  (436,)\n"
     ]
    }
   ],
   "source": [
    "print('train: ', train_examples.shape)\n",
    "print('test: ', test_examples.shape)\n",
    "print('train label: ', train_labels.shape)\n",
    "print('test label: ', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def create_reg_model(lr=0.005):\n",
    "\n",
    "\t# Working\n",
    "\twith tf.device('/cpu:0'):\n",
    "\t\tdata_augmentation = tf.keras.Sequential([ \n",
    "\t\t\ttf.keras.layers.RandomFlip(\"horizontal\", input_shape=(18, 18, 3)),\n",
    "\t  \t\ttf.keras.layers.RandomRotation(0.1),\n",
    "\t\t    tf.keras.layers.RandomZoom(0.1)\n",
    "\t\t\t])\n",
    "\n",
    "\n",
    "\tmodel = tf.keras.Sequential([\n",
    "\t\t# data_augmentation,\n",
    "\t  \t# tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(18, 18,3)),\n",
    "\t\ttf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.MaxPooling2D((2,2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t  \ttf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.MaxPooling2D(),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Flatten(),\n",
    "\t\ttf.keras.layers.Dense(128, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(64, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(32, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "\t])\n",
    "\n",
    "\t# opt = tf.keras.optimizers.SGD(lr=0.005, momentum=0.9)\n",
    "\topt = tf.keras.optimizers.Adam(lr=lr)\n",
    "\tmodel.compile(optimizer=opt, loss=rmse, metrics=[rmse])\n",
    "\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_reg_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 20:50:03.557976: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - ETA: 0s - loss: 95512.2734 - rmse: 95671.0078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 20:50:04.762280: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 2s 16ms/step - loss: 95512.2734 - rmse: 95671.0078 - val_loss: 46192.8984 - val_rmse: 46322.1328\n",
      "Epoch 2/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 48993.8359 - rmse: 48968.6094 - val_loss: 40018.5352 - val_rmse: 39877.3320\n",
      "Epoch 3/120\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 44634.4922 - rmse: 44568.5156 - val_loss: 36631.5977 - val_rmse: 36475.7930\n",
      "Epoch 4/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 41324.1562 - rmse: 41232.6250 - val_loss: 35772.6680 - val_rmse: 35660.4805\n",
      "Epoch 5/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 40260.8516 - rmse: 40181.1016 - val_loss: 33804.9883 - val_rmse: 33669.9688\n",
      "Epoch 6/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 36824.9883 - rmse: 36819.2109 - val_loss: 31658.4102 - val_rmse: 31540.8496\n",
      "Epoch 7/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 35794.9961 - rmse: 35824.4766 - val_loss: 39457.8867 - val_rmse: 39511.7734\n",
      "Epoch 8/120\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 36456.4180 - rmse: 36491.4258 - val_loss: 31587.5918 - val_rmse: 31604.9160\n",
      "Epoch 9/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 33954.4102 - rmse: 33964.7031 - val_loss: 35294.1758 - val_rmse: 35142.1875\n",
      "Epoch 10/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 31418.3574 - rmse: 31508.3105 - val_loss: 26102.0703 - val_rmse: 26153.9121\n",
      "Epoch 11/120\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 29113.6914 - rmse: 28985.0156 - val_loss: 23952.8477 - val_rmse: 24025.8066\n",
      "Epoch 12/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 28453.0723 - rmse: 28533.9297 - val_loss: 22425.4453 - val_rmse: 22464.0918\n",
      "Epoch 13/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 28661.2656 - rmse: 28633.8555 - val_loss: 25091.2910 - val_rmse: 25185.2734\n",
      "Epoch 14/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 29438.0000 - rmse: 29775.2754 - val_loss: 38126.5469 - val_rmse: 38047.5547\n",
      "Epoch 15/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 27893.2754 - rmse: 27814.7285 - val_loss: 23553.4668 - val_rmse: 23569.2461\n",
      "Epoch 16/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 24095.7871 - rmse: 24241.9219 - val_loss: 20403.5020 - val_rmse: 20448.2441\n",
      "Epoch 17/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 26291.8926 - rmse: 26284.8887 - val_loss: 20325.1621 - val_rmse: 20399.5684\n",
      "Epoch 18/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 25577.7168 - rmse: 25570.6328 - val_loss: 19151.4688 - val_rmse: 19257.6230\n",
      "Epoch 19/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 25278.9375 - rmse: 25226.4434 - val_loss: 20786.2344 - val_rmse: 20796.8262\n",
      "Epoch 20/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 23119.7109 - rmse: 23042.9141 - val_loss: 16017.9590 - val_rmse: 16085.1426\n",
      "Epoch 21/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 24416.8535 - rmse: 24398.5137 - val_loss: 24754.3105 - val_rmse: 24722.6211\n",
      "Epoch 22/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 27443.7949 - rmse: 27487.1152 - val_loss: 19378.5039 - val_rmse: 19447.2012\n",
      "Epoch 23/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 21680.0059 - rmse: 21762.1367 - val_loss: 21152.7656 - val_rmse: 21127.4453\n",
      "Epoch 24/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 23651.9922 - rmse: 23591.8164 - val_loss: 15660.5400 - val_rmse: 15703.6895\n",
      "Epoch 25/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 22001.6602 - rmse: 22035.4805 - val_loss: 15746.9199 - val_rmse: 15814.4805\n",
      "Epoch 26/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 20638.6875 - rmse: 20683.6035 - val_loss: 15151.1133 - val_rmse: 15192.2588\n",
      "Epoch 27/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 23763.2090 - rmse: 23692.4688 - val_loss: 15845.9795 - val_rmse: 15915.4932\n",
      "Epoch 28/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 22249.9668 - rmse: 22322.1523 - val_loss: 16032.6719 - val_rmse: 16033.6533\n",
      "Epoch 29/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 21479.7598 - rmse: 21380.6074 - val_loss: 18941.9707 - val_rmse: 18945.9316\n",
      "Epoch 30/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 21876.9922 - rmse: 21893.3086 - val_loss: 26987.1465 - val_rmse: 27057.7363\n",
      "Epoch 31/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 21493.8730 - rmse: 21435.2832 - val_loss: 14860.7686 - val_rmse: 14916.5703\n",
      "Epoch 32/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 20365.9102 - rmse: 20387.4336 - val_loss: 18645.1777 - val_rmse: 18653.7715\n",
      "Epoch 33/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 21381.6758 - rmse: 21403.4434 - val_loss: 20716.9238 - val_rmse: 20641.0820\n",
      "Epoch 34/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 22205.9199 - rmse: 22209.9492 - val_loss: 14165.0137 - val_rmse: 14179.2979\n",
      "Epoch 35/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 21670.5840 - rmse: 21619.2910 - val_loss: 18145.0176 - val_rmse: 18115.3906\n",
      "Epoch 36/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 20744.3184 - rmse: 20677.6211 - val_loss: 13706.9336 - val_rmse: 13729.6826\n",
      "Epoch 37/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 21772.5742 - rmse: 21762.4551 - val_loss: 14946.7451 - val_rmse: 14961.9736\n",
      "Epoch 38/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 19859.1543 - rmse: 19831.4512 - val_loss: 20584.1543 - val_rmse: 20523.2090\n",
      "Epoch 39/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 19717.6191 - rmse: 19739.7207 - val_loss: 16679.6250 - val_rmse: 16584.1465\n",
      "Epoch 40/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 19528.4902 - rmse: 19548.8398 - val_loss: 13858.1592 - val_rmse: 13825.5947\n",
      "Epoch 41/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 21933.8242 - rmse: 21892.5703 - val_loss: 14922.7393 - val_rmse: 14979.2354\n",
      "Epoch 42/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 21234.7188 - rmse: 21196.0059 - val_loss: 14194.9893 - val_rmse: 14225.3760\n",
      "Epoch 43/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 19347.4258 - rmse: 19377.3086 - val_loss: 18241.2051 - val_rmse: 18324.5547\n",
      "Epoch 44/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 21079.3398 - rmse: 21059.6309 - val_loss: 14958.9141 - val_rmse: 15012.6895\n",
      "Epoch 45/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 20960.1758 - rmse: 20942.0215 - val_loss: 14197.2178 - val_rmse: 14247.1650\n",
      "Epoch 46/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 22526.3477 - rmse: 22456.1133 - val_loss: 14235.0332 - val_rmse: 14255.0020\n",
      "Epoch 47/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 20290.2129 - rmse: 20298.9570 - val_loss: 13904.5479 - val_rmse: 13931.1729\n",
      "Epoch 48/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 19977.7422 - rmse: 20009.9258 - val_loss: 24204.5977 - val_rmse: 24289.4961\n",
      "Epoch 49/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 21921.2207 - rmse: 21847.2891 - val_loss: 16958.4785 - val_rmse: 16895.2031\n",
      "Epoch 50/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 19027.3477 - rmse: 19094.8477 - val_loss: 13885.5430 - val_rmse: 13936.1416\n",
      "Epoch 51/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 19545.3477 - rmse: 19520.4922 - val_loss: 14580.0039 - val_rmse: 14638.4150\n",
      "Epoch 52/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 18623.1758 - rmse: 18632.8555 - val_loss: 17689.4043 - val_rmse: 17575.6250\n",
      "Epoch 53/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 19039.8457 - rmse: 18978.9258 - val_loss: 13951.2607 - val_rmse: 13999.4873\n",
      "Epoch 54/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 19559.1191 - rmse: 19595.5977 - val_loss: 13694.9023 - val_rmse: 13713.6523\n",
      "Epoch 55/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 19518.2754 - rmse: 19498.6133 - val_loss: 15383.1807 - val_rmse: 15469.2949\n",
      "Epoch 56/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 19636.8496 - rmse: 19659.2754 - val_loss: 13918.1777 - val_rmse: 13933.8682\n",
      "Epoch 57/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 19880.5547 - rmse: 19811.6953 - val_loss: 13592.4336 - val_rmse: 13611.1230\n",
      "Epoch 58/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 18496.4102 - rmse: 18513.0840 - val_loss: 23612.0273 - val_rmse: 23459.8047\n",
      "Epoch 59/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 18661.3008 - rmse: 18701.8008 - val_loss: 13906.5938 - val_rmse: 13865.4736\n",
      "Epoch 60/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 18338.7656 - rmse: 18327.4336 - val_loss: 13797.5781 - val_rmse: 13771.6836\n",
      "Epoch 61/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 19875.8730 - rmse: 19867.3457 - val_loss: 17986.2539 - val_rmse: 17925.7949\n",
      "Epoch 62/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 19659.8398 - rmse: 19626.4941 - val_loss: 18450.8242 - val_rmse: 18378.5195\n",
      "Epoch 63/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 17919.1250 - rmse: 18215.0742 - val_loss: 14947.7988 - val_rmse: 14938.9043\n",
      "Epoch 64/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 18879.7852 - rmse: 18877.1035 - val_loss: 15979.7275 - val_rmse: 15887.2109\n",
      "Epoch 65/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 17383.4824 - rmse: 17327.4727 - val_loss: 13063.0156 - val_rmse: 13090.5596\n",
      "Epoch 66/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 18660.5176 - rmse: 18605.4629 - val_loss: 25235.4707 - val_rmse: 25114.4043\n",
      "Epoch 67/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 19605.8145 - rmse: 19627.0098 - val_loss: 14124.4414 - val_rmse: 14078.7275\n",
      "Epoch 68/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 18700.0703 - rmse: 18723.2305 - val_loss: 13810.1523 - val_rmse: 13812.6387\n",
      "Epoch 69/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 18404.3359 - rmse: 18377.4531 - val_loss: 22239.2285 - val_rmse: 22104.4258\n",
      "Epoch 70/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 18152.1387 - rmse: 18188.7617 - val_loss: 23389.2520 - val_rmse: 23237.0879\n",
      "Epoch 71/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 17775.7637 - rmse: 17948.2871 - val_loss: 17415.8242 - val_rmse: 17342.2539\n",
      "Epoch 72/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 17735.5293 - rmse: 17684.0117 - val_loss: 13538.9316 - val_rmse: 13576.4883\n",
      "Epoch 73/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 18121.9219 - rmse: 18077.6719 - val_loss: 17702.3125 - val_rmse: 17627.4473\n",
      "Epoch 74/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 17652.1680 - rmse: 17592.1582 - val_loss: 16056.3154 - val_rmse: 15970.3516\n",
      "Epoch 75/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 17617.6680 - rmse: 17580.0117 - val_loss: 20180.7539 - val_rmse: 20081.2871\n",
      "Epoch 76/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 17394.2422 - rmse: 17351.7168 - val_loss: 13087.9570 - val_rmse: 13107.4346\n",
      "Epoch 77/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 20028.9316 - rmse: 20028.3594 - val_loss: 14198.2100 - val_rmse: 14151.5186\n",
      "Epoch 78/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 17229.8613 - rmse: 17236.7734 - val_loss: 13999.0596 - val_rmse: 14044.6582\n",
      "Epoch 79/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 17257.1387 - rmse: 17275.7031 - val_loss: 20921.1152 - val_rmse: 20810.3555\n",
      "Epoch 80/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 18039.2109 - rmse: 18053.7949 - val_loss: 25681.1992 - val_rmse: 25527.9746\n",
      "Epoch 81/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 17921.4883 - rmse: 18024.6758 - val_loss: 24268.7598 - val_rmse: 24167.3184\n",
      "Epoch 82/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 17812.4805 - rmse: 17778.1875 - val_loss: 14021.0586 - val_rmse: 14087.1738\n",
      "Epoch 83/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 16874.9316 - rmse: 16940.3926 - val_loss: 12628.0537 - val_rmse: 12659.4150\n",
      "Epoch 84/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 17759.1230 - rmse: 17684.2617 - val_loss: 21469.3652 - val_rmse: 21324.2148\n",
      "Epoch 85/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 17816.3574 - rmse: 17802.1758 - val_loss: 16038.1973 - val_rmse: 15992.8047\n",
      "Epoch 86/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 17881.3125 - rmse: 17987.1133 - val_loss: 13135.8291 - val_rmse: 13150.0674\n",
      "Epoch 87/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 19426.8262 - rmse: 19423.3730 - val_loss: 24357.1211 - val_rmse: 24218.6543\n",
      "Epoch 88/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 17311.5117 - rmse: 17224.4980 - val_loss: 15621.3701 - val_rmse: 15543.6777\n",
      "Epoch 89/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 16390.3770 - rmse: 16558.4473 - val_loss: 14504.7246 - val_rmse: 14595.2275\n",
      "Epoch 90/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 17580.0312 - rmse: 17532.3887 - val_loss: 12831.5098 - val_rmse: 12841.0137\n",
      "Epoch 91/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 16995.8789 - rmse: 17125.3516 - val_loss: 15359.4678 - val_rmse: 15314.8262\n",
      "Epoch 92/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 17590.4160 - rmse: 17662.3457 - val_loss: 18027.1211 - val_rmse: 17954.5195\n",
      "Epoch 93/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 20665.0723 - rmse: 20600.7461 - val_loss: 20374.1094 - val_rmse: 20442.2930\n",
      "Epoch 94/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 19574.1074 - rmse: 19597.4648 - val_loss: 31260.7441 - val_rmse: 31133.6445\n",
      "Epoch 95/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 17132.6445 - rmse: 17158.3984 - val_loss: 13564.7324 - val_rmse: 13545.8086\n",
      "Epoch 96/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 18145.2832 - rmse: 18195.0781 - val_loss: 13174.7021 - val_rmse: 13212.0967\n",
      "Epoch 97/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 17664.4492 - rmse: 17664.3945 - val_loss: 21129.8828 - val_rmse: 21030.0781\n",
      "Epoch 98/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 16864.7812 - rmse: 16991.9043 - val_loss: 23998.3242 - val_rmse: 23849.3008\n",
      "Epoch 99/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 17724.1016 - rmse: 17704.2598 - val_loss: 12947.5918 - val_rmse: 12974.8428\n",
      "Epoch 100/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 17041.4395 - rmse: 17056.6465 - val_loss: 13106.2168 - val_rmse: 13119.2334\n",
      "Epoch 101/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 17865.4316 - rmse: 17831.7578 - val_loss: 13233.0938 - val_rmse: 13209.4258\n",
      "Epoch 102/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 15710.8018 - rmse: 15703.5693 - val_loss: 15138.8252 - val_rmse: 15087.4512\n",
      "Epoch 103/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 16153.0869 - rmse: 16117.2803 - val_loss: 15601.6514 - val_rmse: 15648.3330\n",
      "Epoch 104/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 16407.9238 - rmse: 16449.6582 - val_loss: 14138.2705 - val_rmse: 14097.2090\n",
      "Epoch 105/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 16355.1875 - rmse: 16474.3145 - val_loss: 13461.6455 - val_rmse: 13510.1201\n",
      "Epoch 106/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 17643.8418 - rmse: 17638.9512 - val_loss: 18723.5645 - val_rmse: 18651.0078\n",
      "Epoch 107/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 15940.9668 - rmse: 15922.1875 - val_loss: 12931.6904 - val_rmse: 12930.5068\n",
      "Epoch 108/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 17265.3633 - rmse: 17260.7910 - val_loss: 16841.8711 - val_rmse: 16820.7969\n",
      "Epoch 109/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 16016.3418 - rmse: 16108.6475 - val_loss: 17276.1816 - val_rmse: 17181.0430\n",
      "Epoch 110/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 15739.7354 - rmse: 15801.7266 - val_loss: 19900.2344 - val_rmse: 19771.7695\n",
      "Epoch 111/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 16091.6914 - rmse: 16073.9053 - val_loss: 20402.5234 - val_rmse: 20312.5352\n",
      "Epoch 112/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 17247.0234 - rmse: 17248.2383 - val_loss: 28854.2852 - val_rmse: 28689.8242\n",
      "Epoch 113/120\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 16678.0352 - rmse: 16733.4668 - val_loss: 13908.6475 - val_rmse: 13876.1553\n",
      "Epoch 114/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 15292.7637 - rmse: 15271.1279 - val_loss: 14338.4707 - val_rmse: 14300.4336\n",
      "Epoch 115/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 15306.3984 - rmse: 15327.4453 - val_loss: 12442.3291 - val_rmse: 12494.1982\n",
      "Epoch 116/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 15254.0996 - rmse: 15263.7520 - val_loss: 12677.2891 - val_rmse: 12729.2998\n",
      "Epoch 117/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 15301.6211 - rmse: 15300.8965 - val_loss: 13088.8008 - val_rmse: 13106.9014\n",
      "Epoch 118/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 16254.4961 - rmse: 16302.8125 - val_loss: 15884.5332 - val_rmse: 15823.7090\n",
      "Epoch 119/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 16350.4717 - rmse: 16344.2266 - val_loss: 17103.6348 - val_rmse: 17021.9961\n",
      "Epoch 120/120\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 15936.5859 - rmse: 15968.8691 - val_loss: 18196.9355 - val_rmse: 18055.7617\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_examples, train_labels, epochs=epochs, validation_data=(test_examples, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 20:59:25.822738: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/evantilu/miniforge3/envs/6998_DL_tf/lib/python3.8/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15d80ca90>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAHiCAYAAAAH/hLvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABZgklEQVR4nO2deZxcVZn3v091VVfv3Ul3Op2kQxbIwhKyEAKyI6iICMLACCMIouI2LjjjguM7MuMwqC+jDjOioiiIvAQEUUBQJIDsSzYIIQnZk87aSXrfq/u8f5xzq25VV1V3J53uzu3n+/nU51aduvfWubfu+d3nec5zzhVjDIqijE5Cw10BRVGGDxUARRnFqAAoyihGBUBRRjEqAIoyilEBUJRRjApACiLypIhcO9jrDiciskVEzj8M+31ORD7l3n9MRJ7qz7oH8TtHiUiziOQcbF2V9ARCANzF4b16RKTN9/ljA9mXMeaDxph7BnvdkYiI3CQiz6cprxCRThE5ob/7MsbcZ4x5/yDVK0mwjDHbjDFFxpjuwdh/mt8TEdkkIu8cjv2PZAIhAO7iKDLGFAHbgA/7yu7z1hOR8PDVckRyL3CaiExLKb8SWGWMeXsY6jQcnAVUAtNF5OSh/OHhviYDIQCZEJFzRKRGRL4hIruBX4vIGBF5XERqRaTOva/2beM3a68TkRdF5Da37mYR+eBBrjtNRJ4XkSYReVpEfiIiv81Q7/7U8bsi8pLb31MiUuH7/hoR2Soi+0XkXzKdH2NMDfAMcE3KVx8H7umrHil1vk5EXvR9fp+IrBWRBhH5X0B83x0tIs+4+u0TkftEpMx9dy9wFPCYs+C+LiJTRcR4jUVEJorIoyJyQEQ2iMinffu+WUQeFJHfuHOzWkQWZjoHjmuBPwJPuPf+4zpeRP7qfmuPiHzLleeIyLdEZKP7nWUiMjm1rm7d1OvkJRH5kYgcAG7Odj7cNpNF5Pfuf9gvIv8rIlFXpzm+9SrFWr/j+jjeOIEWAEcVMBaYAtyAPeZfu89HAW3A/2bZ/hRgHVAB/AC4S0TkINb9f8DrQDlwM70bnZ/+1PEfgE9g71y5wD8DiMhxwE/d/ie630vbaB33+OsiIrOAecD9/axHL5wYPQx8G3suNgKn+1cBbnX1OxaYjD0nGGOuIdmK+0Gan7gfqHHbXw78p4ic5/v+YmAxUAY8mq3OIlLg9nGfe10pIrnuu2LgaeDP7reOAZa4Tb8KXAVcCJQA1wOt2c6Lj1OATdj/7haynA+xcY/Hga3AVGASsNgY0+GO8Wrffq8CnjbG1PazHmCMCdQL2AKc796fA3QCeVnWnwfU+T4/B3zKvb8O2OD7rgAwQNVA1sU2nhhQ4Pv+t8Bv+3lM6er4bd/nzwN/du//1V0g3neF7hycn2HfBUAjcJr7fAvwx4M8Vy+69x8HXvWtJ9gG+6kM+/0IsCLdf+g+T3XnMoxtHN1Ase/7W4G73fubsY3A++44oC3Lub0aqHX7jgL1wKXuu6v89UrZbh1wSZryeF2znKdtffzf8fMBvMerX5r1TgG2AyH3eSnw9wNpL6PBAqg1xrR7H0SkQER+7kzkRuB5oEwyR5h3e2+MMZ7CFw1w3YnAAV8Z2D8uLf2s427f+1ZfnSb6922MaQH2Z/otV6ffAR931srHsFbBwZwrj9Q6GP9nZ6ouFpEdbr+/xVoK/cE7l02+sq3YO6NH6rnJk8y+9rXAg8aYmLF31d+TcAMmY62XdGT7ri+S/vs+zsdkYKsxJpa6E2PMa0ALcLaIzMZaKI8OpCKjQQBShzv+EzALOMUYU4INAIHPRz0M7ALGOnPTY3KW9Q+ljrv8+3a/Wd7HNvcAfw+8DyjGmpyHUo/UOgjJx3sr9n850e336pR9ZhuiuhN7Lot9ZUcBO/qoUy9cPOO9wNUisltsnOhy4ELnxmwHjs6weabvWtzS/19XpayTenzZzsd24KgsAnaPW/8a4CH/za4/jAYBSKUY68vWi8hY4DuH+weNMVux5tnNIpIrIu8BPnyY6vgQcJGInOF82X+n7//5BazpeyfWfeg8xHr8CTheRC5zF+6XSG4ExUCz2+8k4Gsp2+8BpqfbsTFmO/AycKuI5InIicAnsf77QLkGeBcrcvPcaybWXbkKK4RVIvIVF3QrFpFT3La/BL4rIjPEcqKIlBvrf+/AikqOiFxPZhHxyHY+XscK6vdEpNAdsz+eci9wKVYEfjPQEzAaBeDHQD6wD3gVG+AZCj6G9ef2A/8BPAB0ZFj3xxxkHY0xq4EvYIOOu4A67AWdbRuDvXimkHwRHVQ9jDH7gCuA72GPdwbwkm+VfwMWAA1Ysfh9yi5uBb4tIvUi8s9pfuIqrK+9E3gE+I4x5q/9qVsK1wJ3GGN2+1/Az4BrnZvxPqxY7wbWA+e6bX8IPAg8hY2h3IU9VwCfxjbi/cDxWMHKRsbzYWzuw4ex5v027H/5Ud/3NcByrAXxwkBPgLjggTLEiMgDwFpjzGG3QJRgIyK/AnYaY7494G1VAIYGsQkmB4DNwPuBPwDvMcasGM56KUc2IjIVWAnMN8ZsHuj2o9EFGC6qsN1BzcDtwOe08SuHgoh8F3gb+L8H0/hBLQBFGdWoBaAooxgVAEUZxRyxo+MqKirM1KlTh7saijLiWbZs2T5jTNoBQkesAEydOpWlS5cOdzUUZcQjIlszfacugKKMYlQAFGUUowKgKKOYIzYGoBw6XV1d1NTU0N4+oAFkygglLy+P6upqIpFIv7dRARjF1NTUUFxczNSpU8k8yZFyJGCMYf/+/dTU1DBtWuoUj5lRF2AU097eTnl5uTb+ACAilJeXD9iaUwEY5WjjDw4H81+qACjDxv79+5k3bx7z5s2jqqqKSZMmxT93dnZm3Xbp0qV86Utf6vM3TjvttEGp63PPPUdpaSnz589n9uzZ/PM/J6YpuPvuuxERlixZEi975JFHEBEeeughAB5//HHmz5/P3LlzOe644/j5z38OwM0335x03PPmzaO+vn5Q6twfNAagDBvl5eWsXLkSsA2hqKgoqWHFYjHC4fSX6MKFC1m4sK/ZvuHll/uai6P/nHnmmTz++OO0tbUxf/58Lr30Uk4/3U7OM2fOHO6//37OO89OTrx48WLmzp0L2GDrDTfcwOuvv051dTUdHR1s2bIlvt8bb7wx6biHErUAlBHFddddx1e/+lXOPfdcvvGNb/D6669z2mmnMX/+fE477TTWrVsH2DvyRRddBFjxuP766znnnHOYPn06t99+e3x/RUVF8fXPOeccLr/8cmbPns3HPvYxb2ZdnnjiCWbPns0ZZ5zBl770pfh+M5Gfn8+8efPYsSMxDeGZZ57J66+/TldXF83NzWzYsIF58+YB0NTURCwWo7zcTs0YjUaZNWvW4JywQ0QtAAWAf3tsNe/sbBzUfR43sYTvfPj4AW/37rvv8vTTT5OTk0NjYyPPP/884XCYp59+mm9961s8/PDDvbZZu3Ytzz77LE1NTcyaNYvPfe5zvbrDVqxYwerVq5k4cSKnn346L730EgsXLuQzn/kMzz//PNOmTeOqq67qs351dXWsX7+es846K14mIpx//vn85S9/oaGhgYsvvpjNm+0Q/bFjx3LxxRczZcoUzjvvPC666CKuuuoqQiF7//3Rj37Eb39rnxEzZswYnn322QGfs4NFLQBlxHHFFVeQk2NnHm9oaOCKK67ghBNO4MYbb2T16tVpt/nQhz5ENBqloqKCyspK9uzZ02udRYsWUV1dTSgUYt68eWzZsoW1a9cyffr0eNdZNgF44YUXOPHEE6mqquKiiy6iqip5st8rr7ySxYsXs3jx4l77+eUvf8mSJUtYtGgRt912G9dff338uxtvvJGVK1eycuXKIW38oBaA4jiYO/XhorCwMP7+//yf/8O5557LI488wpYtWzjnnHPSbhONRuPvc3JyiMV6TaOfdp2BTIjjxQDeffddzjjjDC699NK4mQ9WYN5++23y8/OZOXNmr+3nzJnDnDlzuOaaa5g2bRp33313v3/7cKEWgDKiaWhoYNIk+8yPw9FgZs+ezaZNm+JBuQceeKDPbWbOnMlNN93E97///V7f3Xrrrfznf/5nUllzczPPPfdc/PPKlSuZMmXKIdV7sFALQBnRfP3rX+faa6/lhz/8Ie9973sHff/5+fnccccdXHDBBVRUVLBo0aJ+bffZz36W2267Le7ne3zwgx/sta4xhh/84Ad85jOfIT8/n8LCwiQx88cAAP7whz8wVHNdHLFzAi5cuNDofACHxpo1azj22GOHuxrDTnNzM0VFRRhj+MIXvsCMGTO48cYbh7taB0W6/1RElhlj0vaZBtYFaO/qpqG1a0A+njI6+cUvfsG8efM4/vjjaWho4DOf+cxwV2nICKwA3PPyFub++1O0dnYPd1WUEY4XhX/nnXe47777KCgo6HujgBBYAQi5vGi9/ytKZgIrAN64iB51ARQlI4EVgLgF0DPMFVGUEUyABcAu1QJQlMwEVgC8sdEqACOXc845h7/85S9JZT/+8Y/5/Oc/n3Ubr/v3wgsvTDt09uabb+a2227L+tt/+MMfeOedd+Kf//Vf/5Wnn356ALVPz5E2bDiwAuBZANr8Ry5XXXUVixcvTipLl0efiSeeeIKysrKD+u1UAfj3f/93zj///IPaVypnnnkmK1asYMWKFTz++OO89NJL8e+8YcMe6YYNP/bYY7z55pusWLEiKfXZP2Zg5cqVB33sfgIrAGoBjHwuv/xyHn/8cTo6OgDYsmULO3fu5IwzzuBzn/scCxcu5Pjjj+c73/lO2u2nTp3Kvn37ALjllluYNWsW559/fnzIMNg+/pNPPpm5c+fyd3/3d7S2tvLyyy/z6KOP8rWvfY158+axceNGrrvuuvhdeMmSJcyfP585c+Zw/fXXx+s3depUvvOd77BgwQLmzJnD2rVrsx7fkTBsOLCpwPEgoLb//vHkN2H3qsHdZ9Uc+OD3Mn5dXl7OokWL+POf/8wll1zC4sWL+ehHP4qIcMsttzB27Fi6u7s577zzeOuttzjxxBPT7mfZsmUsXryYFStWEIvFWLBgASeddBIAl112GZ/+9KcB+Pa3v81dd93FF7/4RS6++GIuuugiLr/88qR9tbe3c91117FkyRJmzpzJxz/+cX7605/yla98BYCKigqWL1/OHXfcwW233cYvf/nLjMd3JAwbDrAFYJdqAYxs/G6A3/x/8MEHWbBgAfPnz2f16tVJ5noqL7zwApdeeikFBQWUlJRw8cUXx797++23OfPMM5kzZw733XdfxuHEHuvWrWPatGnx0XzXXnstzz//fPz7yy67DICTTjopaVaf1PocKcOGA2wB2KW2/36S5U59OPnIRz7CV7/6VZYvX05bWxsLFixg8+bN3HbbbbzxxhuMGTOG6667rs/ZbjNNiHndddfxhz/8gblz53L33XcnjcpLR1+p496Q4kxDjuHIGjYcYAtAYwBHAkVFRZxzzjlcf/318bthY2MjhYWFlJaWsmfPHp588sms+zjrrLN45JFHaGtro6mpicceeyz+XVNTExMmTKCrq4v77rsvXl5cXExTU1Ovfc2ePZstW7awYcMGAO69917OPvvsgzq2I2HYcIAtAI0BHClcddVVXHbZZXFXYO7cucyfP5/jjz+e6dOnxyfezMSCBQv46Ec/yrx585gyZQpnnnlm/Lvvfve7nHLKKUyZMoU5c+bEG/2VV17Jpz/9aW6//fZ48A/s03V+/etfc8UVVxCLxTj55JP57Gc/e9DHNtKHDQd2OPDvl9fw1Qff5G9fO4cp5YUZ1xvN6HDg4KHDgR2JIODw1kNRRjKBFYCEC6AKoCiZCKwAJIKAw1wRRRnBBFYAEt2AqgDZ0PMTHA7mvwywAKgF0Bd5eXns379fRSAAeI8Hz8vLG9B2ge0G9NJCNA8gM9XV1dTU1FBbWzvcVVEGgby8PKqrqwe0TXAFQPMA+iQSicSfiKOMTgLsAtilWgCKkpkAC4BaAIrSF4EVAB0NqCh9E1gBCOlgIEXpk8AKgOiUYIrSJ4EVAE0FVpS+CbwAaCKQomQmsAIQDwKqAihKRgIvANr8FSUzgRUA7QVQlL4JvABo+1eUzARYAOxSLQBFyUxgBUCnBFOUvgmwAGgegKL0RWAFQGMAitI3ARYAu9QYgKJkJrACIGgmoKL0RXAFQC0ARemTwAqAxgAUpW+CKwDuyLQXQFEyE1wB0NGAitIngRUAnRZcUfomuALgxQCGuR6KMpIJrADoo8EUpW8CLAA6HFhR+iL4AtAzzBVRlBFMYAVAE4EUpW8CLwDa/BUlM4EVAJ0WXFH6JvACoIlAipKZAAuAXWoMQFEyE1gBQKcEU5Q+CawAhOJRQFUARclE4AVALQBFyUyABcAuNQagKJkJrADolGCK0jfBFQCdEERR+iSwAqBTgilK3wRYAOxSYwCKkpkAC4DGABSlLwIrAB5qAShKZgIrAPFEIEVRMhJgAbDLHvUBFCUjARYAjQEoSl8EVgB0RiBF6ZsAC4BOCKIofRFYAQAbB9DmryiZCbgAiLoAipKFUSAAw10LRRm5BFoAEA0CKko2+iUAInKjiKwWkbdF5H4RyRORsSLyVxFZ75ZjfOvfJCIbRGSdiHzAV36SiKxy390uLlInIlERecCVvyYiUwfl4EQHAylKNvoUABGZBHwJWGiMOQHIAa4EvgksMcbMAJa4z4jIce7744ELgDtEJMft7qfADcAM97rAlX8SqDPGHAP8CPj+oByciPYCKEoW+usChIF8EQkDBcBO4BLgHvf9PcBH3PtLgMXGmA5jzGZgA7BIRCYAJcaYV4xtlb9J2cbb10PAeZ51cChoDEBRstOnABhjdgC3AduAXUCDMeYpYLwxZpdbZxdQ6TaZBGz37aLGlU1y71PLk7YxxsSABqD84A4pgWgMQFGy0h8XYAz2Dj0NmAgUisjV2TZJU2aylGfbJrUuN4jIUhFZWltbm73ibqfa/hUlM/1xAc4HNhtjao0xXcDvgdOAPc6sxy33uvVrgMm+7auxLkONe59anrSNczNKgQOpFTHG3GmMWWiMWThu3Li+Dy6kMQBFyUZ/BGAbcKqIFDi//DxgDfAocK1b51rgj+79o8CVLrI/DRvse925CU0icqrbz8dTtvH2dTnwjBmElqsxAEXJTrivFYwxr4nIQ8ByIAasAO4EioAHReSTWJG4wq2/WkQeBN5x63/BGNPtdvc54G4gH3jSvQDuAu4VkQ3YO/+Vg3FwIY0BKEpW+hQAAGPMd4DvpBR3YK2BdOvfAtySpnwpcEKa8nacgAwmohaAomQl0JmANgioCqAomQi0ANhEoOGuhaKMXAIuABoDUJRsBFoANAagKNkJuABoDEBRshFoAdAJQRQlOwEXAJ0STFGyEXAB0BiAomQj0AKgowEVJTsBFwAdDKQo2Qi0AOiUYIqSnYALgPYCKEo2Ai0AmgikKNkJtACENBFIUbISaAGwvQDDXQtFGbkEWgB0WnBFyU6gBUBjAIqSnUALgA4HVpTsBFoAdFpwRclOoAVA8wAUJTuBFwBt/4qSmUALgA4GUpTsBFoA1AJQlOwEWgDUAlCU7ARaAEIiOiOQomQh0AKgFoCiZCfQAqBTgilKdgIuADoaUFGyEWgBEE0EUpSsBFoAdEowRclOoAVARwMqSnYCLQAaA1CU7ARcADQGoCjZCLQA6JRgipKdgAuATgmmKNkItADoYCBFyU7ABUBTgRUlG4EWAEFjAIqSjUALgPYCKEp2Ai0AojEARclKoAVAE4EUJTsBFwBNBVaUbARaAHRCEEXJTsAFQKcEU5RsBFoANAagKNkJuABoDEBRshFwAdAYgKJkI9ACICL0qAmgKBkJuACgQUBFyUKgBUBHAypKdgIuABoDUJRsBFoAdFpwRclOwAVAhwMrSjYCLQAhjQIqSlYCLgAaA1CUbARcADQGoCjZCLQA6JRgipKdYAuACKADghQlE4EWgFBcAIa5IooyQgm4ANilxgEUJT3BFgCnABoHUJT0BFoAPNQCUJT0BFoAvBiAoijpCbgA2KVaAIqSnoALgMYAFCUbgRYAUQtAUbIScAFweQA9w1wRRRmhBFoAvBiA0SGBipKWgAuAxgAUJRsBFwC71BiAoqQn0AJA3AJQAVCUdARaADwLQEMAipKegAuAxgAUJRsBFwC7VBdAUdITaAEQjQEoSlaCLQBuqe1fUdITaAHQGYEUJTvBFgB3dOoCKEp6gi0AGgNQlKwEWgA8tBtQUdITaAEI6bTgipKV0SEAw1wPRRmpBFwA7FJjAIqSnkALQDwRSCcEUZS0BFwA7FItAEVJT6AFQKcFV5TsBFwA7FItAEVJT78EQETKROQhEVkrImtE5D0iMlZE/ioi691yjG/9m0Rkg4isE5EP+MpPEpFV7rvbxTnpIhIVkQdc+WsiMnVQDk6HAytKVvprAfw38GdjzGxgLrAG+CawxBgzA1jiPiMixwFXAscDFwB3iEiO289PgRuAGe51gSv/JFBnjDkG+BHw/UM8Lmxd7FItAEVJT58CICIlwFnAXQDGmE5jTD1wCXCPW+0e4CPu/SXAYmNMhzFmM7ABWCQiE4ASY8wrxmbm/CZlG29fDwHnedbBoSCaCKQoWemPBTAdqAV+LSIrROSXIlIIjDfG7AJwy0q3/iRgu2/7Glc2yb1PLU/axhgTAxqA8tSKiMgNIrJURJbW1tb2fXDetODa/hUlLf0RgDCwAPipMWY+0IIz9zOQ7s5tspRn2ya5wJg7jTELjTELx40bl73WaAxAUfqiPwJQA9QYY15znx/CCsIeZ9bjlnt960/2bV8N7HTl1WnKk7YRkTBQChwY6MGkojEARclOnwJgjNkNbBeRWa7oPOAd4FHgWld2LfBH9/5R4EoX2Z+GDfa97tyEJhE51fn3H0/ZxtvX5cAzZhAcd0GHAytKNsL9XO+LwH0ikgtsAj6BFY8HReSTwDbgCgBjzGoReRArEjHgC8aYbrefzwF3A/nAk+4FNsB4r4hswN75rzzE4wI0BqAofdEvATDGrAQWpvnqvAzr3wLckqZ8KXBCmvJ2nIAMJqGQTgmmKNnQTEBFGcUEWgB0WnBFyU6wBcAttf0rSnoCLQA6KaiiZGdUCIC2f0VJT6AFQBOBFCU7gRYATQVWlOwEWgAkngikCqAo6Qi0AOi04IqSnYALgF1qDEBR0hNoARCNAShKVgItACGNAShKVgItAJoKrCjZCbQA6HBgRclOwAVAYwCKko1AC4BmAipKdgIuADotuKJkI9ACkMgDGN56KMpIJeACoKMBFSUbgRYAjQEoSnYCLQAhjQEoSlYCLQDelGAaA1CU9ARaANQCUJTsjAoBUAtAUdITaAEQd3QaBFSU9ARaALQbUFGyE2gBSAQBVQEUJR2BFgCdEkxRshNoAdBEIEXJTqAFQGMAipKdQAtA3ALQfkBFSUugBUDzABQlOwEXALs0GgZUlLQEWgB0WnBFyU6gBQCsFaBjARQlPYEXABHRbkBFyUDgBcBaAMNdC0UZmQReAKwFMNy1UJSRSeAFQGMAipKZUSAAGgNQlEwEXgAE7QZUlEwEXgBCIhoEVJQMBF4ARHQ0oKJkIvACEAqJBgEVJQOBFwCNAShKZgIvANoLoCiZCb4AhFQAFCUTgReA4miYxvbYcFdDUUYkgReAsoII9a2dw10NRRmRBF4AxhTkUtfSNdzVUJQRSeAFoKwgVy0ARclA4AVgTEGEula1ABQlHcEXgMJc2rq6ae/qHu6qKMqII/ACUFYQAaBerQBF6UXgBWBMQS4AdRoHUJReBF4APAtABUBRehN4AfAsAHUBFKU3o0YA1AJQlN4EVwC2vQpLvktZnj1EtQAUpTfBFYAdy+CF28gzbeRHcqhrUQtAUVIJrgDkFtplR7MmAylKBgIsAEV22dms6cCKkoHgCkC02C47mhlTGNEgoKKkIbgCELcAmpwFoC6AoqQSXAGIOgGIxwDUAlCUVIIrAL4YwJiCXBrauujR2UEVJYngCkA8BmBdgB4Dje3qBiiKn+AKgM8CGFtoxwMc0FwARUkiuAIQyQcJQUczFUVRAGqbOoa5UooysgiuAIhAbjF0NlNZnAfAXhUARUkiuAIAtiego5nKYmsBqAAoSjLBFoDcIpcHECE3J8TepvbhrpGijCiCLQDOAhARxhVHqW1UC0BR/ARbAHKLoLMZgMqSqLoAipJCsAUgWgwdTgCKo+oCKEoKwRYAFwMAqCzOUwtAUVIItgC4GABYC6C+tYuOmD4fQFE8gi0AKTEA0GQgRfETbAGIFkF3J8Q6NRlIUdIQbAHIdQOCOpsZ5yUDaVegosQJtgDE5wRo8rkA2hOgKB7BFgDfiMDywighURdAUfwEWwB8swLlhISKoqi6AIriI9gCEI8B2FyACWX5bNrXPIwVUpSRRbAFwGcBAJw1o4JlW+t0YhBFcQRbAHwxAID3HTeeHgPPrN07jJVSlJFDvwVARHJEZIWIPO4+jxWRv4rIercc41v3JhHZICLrROQDvvKTRGSV++52ERFXHhWRB1z5ayIydVCOzvdsAIA5k0qpKsnjqdW7B2X3inKkMxAL4MvAGt/nbwJLjDEzgCXuMyJyHHAlcDxwAXCHiOS4bX4K3ADMcK8LXPkngTpjzDHAj4DvH9TRpOI9HszFAESE9x03nufX19LWqSnBitIvARCRauBDwC99xZcA97j39wAf8ZUvNsZ0GGM2AxuARSIyASgxxrxijDHAb1K28fb1EHCeZx0cEuEohCJxCwDgvbMrae/qYeX2+kPevaIc6fTXAvgx8HWgx1c23hizC8AtK135JGC7b70aVzbJvU8tT9rGGBMDGoDy1EqIyA0islREltbW1vav5tHEeACAyWMLANjTqAlBitKnAIjIRcBeY8yyfu4z3Z3bZCnPtk1ygTF3GmMWGmMWjhs3rn+1KSiH1v3xj15GYHxugC4VAmX00h8L4HTgYhHZAiwG3isivwX2OLMet/RC6zXAZN/21cBOV16dpjxpGxEJA6XAgYM4nt4UVUHTnvjH4miYvEjIJgQ174XvHQWbXxiUn1KUI40+BcAYc5MxptoYMxUb3HvGGHM18ChwrVvtWuCP7v2jwJUusj8NG+x73bkJTSJyqvPvP56yjbevy91vDM5zvIoqoTkhAN78gHubOqBpF3R3QN2WQfkpRTnSCB/Ctt8DHhSRTwLbgCsAjDGrReRB4B0gBnzBGOOF3D8H3A3kA0+6F8BdwL0isgF757/yEOqVTHEVrH8qqcjODtQOXj5QV+ug/ZyiHEkMSACMMc8Bz7n3+4HzMqx3C3BLmvKlwAlpyttxAjLoFFXaIGBHczwzsLI4yrt7mqDL6VJny2H5aUUZ6QQ7ExBsDACS3IBKzwXw7vxdbcNQMUUZfkaBALjeyeZE+m9lSR5N7TE6292dX10AZZQSfAEo9iyARPqvNztQU2ODLeivALTVD2LFFGX4Cb4AFI23S58F4AlAS7NNEaazHwJQsxR+MB3qtg52DRVl2Ai+AOSPhVAYmhIWgPew0LZWJwD9sQAaasB0JwmJohzpBF8AQiEorEyOAbgZgtsHIgDdXW6pcwkowSH4AgBQPD4pBlBemEtOSOiKBwH70QvQ7aYSUwFQAsToEICiqqRuwFBIqCjKJeYJQH/yALyG3xM7DBVUlOFhlAhAZdJ4ALBuQE+HlwfQDxcg5gRALQAlQIwOASiugtZ90JOYBGRiWR5d7W6YcL9cABUAJXiMDgEoqgTTkxQIfO/sSsTd+Tvbmvj+n9dm30c8BtB1uGqpKEPO6BCAqrl2ueGv8aL3HVdFgdi7eU9nK3e9sJmeniwDEOO9ACoASnAYHQJQvRDGHQvL7o4XjS3MpSJqXYI8OunqjmV/alBMewGU4DE6BEAETroOdiyDXW/Fi8fkJiL6eXSyvS5LMFAtACWAjA4BAJj7UQjnwfLfxIuKQ4m7eQEdbD+QTQDUAlCCx+gRgPwxMOU0qHk9XhSKtcUfHpIvnWw/kKU3QHsBlAAyegQAbByg9l3ocZMbd7baSUOB6sIe2Lksc1JQTBOBlOAxygRgFsTaoGGbzQno7oDCCgCOLWrjC5s+Dyv/X/pt1QJQAsgoE4DZdlm7LpH9V2AF4Li8fYTphra69NuqACgBZJQJwEy7rF2byP5zFsC0kE0S6u7I4ALEBUB7AZTgMLoEIH+MHRhUuy7h67sYQFWPHS3Y0tKcftuYZgIqwWN0CQDYOEAaC2BMh31GSWsmAdD5AJQAMgoFYLbtCYhbAFYA8lu2AdDemkkA1AJQgscoFIBZ9nHhBzbaz84CCLkHiHa09xUDUAtACQ6jUwAAdq6wy4LkhxDHOjIkA3l3/h61AJTgMPoEoHyGXe560y5zi2yKsKM70wzBGgRUAsjoE4CiSoiWwu5V9nMkHyIF8a8l0+QgGgRUAsjoEwARqDjGPi8QbOP3CUC4p4Om9jR3eR0MpASQ0ScAkHADAHIL7MsRJcOgIE0EUgLI6BSAimMS78P51g0AuiOF5Esn29INC46pACjBY5QKgEsJDufbB4dECu3nsink0cm2A2m6ArUbUAkgo1MAPBfA3fm9Zc7YqeSlswB6uu1jwUAtACVQjE4BGDsdEMh1d34vBlA2hXw62bY/RQD8d321AJQAER7uCgwLkTwoOwrCUfe50OYCFFUCsOdAffL6Md9koZoIpASI0SkAABPnJcb+H3WqbdjOFdhf30B3jyEnJPZ7v9mvLoASIEanCwDw4dvh8rvt+4WfgMt/lYgFdHewp7E9sa6XA5CTqy7AaMGYRLJYgBm9ApBfBoXJ4wAIWwHIk87kGYK9Rp9bpBbAaGHLi/CzM+zcEQFm9ApAOiJ2TEA+nWyv8yUDxfwCoBbAqKB1v1221Q9rNQ43KgB+nAWQn9ECKFQLYLTgBX4DHvRVAfDjYgATCsksAKY76SnDSkCJuRhQwKeBVwHw4wRgchHJjwnzBCBa5D4H+66g4Mv8VAEYPbh5AawF4I8BOHMw1xMAjQMEnrgFEGyxVwHw4yyA8fmGPU3tdMSS039f2OpEIeBmoUJCAAJu7akA+HECUJlvMAZ2eD0BLg9ga5NbTy2A4KNBwFGIcwEqovbZgfGuQNfgm8lP+qwEmLgLEOyArwqAH2cBjMm1f7rXE9DYbIcHtxo3d2DAzUKFUTP/gwqAH2cBFOd0EQ2HeHWTTQZZv/MAADl52gswatAg4ChEBML5SKyNG86azuNv7eKBN7bxTk0tAMdUjweg2z86UAkmQz0LdP12eOrbiUfXDxEqAKlE8qCrnS+dN4P5R5XxjYdXsWVPPQBjx9qxA3vqmrLsQAkEQx0DWP8XePl/oLFmaH7PMXqHA2cinA+xNiI5IX76sZN4eHkNlzSvhGVQWT4WgB37Gpg4vLVUDjdD3QvgTUc/xO6lWgCpRPKhy6p/VWkeXzj3GKqLrU5OHGctgB37G4atesoQMdTPglQBGCFE8hN/hkd3J0gO+QU2CLj7gLoAgSduAQxR0leXSz0f4qCjCkAq4TyIpQpAh50+LCcCwP6mDA8QVYLDUA8G6krOORkqVABS8bkAcbq7bOMPWQFoa8vw+LAUPnXPUn7zypZBrqAyJAx1L4BnAQzx4CMVgFQi+b0tgFgH5ETtlGBAW3sbxpisu+npMTy3bi/Lt9Ydrpoqh5OhzgNQC2CEEM5LEwPoso3fuQAm1kVLZ/buof0tncR6DE3tA1D0tU/AvZfa+eiU4SU2xMOBVQBGCGmDgB0Qzo1bABHpprYpezKQN6loU8cALqDtr8HGZ3S04UhgyGMAXhBQXYDhJZKf+PM9ujuTLIAIMfY1ZxeA3Q1OAAZiAXjCo5mGw8+w5QGoBTC8hNNYALFkAcglxr6mDhraupKnD/ex25U3dwzgAupyvQs62nD4ic8HMMQWgArAMBNJFwPwBMC6AGFi1DZ38G+Prubi/30xMXGIj7gLcDAWgArA8GJMIhFoyC0AdQGGl4Jy+6e3+aL33Z0uD8AKQK50s6+pg9U7G9nT2MEfVuzotRu/C9BXj0EcdQFGBn4B1jyAUUb5MXa5b0OirLvT5QHYlOCSiGF3Yzub91mT/c7nN9HTk9zIPRegu8fQ3tXPEV7DZAYqKfhjQEOeB6ACMLxUzLTLfe8myrw8ABEIRSjNNSzfVk9ndw9nzqhgY20LDy9PHsXljw00tffzIupsTfye0n9e+OHgPsYr6WGwQ2wBaC/AMFM2xWb87V+fKPMyAQFycimOGDbsbQbgS+fNYNHUsdz0+1UsWbMnvsnuhnbKCuw2jf2NA3Qlz0Go9INYByz5N3jljkHc5xBbAMaoBTBiyAnD2GmwzycAsfbEo8RzIhRHEub+rKpifnndQmZVFfNPv3sTYwytnTEa22PMqLSDh5r7mwvgXQSxgLoAa5+AlfcP7j69WM3WlwZvn0NtAQyHy+FQAUhH+YyEABgDTbuhyM4GRE6EwrAVgMriKCV5EUryIlx58mTqW7vY29QRDwAe4wSg3y5A/C4QUAtg6a/spBeDiffsvvqt0LhzcPY51ALg73VSARgBVMyAA5tsl0xbne2fL6223+XkUhi2QT2vgQNMq7DvN9Y207zxFf43cjszxhUA0NxvFyDgFkB3x+CLm7+3ZuvLg7NPvwAMRYPsSvMUqiFCBSAdFTNsV2D9VmjYbstKJ9tlToT8HNvvnyQA4woB2LyvhZytL3BRzqvMLrEXT79zAYIeA4h1Dr64HRYBcCZ5KDw0eQB+C0DnAxgBlM+wy/0b7GSNAGWeAOSSH+ptAUwoySMvEmJzbQvNzTZAeHSJFYrG/rgA3bGE+g9VL8C7f4Fnbx2a3wIrbKlp1odKe71djj0atr3S9/prn+g93DsVr465RUPkAvgtABWA4afCCcC+d3tbAKEIJbmGWeOLOe3oivgmoZAwtbyQzfta2F9Xb3cT9tKB+3ERDcdF8M6j8PrPh+a3wFkAh8kFmHkB7H0n0ZWajrotsPgqWPt49n16dYwWD01mXlIMQF2A4adgLBRWwp53rAUQzrcZggA5EaLSzV9uPCvJAgCYPq6Qd/c20djUaFftaKAgN6d/LkDSRTBEFkCsHTqHcHajwxYDEBgz1X7OdjwdzW7Zx5RuXh1zC3ub5D098NfvWDEZLNQCGIFMnA87l1sLoGyyTQICmw6cQaWnVRSy/UAb4R53AbU3UJwXprk9xpvb63l3T+LCa2jr4oIfP8+b2+ttgf8iGKogYKzdHstQ5Z/HOu1vDuZ8B211kF8Wf6pTVhfDa9ipYz1SifkEILVBHtgEL/3Yuk+DhfYCjEAmnQS162DvmkQPANjBQhnMzOmuJyAPvwBEaOro4sYHVvLtR96Or7t+TxNrdzfxxKpdtmBYLACvQRyEFdDdBQ9cDbvf7nvd+DaHYZqttnrIHxN/qlNWFyN+vFncBEiJAaQM9PJcwsF0ZdQFGIFMWgAYmxHo+f9g8wGad6fdxOsJKHfPFqSjkaJomL2NHWze38KqHQ3Eum0AcZfLFXh9i33sWLIFMIQuAGT3mzPRtBvWPDawyLt3XKl36a0vw+NfPTjLoK0O8soSiVqp07kl/X57+t/PVM9ocW8XoHFH8jqDgScAuUXaCzBimLgg8b7MJwDFE+zFn+ZinV5hBaAizw3+cS7AWzsaMAbaurpZ71KIvWSht3c00NbZPTx9wd5FfDBxgHhj7t8EqUDiuFKP75WfwNK7Dq6HoK3OWgBxFyCbBeB+t08XwGcBpForDW7Mx2BaaV598krVBRgxFJbbcQEApUclyksm2gu49UCvTcoKcvnWhbOp9mKDTgA6Y4nRgJ7P740W7Oo2rNxen3xRDrUFcDAugNfw++pSS9omjQUQ64RNz9n3fQXn0tFe71wAzwLIUp/48fblAjihSBcEjLsAg9id6dUnWqIuwIhikrMC/DGA4gl22ZQ+7fSGs44mH/cntjdQHLUDgvIjOZTmR3izxj5VaHdDO+OKo4jAG1sOJN+Fh9wCOAgXIJM5n4mebjDdydsCbH8VOvsZnU+HFwT0YgDZBMk7r/3JA5Acu89eMYDD6AJEi9UCGFFUL7JLr4sJrAUA0Lgr83beH9reSHGenUNgZlUxJ1aXxi2AXQ1tzKgsYtb4YisAQ2QBfOuRVXz6N0vd73gxgIOwAOKTl/RTAPzH5H+//qnE+47GgdWhp8cXBBxMC6DdNv6ccGYXYFAFoNV2NYejKgAjioWfgGsfg9JJibLiKrts6o8ANFDkBODYqmLmVpexbk8TbZ3d7G5op6o0j1Onl/PGlgN0tLm7YCjcywJYu7uRD//Pi2zbfxB36hSWrNnDa5v221mKDskF6GeXmof/mPyNdP1fIbfYvh+oBdDRCBgnAP3oBuxvnWPuSVChSLILYMxhEoA2G8PIiWgQcEQRyYdpZyWXFfVDAGIJASjOsy7A7Kpi5k4uo7vH8FZNPXubOphQmseFcybQ3tXD+h17AejKLe0lAL98YTOrdjTw/T+v7bPKTe1dvLZpf9rvdjW0saexg8b2GPtbOuONpbWlie6eAUbgvWPsb0PwH5P3vqMZatfC0efaz+0DtAC8LMCkXoB+dAP2Fbj0LIBQGEyPtTS834sN0PLpD11tECnImmNyuFABGCjhXCgcl33oaZdfAKwFMKuqhAVHlQHw5Nu7ifUYqkryWDhlDBNK89joBGBbay576xINoaG1i8fe3MmYggh/WrWLZe5JQ39cuYP/fcY3Z4Hjvte28dE7X2VVTe8nGK/cVh9/v3lfS7xB/PDx5Ty0bHu/TwEw8F6AJBcgxRQvc0HWgVoAngAk5QH0oxuwLwsgPgek/e/i4wEatievM1h0tdqbTSiNy3GYUQE4GLyuwHT09CQutPYGzpxRwVWLjmL+UWWUF0U5prKIx9+y1kNVaT6hkPDhuRPZta+ODhOhS3JZU7OP/e65A79fUUNHrIdffHwh44qj/PhpO1XZfy9Zz8/+tqnXhKMbXTfjz57f2KtqK72sQ2DT3qZ4PUOxVtbsGmDj6xpgL0CSC+AF45wAeHMtHJIA9MMCGEgQ0HMBIGGWe+Z/KHIYLIB8ZwGoAIx8iidk7AVI6kPuamFCUYRbL5tDXiQHgEXTxsYfKjKh1N61PnziRPLooDOUx5TKMUh3J//zjJ2U9IE3tjN3chkLp47lqkVH8eKGfby2aT+balto7oixsyH5Qtzq4gRPrtoVn7TUY8W2ek6sLiU3J8TWvfXx8gLpYEf9APrz/cd5UEHAlASkuAAM0AXwRgJmSgV+60E7+i/1d/sMAiY/DTreKL0egLHTBj8IqC7AEUTJhMy9AN6d0buo96yCP3w+fsGcMm1sfNUqJwAnTCrhjCkF5BcWk59fQGUBPLV6Nxtrm1m7u4lL59meh4vnTsAYuOmRxASY/vEFAFsPtHD2zHGEc0L89tWtAKze2cBz6/ayakcDC44aw5TyArbXJsbR59PBzsMtAP7Emfid2P1mfpm9+A/FAnBTtifd3V+6HV77qa/OA0gE8mIAkOwC5EShZNJhCgKqC3BkUDwRWvf1njmmvSFxd/HyBZb+GlbeF59i7OSpVgByc0KMLbAXrYhwTFmIcLQAciJU5MPOhnZ++JQ1999/vA08HlNZzLETSthU28KkMnvHe3d3otG0dXazp7GDk6eOYW51KW/V1APwpftXcN2v36Ctq5v5R5UxfVwhu/bVx7crpP3gBaC/vQCxNL0A3rmK5Ns+8IMVgLwyO1grnJcsSF2tyYHFuGj1txcgRQCadtteoEj+YRAAZwFoL8ARQIlr3M2JWYB5/v/CT89IXGTFzgLY/Lxdttug3MSyfKrH5FNZEiUUksT23kUQjlKWawgJ/GnVLv6uai8TexLxhovnOmtg3kQqi6O8u6c5/t22A7ZBHVVeyOyqEtbubqKpvYuNtS1ccHwVN5w1nfOPHc/0cUXU1ieChAXSQV1rF62dA3mK0SFYAKniESmwWXADdQHa6m2jj7gAYDivt6vR7guGploeGejqbCcWyu3tAnQ223qGo4OfCRiPAYwwF0BEJovIsyKyRkRWi8iXXflYEfmriKx3yzG+bW4SkQ0isk5EPuArP0lEVrnvbhexY2xFJCoiD7jy10Rk6mE41sGj2CUDrfgt7HfBtk1/g4ZtiTuOZwHUbbZL38V93WlTuWy+L7cAkvzAcE9X3FK4ue178Owt8dUuWzCJ+UeV8XcLqjl+fJSr138Zdq4AYMt+6/NPLS9g9oRimtpjLFljexf+/uRqvnXhsRRGw0yvKCSnJ3GhTSywgcQBWQEDjgFkCQIerAXQ2Wy38wjnJd/du1qTRcVf557MD2upqa1j1e723kFAr6GG8wZ/LEAkb8T2AsSAfzLGHAucCnxBRI4DvgksMcbMAJa4z7jvrgSOBy4A7hCRHLevnwI3ADPc6wJX/kmgzhhzDPAj4PuDcGyHj8pj7cCNv30ffnOJ/dN2vWm/8/IDvBiAh88U/dSZ0/nq+2clfx+/uKLQ3cmFcyZQKi0Ud+xOmvdufEkej3z+dI6pLGJBaSvzu1bQs8laGV6i0JSxhcyusg3joWU2cn38xNL4PqaPKyJK4kKbWGDTXXfUDySv37uLD4YFkO8sgIEKQIvN1/cIR5MtgK42awF4PSXpApEpGGPIizWypSlEm5cF7M2X0NkKuc5UHywXoLvLilQ8CDjCBMAYs8sYs9y9bwLWAJOAS4B73Gr3AB9x7y8BFhtjOowxm4ENwCIRmQCUGGNeMbbv6jcp23j7egg4z7MORiSlk+Drm+GC79vA0NsPJ+48ngB4GYMe7b375ZOI+4FWAK4+dQp/+odK+11Hc9pNZpbZu1jzAesibNnfQllBhNKCCDPHWwF4aeM+KoqiVBZH49udMKmE48blxj9XRO2VfnAWwEHkAXSnWgAFzgLI4AIsu9vGUrBZkS+sr7XlnS2JLEJIjgF43bE9sfTPXMzgBjQ0NTOeOrb2VPD2blc/LwbQ1QqRwt6xhkPhtZ9ZS+bo8xKZgIM5YUofDCgG4Ezz+cBrwHhjzC6wIgG4q5VJgD+rpMaVTXLvU8uTtjHGxIAGoDzN798gIktFZGltbe1Aqj74hHLguIvt+7/9IFHujRdPFYCOvgTA3V3C9u6SExKqu7bZ7zrTC8C0IisAW7dvZW9jO1v3tzJlrJ2KvDgvwuSx+RgDx08swa+n0XAO3/2wnfewTfLJN+3khIQddX035p4ew7Pr9lJb547H16hW72zgqw+sZPuBNN1s6VKBkyyALC7AsrttIBX40V/f5Wu/e8uWdzQlWwARXwzA30A98U0ahZj+WPft2ERIDNt7Klm23Z33Xi5AlEGZtalxJzz3PZjxAZj5gUTMYQgfD9ZvARCRIuBh4CvGmGzRmnR3bpOlPNs2yQXG3GmMWWiMWThu3Li+qnz4KZkI4+fAAV/SjZchWDgOEOsKhPP7aQHkOwvAXcS16+wygwBMKbIXyt7dOznjB8+ybGsdU8oTDWJ2VQlg7/ipFIi9qPNKKpCuFqpK8thZ30ZnrCcpuaits5v7XtvK2zsaeOzNnXzwv1/gE79+g7e2uACob4qvB9/Yzu9X7ODC21/glY0p6cg+oVi2aQ97m9qTg4B5WVyA1v3xAUvbD7Sxp6mdru6eNC5Ani9BydfAvXPvF6EMFkDzbttbE66YysodLo8i1QXwgoCHeqdefq89hg9+3/ZieF2ZQxgI7JcAiEgE2/jvM8b83hXvcWY9brnXldcAvhk0qAZ2uvLqNOVJ24hIGCgFeg+4H4nMON8uxx5tl54A5Bbai3rCXLvsK8+90wUBw7mJu0uty/3P4AJEY7b8tAmGD82ZQFtXN8dOSDT2Y10c4ASf/x/HNUgpGAudrUwsy+PNmnrO+P4z3L4k8WTknzy7gX955G0u+p8X+eL9K+gxhitOqiYnjU+/sqaB2VXFFEXD/Dw1E9F3Ub+zbS93vbjZ3lFDEXvnixbbc5SuUbUeiIvgjvo2jHETqnS2QNQ3Mas/BuC/wzvXwvjjFRmSgTr32aDtexYupNWLASRZAIUu69Acur9+YJPNKRg7zX72go4jSQCcL34XsMYY80PfV48C17r31wJ/9JVf6SL707DBvtedm9AkIqe6fX48ZRtvX5cDz5jUHNeRyjHvc0snBF4MIFIAZ9wIp3ymf11cSdlg/bMAvH3mdx7gRx+dx4vfOJfrz5ga//q0YyoojoY5acqY3tt65nBBOXS1Mqksn421Lext6uDeV7cS6+6htqmDX720mfcfN57/umIud15zEn/5yll89f0zk4KIxNrpiHWzZmcjZ88cxzmzxrF8a13yI9Ndw2wPFZBLjD+u2EmPd8yQmH4rNbjW1W6Pv9NmPja02d/dWd9my3P9ApCfPj/BWQBtba00mz7mDajbQoeJcOLsWXSRkgfg7wWAQ+8JqN8GY6YkPse7HYfOBQj3Y53TgWuAVSKy0pV9C/ge8KCIfBLYBlwBYIxZLSIPAu9gexC+YIw3EwSfA+4G8oEn3QuswNwrIhuwd/4rD+2whpCjToVzboK5V9k59j0LIJxnBQBsj0E2F6C7y178kQI7+sy4ce4N221ZV6u9KHJS/i7Pqmix5nb1mIKkr0+dXs6qf/sAafEaWv5Y6Gxh8hibWHT5SdU8tKyGFzbs45k1e+mI9XDThccyrSJhak8ozacuEgOvJ62rnbX7hM7uHuZNLqOls5v7X9/O+r3NzBxfZOMPrrHU9+RTGulmd2M7e/fXUeWl8Ead5dLRlOjXB2v+A3S2JMUodjZ4ApDaC5BZADo72mimiCLaM1oA0aYadocqmVxeRA+u86q7y1plPTHXC+Abd+Dvhhwo9dtg2pmJz8PgAvQpAMaYF0nvowOcl2GbW4Bb0pQvBU5IU96OE5AjjlAOnPNN+z5amgj2eRc29O0CeL5vtChxAXvPu5843z75trPJprwmbef22dWSiCH0F6+BFIwF080nTp3IqUdXcPLUsTy9Zg//+se32X6gjWvfMyWp8XuU5faAq+q++gZW1tgGPndyWXwKtKVbD3Dzo6uZWlHIrWPdY9J68jhxUh5FO8PsrD3gEwBvToBGKPLFdzwBiLWzY3/iHO6sa3MxAL8FkJdVALq7Omg0hSD7MsYAStpr2B+pYkpOiLLiAujA9SR4PRaF9Gvykb6IddrxJN5ISPBZACPIBVAGQH6ZXeZErTB49GUBxHsOJiTuLnvcdNvetGTp4gD+fbbsG1hdPQvAPfBkbCTG6cdUkBsOcfHciWw/0MZ5syv59kXHpd28OCdGzNjL5xO/eJ6Hl9cwrjjKhNI8ppQXUFEU5ed/28Qrm/bz3Lq9EOsgJmE6JZfxhcJFJ05gX10dzT3urue3APy0JoKJe/bbsFAkR9hT1+DuyBnyAPx3+PYGenoM0t1Og3HrZ+gFqOjaRVO+DVWNK3XrdnclJy31ZwpyP8/eCo9+KbmsscZaekkC4M7FSOwFUPqBd4f2m7DQdwzAG2VWWm2DgAB1diAP4461y3RxAP8+W9J0i8Y6YN2f0/+md/fKd4OTfNOC/eO5x/D1C2bxk48tIJKT/hIpCMVoxDaQqgLhrZoG5laXIYC8cBsfG7c5npq8q6GdlrZWOk2Y3LwCcro7uOnCYynP7WZDfTd/XLkD4wXzUs+TTwD2HThAbjjEjMpiDtS55Ci/CR7JTz9VWXsDG2ubiZgYnRG7fld771mQTGsdxbTQVWxj2ONKXZ16YomRi7mFif+ovwKw7WXY8mJyWb3r4i3zxQC8sQdqARyheBZAJNkX79MF8CyAkokJC6CxxpqbRb5koL1r7cujvdFOXglJDSXOmsfg/o9C7bu9v4tbAL0FoLIkj8+fc0x8CHM6crrbKR1rTfV/u3A644qjnDNrHKy4F575Dz7c8zRgU5cBtu+to92EKSgohO5OSvMjnDg+l1BuAV9evJKb/rTFHae1AGqbOmjpiCUd14G6Oia5sRSNDfW2MLeQhtYu9jd3YHIyWAAdjSzbWkeUTsorbIZmXUPv/6N5t+v9cHNAVpZZAdjX2MK/Pvy6/S5S4LMA+ukCtNX3Fra4AKSxAPorALveOuR8BBWAwSRuAaT44nml1uTM9Gc17rDqXzQ+4V827LC+sGfidjbBn/7Jvjw6GhMXUDoXwJu0xD+TjUes3V5wng890HkBYx3kFNjjnVgIr910HlfPiMGT3wBgSrSZr31gFjdffDzhkPDuzv10EaakqDDecCI97cyZOoH/vHQOb9baHoO3N9WweV8L5/3Xc1x556t0NyeOq6Ghnkll+Uwsy6e5sd4eNnmc+YNnOOk/nuauV3divP55zxLIyaW96QC/X7aNXOmmcrxN0Kpr6O2S1e+0XZfRStulWzXGnptn39nB6i32XPZ4k3dCUkPNOqVae0Nv16Z+mxXvEt+YkLgA9MMF6GyFu94PT3+n73WzoAIwmHgCEE4RgKjrh8/kBjTssP5/KCdxETTU2GQir4F2ttguRv8IxPZGGDvdvm9NIwDe3dO/jUesw9Yz11krA50aPNZmh+G6fYVCAu/80d55qxcRad3LF849hpK8CLOqiu0IO4lQVFiYNC5fIvn8wylHceenzgHgwZfe4YqfvYLpibF6Rx2rN2yO/2RzU4MTgLy4S/Ti1nYa22N85fwZ5OYVIBgWv7opLgAHZAyvrd3Cmhp7LkrG2Cc6Nzb2/i/a922y61TZczphjHUXVmyppSRs6/z0xua4lbZ0g+3yvf/1bZz0H3/NOJrStDdY0fPfAOq32cbv79nJGYALsPEZ+x/MzNDL009UAAaTbBYAZA4ENu5ITDfuCUDLXisAcd+42d7l23z5UR2NdlxCTm56C8AThXTTl3nTXsUtDJ8FsHMl3HtZ5r7y7pj1iz2Xxwuote63bsvEeUmiM3dyGVFi5ETykNSuOucuTZ5g78yLJkZoau/ipQm386uq37O9Zlt8P11tTUwaYy2AArGm/p/WNTL/qDK+cv5Mrjp9JgC/eu4dujpc5mCshKmFMZ74/MkA5OUX00mY5mZ7R16/p4n/fno9se4e6ndsoNEUMLHKjuScONYGJnu6Y3xotj3Wu17bTbvrPPvJ06tZs6uRn/9tI/WtXWyqTWNF9fRgPPfPfwOo25qcAwC+IGA/EozWPm4FeMrpfa+bBRWAwSSjALgId1YBcKagF2ACZwG4IFdbne1ibKtLDGVtb7QBxoKKDALgxCKjBZBnGywkuwAbl9hXpkdgew3YswA8oWjdb3sViird5Ci2fF51Gbl0kRv1htL6BgN55yochZxcLppZyJvfeT8lTes5M7SKOWNitORYAS2kPe4CFGJFZ0O94dr3TAUgkmv3daChiafftHWfVD2VKQVdTC5x8YxwLl0Spa3VWhD/9dS7/Ojpd/ny4pU07tlMc/5ExrmBUyWFdn9hujlpgi3b257D/zxvh7Tk0sXnfruMLW4U5sba3oHajtYGQi5horOlgdqmDt7Z2WgtAJ//3xnrYcN+79FpfQhAdxesexJmXpDoOjxIVAAGE69BpApANIsAGGOTh7xnD3hBQEi2AOpdr4DpsULQ3eXM8FL7GLN0LkBLfyyANC6Al8zUsrf3duBLIhqT2Bc4ARibGArttj93diUTikIUF3pDaf0WgO9c5Y+BtjrycoDWA+Qc2MBRoVoKK22q7PnHFPLe2ZVMGVtAccjWYezYsXxwjht45YJzlfmGzbttzKF83AR75/V+M5xHd04erS1N7G1q59l1e+MzLh8VqqWiekaiPi4qPyYKU5wOHzeliifW2B6I980oZcv+ViqKchGBjWksgFUbEhbM9t27+M8n1vD1nz+MadqVJAD/77WtfPl3ruu3Lxdg68t2PsRjL8q+Xj9QARhM+nIB0sUAWg/Yi7PEDZMI+wSgqNJe1JKTfDduPZDoVchqAbgYQFoBcBaAP8bg4XVLNmcSAN9cfpAiAOUJAWiylse44ihzxucR8SyAWIcL1LUm95gUjnNuTj12LJhJulN+5LgyxhTmUl4U5Zvn2a66X336XKJhd3d35/1Dx40hnw5MOB/JK7HC6zWqnCjR/CJyejr47L3L6Ij18JN/WMA1pxzFtPABcst7d8t9+IRKQu6Yrz37eDqNLb/w2LFUFOVy/RnTqB6Tz6Y0FsCbG7bG32/buYf1G9bzM/6DrryxNnvU8cy62njqcXtHH92L65605/HotHl4A0IFYDCJC0CabkBI3xXY6EZIp8YAAAor7CixaBEcSATD4u6At+/Cij6CgFksgGgxIMmxhca+BMBdoHEXwBcD8AuA3/Xo7rTH5iXreKLhF8vCCpvPkHos3p3SlwtREbENOrfAPx+AFc8r51cyb0IekbxCW8eu1kQUPhwlml9IdREs31ZPZXGUU6eX890PTCIca0mbmTdzXDTuIi2aMYkfXGkfGVeQE+Olb76Xz519NNMritLGANZuSfTAvL1pO3/X9jvG0cBvj/6veAygvaub1zbtZ/Yk2yX79Ko+ntGw8Rnr++cWZF+vH6gADCbxXoCURKBsQcB4EpAXA0hxAcDGAeoTd5JeFkDhOGiuTR5J19OdmEmoaU/vUXaeBZATsQ3W/6ATTwAyuQD+h1kiPgvgQGYB8CbaDLvhzv7EGv/xttT2tmaKJ1jx8FspnS32t/1i6857edQwvyqKRPIS595LlApHIZLPzDHWarhwzgTbg+H1y5f6BrLGpwTrtvUNhSGcy+mzJ8WPKRrOQUQ4elwRm/e1xAdAPbt2Lzc/upqGA4kErS07djFBDlATmsADNYnpLl7ffICOWA9Xvcd2P67bkf7JToDtHdq3DuM9TekQUQEYTDIlAuW6hpLOBYgnATkXIMkCcElA/jECYO/W3r7ySmwAsaslWWDa6gBjH20ea+v9254FANb68ASgq91nOfRhAUTy3Qy5rouro9EKQGGFPV7/9n4LABJz+idZAE4AvN/35l4sKLdCkSoAuUXWQvKI5+i3JdwLz/ry6hKOQtgOSPrZ1Qv44nuPseVeroTfAvDSubu7XLzCiVV8MFDiP5k+rpC2rm52Nbbz8LIarr/nDR5cup1jShJPFy6kjfE5jeSVVrJuTxO7GqyQPv9uLbnhEPOm2v97X6Md+ZhKe1c3P//1XQB88sWS+PaHggrAYJIpFTgUcuPdUyyAzhaoecPeaby7fVoLoCh5u1QLwHt8uT/hx2tE44+3y6aUngDPAoBkAfAECfqOAYTzbH39olEw1loVBeXJrkf8YRvu+Nrq7TIpBlBhzXzvCTzHvDdRnluULACpswGB7wGhzsUI+y0Adyw5UZcy3MoFJ0ygvMjVJ11mnoi96/d0WYH191hAUr/+9HG2Lnf+bSNfe+hNTj+6guX/5318/ewJ8XWKaWNCpIXicits97+2jeaOGE+v2cOiqWPJj9r/I0ws/hRpP8+tq2XC/ldoCJfzest4PvHrN2hqP7Q5CVQABpNIPpz9TTjukt7f5ZXC3nfgtTttg6nfDj+eA289YIcUh9xf4VkAkpMQFK8noKAcJNTbAvDMVq/hQMKMjgvAruT6JFkAkxIN3xOCSEGWXoBERN2OwW/zCYAzbYurrIAs+S689nNnAUQTv+m5J0kWgLN4atfY5awP2WXpZGcB+IJsqZOBQPIoPS/HwDuH3oNcwlEr0Kk5DvXbrMikjrgMRawF4M0GBG72nmiSBXDMOFuXe17ZygmTSvnFxxfaVGpn6fSEIhRLK2NMA8Vjx/OhEydw+zMbOO+/nmN7XRvXvGdKPOaQS4zlW+tI5S+rdnBmztsUH/c+fnr1SWzY28xtf1nXa72B0J/5AJSBcO5N6cvzyuwzAjY/bxtv6wF7F7zmDzDt7MR6ngAUViREwbMACiutL996wEb+wWYZeqapXwDiFoAbzZeaC+C3AEon2Tq1NyYEoOrE5LiDH6/xRPIS8/ClCkBRpZ3QZP1fbWJQrMPmOGQVAGfx1K6zbtOsD8IXl0P50RlcgFQLwDdKr6vVnqPU6dnDeYk5FvzUb7d3/9S5aHMiNgbgdwGg1wzE44qjFEfDTMvZy/1VT5IvJwFuKrhoKZIT4YJJEaLbm6Cokv++YB6FuTk89c4e7v7EyZw5Y1w8LlJVlMPL25IFoCPWzdq1qxkjTTDlNM6cMY5fXrsw/WQvA0AFYKi44FZ7l1n9e/vIKtMDJ1yWeDS2h9dAvMYAPgGosOZoqgUgOfaOlOQCOAug0rMAUnoCYm3JFgBYK8HrlZg4D3YstUlHoRC88F+2zmd9LcUCcPPweQJQ6ISpaLyNVoMVprgF4BppXABSugHBDngqLLeNsdxNtdZLAJp7u0ae69XVZkUqkud6VyTx/Aav56P1gL2ze4k0KYk5cdK5AN5+fDMCiQi3XXEip7/yKQrffhHmXQrHnGcFIK8UyQkzNeQCggXlhHNC/ODyudzaY8jxHhDj6nJUaYSfbK9nV0MbgjC+JMpLG/ZR2rUHcon3Hpwzq5JDRQVgqPBmfpkwF37m0jff84+918tJIwBRnwDEOlwMoMGa394FXDrJ3sU8vAY5dpptZNksAK8LsnGHtQDyyuyIuJ6YNWELxsLSu+0Ff+Y/+wQgmpiEI50F4NG029Yz3FcQ0IlHRwNU+BJywDZ2v8B1NkNRyszL/lF68SnWItYd8Xz8cNT2n7/xS/swF29Ox/pt1hVLxXtYh98F8H4rZTjwB3KWwQ437Hfn8iQBIJRj5wCEpP82x/90KJd3UF2SQ/22Lt5zqxXQ4rwwrZ3dXJnrumr9PRWHiArAUFN1Aiz6jG0wE+f1/j4UshdCOgugoMLe3Rp3WAvAi3CDDQQ21Fhfd8PTdpqw3GJ7wReN7yMG4AnATtstWTIp8fvNe+yduME1oMYdPgHwegE6EmnHng/tNc7SybbhehZATqoLkBIE9PCEJH4OUiyAjmYYm+oC+Kbq8mcZlkxKHH9O1DbMaKl9nsOM8xN5FW4YcBI5kcSzBfz/iT+j0ePZW2DcbCsYO5a746y3AiACu1b2Pk4/bmbgYyry+Pw5RzOhNI/uHsOG2mbK8nP5+9bX4E0SQd9BQAVgOLjwB9m/LxyXfDH6LYDOFtj9tr1j+e+ypZNh47Pw8u3w6h12e2+s/4QT7Xdeo+jpsQ3Su2N6fnLjTtvASyf5+vL3Js81sGNZIgbg9eu3N9p18koTFsm4mTaAdtqX4MmvJa8Pvl4AnwWQW2j97K6W3o1kQDGANidwbt+l1dad8dfh2A/Dmkeh60eJLMvUwTngXIBYGhcgxQLoaLJB3vd+G/ZtgE3P2nhNe0Ni1l8Pv5D0+r0IUenm6x+Y3fu7P+6zwurvKTpEtBdgJPKpJYkJRSExIKig3DbqllrY9ipMOSOxTulke5db96T9XLclcRc9+VM2bvD2w/az57t6F1I4ai/K+q1umOrEhLi01CbmJ5SQFQB/Fp83E6+XBehx9HnwtQ1w9HsTZTmRNEHAlJwJbz7AXhZAUW8BSJ2QMycXm5jUkTzQyH/H9H7/hMusFbXhaZ8ATKUXOWl6Abz9+AXAm8G58jiYdJK1nBp3JlwAf11Tjy3d76WjoWZQ7/6gAjAyKZ2UfLH5LYD8MbYBx9ph+jm+baoBY6Pd3h3du4tOPdNemK/93LoGXkPyZyyWTILVf7C++dHvTXYBdr9tP0+YBzWeALgHWUS8IOC+5AtbxCZGeRmOkNwNmC4GAInfzeQCGGNfqTMCe78ZzrPuQU8sIS7pJt2YdrYVr60vJ6ZfK0tnAURcEDBdL4DPBdiz2i49AQArlnEBcO5aKJxIoU6H51p4LoSf+u1QNnj+P6gAHBnEewHGJcx6yYGpvrHg/jvDR+6w33tdhSKw6NOw+y34v9Ph567bMZIiAJ3N9gEnsy+yQhOKWBdg91sw/gSoXmifRNzZ6gYpScIUTrUAPCL5iXqEc3v3AqSmTXsCkM4FwNiehedute9TewHAuST1ycfnnZucaKKbLydsH/K6Z1XCWvLHVDxCYTv/QdpeAN+ovb1rrECUTbFxnlAEtr9mZ3LyWwAF5Ynu3XTkROCtB+EX51qx9ejpcRbA4AqAxgCOBI56D5xwue1B8BJ8qk9ONiu9C6PsKJh+Llx+F1TMTHw/72p75+xogudvs2VJFoALBJ7+pUQKbFGlvYvVroVTPmtF4PU7bYQ77j7kWXO7pcfmDqSjtNpaCDlRNwmp65aLFPTud/cafkE6AQAe+0oiIOmlXvuJFifyIeIugDfOIkVsqubYOIDkpL/7gxWKrtbEMwE8wnlJT21m72qonO2CuFE7m7PncuWVJdyubP4/WAHwMi13rYBqZ0201Np9DLIAqAVwJFA83jboaHHCAvCb/2AbmeTYJxWJwPGXJrIAwd59T/4knPEVuOb3UH4MjJuV+H762XDUaXDilYmyRZ+GLS/YO13VibYrM1IA215JNK78MTa+0LQzUbdUvDtwOAolE+Di/7GfU014yOICuLt9wzYbWPz731hRTGXCXJteDT4XwPv93OR1q+bYRlyzNL3/D/ZO7uVcZEkEYu8aa1F4nPHVRM+D3wXI5v+DFb4J8+w2nlsBCVEbZBdALYAjjcrj7cNCTrgsuTySBx97EKrm9r2PyYvgi8uSy479sH35OeNGe/d65SfW3SiZCFc/DL+9PNG4zvyqjXJvfdmKTjriJrhrgAuuseLkH3fgEXcB0sQAPOZfY3sZ0nHUqXa6LEjc8QvHuYFIaSwAsGZ6JgHIifge9+ZzAfwzEDfX2jt0pU9wZ34App1lMz/zSqErknx8mfjY72w977vcxl48PKtHXYBRTmE53PBc+u+85xMOJgs/YV8eU06DTz6V6BrMLYT5V9tXJvwWgMekBYmHnvg57iPWTRmT0nXmCUD5jMyNH2DyKYn3nkiFQla8vCnUPfwWUiYBmHom/O17yXWAZAtg7zt26bcAROADt8LvrrPp2PvW2/JMOQAenhU1/gT7SHQvE9NL8lILQBl2qno93S07qRZANkomwNlf713uuQCzL8y+/YS5iUes++/YpZMTyUoe0WIrNHWbMwvA2d+w7s3y3yRH78N5NsL/yOfg3T/bMr+ggD1PX3T5B14qdl8C4DH+eBuUrd9qLay6LdaN8EY3DhIqAMrhp/pk+4Qjf8xhoFTMtHkP86/Jvl44al2k7a8mC8DpX04/IUvVHCcAGYKAoRBc9N9w/GUw1Zd3kVtgewbWPAazPwTHXZycmJVKPAbQXwFwIrtntXVBVtx7WCw8FQDl8FNaDV949dD2kV8Gn/hT/9Y96pTeAjDjfenXnXqmDWp6gcJ0hEK9B22d/Gmb9jv7ovTdh6lUzLAp4P2dx79yNiDWDdj2iu2l+Mgd/dt2AGgvgBI8Zn/Ydof2J2vu5E/BV1b1fvR6X5RNhnn/0L/GDzaYeOEPEt2tfZFbaEdCrnvCWg0f+13vuQoGAbUAlOAx+WTbqPtDKASh/L7XGw7e849wYCOcc1P6LtNBQAVAUUYq/t6Xw4S6AIoyilEBUJRRjAqAooxiVAAUZRSjAqAooxgVAEUZxagAKMooRgVAUUYxKgCKMopRAVCUUYwKgKKMYlQAFGUUowKgKKMYFQBFGcWoACjKKEYFQFFGMSoAijKKUQFQlFGMGGOGuw4HhYjUAlv7WK0C2DcE1TkYtG4Hx0it20itF8AUY0zaRxIdsQLQH0RkqTFm4XDXIx1at4NjpNZtpNarL9QFUJRRjAqAooxigi4Adw53BbKgdTs4RmrdRmq9shLoGICiKNkJugWgKEoWAisAInKBiKwTkQ0i8s1hrMdkEXlWRNaIyGoR+bIrv1lEdojISvfq47nXh61+W0RklavDUlc2VkT+KiLr3XLwH0rXd71m+c7NShFpFJGvDNd5E5FficheEXnbV5bxPInITe7aWyci/Xwi6NATSBdARHKAd4H3ATXAG8BVxph3hqEuE4AJxpjlIlIMLAM+Avw90GyMuW2o65RSvy3AQmPMPl/ZD4ADxpjvOfEcY4z5xjDWMQfYAZwCfIJhOG8ichbQDPzGGHOCK0t7nkTkOOB+YBEwEXgamGmM6R7KOveHoFoAi4ANxphNxphOYDFwyXBUxBizyxiz3L1vAtYAk4ajLgPgEuAe9/4erGANJ+cBG40xfSV+HTaMMc8DB1KKM52nS4DFxpgOY8xmYAP2mhxxBFUAJgHbfZ9rGAGNTkSmAvOB11zRP4rIW868HHIz22GAp0RkmYjc4MrGG2N2gRUwoHKY6uZxJfaO6jESzhtkPk8j8vpLR1AFQNKUDauvIyJFwMPAV4wxjcBPgaOBecAu4L+GqWqnG2MWAB8EvuBM3RGDiOQCFwO/c0Uj5bxlY8Rdf5kIqgDUAJN9n6uBncNUF0Qkgm389xljfg9gjNljjOk2xvQAv2CYTERjzE633As84uqxx8UuvBjG3uGom+ODwHJjzB4YOefNkek8jajrLxtBFYA3gBkiMs3dQa4EHh2OioiIAHcBa4wxP/SVT/Ctdinwduq2Q1C3QheYREQKgfe7ejwKXOtWuxb441DXzcdV+Mz/kXDefGQ6T48CV4pIVESmATOA14ehfn1jjAnkC7gQ2xOwEfiXYazHGVjz7y1gpXtdCNwLrHLlj2J7Coa6btOBN91rtXeegHJgCbDeLccO07krAPYDpb6yYTlvWBHaBXRh7/CfzHaegH9x19464IPDdf319QpkN6CiKP0jqC6Aoij9QAVAUUYxKgCKMopRAVCUUYwKgKKMYlQAFGUUowKgKKMYFQBFGcX8f0h4sv3Au90uAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUwAAAEJCAYAAAD/+x6AAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAyPUlEQVR4nO3deZzN9f7A8dfbGIyUIUsMoptoFeaKdCtaaLGlouWmjbppL6XyUyqltN0Wt3RbFCWFMZaSayv7coekEi03hkhIMTHL+/fH93s4Zs45852Zs868n4+HxznnO9/z/b7V9O7z/Szvj6gqxhhjilcp1gEYY0yisIRpjDEeWcI0xhiPLGEaY4xHljCNMcYjS5jGGONRRBOmiPwoImtEZJWIrHCP1RaRWSKy3n2t5Xf+AyKyQUTWiUgXv+N/RDJOY4zxIhotzE6qeqqqprufBwOzVbU5MNv9jIicAPQFTgS6AqNEJCkK8RljjCeVY3DPHsDZ7vsxwDzgfvf4eFXdB/wgIhuAdsBi3xdFpA4wFXhcVacHu0GdOnW0adOmkYjdGFOe7NwJP/zAStXtqlq3uNMjnTAV+FREFHhNVUcD9VV1C4CqbhGReu65acASv+9uco8BICL1gUxgiKrOKnwjERkADABo0qQJK1asiMTfxxhTHuTnw5AhMGIEnH46smjR/7x8LdIJs6OqbnaT4iwR+SbEuRLgmG/dZjLO4/tAVZ0f6MtuMh4NkJ6ebus9jTGB7doFV14JH38MAwbASy9B1aqevhrRPkxV3ey+bgMm4zxibxWRBgDu6zb39E1AY7+vNwI2u+/zgJVAF4wxprS+/hratYNZs+Bf/4LXXoMqVTx/PWIJU0QOE5HDfe+B84EvcR6r+7mn9QOmuO8zgb4iUlVEmgHNgWXuzxS4HmgpIoMjFbMxphybOhVOOw1++w3mzIGbby7xJSL5SF4fmCwivvu8p6qfiMhyYIKI3AD8BFwGoKprRWQC8BVOi3Kgqub7Lqaq+SLSF5gqIrtVdVQEYzfGlBcFBfDEEzB0KLRpA5MnQ+PGxX8vACmP5d3S09PVBn2MMfzxB1x7LUycCFdfDaNHQ0pKkdNEZKXf1MegYjGtyBhjIu/776FnT1i7Fp55Bu6+GyTQ2LJ3ljCNMeXPf/4Dl1/uvP/kEzjvvLBc1taSG2PKD1V47jno0gUaNoTly8OWLMESpjGmvMjJgWuugXvugR49YPFi+MtfwnoLS5jGmMS3cSOceSaMHQuPPgoffQSHHx7221gfpjEmsS1YAL17Oy3MKVOge/eI3cpamMaYxPXaa9C5M9SsCUuXRjRZgiVMY0wi2r/fWalz881wzjmwbBkcf3zEb2sJ0xiTWLZudZLka6/B/ffDtGmQmhqVW1sfpjEmcaxY4UxG37ED3n8f+vaN6u0tYRpj4kJGVjYjZ65j864cGqamMKhLC3q2Tjt4wrvvQv/+UL8+LFwIrVtHPUZ7JDfGxFxGVjYPTFpD9q4cFMjelcMDk9aQkZUNeXnO3MprroEOHZxWZgySJVjCNMbEgZEz15GTm3/IsZzcfJ4fv5jlx58Gzz3Hhx16MeWpt6BusTtJRIw9khtjYsL/ETxQzbQWv/zI6xMfo/4fvzLogtv58JTzSZn6DZqcfOijehRZC9MYE3WFH8EL67puIZPevZeq+bn0vWIEH55yPuC0OkfOXBfdYP1YC9MYE3WBHsEBRAu46/Nx3L74A/7bsAU393yQbYcfecg5m3flRCvMIixhGmOiLlDSq7FvL89Pe4bzNixjanpXHrtgINv2F61f2TC1aAHgaLFHcmNM1BVOes12ZJPxzt10+n4FvPQS3ZbN4MFerUlJTjrkvJTkJAZ1aRHNUA9hCdMYE3WDurQ4kAzP/m45U965m9p/7mbJq+Ph1ltBhJ6t03jykpNJS01BgLTUFJ685OSYDfiAPZIbY2KgZ+s0UGXLg8O4aeYbbGjwF354fSxdLjqtyHmxTJCFWcI0xkTfnj30fPpemPkB9OnDcW++yXHVq8c6qmLZI7kxJrp+/BE6doQJE2DECGdNeAIkS7AWpjEmmubOhcsuc5Y7zpgBXbvGOqISsRamMSbyVOGll5wNyerVczYnS7BkCZYwjTGR9uefcMMNcPvtcNFFsGQJNG8e66hKxR7JjanAii2pVlabN8MllzjbRwwdCg8/DJUSt51mCdOYCsq3ntu3RNFXUg0IT9JcvNhJlr//DhMnOu8TXOKmemNMmQQrqRaW4hZvvAFnn+2Mfi9ZUi6SJVjCNKbCClbEokzFLXJznZU6N94IZ53lDO6cdFLprxdnLGEaU0EFK2JR6uIWv/zijIK/8opTIX3GDKhduwwRxh9LmMZUUP7ruX1KXdwiKwvS053BnXffhWeegcrlb4ik/P2NjDGe+AZ2yjxK/v77zrShI4+EBQugbdsIRBsfLGEak6DCMSWoTMUt8vPhwQfh6afhjDPgo4+cHR3LMUuYxiSgiE8JKs7OnXDllfDJJ/CPf8ALL0CVKpG/b4xFvA9TRJJEJEtEprmfa4vILBFZ777W8jv3ARHZICLrRKSL3/E/Ih2nMYkkolOCivPVV9CuHcyeDa+9BqNGVYhkCdEZ9LkD+Nrv82Bgtqo2B2a7nxGRE4C+wIlAV2CUiCRhjCkiIlOCvJgyBU47zZmMPncuDBgQ2fvFmYgmTBFpBFwE/NvvcA9gjPt+DNDT7/h4Vd2nqj8AG4B2ha5XR0QWi8hFkYzbmHgX9ilBxSkogGHDoGdPaNkSVqxwSrRVMJFuYb4A3AcU+B2rr6pbANzXeu7xNGCj33mb3GMAiEh9YDowVFWnRzBmY+JeWKcEFef336F3b3jkEfj73+Gzz6BRo/DfJwFELGGKyMXANlVd6fUrAY75tixOxnl8v09VZwW53wARWSEiK3755ZeSB2xMAonafjcbNkCHDjB1Kjz/PIwZAymx27Ux1iI5St4R6C4iFwLVgCNEZCywVUQaqOoWEWkAbHPP3wQ09vt+I2Cz+z4PWAl0AeYHupmqjgZGA6SnpwfaG96YciXi+918+in06eNUF5o5E845J3L3ShARa2Gq6gOq2khVm+IM5sxR1auBTKCfe1o/YIr7PhPoKyJVRaQZ0BxY5rsccD3QUkQGRypmYwxOsd9nnoELLoDGjZ314JYsgdjMwxwBTBCRG4CfgMsAVHWtiEwAvsJpUQ5U1QPzJlQ1X0T6AlNFZLeqjopB7MZEXMRrVIaSkwP9+8O4cXDppfDWW1CjRnTunQBEtfw9vaanp+uKFStiHYYxJVZ4Qjo4gzlR2Y/7p5+gVy9nXfhjjzmreCTQ0EL5IyIrVTW9uPOs+IYxcSRmE9I/+8wpnrF+PWRmwkMPVZhkWRKWMI2JI1GfkK4K//qX00dZqxYsWwYXXxyZe5UDljCNiQMZWdl0HDGHYB1kEZmQvm8f3HQT3HILnH++U5qtZcvw36ccseIbxsRYoH5LfxGZkL5lizOos2iR01f56KOQZCuRi2MJ05gYC9Rv6ZMWiVHyZcucwZ1du2DCBLjssvBdu5yzhGlMjAXrnxRg4eDO4b3ZmDHOY3iDBk7rslWr8F6/nLM+TGNiLCqFNPLy4M474dprnaIZy5dbsiwFS5jGxFinlnVLdLzEtm+HLl3gn/+EO+5wljnWqROea1cw9khuTIzN/SZwsZhgx0vkiy+gRw9nkOett5wWpik1S5jGxFhp5l56Wj754YdOgkxNdSamt2sX6FKmBCxhGlNC4V7r3TA1hewAybFwH6bvvtm7chAO1j4ssp9Pfj4MHQpPPOGUZps40RnkMWVmfZjGlIBvzmT2rhyUg8kqIyu71Nf0UgzY/75AkQnuB5ZP/vab8wj+xBNw443ONhKWLMPGEqYxJRCJtd5eigGHmqvpU+279c5+OzNnwiuvwOjRULVqqeMyRdkjuTElEKm13sUVAy7u+p03LOPFac/CEdWd3RzPPLNM8ZjALGEaUwLF9TdGqpZlsPuiyi1LPuTez95ld8uTYOY0aNKkzPczgVnCNKYY/kmwZkoyyUlCbv7BXkRff2PhNeFFBmPKYFCXFkXWmx+2P4enZ7zAResWsrFrDxpPfA+qVy/TfUxo1odpTAiFB3l25eSCQq3qyUX6GyNZy7JwP2d6wS4WTf0/Llq/GEaOpPGMyZYso8BamMaEECgJ5hYo1atUJmvo+Yccj3QtywP9nLNnw+W3O3uFz5jhrOIxUWEtTGNCKEkSrJmSHPDcsK0JV4UXXnAS5FFHOevBLVlGlSVMY0IIluwqiRwy9zIjK5vdf+YWOS+5koSnluWff8J118Fdd0G3brBkCRx7bNmva0rEEqYxIQSaVA6Qr3rIhPVHMtdSEKBcenKSlH2UfNMmZ5rQmDHwyCPOyp3DDy/bNU2pWMI0JgTfYEtSgA3B/Ad0duUUbV0C7M0tKFsAixY5m5N9/TVMngwPPwyV7D/bWLF/8sYUo2frNPKDbEcdsc3JAF5/Hc4+22lNLlkCPXtG7l7GE88JU0QOi2QgxsSrjKxsgm046+vjrFU98IBPsOMh7d/vbEw2YAB07uxsKXHiiSW/jgm7YhOmiJwuIl8BX7ufW4nIqIhHZkycGDlzXcDdHAUODOg83O1EkpMOTavJScLD3UqY6LZuhXPPdba+HTQIpk93tr81ccHLPMzngS5AJoCqrhYRW6hqKoyASxJxKgb5BnR8r2VaFrlypfPYvX07jBsHV15ZxshNuHmauK6qG+XQTu/QZVOMKUeSRAL2YRYeCCqugEZI48Y55djq1YOFC6FNm9Jdx0SUlz7MjSJyOqAiUkVE7sV9PDemIgg24BPseInk5cG998LVVzsV0Zcvt2QZx7wkzJuBgUAasAk41f1sTIWQFmTyerDjnu3YARdeCM8+CwMHwn/+47QwTdwqNmGq6nZVvUpV66tqPVW9WlV/jUZwxsQDLxXRS+zLL50W5bx5zvShl1+G5FKMqJuo8jJKPkZEUv0+1xKRNyMalTExkJGVTccRc2g2eDodR8w5sIrHS0X0Epk8Gdq3hz17YP58p+/SJAQvgz6nqOou3wdV3SkirSMXkjGRE6zAb3G1LMs0oONTUADDhsGjjzqty0mTIK3sxYVN9HhJmJVEpJaq7gQQkdoev2dMXAmVFEPVsgxHxXR274ZrroEpU5ytb//1L6hWrezXNVHlJfE9CywSkY/cz5cBwyMXkjGlF2qLiFBJMaK1LNevd3Zy/PZb+Oc/4bbbIMDadBP/ik2YqvqOiKwAOuMsbrhEVb+KeGTGlFBxj9XBkl+wiekQhlqWn3wCfftC5crw6afOUkeTsIIO+ojIEe5rbeBn4D1gHPCzeywkEakmIstEZLWIrBWRYb7ricgsEVnvvtby+84DIrJBRNaJSBe/43+U/q9oKoKMrGzumbA65BYRJU1+ZaplqQpPP+1MGzr6aGd+pSXLhBdqlPw993UlsMLvj+9zcfYBnVW1Fc7cza4i0h4YDMxW1ebAbPczInIC0Bc4EegKjBKRooUIjSnE17IsrqJQp5Z1gxbRCKi0T81798JVV8H998Nllzkl2po1K+XFTDwJmjBV9WJx1kOeparH+P1ppqrHFHdhdfhahsnuHwV6AGPc42OAnu77HsB4Vd2nqj8AG4B2/tcUkToislhELirB39GUc4H6Jv01TE0hIyubiSuzAxbRCCY3X0u+gdn//gcdO8L48fDkk87rYVboq7wIOQ9TVRWYXNqLi0iSiKwCtgGzVHUpUF9Vt7jX3wL4ljakARv9vr7JPea7Vn1gOjBUVacHuNcAEVkhIit++eWX0oZsElCogRnfBPPikmpprl3E/PlOsd8ffoBp02DwYBvcKWe8LI1cIiJ/Lc3FVTVfVU8FGgHtROSkEKcH+s3yNQiScR7f71PVWUHuNVpV01U1vW7duqUJ1ySoYH2TSSIHJpiXdrTbU7+nKrzyilOWrU4dWLrU6bs05Y6XhNkJJ2l+JyJfiMgaEfmiJDdxJ77Pw+mb3CoiDQDc123uaZuAxn5fawRsdt/n4fSd2hZ5pohgSxefvbzVgSlFpRnt9rT8cd8+Z6XOrbfCBRc4ldFbhGHTMxOXvCTMC4BjcKYVdQMudl9DEpG6viWVIpICnAt8g1NXs597Wj9givs+E+grIlVFpBnQHFjm/kyB64GWIjLYQ8ymnPNfxjhy5jp6t00LuXQx2GZmPrWqJ3N1+yYlW/64ebOzhcSbb8KQIZCRATVrhuuvaOJQ0HmYIlIPeBA4FlgDPKmqu0tw7QbAGHekuxIwQVWnichiYIKI3AD8hDMRHlVdKyITgK9wWpQDVfVAp5Oq5otIX2CqiOxWVav6XkEFmm85cWV2yATnO37PhNUBR9OrV6nM4z1P9h7E0qXQq5ezgufDD+HSS0v+FzEJRzTIVAwR+QTnMfgznFbl4ap6bfRCK7309HRdscLLzCeTiDqOmBNwsnlaagoLB4ee69hs8PSg2038MMLj5Iu33oKbb3bWgU+ZAieXINGauCQiK1U1vbjzQj2SH6WqD6nqTFW9DTglfOEZU3rBVub4jgerOgTB+zIVipxbRG4u3H47XH89/O1vzmR0S5YVSqiEKW4pt9ruyp6kQp+NiYlAe4T7jvse17N35aAcXB7pS4Sh+jILn3uI7dvh/PPhpZfgrrucJY9HHhmuv5JJEKHWktfEeST3/+38r/uqOANBxkRdqC0jQhXYgIOT3IPt0xOwQtGqVc7mZD//DGPGOFWHTIUUNGGqatMoxmGMZ2mpKQEfy4XQj+v+A0Wh9uM5ZM7mBx/Addc5rckFC5yJ6abC8jKtyJi4MqhLi4C/uMUte/S60qdhagrk58MDDziVhtq0cforLVlWeFYI2MStULUtCyJ0z5TkJB48/Sjo1g0+/hgGDHD6LatUidAdTSKxhGniUnHV0cMlNSWZw6pWPpCUhzWvxLk39nTWg7/6Ktx0U9juZRJfqInrIUfCVXVH+MMxiS5Uq7AkSlMdvaRSkpN4pPuJB+PLzHT2B09Jgblz4YwzwnIfU36EamGuxOkWEqAJsNN9n4qzQscK/JlDFFfxvCRCDd4EG/QpCf/CHBQUwPDhMHQotG3r7OrYuHHxFzEVTqh6mL66lzOBbqpaR1WPxFn1MylaAZrEUdyUnpIINdeyuHXhXhSoOsnyjz+cIr9Dhzqty88/t2RpgvIySv5XVZ3h+6CqHwNnRS4kk6jCuZFYqLmWhfcJL42GqSnw3XfQoYNTNOPZZ+Gdd5zHcWOC8DLos11EhgBjcR7RrwZ+jWhUJiE1DPKoXJrSasEeu5NEaDZ4+iH9o6cO+5RdObmer52SnMTTqdvgr27BjE8+gfPOK3GMpuLx0sK8AqiLU3l9svv+ikgGZRJTsLqUpdlIrFPLwEWg81WLLHl8pPuJRX6RK8GBcm1w8BE/rWY1JuxdTMeBVznFM5Yvt2RpPPOyze4O4A4RqeG3R48xRfjv/13WUfK53xS/zYivf3RQlxYkJQkF+Qcf45OShPSjax9asi0nx5lXOXYsXHKJs8yxRo0Sx2YqrmITpoicDvwbqAE0EZFWwE2qekukgzOJp2frtBInyEBTkbz2e2bvymHkzHXk5h/a5+nbwOxALBs3OvUrV66ERx+Fhx6CSrbQzZSMlz7M53G2hsgEUNXVInJmRKMyFUawqUip1ZPZubf4fskkkeIHmxYsgN69nRbmlCnQvXvY4jcVi6f/xarqxkKHSr79njEBBJuKtMtDsgSnTzPYoFLD1BR47TXo3NnZOmLpUkuWpky8JMyN7mO5ikgVEbkX+DrCcZkKIljr0Ov+4WnuI3zhwaYjKhUwdvmbTmX0c8+FZcvg+OPLGK2p6LwkzJuBgTh7hG8CTgWs/9KERWmmHPn4RuALz8s8KSmHuR8/RrNJ45y9wadOhdTUsMVsKi4vfZgtVPUq/wMi0hFYGJmQTEUyqEuLQ/owS8J/07MDg00rVjjFfnfsgPHjoU+fMEdsKjIvCfMloI2HY8Z45j8ynlo9GVBycktWtK3IaPy770L//nDUUbBoEZx6atjiNQZCVyvqAJwO1BWRu/1+dARQtoW8pkIIVrmo8Mj4zr25JV7imOb/KJ+XB/fdB88/7+wTPmEC1A088d2YsgjVwqyCM/eyMnC43/HdgG3CbEIKNF3org9WcecHqwLup+N1kAcKrR769VfnsXv2bLjtNmdNeHJymP4Wxhwq1J4+84H5IvK2qv4vijGZciDQdCFfUgy1n05hlQSuPK0Jc7/5pejqoTVroEcPyM6GN9909t4xJoK8jJL/W0RSfR/crXZnRi4kUx6Eq8hvsDJvTJzoVBr680+YP9+SpYkKLwmzjqru8n1Q1Z1AvYhFZMqFkk4XSq4kVE8u+uuYW6CMW/LTgX3GN+/cw5bbB8Gll8LJJzuj4u3bhylqY0LzkjALRKSJ74OIHE3JupxMBVTiIr8Ce4OMkvt+2Wrs28voSY/zjwXvMzW9K8ybBw0bljlWY7zyMq3oIWCBiMx3P58JDIhcSKY86Nk6jRX/28H7Szd66rPMzdeAg0E+x/y6idGTHufoXVv4v/NuZmzri+hWtWq4wzYmJC/l3T4RkTZAe5w9fe5S1e0Rj8zEFa+bm/nOK82eO/mqpCQnHTJYJMBZ3y3nxcyR5CZV5u99HmNJk1MOnVZkTJQEfSQXkZbuaxucTdA2A9k4Jd5s0noF4psi5OtH9C/eG+y80khLTTlkiWNazWq8lj2LNz96lI2pR9G93wssaXJKqYsSG1NWoVqY9wD9gWcD/EyBzhGJyMSdUJub+VqZGVnZ3DNhdYmmDPnzXxfes3Ua7NkD118PEyawqUsPbjt9AJv36oFiG6UpSmxMWYWah9nffe0UvXBMPCqu3qSvZVnaZJkkQu+2foWHf/zRWQ/+xRfw1FM0GjSIOcGmFxkTRaGWRl4S6ouqalvtVhDBNjdToPWjn7Jrb26Zpk3kqzN1CODxmtudbW/z8mDGDOjatQxXNia8Qj2Sd3Nf6+GsKZ/jfu4EzMP2Jq8wOrWsy7glPwVMil6qonuhqiS//DIF896g0nHHOZXRmzcPy7WNCZeggz6qep2qXofTkDhBVXuram/gRC8XFpHGIjJXRL4WkbUicod7vLaIzBKR9e5rLb/vPCAiG0RknYh08Ttum6/FSEZWNhNXZkd04m3VvP2MnPFPHp49moXHnQZLlliyNHHJy8T1pqq6xe/zVuA4D9/LA+5R1eNxpiQNFJETgMHAbFVtDsx2P+P+rC9OQu4KjBIRq4oUY4EGfMKp3u+/8sF7g7nsy//wz9OvoF+3wXDEERG7nzFl4SVhzhORmSJyrYj0A6YDc4v7kqpuUdX/uu9/x9nWIg3oAYxxTxsD9HTf9wDGq+o+Vf0B2AC087+miNQRkcUicpGHuE0YhGtNeCBtsr9m2pg7ab79J27q+SDP/+0qGtQ6LGL3M6asvExcv1VEeuGs8AEYraqTS3ITEWkKtAaWAvV9LVZV3SIivnXpacASv69tco/5rlEfZ+fKIao6K8A9BuCuQGrSpEnhH5tSqpmSzK6c8PRT+rt89ac8NmsUPx9eh6v7PMa3dZva/EoT97wsjQT4L/C7qv5HRKqLyOFuq7FYIlIDmAjcqaq7Jfj0kEA/8HWdJeM8vg90y84VPVF1NDAaID093da6h0FGVja7/wxvsqycn8fQ2a9zTdZ0Pmvamif+PpT1+5NtfqVJCMUmTBHpj9Nyqw38BafV9ypwjofvJuMky3F+05C2ikgDt3XZANjmHt8ENPb7eiOc1UXg9IeuxNkfPWDCNOGTkZXNI5lrw96yPHLPLkZNGcFpG79k9F978Uyn6/j20W7Ff9GYOOGlD3Mg0BGn0jqquh4P5d3EaUq+AXytqs/5/SgT6Oe+7wdM8TveV0SqikgzoDmwzP2ZAtcDLUVksIeYTSlkZGVz6rBPufODVWFPlif+vIEp79xFqy3fcsfF9/BE5xvYL15+/YyJH14eyfep6n7fo7SIVMZbebeOwN+BNSKyyj32IDACmCAiNwA/AZcBqOpaEZkAfIXTohyoqgeGZ1U1X0T6AlNFZLeqjvLyFzTeZGRlM+ij1eTmh783o/tX83nq4xfZkXIEl171NF8edSwAqSm2lYRJLF4S5nwReRBIEZHzcPYkn1rcl1R1AYH7JSHI47yqDgeGBzhew33dj/NYbsJs2NS1YU+WlQryuW/+GG5eNomljU7klp4P8OthqQd+bqsdTaLxkjDvB24E1gA3ATOAf0cyKBN94VqxA07VoYU3tebbcy7muKxFvNv6Qh49pz+5SYe2KHeF8Z7GREPIhCkilYAvVPUk4PXohGSiJSMrm2FT14Y1WQrw6LFAu3Yc97//kTXkKR7JOylgYY6SbmNhTKyF7HVX1QJgtf8WFaZ8yMjK5u4Jq8KaLAFu+/1LzrmuB/z+O8ydS+vH7uPZy1sV2a7C5lyaROTlkbwBsFZElgF7fAdVtXvEojIR90jmWgrC2GUpWsDtC8dz18L32HliK2p9Mg0aNQI4MLfSS8V2Y+KZl4Q5LOJRmKgL57Shw/bt5bnpz9Fl/RImntSZly8fxFw3WfocKAxsTAILVQ+zGnAzcCzOgM8bqpoXrcBM5BTeWqIsjt65mdcnPs4xOzbxaOf+vJneHdkTuWIdxsRSqBbmGCAX+By4ADgBuCMaQZnIGZKx5kCx3rI68/uVvJT5NAVSiWsuf5RFTU8FbDDHlF+hEuYJqnoygIi8wcFVNyaBRGSZoyr9l01m8Py3+bZOEwZcMoSNqUcBNphjyrdQCfPAf2GqmheiaIaJUxlZ2Qz6cDW5YRzdqZb7JyM+eYmeX81nWoszGHThneRUqQY4U4qevORk66s05VaohNlKRHa77wVnpc9u972qqlV5jXMjZ64La7JsuHsboycN54St3/P0mdcwqv1lRZbrWLI05VmoXSOt2nkCysjKPjB9J5wLHdtt/JJRGU9SJS+XG3v/H3OObVfkHOu7NOWd13qYJgH4trsN65YSqlydNYOHZ4/mp9QGDLjkIX5tdAwpeQWH3Mf6Lk1FYAmzHAn3/jtV8nIZNutfXPHFp8z+y1+5s9u95NU4gie7n3jgfjYR3VQkljDLkUB7h5dW3T928OrkJ2i7+Rte7nA5z51xFZJUmWf9BnUsQZqKxhJmOeArohEurTav47XJwzli3x5u6TGYGS3PICU5yUbATYVnCTPBhbvfsvea2Twx82W21ahN76tH8nW9Y0gSsWRpDJYwE5ZvNDxcj+FJBfk8NOcNrl+ZycKjT+HW7vezs3pNa1ka48cSZoz5TwMqbvDEP0kK3vYJ8aLW3t94OfMpOv7vC95s253hnW8gv5Izq8ySpTEHWcKMocKP09m7cnhg0hqg6IBK4XPDlSyP3/Y9oycNp94fO7jnwruYePLB3UPSUlMsWRrjx7bti6FA04BycvMZOXOdp3PL6sJvFjBx7CCS83O5/MoRhyRLm1dpTFHWwoyhzUH6HwMdD+eUoUoF+dy9YBy3Lp7AyoYtubnXg/xSozZJIhSo2rxKY4KwhBlDDVNTAibCwksMM7Kyw9Znefi+Pbww9RnO+W4577XqwiPn3sz+ysk2uGOMB/ZIHkODurTwtNfNyJnrwpIs//LrRjLeuZszf/gvQ86/hQe73Mr+yskI0LutVUQ3pjjWwoyhYHvdAHQcMYfNu3JIrZ4clo3KOm9YxgtTn2F/5WSu6jucZY1POvAzBeZ+80uZ72FMeWcJM8YK73VTeDS8zMlSlYGLJ3DP52NZW/8YbrrkITYfUa/IacH6U40xB1nCjDPhHA2vvj+HkTNe4KJ1C8k44SxWD32arau2g+0RbkypWMKME+HeSqLRrp95fdLjHLf9J4affT0fnX05WX3a0eq4okspbQqRMd5YwowD4d5K4vQfV/HKlKeopAVcd+nDLG/Rjie7O32Wtke4MaVnCTMODJu6NjzJUpXrV2Ty4Nw3+O7IRs7mZLUb8lyh6UK2R7gxpWMJM8aGZKwJyyh41dx9PPHpK/T+cg4zm7fn7ovuZk/V6qRWS7bkaEyYWMKMoYys7LDsEX7U7u28Nnk4rX5ez/Mdr+TFjn1RcabY/hbO7XWNqeAsYcZQOCakt930Fa9mPEFK7j76XzKEWc3bH/JzG/02JnwsYcZQWdeHX7HqE4bNepXsmnW5ou8TbKjT5JCf2+i3MeFlCTNGhmSsKfV3k/Nzefg/o7l61cfMb9aG27rfx+5qNQBIEiFflTQb/TYm7CKWMEXkTeBiYJuqnuQeqw18ADQFfgQuV9Wd7s8eAG4A8oHbVXWme/wPVa0RqTijoXCR4E4t6zK2lH2Xdfbs5JWMEZy2aS2vntabZ866hmf6trXEaEwURLL4xttA10LHBgOzVbU5MNv9jIicAPQFTnS/M0pEkigHfEsds3floDiP4aVNlif9vIHMMXdxys8buL3bIJ4753pLlsZEUcQSpqp+BuwodLgHMMZ9Pwbo6Xd8vKruU9UfgA1AO/8vikgdEVksIhdFKuZICNdSxx5r5/LRuPsoEOG6G5+j87A7+Hb4hZYsjYmiaPdh1lfVLQCqukVEfFUg0oAlfudtco8BICL1gUxgiKrOilawZRGuTcqSCvK5f97bDFg+maWNT+LXt8Yy/pxWYYrSGFMS8TLoIwGO+WbcJOM8vg9U1flBLyAyABgA0KRJk2CnRUW4tr6tmfM7L2U+zZk/ZjGmzUU83rk/6y1ZGhMz0S4gvFVEGgC4r9vc45uAxn7nNQI2u+/zgJVAl1AXVtXRqpququl169YNb9QlFI7H8ON++ZHMd+7itI1ruK/r7Tx83j+od+ThYYrQGFMa0U6YmUA/930/YIrf8b4iUlVEmgHNgWXuzxS4HmgpIoOjGWxpZGRll/kxvMu6RUx+915ScvdxxRVPMqHV+Tan0pg4EMlpRe8DZwN1RGQT8DAwApggIjcAPwGXAajqWhGZAHyF06IcqKoHmmiqmi8ifYGpIrJbVUdFKu6y8D2Kl5ZoAXcueJ87Fr3PqgbHcVOvB9l6eB3A9gc3Jh5ELGGq6hVBfnROoIOqOhwYHuB4Dfd1P8U8lsdSRlY290xYTX6A4rxe1Ni3l+enPct5G5Yy4eRz+b/zb2Ff5SqA7Q9uTLyIl0GfhOZrWZY2WTbdkc3rkx6n2Y5sHj73Jsa0uRjEGQcToFPL2PbJGmMcljDDoCyDPGd9v5KXMp8mr1ISf+/zOIuPPuWQnyswcWU26UfXtlamMTFmCdOjwssb/ddpl2oDMVVuXjqR++aP4Zt6TRlwyRA21awf8NSc3HxGzlxnCdOYGLOE6UHheZXZu3IODO70bJ1Gw9SUEo2MV8v9k6c/fpHuX3/GtJZ/Y9AFd5BTpVrI79iujsbEXrSnFSWkQI/cvlYflKyPMe23bUwcex8Xf/05T53Vj1u731dssgSra2lMPLAWpgfBWnebd+WQkZXN+0s3erpO+5++4JWMESQX5HP9pUOZ95e/AlBJoGZKMrv25lIzJZk9+/PIzT84gGRzMI2JD5YwPQj2yJ1aPdnb6Lgq1/x3GkNnv86PtRrSv/f/sf+YY5EguzaG6i81xsSOJUwPBnVpEXAv7z9z88nJLQj53Sp5uTz26Sj6rJnFrGPbcdfF91Kz/pEsHNw56HdsV0dj4pMlTA8K7+WdWj252GRZCWj8506e//Bx2mxex4sd+vD8366iWpVke7w2JkFZwvTI1+o7OGIeumXZ9ud1vDP9aZL3/M5DVz3Me43+ao/XxiQ4S5gl4HX542VffMrjn45ia40j2fD2FIZffm7RNZ/GmIRjCdODjKxshk1dy869off4rpyfx5A5/+ba/05jwdGtuLXH/exbk8eTzbOtVWlMOWAJsxheiwHX3vsbr0wZQYef1vD6X3sy4uzryK+UBLZKx5hywxJmMbysEz9h6/eMnvQ4dffs5K6L7mbySYeOgNsqHWPKB0uYxSgu2XX/5nOem/lP9tesRZ9eI1l11LFFzrFVOsaUD7Y0shjBkl2lgnwe+nwML055ispt21J9dRbX3nEpKcmH7g5sq3SMKT8sYRZjUJcWRZLgEX/+wTuTH6f/og/hpptgzhyoX5+erdN48pKTSUtNQXAK/1qldGPKD3skD8J/eWJq9WSqVq7Ebzm5dNi/jdGTH6PG5o3w6qtOwvRjq3SMKb8sYQZQeGR8595cUpKTeP+obbQfegekpMDcuXDGGTGO1BgTTZYwAyg8Mi5awI3z3qf9gnHQti1MngyNG4e4gjGmPLKEGYD/yPhh+/byzIwXuODbRUw+sRO9Pp/utDCNMRWOJcwAfOXcmuzcwuuTHuPYXzfxWKcb+OS8K+hlydKYCstGyQMY1KUF52xcTeY7d1H/jx1cc/mjvNfxUgZ1bRnr0IwxMWQtzMJU6TlnPD3G/x/f1z2a63o+RH7TZjxpVYaMqfAsYfrLyYEBA2DsWKR3b/7y9tt8VqNGrKMyxsQJeyT32bgR/vY3GDsWHnsMJkwAS5bGGD/WwgT4/HO49FKnhZmZCd26xToiY0wcshbmq69C585QsyYsXWrJ0hgTVMVNmPv3O8sa//EPOO88WLYMjj8+1lEZY+JYxUyYP//stCpHj4bBg2HqVEhNjXVUxpg4V/H6MJcvh169YOdO+OADuPzyWEdkjEkQFauF+c47zkh45cqwcKElS2NMiVSMhJmXB3ffDf36QYcOTivz1FNjHZUxJsGU/0fyX3+FPn1g9my4/XZ45hlITo51VMaYBFS+E+aaNdCjB2Rnw5tvwnXXxToiY0wCi7tHchHpKiLrRGSDiAx2j80TkfQSXWjiROfx+88/Yf58S5bGmDKLq4QpIknAK8AFwAnAFSJyQokvtHmzs3Ln5JNh5Upo3z7MkRpjKqK4SphAO2CDqn6vqvuB8UAP3w9FpJKIjBGRx0NeZcsWuOEGmDcPGjSIaMDGmIoj3vow04CNfp83Aae57ysD44AvVXV44S+KyABggPtxn7zxxpe88UYkYw2XOsD2WAdRAokUr8UaOYkUr5dYj/ZyoXhLmBLgmLqvrwETAiVLAFUdDYwGEJEVqlqyPs8YSaRYIbHitVgjJ5HiDWes8fZIvgnw312sEbDZfb8I6CQi1aIelTHGEH8JcznQXESaiUgVoC+Q6f7sDWAG8KGIxFvL2BhTAcRVwlTVPOBWYCbwNc4j+Fq/nz8H/Bd4V0RCxT46ooGGVyLFCokVr8UaOYkUb9hiFVUt/ixjjDHx1cI0xph4ZgnTGGM8KhcJM2zLKcMXz5sisk1EvvQ7VltEZonIeve1lt/PHnBjXyciXfyO/xGFWBuLyFwR+VpE1orIHfEar4hUE5FlIrLajXVYvMbqd58kEckSkWkJEOuPIrJGRFaJyIp4jldEUkXkIxH5xv3d7RCVWFU1of8AScB3wDFAFWA1zrLKeUB6jGI6E2iDM8ned+xpYLD7fjDwlPv+BDfmqkAz9++S5P7sjyjE2gBo474/HPjWjSnu4sWZp1vDfZ8MLAXax2OsfjHfDbwHTIvn3wP3Pj8CdQodi8t4gTHAje77KkBqNGKN+L+EKPyD6wDM9Pv8gPtnHpCO04oeAzwe5biacmjCXAc0cN83ANb5x+t33kygg/+/TJyVCouBi6IQ9xTgvHiPF6iOM2PitHiNFWce8WygMwcTZlzG6l7/R4omzLiLFzgC+AF30DqasZaHR/JAyynT3Pe+5ZTfquqQaAdWSH1V3QLgvtZzj4eKHxGpD0wHhqrq9EgGKCJNgdY4Lbe4jNd9xF0FbANmqWrcxgq8ANwHFPgdi9dYwVlV96mIrHSXGsdrvMcAvwBvud0d/xaRw6IRa3lImMUtpwy49jyOhIo/GaeFcp+qzopoECI1gInAnaq6O9SpAY5FLV5VzVfVU3Fab+1E5KQQp8csVhG5GNimqiu9fiXAsWj/HnRU1TY41cIGisiZIc6NZbyVcbq8/qWqrYE9OI/gwYQt1vKQMBNlOeVWEWkA4L5uc4+Hij8PWAl0IYJEJBknWY5T1UnxHi+Aqu7C6XbpGqexdgS6i8iPOFW3OovI2DiNFQBV3ey+bgMm41QPi8d4NwGb3KcLgI9wEmjEYy0PCTNRllNmAv3c9/1w+gp9x/uKSFURaQY0B5a5P1PgeqCluKP/4SYigvPP6Wt1VlLFbbwiUldEUt33KcC5wDfxGKuqPqCqjVS1Kc7v5BxVvToeYwUQkcNE5HDfe+B84Mt4jFdVfwY2ikgL99A5wFdRiTUSncfR/gNciDO6+x3wkHtsHu4oOTAMeB+oFKV43ge2ALk4/3e7ATgSp+m/3n2t7Xf+Q27s64AL/I77OqSr4HRU3xKBWM9wf2m+AFa5fy6Mx3iBU4AsN9YvcfqciMdYC8V9NgcHfeIyVpx+wdXun7V+/x3Fa7ynAivc34UMoFY0YrWlkcYY41F5eCQ3xpiosIRpjDEeWcI0xhiPLGEaY4xHljCNMcYjS5gmpkSkl4ioiLT0cO6dIlK9DPe6VkReDnL8F7dKz1ci0j/I97tHah6kSQyWME2sXQEswJncXZw7cYpuRMIH6iy5PBt4wl1ffICIVFbVTFUdEaH7mwRgCdPEjLt+vSPOxP6+fseTROQZtzbjFyJym4jcDjQE5orIXPe8P/y+c6mIvO2+7yYiS93CDP8pnPxCUWdZ4HfA0SLytog8597vKf8WqojUF5HJ4tTmXC0ip7vHrxanZucqEXlNRJLK+I/JxBFLmCaWegKfqOq3wA4RaeMeH4BTt7C1qp6Cs8b9RZz1v51UtVMx110AtFenMMN4nIpBnojIMTirXja4h44DzlXVewqd+iIwX1Vb4axjXisixwN9cIpYnArkA1d5vbeJf7FeX20qtitwSqCBk9iuwKlxeS7wqjq7iKKqO0p43UbAB24Bhio4tROL00dEzgD2ATep6g5nmT0fqmp+gPM7A9e48eUDv4nI34G2wHL3uykcLABhygFLmCYmRORInKRzkogoTuV8FZH7cMpxeVmz63+Of0Wql4DnVDVTRM4GHvFwrQ9U9dYAx/d4+K6PAGNU9YESfMckEHskN7FyKfCOqh6tqk1VtTFOS/AM4FPgZl+FKRGp7X7nd5xtNHy2isjx4uxR38vveE0g233fj8iYDfzDjS9JRI5wj10qIvV8cYvI0RG6v4kBS5gmVq7AqbnobyJwJfBv4CfgCxFZ7R4DGA187Bv0wSkaOw2Yg1MdyucRnJJ+nwPbIxI93IFTa3UNTj3FE1X1K2AITtXyL4BZOFslmHLCqhUZY4xH1sI0xhiPLGEaY4xHljCNMcYjS5jGGOORJUxjjPHIEqYxxnhkCdMYYzz6fxvixaLs9rb+AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_rmse = history.history['rmse']\n",
    "val_rmse = history.history['val_rmse']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_rmse, label='Training RMSE')\n",
    "plt.plot(epochs_range, val_rmse, label='Validation RMSE')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "\n",
    "'''How far are predictions from real values?'''\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def format_tick_labels(x, pos):\n",
    "    return '{:.0f}k'.format(x/1000)\n",
    "\n",
    "predictions = model.predict(test_examples)\n",
    "\n",
    "xlims = [0, max(test_labels)*1.1]\n",
    "ylims = [0, max(predictions)*1.1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(test_labels, predictions)\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "ax.set_xlim(xlims)\n",
    "ax.set_ylim(ylims)\n",
    "ax.set_xlabel('Actual Price')\n",
    "ax.set_ylabel('Predicted Price')\n",
    "\n",
    "ax.plot(xlims, ylims, 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold CV Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:01:58.771360: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 101276.9141 - rmse: 101016.1172"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:01:59.988228: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 55759.26562, saving model to ./ckpt/5_class_lr005/val_rmse_55878.hdf5\n",
      "70/70 [==============================] - 2s 18ms/step - loss: 101276.9141 - rmse: 101016.1172 - val_loss: 55759.2656 - val_rmse: 55878.2852\n",
      "Epoch 2/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 53699.6992 - rmse: 53699.6992\n",
      "Epoch 2: val_loss improved from 55759.26562 to 50398.23828, saving model to ./ckpt/5_class_lr005/val_rmse_50269.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 54629.2852 - rmse: 54733.3594 - val_loss: 50398.2383 - val_rmse: 50268.8984\n",
      "Epoch 3/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 47985.3516 - rmse: 47985.3516\n",
      "Epoch 3: val_loss did not improve from 50398.23828\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 47690.5352 - rmse: 47936.6758 - val_loss: 51784.5195 - val_rmse: 51601.8203\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 43200.7383 - rmse: 43315.7891\n",
      "Epoch 4: val_loss improved from 50398.23828 to 41578.92578, saving model to ./ckpt/5_class_lr005/val_rmse_41513.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 43200.7383 - rmse: 43315.7891 - val_loss: 41578.9258 - val_rmse: 41512.5664\n",
      "Epoch 5/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 44101.5000 - rmse: 44101.5000\n",
      "Epoch 5: val_loss did not improve from 41578.92578\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 44101.0664 - rmse: 44103.1953 - val_loss: 44944.7461 - val_rmse: 44929.3125\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 40210.0898 - rmse: 40160.0938\n",
      "Epoch 6: val_loss did not improve from 41578.92578\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 40210.0898 - rmse: 40160.0938 - val_loss: 50620.9375 - val_rmse: 50518.9805\n",
      "Epoch 7/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 38321.6016 - rmse: 38321.6016\n",
      "Epoch 7: val_loss improved from 41578.92578 to 36729.53125, saving model to ./ckpt/5_class_lr005/val_rmse_36641.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 38356.5156 - rmse: 38477.4922 - val_loss: 36729.5312 - val_rmse: 36641.2383\n",
      "Epoch 8/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 34997.9922 - rmse: 34997.9922\n",
      "Epoch 8: val_loss improved from 36729.53125 to 35604.27344, saving model to ./ckpt/5_class_lr005/val_rmse_35534.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 34895.6445 - rmse: 34854.1523 - val_loss: 35604.2734 - val_rmse: 35534.0508\n",
      "Epoch 9/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 36127.8828 - rmse: 36127.8828\n",
      "Epoch 9: val_loss did not improve from 35604.27344\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 36148.3633 - rmse: 36162.1680 - val_loss: 43142.8633 - val_rmse: 43099.2773\n",
      "Epoch 10/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 34731.7852 - rmse: 34731.7852\n",
      "Epoch 10: val_loss improved from 35604.27344 to 33494.91016, saving model to ./ckpt/5_class_lr005/val_rmse_33409.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 34653.4258 - rmse: 34854.4648 - val_loss: 33494.9102 - val_rmse: 33408.8906\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 33433.3125 - rmse: 33456.0391\n",
      "Epoch 11: val_loss improved from 33494.91016 to 32134.09375, saving model to ./ckpt/5_class_lr005/val_rmse_32059.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 33433.3125 - rmse: 33456.0391 - val_loss: 32134.0938 - val_rmse: 32059.1816\n",
      "Epoch 12/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 32838.9961 - rmse: 32838.9961\n",
      "Epoch 12: val_loss did not improve from 32134.09375\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 32814.1680 - rmse: 32836.8555 - val_loss: 34797.6445 - val_rmse: 34712.4805\n",
      "Epoch 13/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 29309.9824 - rmse: 29309.9824\n",
      "Epoch 13: val_loss improved from 32134.09375 to 30095.25586, saving model to ./ckpt/5_class_lr005/val_rmse_29946.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 29283.3145 - rmse: 29333.4297 - val_loss: 30095.2559 - val_rmse: 29946.0332\n",
      "Epoch 14/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 28936.4277 - rmse: 28936.4277\n",
      "Epoch 14: val_loss did not improve from 30095.25586\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 29552.4297 - rmse: 29567.9414 - val_loss: 34412.2578 - val_rmse: 34270.1172\n",
      "Epoch 15/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 29528.7559 - rmse: 29528.7559\n",
      "Epoch 15: val_loss improved from 30095.25586 to 28866.91211, saving model to ./ckpt/5_class_lr005/val_rmse_28664.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 29647.5117 - rmse: 29685.0254 - val_loss: 28866.9121 - val_rmse: 28664.0859\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 28041.3691 - rmse: 28041.3691\n",
      "Epoch 16: val_loss did not improve from 28866.91211\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 28241.6445 - rmse: 28300.5684 - val_loss: 32723.3652 - val_rmse: 32676.1934\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25917.7070 - rmse: 25920.7988\n",
      "Epoch 17: val_loss improved from 28866.91211 to 26955.58203, saving model to ./ckpt/5_class_lr005/val_rmse_26777.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25917.7070 - rmse: 25920.7988 - val_loss: 26955.5820 - val_rmse: 26777.1777\n",
      "Epoch 18/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25612.6074 - rmse: 25612.6074\n",
      "Epoch 18: val_loss did not improve from 26955.58203\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25897.8750 - rmse: 25966.7402 - val_loss: 29467.4746 - val_rmse: 29346.1680\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 24405.2910 - rmse: 24405.2910\n",
      "Epoch 19: val_loss improved from 26955.58203 to 25732.70703, saving model to ./ckpt/5_class_lr005/val_rmse_25703.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 24312.0488 - rmse: 24296.3027 - val_loss: 25732.7070 - val_rmse: 25703.4844\n",
      "Epoch 20/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 24316.0840 - rmse: 24316.0840\n",
      "Epoch 20: val_loss did not improve from 25732.70703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 24328.8438 - rmse: 24341.7383 - val_loss: 28825.6016 - val_rmse: 28713.4297\n",
      "Epoch 21/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23857.5703 - rmse: 23857.5703\n",
      "Epoch 21: val_loss improved from 25732.70703 to 24452.88672, saving model to ./ckpt/5_class_lr005/val_rmse_24365.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23773.9688 - rmse: 23765.5859 - val_loss: 24452.8867 - val_rmse: 24365.1855\n",
      "Epoch 22/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 26688.6934 - rmse: 26688.6934\n",
      "Epoch 22: val_loss did not improve from 24452.88672\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26624.4258 - rmse: 26627.4492 - val_loss: 42805.1680 - val_rmse: 42823.9492\n",
      "Epoch 23/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23757.9746 - rmse: 23757.9746\n",
      "Epoch 23: val_loss improved from 24452.88672 to 23576.20703, saving model to ./ckpt/5_class_lr005/val_rmse_23478.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23863.4766 - rmse: 23932.0684 - val_loss: 23576.2070 - val_rmse: 23478.0488\n",
      "Epoch 24/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24625.1758 - rmse: 24729.1699\n",
      "Epoch 24: val_loss did not improve from 23576.20703\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 24625.1758 - rmse: 24729.1699 - val_loss: 25445.7305 - val_rmse: 25337.8203\n",
      "Epoch 25/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 26006.9316 - rmse: 26006.9316\n",
      "Epoch 25: val_loss did not improve from 23576.20703\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25976.2051 - rmse: 25955.4805 - val_loss: 23949.5410 - val_rmse: 23816.9492\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21865.2969 - rmse: 21865.2969\n",
      "Epoch 26: val_loss did not improve from 23576.20703\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22038.7031 - rmse: 22155.6562 - val_loss: 30888.3594 - val_rmse: 30921.2227\n",
      "Epoch 27/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21861.1543 - rmse: 21861.1543\n",
      "Epoch 27: val_loss did not improve from 23576.20703\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21937.7715 - rmse: 21989.4453 - val_loss: 24539.5000 - val_rmse: 24448.8262\n",
      "Epoch 28/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22546.0527 - rmse: 22546.0527\n",
      "Epoch 28: val_loss did not improve from 23576.20703\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22396.7578 - rmse: 22429.6797 - val_loss: 25322.9727 - val_rmse: 25265.1270\n",
      "Epoch 29/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22871.8926 - rmse: 22871.8926\n",
      "Epoch 29: val_loss did not improve from 23576.20703\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22814.5078 - rmse: 22877.2188 - val_loss: 24323.7480 - val_rmse: 24163.1484\n",
      "Epoch 30/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20798.9219 - rmse: 20798.9219\n",
      "Epoch 30: val_loss improved from 23576.20703 to 21566.39062, saving model to ./ckpt/5_class_lr005/val_rmse_21480.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20685.5742 - rmse: 20697.2051 - val_loss: 21566.3906 - val_rmse: 21479.7539\n",
      "Epoch 31/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23044.3027 - rmse: 23044.3027\n",
      "Epoch 31: val_loss did not improve from 21566.39062\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23101.3672 - rmse: 23125.2207 - val_loss: 24290.9062 - val_rmse: 24169.8203\n",
      "Epoch 32/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20714.4102 - rmse: 20714.4102\n",
      "Epoch 32: val_loss did not improve from 21566.39062\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20750.2402 - rmse: 20774.4043 - val_loss: 28392.4238 - val_rmse: 28344.4668\n",
      "Epoch 33/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 22184.8984 - rmse: 22184.8984\n",
      "Epoch 33: val_loss did not improve from 21566.39062\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22158.2773 - rmse: 22173.3086 - val_loss: 21838.2891 - val_rmse: 21756.7480\n",
      "Epoch 34/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19841.6113 - rmse: 19841.6113\n",
      "Epoch 34: val_loss did not improve from 21566.39062\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19787.2012 - rmse: 19808.0117 - val_loss: 23376.7910 - val_rmse: 23348.3438\n",
      "Epoch 35/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21629.7812 - rmse: 21629.7812\n",
      "Epoch 35: val_loss did not improve from 21566.39062\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21619.6426 - rmse: 21612.8027 - val_loss: 22282.6133 - val_rmse: 22196.8535\n",
      "Epoch 36/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20300.0566 - rmse: 20300.0566\n",
      "Epoch 36: val_loss did not improve from 21566.39062\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20293.1914 - rmse: 20334.5234 - val_loss: 23593.1387 - val_rmse: 23526.6270\n",
      "Epoch 37/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20876.9258 - rmse: 20876.9258\n",
      "Epoch 37: val_loss did not improve from 21566.39062\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20623.8594 - rmse: 20620.1875 - val_loss: 26162.7656 - val_rmse: 26203.0391\n",
      "Epoch 38/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19244.1836 - rmse: 19244.1836\n",
      "Epoch 38: val_loss did not improve from 21566.39062\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19621.6230 - rmse: 19823.7129 - val_loss: 23417.4492 - val_rmse: 23453.8770\n",
      "Epoch 39/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20230.5938 - rmse: 20230.5938\n",
      "Epoch 39: val_loss did not improve from 21566.39062\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21023.7520 - rmse: 21080.1602 - val_loss: 23220.3633 - val_rmse: 23158.2441\n",
      "Epoch 40/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21576.5566 - rmse: 21576.5566\n",
      "Epoch 40: val_loss did not improve from 21566.39062\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21610.9395 - rmse: 21601.7461 - val_loss: 25831.3926 - val_rmse: 25922.5312\n",
      "Epoch 41/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19672.8730 - rmse: 19672.8730\n",
      "Epoch 41: val_loss did not improve from 21566.39062\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19780.3887 - rmse: 19852.9004 - val_loss: 24195.9062 - val_rmse: 24234.5723\n",
      "Epoch 42/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21066.0000 - rmse: 21066.0000\n",
      "Epoch 42: val_loss did not improve from 21566.39062\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21225.1777 - rmse: 21227.2715 - val_loss: 21981.7324 - val_rmse: 21903.5039\n",
      "Epoch 43/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 22942.7383 - rmse: 22942.7383\n",
      "Epoch 43: val_loss improved from 21566.39062 to 21471.92383, saving model to ./ckpt/5_class_lr005/val_rmse_21386.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22735.4980 - rmse: 22716.0547 - val_loss: 21471.9238 - val_rmse: 21385.5059\n",
      "Epoch 44/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19501.3340 - rmse: 19707.2637\n",
      "Epoch 44: val_loss did not improve from 21471.92383\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19501.3340 - rmse: 19707.2637 - val_loss: 31413.4102 - val_rmse: 31296.7207\n",
      "Epoch 45/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21177.1973 - rmse: 21177.1973\n",
      "Epoch 45: val_loss did not improve from 21471.92383\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21181.2754 - rmse: 21227.2539 - val_loss: 21496.7988 - val_rmse: 21503.4590\n",
      "Epoch 46/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21210.3574 - rmse: 21236.7031\n",
      "Epoch 46: val_loss did not improve from 21471.92383\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21210.3574 - rmse: 21236.7031 - val_loss: 23342.3438 - val_rmse: 23397.3652\n",
      "Epoch 47/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20469.3027 - rmse: 20469.3027\n",
      "Epoch 47: val_loss did not improve from 21471.92383\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20373.5957 - rmse: 20345.6250 - val_loss: 22638.2012 - val_rmse: 22640.6328\n",
      "Epoch 48/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18795.1582 - rmse: 18795.1582\n",
      "Epoch 48: val_loss improved from 21471.92383 to 21303.76172, saving model to ./ckpt/5_class_lr005/val_rmse_21185.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18877.4688 - rmse: 18891.3242 - val_loss: 21303.7617 - val_rmse: 21184.5352\n",
      "Epoch 49/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20946.7344 - rmse: 20925.0938\n",
      "Epoch 49: val_loss did not improve from 21303.76172\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20946.7344 - rmse: 20925.0938 - val_loss: 38974.5469 - val_rmse: 39118.8516\n",
      "Epoch 50/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20558.9863 - rmse: 20558.9863\n",
      "Epoch 50: val_loss did not improve from 21303.76172\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20488.4277 - rmse: 20500.2930 - val_loss: 25057.9727 - val_rmse: 25116.7715\n",
      "Epoch 51/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21291.2871 - rmse: 21291.2871\n",
      "Epoch 51: val_loss did not improve from 21303.76172\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21213.8242 - rmse: 21258.2930 - val_loss: 27089.2266 - val_rmse: 26973.5273\n",
      "Epoch 52/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19601.7246 - rmse: 19601.7246\n",
      "Epoch 52: val_loss did not improve from 21303.76172\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19523.3262 - rmse: 19522.5059 - val_loss: 22568.1992 - val_rmse: 22683.5547\n",
      "Epoch 53/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18734.4844 - rmse: 18734.4844\n",
      "Epoch 53: val_loss did not improve from 21303.76172\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18758.8555 - rmse: 18775.2910 - val_loss: 21323.6367 - val_rmse: 21257.9629\n",
      "Epoch 54/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17860.9805 - rmse: 17860.9805\n",
      "Epoch 54: val_loss did not improve from 21303.76172\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18003.0605 - rmse: 18036.2363 - val_loss: 25258.1680 - val_rmse: 25353.2578\n",
      "Epoch 55/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19225.8086 - rmse: 19225.8086\n",
      "Epoch 55: val_loss did not improve from 21303.76172\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19231.7656 - rmse: 19235.7832 - val_loss: 22197.6191 - val_rmse: 22187.8457\n",
      "Epoch 56/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19279.7129 - rmse: 19279.7129\n",
      "Epoch 56: val_loss did not improve from 21303.76172\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19172.1152 - rmse: 19162.5723 - val_loss: 22261.4043 - val_rmse: 22172.3340\n",
      "Epoch 57/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17918.2852 - rmse: 17918.2852\n",
      "Epoch 57: val_loss did not improve from 21303.76172\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18091.9512 - rmse: 18087.9922 - val_loss: 22286.8418 - val_rmse: 22185.0879\n",
      "Epoch 58/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18340.9531 - rmse: 18340.9531\n",
      "Epoch 58: val_loss did not improve from 21303.76172\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18458.9023 - rmse: 18538.4492 - val_loss: 22679.5801 - val_rmse: 22529.1309\n",
      "Epoch 59/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19527.1211 - rmse: 19506.3809\n",
      "Epoch 59: val_loss did not improve from 21303.76172\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19527.1211 - rmse: 19506.3809 - val_loss: 24935.6973 - val_rmse: 25008.5059\n",
      "Epoch 60/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16994.1367 - rmse: 16994.1367\n",
      "Epoch 60: val_loss improved from 21303.76172 to 20845.37500, saving model to ./ckpt/5_class_lr005/val_rmse_20802.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17066.3242 - rmse: 17121.8711 - val_loss: 20845.3750 - val_rmse: 20801.5625\n",
      "Epoch 61/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18921.6348 - rmse: 18921.6348\n",
      "Epoch 61: val_loss did not improve from 20845.37500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18914.5859 - rmse: 18958.0312 - val_loss: 25183.2559 - val_rmse: 25266.8516\n",
      "Epoch 62/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17019.9160 - rmse: 17019.9160\n",
      "Epoch 62: val_loss improved from 20845.37500 to 20650.58789, saving model to ./ckpt/5_class_lr005/val_rmse_20612.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17084.4941 - rmse: 17120.0703 - val_loss: 20650.5879 - val_rmse: 20612.0039\n",
      "Epoch 63/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17441.3594 - rmse: 17441.3594\n",
      "Epoch 63: val_loss did not improve from 20650.58789\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17496.8281 - rmse: 17534.2402 - val_loss: 21327.9102 - val_rmse: 21364.8516\n",
      "Epoch 64/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17839.2109 - rmse: 17839.2109\n",
      "Epoch 64: val_loss did not improve from 20650.58789\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17811.1855 - rmse: 17831.9688 - val_loss: 24449.3438 - val_rmse: 24511.5449\n",
      "Epoch 65/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17640.7930 - rmse: 17640.7930\n",
      "Epoch 65: val_loss did not improve from 20650.58789\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17497.2598 - rmse: 17502.6582 - val_loss: 21881.0957 - val_rmse: 21913.0449\n",
      "Epoch 66/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18036.5664 - rmse: 18036.5664\n",
      "Epoch 66: val_loss did not improve from 20650.58789\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18129.4746 - rmse: 18147.3555 - val_loss: 20877.7793 - val_rmse: 20846.6992\n",
      "Epoch 67/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17826.5547 - rmse: 17826.5547\n",
      "Epoch 67: val_loss did not improve from 20650.58789\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17834.8359 - rmse: 17861.4902 - val_loss: 23320.8203 - val_rmse: 23373.0215\n",
      "Epoch 68/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18778.3496 - rmse: 18778.3496\n",
      "Epoch 68: val_loss did not improve from 20650.58789\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18826.8770 - rmse: 18859.6055 - val_loss: 20960.5488 - val_rmse: 20988.0215\n",
      "Epoch 69/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18377.7402 - rmse: 18377.7402\n",
      "Epoch 69: val_loss did not improve from 20650.58789\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18480.2715 - rmse: 18523.5332 - val_loss: 43114.2422 - val_rmse: 43330.8516\n",
      "Epoch 70/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18931.3105 - rmse: 18931.3105\n",
      "Epoch 70: val_loss did not improve from 20650.58789\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18699.1016 - rmse: 18730.0547 - val_loss: 21500.4180 - val_rmse: 21348.1230\n",
      "Epoch 71/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18830.5742 - rmse: 18830.5742\n",
      "Epoch 71: val_loss did not improve from 20650.58789\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18693.0176 - rmse: 18667.1777 - val_loss: 22781.4258 - val_rmse: 22673.0117\n",
      "Epoch 72/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17080.1172 - rmse: 17080.1172\n",
      "Epoch 72: val_loss did not improve from 20650.58789\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17087.3066 - rmse: 17092.1562 - val_loss: 21815.0801 - val_rmse: 21847.4922\n",
      "Epoch 73/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17799.6855 - rmse: 17799.6855\n",
      "Epoch 73: val_loss did not improve from 20650.58789\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17767.0312 - rmse: 17745.0059 - val_loss: 23618.2969 - val_rmse: 23739.5820\n",
      "Epoch 74/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19626.0859 - rmse: 19626.0859\n",
      "Epoch 74: val_loss did not improve from 20650.58789\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19288.6445 - rmse: 19287.1816 - val_loss: 21861.9043 - val_rmse: 21955.5664\n",
      "Epoch 75/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17669.2188 - rmse: 17702.4219\n",
      "Epoch 75: val_loss did not improve from 20650.58789\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 17669.2188 - rmse: 17702.4219 - val_loss: 32549.7695 - val_rmse: 32701.0508\n",
      "Epoch 76/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18936.2715 - rmse: 18936.2715\n",
      "Epoch 76: val_loss improved from 20650.58789 to 20501.22461, saving model to ./ckpt/5_class_lr005/val_rmse_20540.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18945.4238 - rmse: 18939.1367 - val_loss: 20501.2246 - val_rmse: 20540.1641\n",
      "Epoch 77/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17548.7422 - rmse: 17548.7422\n",
      "Epoch 77: val_loss did not improve from 20501.22461\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17963.7031 - rmse: 18057.9785 - val_loss: 24207.8750 - val_rmse: 24275.5254\n",
      "Epoch 78/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18811.0898 - rmse: 18853.9492\n",
      "Epoch 78: val_loss did not improve from 20501.22461\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18811.0898 - rmse: 18853.9492 - val_loss: 29107.5312 - val_rmse: 29293.3086\n",
      "Epoch 79/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16988.3027 - rmse: 16988.3027\n",
      "Epoch 79: val_loss did not improve from 20501.22461\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17111.0605 - rmse: 17161.4668 - val_loss: 20767.2500 - val_rmse: 20752.4980\n",
      "Epoch 80/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16590.7012 - rmse: 16587.3496\n",
      "Epoch 80: val_loss did not improve from 20501.22461\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16590.7012 - rmse: 16587.3496 - val_loss: 21858.0059 - val_rmse: 22004.1328\n",
      "Epoch 81/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16993.9746 - rmse: 16993.9746\n",
      "Epoch 81: val_loss did not improve from 20501.22461\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17022.2871 - rmse: 17041.3809 - val_loss: 34789.5586 - val_rmse: 34975.9531\n",
      "Epoch 82/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18062.7930 - rmse: 18062.7930\n",
      "Epoch 82: val_loss did not improve from 20501.22461\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17912.6582 - rmse: 17872.6992 - val_loss: 22732.6699 - val_rmse: 22827.2930\n",
      "Epoch 83/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17516.6797 - rmse: 17516.6797\n",
      "Epoch 83: val_loss did not improve from 20501.22461\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17579.6289 - rmse: 17653.9082 - val_loss: 20558.0234 - val_rmse: 20510.2715\n",
      "Epoch 84/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16616.5000 - rmse: 16663.2793\n",
      "Epoch 84: val_loss improved from 20501.22461 to 20429.75391, saving model to ./ckpt/5_class_lr005/val_rmse_20398.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16616.5000 - rmse: 16663.2793 - val_loss: 20429.7539 - val_rmse: 20397.5332\n",
      "Epoch 85/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17734.8242 - rmse: 17734.8242\n",
      "Epoch 85: val_loss did not improve from 20429.75391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17732.1602 - rmse: 17769.1582 - val_loss: 24644.4219 - val_rmse: 24764.2695\n",
      "Epoch 86/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17988.3066 - rmse: 17996.8906\n",
      "Epoch 86: val_loss improved from 20429.75391 to 20200.42383, saving model to ./ckpt/5_class_lr005/val_rmse_20221.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17988.3066 - rmse: 17996.8906 - val_loss: 20200.4238 - val_rmse: 20221.2539\n",
      "Epoch 87/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16132.3164 - rmse: 16132.3164\n",
      "Epoch 87: val_loss did not improve from 20200.42383\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16115.8564 - rmse: 16130.6162 - val_loss: 20216.0859 - val_rmse: 20266.6602\n",
      "Epoch 88/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16088.9492 - rmse: 16088.9492\n",
      "Epoch 88: val_loss did not improve from 20200.42383\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16066.2363 - rmse: 16031.2070 - val_loss: 20533.9648 - val_rmse: 20525.3516\n",
      "Epoch 89/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17286.3848 - rmse: 17286.3848\n",
      "Epoch 89: val_loss did not improve from 20200.42383\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17182.8477 - rmse: 17196.8223 - val_loss: 22275.9473 - val_rmse: 22372.6523\n",
      "Epoch 90/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16184.7998 - rmse: 16165.3535\n",
      "Epoch 90: val_loss did not improve from 20200.42383\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16184.7998 - rmse: 16165.3535 - val_loss: 20895.0820 - val_rmse: 20974.2441\n",
      "Epoch 91/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15523.1797 - rmse: 15523.1797\n",
      "Epoch 91: val_loss did not improve from 20200.42383\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15678.2012 - rmse: 15782.7520 - val_loss: 20380.8184 - val_rmse: 20317.9277\n",
      "Epoch 92/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17242.5312 - rmse: 17242.5312\n",
      "Epoch 92: val_loss did not improve from 20200.42383\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 17133.9258 - rmse: 17120.0586 - val_loss: 29497.3398 - val_rmse: 29742.8359\n",
      "Epoch 93/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15782.4062 - rmse: 15782.4062\n",
      "Epoch 93: val_loss did not improve from 20200.42383\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16115.3535 - rmse: 16113.8145 - val_loss: 21297.6211 - val_rmse: 21161.2051\n",
      "Epoch 94/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17142.8770 - rmse: 17142.8770\n",
      "Epoch 94: val_loss did not improve from 20200.42383\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17115.8672 - rmse: 17097.6523 - val_loss: 21261.5781 - val_rmse: 21187.6094\n",
      "Epoch 95/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16078.6963 - rmse: 16078.6963\n",
      "Epoch 95: val_loss did not improve from 20200.42383\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16065.1768 - rmse: 16056.0576 - val_loss: 22047.3574 - val_rmse: 22117.5156\n",
      "Epoch 96/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16098.0391 - rmse: 16123.3320\n",
      "Epoch 96: val_loss improved from 20200.42383 to 19524.75391, saving model to ./ckpt/5_class_lr005/val_rmse_19510.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16098.0391 - rmse: 16123.3320 - val_loss: 19524.7539 - val_rmse: 19510.1914\n",
      "Epoch 97/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16036.7695 - rmse: 16036.7695\n",
      "Epoch 97: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16236.3252 - rmse: 16287.3018 - val_loss: 21079.0801 - val_rmse: 21001.0391\n",
      "Epoch 98/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17378.8750 - rmse: 17498.4785\n",
      "Epoch 98: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 17378.8750 - rmse: 17498.4785 - val_loss: 28094.8242 - val_rmse: 27818.6094\n",
      "Epoch 99/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18009.0723 - rmse: 18009.0723\n",
      "Epoch 99: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17969.8164 - rmse: 17943.3398 - val_loss: 29976.3496 - val_rmse: 30163.5781\n",
      "Epoch 100/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17177.5586 - rmse: 17223.8359\n",
      "Epoch 100: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17177.5586 - rmse: 17223.8359 - val_loss: 27756.5723 - val_rmse: 27870.4570\n",
      "Epoch 101/120\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 15536.3018 - rmse: 15536.3018\n",
      "Epoch 101: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15695.2012 - rmse: 15705.4893 - val_loss: 24571.2852 - val_rmse: 24671.0312\n",
      "Epoch 102/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16572.2402 - rmse: 16572.2402\n",
      "Epoch 102: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16566.0508 - rmse: 16566.1230 - val_loss: 20484.5332 - val_rmse: 20501.8691\n",
      "Epoch 103/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15073.6992 - rmse: 15073.6992\n",
      "Epoch 103: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15053.2705 - rmse: 15028.2695 - val_loss: 26299.2480 - val_rmse: 26424.3281\n",
      "Epoch 104/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19060.0957 - rmse: 19060.0957\n",
      "Epoch 104: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19010.3105 - rmse: 18987.7051 - val_loss: 24752.3008 - val_rmse: 24895.5762\n",
      "Epoch 105/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15838.7598 - rmse: 15838.7598\n",
      "Epoch 105: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15841.4766 - rmse: 15847.5859 - val_loss: 21609.6484 - val_rmse: 21687.2695\n",
      "Epoch 106/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16493.0977 - rmse: 16493.0977\n",
      "Epoch 106: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16621.1992 - rmse: 16652.7422 - val_loss: 19713.0137 - val_rmse: 19727.1113\n",
      "Epoch 107/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16079.9814 - rmse: 16079.9814\n",
      "Epoch 107: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16059.3506 - rmse: 16101.2090 - val_loss: 27314.0430 - val_rmse: 27473.4043\n",
      "Epoch 108/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15500.7725 - rmse: 15552.8037\n",
      "Epoch 108: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15500.7725 - rmse: 15552.8037 - val_loss: 21793.4648 - val_rmse: 21865.2266\n",
      "Epoch 109/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 14329.1445 - rmse: 14329.1445\n",
      "Epoch 109: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14476.5615 - rmse: 14488.2588 - val_loss: 32959.4688 - val_rmse: 33162.5781\n",
      "Epoch 110/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16292.3584 - rmse: 16292.3584\n",
      "Epoch 110: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16208.0635 - rmse: 16205.2803 - val_loss: 23159.5020 - val_rmse: 23270.4922\n",
      "Epoch 111/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16093.1406 - rmse: 16070.7646\n",
      "Epoch 111: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16093.1406 - rmse: 16070.7646 - val_loss: 20286.2715 - val_rmse: 20369.1543\n",
      "Epoch 112/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16042.2861 - rmse: 16042.2861\n",
      "Epoch 112: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15911.8857 - rmse: 15896.9434 - val_loss: 23759.8496 - val_rmse: 23846.4492\n",
      "Epoch 113/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15488.9180 - rmse: 15488.9180\n",
      "Epoch 113: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15342.9805 - rmse: 15352.3916 - val_loss: 23268.2188 - val_rmse: 23427.0703\n",
      "Epoch 114/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16091.8301 - rmse: 16186.3486\n",
      "Epoch 114: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16091.8301 - rmse: 16186.3486 - val_loss: 20052.1387 - val_rmse: 20074.0645\n",
      "Epoch 115/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16587.8848 - rmse: 16596.1738\n",
      "Epoch 115: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16587.8848 - rmse: 16596.1738 - val_loss: 26991.7402 - val_rmse: 27122.0332\n",
      "Epoch 116/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15037.4619 - rmse: 15037.4619\n",
      "Epoch 116: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15003.0947 - rmse: 15012.2461 - val_loss: 21950.7715 - val_rmse: 22081.4922\n",
      "Epoch 117/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15435.8594 - rmse: 15435.8594\n",
      "Epoch 117: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15454.8613 - rmse: 15475.8750 - val_loss: 22317.8223 - val_rmse: 22389.4453\n",
      "Epoch 118/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15992.6611 - rmse: 16042.7559\n",
      "Epoch 118: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15992.6611 - rmse: 16042.7559 - val_loss: 42573.3086 - val_rmse: 42786.3828\n",
      "Epoch 119/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16544.4141 - rmse: 16544.4141\n",
      "Epoch 119: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16490.5508 - rmse: 16501.8906 - val_loss: 19701.9395 - val_rmse: 19710.8672\n",
      "Epoch 120/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15695.5664 - rmse: 15695.5664\n",
      "Epoch 120: val_loss did not improve from 19524.75391\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15782.8975 - rmse: 15823.4395 - val_loss: 33315.6211 - val_rmse: 33516.8125\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 30s - loss: 221698.8281 - rmse: 221698.8281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:03:24.194863: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 99976.8438 - rmse: 99958.3750  \n",
      "Epoch 1: val_loss improved from inf to 61005.45703, saving model to ./ckpt/5_class_lr005/val_rmse_61484.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 99976.8438 - rmse: 99958.3750 - val_loss: 61005.4570 - val_rmse: 61484.0898\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 64392.9766 - rmse: 64392.9766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:03:25.154680: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 52371.5898 - rmse: 52371.5898\n",
      "Epoch 2: val_loss improved from 61005.45703 to 47753.56250, saving model to ./ckpt/5_class_lr005/val_rmse_48205.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 52340.2422 - rmse: 52229.4023 - val_loss: 47753.5625 - val_rmse: 48204.9414\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 46270.3516 - rmse: 46270.3516\n",
      "Epoch 3: val_loss improved from 47753.56250 to 46550.01953, saving model to ./ckpt/5_class_lr005/val_rmse_46967.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 45827.8555 - rmse: 45715.5000 - val_loss: 46550.0195 - val_rmse: 46967.2031\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 45275.9023 - rmse: 45275.9023\n",
      "Epoch 4: val_loss improved from 46550.01953 to 44515.36328, saving model to ./ckpt/5_class_lr005/val_rmse_44852.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 45162.0859 - rmse: 45085.3203 - val_loss: 44515.3633 - val_rmse: 44851.7578\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 41663.0977 - rmse: 41639.5078\n",
      "Epoch 5: val_loss improved from 44515.36328 to 40116.71875, saving model to ./ckpt/5_class_lr005/val_rmse_40347.hdf5\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 41663.0977 - rmse: 41639.5078 - val_loss: 40116.7188 - val_rmse: 40346.5586\n",
      "Epoch 6/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 37874.0039 - rmse: 37874.0039\n",
      "Epoch 6: val_loss did not improve from 40116.71875\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 38018.7852 - rmse: 38061.0469 - val_loss: 43867.9258 - val_rmse: 43938.7773\n",
      "Epoch 7/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 36758.6445 - rmse: 36758.6445\n",
      "Epoch 7: val_loss improved from 40116.71875 to 36084.13281, saving model to ./ckpt/5_class_lr005/val_rmse_36226.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 36496.0430 - rmse: 36474.2031 - val_loss: 36084.1328 - val_rmse: 36225.9453\n",
      "Epoch 8/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 34266.3867 - rmse: 34266.3867\n",
      "Epoch 8: val_loss improved from 36084.13281 to 34407.27734, saving model to ./ckpt/5_class_lr005/val_rmse_34483.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 34376.6172 - rmse: 34450.6328 - val_loss: 34407.2773 - val_rmse: 34482.9023\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 35375.0703 - rmse: 35359.8945\n",
      "Epoch 9: val_loss did not improve from 34407.27734\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 35375.0703 - rmse: 35359.8945 - val_loss: 37829.7812 - val_rmse: 37944.0156\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 31344.7598 - rmse: 31365.0039\n",
      "Epoch 10: val_loss did not improve from 34407.27734\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 31344.7598 - rmse: 31365.0039 - val_loss: 35058.7969 - val_rmse: 35119.3398\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 31722.1855 - rmse: 31781.6250\n",
      "Epoch 11: val_loss improved from 34407.27734 to 32182.71680, saving model to ./ckpt/5_class_lr005/val_rmse_32219.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 31722.1855 - rmse: 31781.6250 - val_loss: 32182.7168 - val_rmse: 32218.6523\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 35557.1016 - rmse: 35522.2773\n",
      "Epoch 12: val_loss improved from 32182.71680 to 30086.25586, saving model to ./ckpt/5_class_lr005/val_rmse_30126.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 35557.1016 - rmse: 35522.2773 - val_loss: 30086.2559 - val_rmse: 30125.7129\n",
      "Epoch 13/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 29800.9844 - rmse: 29800.9844\n",
      "Epoch 13: val_loss did not improve from 30086.25586\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 29753.9980 - rmse: 29681.1504 - val_loss: 33255.4609 - val_rmse: 33284.3008\n",
      "Epoch 14/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 28717.8574 - rmse: 28717.8574\n",
      "Epoch 14: val_loss did not improve from 30086.25586\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 28811.7129 - rmse: 28851.1582 - val_loss: 40086.9141 - val_rmse: 40046.7383\n",
      "Epoch 15/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 28862.1328 - rmse: 28862.1328\n",
      "Epoch 15: val_loss improved from 30086.25586 to 27499.32227, saving model to ./ckpt/5_class_lr005/val_rmse_27516.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 29052.0605 - rmse: 29099.4219 - val_loss: 27499.3223 - val_rmse: 27516.0645\n",
      "Epoch 16/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 33068.5352 - rmse: 33068.1680\n",
      "Epoch 16: val_loss did not improve from 27499.32227\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 33068.5352 - rmse: 33068.1680 - val_loss: 33569.1875 - val_rmse: 33542.4023\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 26165.7246 - rmse: 26165.7246\n",
      "Epoch 17: val_loss did not improve from 27499.32227\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26111.6270 - rmse: 26075.1406 - val_loss: 34872.5430 - val_rmse: 34818.4219\n",
      "Epoch 18/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 26794.3418 - rmse: 26794.3418\n",
      "Epoch 18: val_loss improved from 27499.32227 to 26816.12500, saving model to ./ckpt/5_class_lr005/val_rmse_26784.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 27160.8848 - rmse: 27182.5391 - val_loss: 26816.1250 - val_rmse: 26784.3340\n",
      "Epoch 19/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 25138.1738 - rmse: 25138.1738\n",
      "Epoch 19: val_loss improved from 26816.12500 to 26437.26367, saving model to ./ckpt/5_class_lr005/val_rmse_26380.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 25201.0586 - rmse: 25243.4688 - val_loss: 26437.2637 - val_rmse: 26380.1016\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27460.6523 - rmse: 27426.6270\n",
      "Epoch 20: val_loss did not improve from 26437.26367\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27460.6523 - rmse: 27426.6270 - val_loss: 31366.3496 - val_rmse: 31299.1777\n",
      "Epoch 21/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 27461.3574 - rmse: 27461.3574\n",
      "Epoch 21: val_loss improved from 26437.26367 to 26019.38867, saving model to ./ckpt/5_class_lr005/val_rmse_25937.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 27557.1133 - rmse: 27513.6328 - val_loss: 26019.3887 - val_rmse: 25936.8125\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 25389.6367 - rmse: 25389.6367\n",
      "Epoch 22: val_loss did not improve from 26019.38867\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 25364.8555 - rmse: 25348.1406 - val_loss: 30735.6211 - val_rmse: 30612.5859\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23551.4180 - rmse: 23551.4180\n",
      "Epoch 23: val_loss did not improve from 26019.38867\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23722.4453 - rmse: 23837.7891 - val_loss: 28050.4121 - val_rmse: 28008.0938\n",
      "Epoch 24/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23959.8418 - rmse: 23959.8418\n",
      "Epoch 24: val_loss did not improve from 26019.38867\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23985.3359 - rmse: 23993.1973 - val_loss: 26450.2676 - val_rmse: 26311.1895\n",
      "Epoch 25/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 26289.5059 - rmse: 26289.5059\n",
      "Epoch 25: val_loss did not improve from 26019.38867\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 26290.3867 - rmse: 26328.5215 - val_loss: 28767.5254 - val_rmse: 28733.7891\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23170.6016 - rmse: 23170.6016\n",
      "Epoch 26: val_loss did not improve from 26019.38867\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23117.4141 - rmse: 23161.8418 - val_loss: 29607.7715 - val_rmse: 29481.0742\n",
      "Epoch 27/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22396.1836 - rmse: 22550.6719\n",
      "Epoch 27: val_loss did not improve from 26019.38867\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22396.1836 - rmse: 22550.6719 - val_loss: 27867.4844 - val_rmse: 27719.3145\n",
      "Epoch 28/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22887.9648 - rmse: 22941.1055\n",
      "Epoch 28: val_loss improved from 26019.38867 to 24303.08008, saving model to ./ckpt/5_class_lr005/val_rmse_24200.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22887.9648 - rmse: 22941.1055 - val_loss: 24303.0801 - val_rmse: 24199.6465\n",
      "Epoch 29/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22509.0215 - rmse: 22509.0215\n",
      "Epoch 29: val_loss improved from 24303.08008 to 23443.95508, saving model to ./ckpt/5_class_lr005/val_rmse_23343.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22594.5039 - rmse: 22675.5586 - val_loss: 23443.9551 - val_rmse: 23343.0684\n",
      "Epoch 30/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20817.4824 - rmse: 20817.4824\n",
      "Epoch 30: val_loss did not improve from 23443.95508\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20940.8965 - rmse: 20942.3496 - val_loss: 32019.0059 - val_rmse: 31803.2578\n",
      "Epoch 31/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22282.7988 - rmse: 22282.7988\n",
      "Epoch 31: val_loss did not improve from 23443.95508\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22089.4941 - rmse: 22078.3926 - val_loss: 24780.0996 - val_rmse: 24657.3125\n",
      "Epoch 32/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23672.7129 - rmse: 23694.2891\n",
      "Epoch 32: val_loss did not improve from 23443.95508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23672.7129 - rmse: 23694.2891 - val_loss: 29004.4492 - val_rmse: 28862.0527\n",
      "Epoch 33/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 21174.3535 - rmse: 21174.3535\n",
      "Epoch 33: val_loss did not improve from 23443.95508\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21093.0801 - rmse: 21083.6328 - val_loss: 24789.6953 - val_rmse: 24623.5352\n",
      "Epoch 34/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21401.5957 - rmse: 21401.5957\n",
      "Epoch 34: val_loss did not improve from 23443.95508\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21209.0449 - rmse: 21204.9609 - val_loss: 26136.2461 - val_rmse: 25957.5645\n",
      "Epoch 35/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20569.4883 - rmse: 20569.4883\n",
      "Epoch 35: val_loss did not improve from 23443.95508\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20728.2715 - rmse: 20769.7871 - val_loss: 35015.0273 - val_rmse: 34846.7461\n",
      "Epoch 36/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21383.2988 - rmse: 21383.2988\n",
      "Epoch 36: val_loss did not improve from 23443.95508\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21419.7383 - rmse: 21504.4316 - val_loss: 29548.1016 - val_rmse: 29379.4102\n",
      "Epoch 37/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19833.0469 - rmse: 19833.0469\n",
      "Epoch 37: val_loss did not improve from 23443.95508\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19838.9082 - rmse: 19842.8613 - val_loss: 26190.2441 - val_rmse: 26023.5488\n",
      "Epoch 38/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19907.6777 - rmse: 19907.6777\n",
      "Epoch 38: val_loss improved from 23443.95508 to 23240.30078, saving model to ./ckpt/5_class_lr005/val_rmse_23151.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19960.1914 - rmse: 19974.8926 - val_loss: 23240.3008 - val_rmse: 23150.9043\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23145.2441 - rmse: 23145.2441\n",
      "Epoch 39: val_loss did not improve from 23240.30078\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23124.1016 - rmse: 23109.8418 - val_loss: 27926.1855 - val_rmse: 27771.8438\n",
      "Epoch 40/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20091.1660 - rmse: 20091.1660\n",
      "Epoch 40: val_loss did not improve from 23240.30078\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20024.3828 - rmse: 20039.1367 - val_loss: 25117.0977 - val_rmse: 24924.4238\n",
      "Epoch 41/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20532.0742 - rmse: 20532.0742\n",
      "Epoch 41: val_loss did not improve from 23240.30078\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20690.7012 - rmse: 20683.2871 - val_loss: 23604.1816 - val_rmse: 23467.4648\n",
      "Epoch 42/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19786.2109 - rmse: 19786.2109\n",
      "Epoch 42: val_loss did not improve from 23240.30078\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19859.3633 - rmse: 19908.6992 - val_loss: 27591.1836 - val_rmse: 27405.4492\n",
      "Epoch 43/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19290.4844 - rmse: 19290.4844\n",
      "Epoch 43: val_loss did not improve from 23240.30078\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19561.4277 - rmse: 19588.3887 - val_loss: 26434.7070 - val_rmse: 26241.7832\n",
      "Epoch 44/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20412.6680 - rmse: 20412.6680\n",
      "Epoch 44: val_loss did not improve from 23240.30078\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20351.9648 - rmse: 20311.0215 - val_loss: 30875.3691 - val_rmse: 30698.6133\n",
      "Epoch 45/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20129.2266 - rmse: 20129.2266\n",
      "Epoch 45: val_loss improved from 23240.30078 to 22429.87500, saving model to ./ckpt/5_class_lr005/val_rmse_22304.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20168.6699 - rmse: 20164.3477 - val_loss: 22429.8750 - val_rmse: 22303.8574\n",
      "Epoch 46/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19436.5684 - rmse: 19546.6250\n",
      "Epoch 46: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19436.5684 - rmse: 19546.6250 - val_loss: 25899.8516 - val_rmse: 25727.5430\n",
      "Epoch 47/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20188.8965 - rmse: 20188.8965\n",
      "Epoch 47: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20207.4395 - rmse: 20226.6660 - val_loss: 22687.7734 - val_rmse: 22566.8281\n",
      "Epoch 48/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20483.0430 - rmse: 20483.0430\n",
      "Epoch 48: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20384.8223 - rmse: 20390.9609 - val_loss: 25216.8086 - val_rmse: 25028.0156\n",
      "Epoch 49/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20445.3223 - rmse: 20445.3223\n",
      "Epoch 49: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20268.1094 - rmse: 20252.0488 - val_loss: 23533.5254 - val_rmse: 23343.7598\n",
      "Epoch 50/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18487.0488 - rmse: 18487.0488\n",
      "Epoch 50: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18676.2441 - rmse: 18803.8438 - val_loss: 25692.1934 - val_rmse: 25524.4707\n",
      "Epoch 51/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19997.7168 - rmse: 19997.7168\n",
      "Epoch 51: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19967.0156 - rmse: 20096.1484 - val_loss: 23119.0977 - val_rmse: 22942.9590\n",
      "Epoch 52/120\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 18403.8223 - rmse: 18403.8223\n",
      "Epoch 52: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18416.5527 - rmse: 18404.0527 - val_loss: 24179.4414 - val_rmse: 24020.8574\n",
      "Epoch 53/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22784.4980 - rmse: 22784.4980\n",
      "Epoch 53: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22792.4180 - rmse: 22797.7559 - val_loss: 39912.4336 - val_rmse: 39811.0977\n",
      "Epoch 54/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19760.3496 - rmse: 19760.3496\n",
      "Epoch 54: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19499.2812 - rmse: 19472.6328 - val_loss: 25441.3691 - val_rmse: 25285.6758\n",
      "Epoch 55/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17874.3105 - rmse: 17874.3105\n",
      "Epoch 55: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18244.1543 - rmse: 18419.9277 - val_loss: 22606.7324 - val_rmse: 22442.8379\n",
      "Epoch 56/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19534.4941 - rmse: 19522.2773\n",
      "Epoch 56: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19534.4941 - rmse: 19522.2773 - val_loss: 23820.5820 - val_rmse: 23674.3496\n",
      "Epoch 57/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18430.5977 - rmse: 18430.5977\n",
      "Epoch 57: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18692.2129 - rmse: 18764.7812 - val_loss: 24984.0098 - val_rmse: 24915.0020\n",
      "Epoch 58/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20341.8184 - rmse: 20341.8184\n",
      "Epoch 58: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20336.3906 - rmse: 20317.0664 - val_loss: 32167.7520 - val_rmse: 31988.8633\n",
      "Epoch 59/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19933.0762 - rmse: 19915.0449\n",
      "Epoch 59: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19933.0762 - rmse: 19915.0449 - val_loss: 23752.3164 - val_rmse: 23606.0273\n",
      "Epoch 60/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17617.0293 - rmse: 17617.0293\n",
      "Epoch 60: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17609.0332 - rmse: 17603.6406 - val_loss: 25988.4707 - val_rmse: 25886.1777\n",
      "Epoch 61/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18101.2422 - rmse: 18101.2422\n",
      "Epoch 61: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18151.5039 - rmse: 18194.7617 - val_loss: 27422.5293 - val_rmse: 27252.9629\n",
      "Epoch 62/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18096.9883 - rmse: 18096.9883\n",
      "Epoch 62: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18529.9414 - rmse: 18608.4824 - val_loss: 33956.3672 - val_rmse: 33805.6641\n",
      "Epoch 63/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18292.2930 - rmse: 18292.2930\n",
      "Epoch 63: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18299.5840 - rmse: 18304.5020 - val_loss: 28384.4180 - val_rmse: 28202.7148\n",
      "Epoch 64/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17638.3418 - rmse: 17639.7812\n",
      "Epoch 64: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17638.3418 - rmse: 17639.7812 - val_loss: 22628.8145 - val_rmse: 22465.7305\n",
      "Epoch 65/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18794.4531 - rmse: 18794.4531\n",
      "Epoch 65: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18991.8613 - rmse: 19079.5332 - val_loss: 28143.8906 - val_rmse: 27930.5332\n",
      "Epoch 66/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17639.8047 - rmse: 17625.1699\n",
      "Epoch 66: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17639.8047 - rmse: 17625.1699 - val_loss: 22867.0352 - val_rmse: 22734.5703\n",
      "Epoch 67/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17016.8379 - rmse: 17016.8379\n",
      "Epoch 67: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17006.8535 - rmse: 17000.1191 - val_loss: 24442.1426 - val_rmse: 24267.8359\n",
      "Epoch 68/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18173.5684 - rmse: 18173.5684\n",
      "Epoch 68: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18122.6875 - rmse: 18142.2969 - val_loss: 37735.6094 - val_rmse: 37621.8125\n",
      "Epoch 69/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18713.1719 - rmse: 18713.1719\n",
      "Epoch 69: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18707.6582 - rmse: 18709.1074 - val_loss: 22430.4414 - val_rmse: 22313.0566\n",
      "Epoch 70/120\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 17365.1582 - rmse: 17365.1582\n",
      "Epoch 70: val_loss did not improve from 22429.87500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17119.3555 - rmse: 17147.0957 - val_loss: 26069.6602 - val_rmse: 25879.0449\n",
      "Epoch 70: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 28s - loss: 166410.5000 - rmse: 166410.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:04:14.825571: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 99603.5625 - rmse: 99221.7266  \n",
      "Epoch 1: val_loss improved from inf to 63218.00391, saving model to ./ckpt/5_class_lr005/val_rmse_62431.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 99603.5625 - rmse: 99221.7266 - val_loss: 63218.0039 - val_rmse: 62431.2422\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 64704.2617 - rmse: 64704.2617"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:04:15.764258: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/70 [==========================>...] - ETA: 0s - loss: 52272.9766 - rmse: 52272.9766\n",
      "Epoch 2: val_loss improved from 63218.00391 to 61370.55078, saving model to ./ckpt/5_class_lr005/val_rmse_61182.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 52055.2773 - rmse: 52019.9727 - val_loss: 61370.5508 - val_rmse: 61182.4492\n",
      "Epoch 3/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 46417.8125 - rmse: 46417.8125\n",
      "Epoch 3: val_loss improved from 61370.55078 to 49498.93750, saving model to ./ckpt/5_class_lr005/val_rmse_49220.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 46255.2227 - rmse: 46183.0430 - val_loss: 49498.9375 - val_rmse: 49219.6289\n",
      "Epoch 4/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 44943.9375 - rmse: 44943.9375\n",
      "Epoch 4: val_loss improved from 49498.93750 to 48845.73047, saving model to ./ckpt/5_class_lr005/val_rmse_48673.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 44469.9922 - rmse: 44421.7734 - val_loss: 48845.7305 - val_rmse: 48672.6367\n",
      "Epoch 5/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 40951.9141 - rmse: 40951.9141\n",
      "Epoch 5: val_loss did not improve from 48845.73047\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 41248.8867 - rmse: 41386.4609 - val_loss: 52881.4141 - val_rmse: 52843.4375\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 40284.8242 - rmse: 40284.8242\n",
      "Epoch 6: val_loss improved from 48845.73047 to 47930.06250, saving model to ./ckpt/5_class_lr005/val_rmse_47843.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 40239.3047 - rmse: 40208.6055 - val_loss: 47930.0625 - val_rmse: 47842.8516\n",
      "Epoch 7/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 41492.7266 - rmse: 41492.7266\n",
      "Epoch 7: val_loss improved from 47930.06250 to 45548.09375, saving model to ./ckpt/5_class_lr005/val_rmse_45417.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 41297.5195 - rmse: 41223.6094 - val_loss: 45548.0938 - val_rmse: 45417.1953\n",
      "Epoch 8/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 36768.2070 - rmse: 36768.2070\n",
      "Epoch 8: val_loss improved from 45548.09375 to 40918.16016, saving model to ./ckpt/5_class_lr005/val_rmse_40668.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 36270.3008 - rmse: 36266.9023 - val_loss: 40918.1602 - val_rmse: 40668.3672\n",
      "Epoch 9/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 35542.8320 - rmse: 35542.8320\n",
      "Epoch 9: val_loss did not improve from 40918.16016\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 35660.8203 - rmse: 35740.3984 - val_loss: 51729.0586 - val_rmse: 51739.9961\n",
      "Epoch 10/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 35996.1133 - rmse: 35996.1133\n",
      "Epoch 10: val_loss did not improve from 40918.16016\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 35972.4688 - rmse: 35913.0312 - val_loss: 48842.9297 - val_rmse: 48391.1016\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 32545.1055 - rmse: 32545.1055\n",
      "Epoch 11: val_loss improved from 40918.16016 to 36658.88672, saving model to ./ckpt/5_class_lr005/val_rmse_36284.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 32539.3418 - rmse: 32535.4531 - val_loss: 36658.8867 - val_rmse: 36284.0547\n",
      "Epoch 12/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 31719.6582 - rmse: 31719.6582\n",
      "Epoch 12: val_loss did not improve from 36658.88672\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 31910.5039 - rmse: 31897.6816 - val_loss: 38950.6953 - val_rmse: 38542.5898\n",
      "Epoch 13/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 30498.8965 - rmse: 30498.8965\n",
      "Epoch 13: val_loss improved from 36658.88672 to 33518.65234, saving model to ./ckpt/5_class_lr005/val_rmse_33219.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 30327.7637 - rmse: 30289.1855 - val_loss: 33518.6523 - val_rmse: 33218.6953\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 28772.3398 - rmse: 28776.8008\n",
      "Epoch 14: val_loss did not improve from 33518.65234\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 28772.3398 - rmse: 28776.8008 - val_loss: 35082.4336 - val_rmse: 34683.7617\n",
      "Epoch 15/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 29139.5605 - rmse: 29139.5605\n",
      "Epoch 15: val_loss improved from 33518.65234 to 32744.54297, saving model to ./ckpt/5_class_lr005/val_rmse_32434.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 29085.8535 - rmse: 29046.3867 - val_loss: 32744.5430 - val_rmse: 32433.5156\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 29480.9434 - rmse: 29480.9434\n",
      "Epoch 16: val_loss did not improve from 32744.54297\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 29529.4785 - rmse: 29562.2109 - val_loss: 38710.5938 - val_rmse: 38572.6875\n",
      "Epoch 17/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 26731.4180 - rmse: 26731.4180\n",
      "Epoch 17: val_loss improved from 32744.54297 to 29549.99414, saving model to ./ckpt/5_class_lr005/val_rmse_29200.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26545.4355 - rmse: 26504.9082 - val_loss: 29549.9941 - val_rmse: 29200.3457\n",
      "Epoch 18/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 26532.7773 - rmse: 26532.7773\n",
      "Epoch 18: val_loss improved from 29549.99414 to 29057.21289, saving model to ./ckpt/5_class_lr005/val_rmse_28743.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 26533.1445 - rmse: 26489.1094 - val_loss: 29057.2129 - val_rmse: 28743.3320\n",
      "Epoch 19/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 28847.5938 - rmse: 28847.5938\n",
      "Epoch 19: val_loss did not improve from 29057.21289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 28708.3281 - rmse: 28720.1172 - val_loss: 29503.7500 - val_rmse: 29176.5391\n",
      "Epoch 20/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 26281.8145 - rmse: 26281.8145\n",
      "Epoch 20: val_loss improved from 29057.21289 to 28532.83398, saving model to ./ckpt/5_class_lr005/val_rmse_28207.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26361.2812 - rmse: 26360.4590 - val_loss: 28532.8340 - val_rmse: 28206.7344\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23570.2637 - rmse: 23570.2637\n",
      "Epoch 21: val_loss improved from 28532.83398 to 27032.74023, saving model to ./ckpt/5_class_lr005/val_rmse_26782.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23523.6660 - rmse: 23547.3965 - val_loss: 27032.7402 - val_rmse: 26781.9922\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23922.3633 - rmse: 23925.0664\n",
      "Epoch 22: val_loss did not improve from 27032.74023\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23922.3633 - rmse: 23925.0664 - val_loss: 27607.7734 - val_rmse: 27359.3750\n",
      "Epoch 23/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23850.9414 - rmse: 23850.9414\n",
      "Epoch 23: val_loss did not improve from 27032.74023\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23638.6270 - rmse: 23651.9805 - val_loss: 27923.0879 - val_rmse: 27640.3027\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23489.1230 - rmse: 23489.1230\n",
      "Epoch 24: val_loss did not improve from 27032.74023\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23739.1797 - rmse: 23748.6406 - val_loss: 27946.4121 - val_rmse: 27735.6602\n",
      "Epoch 25/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 24491.2930 - rmse: 24491.2930\n",
      "Epoch 25: val_loss improved from 27032.74023 to 26601.96680, saving model to ./ckpt/5_class_lr005/val_rmse_26314.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 24577.0059 - rmse: 24671.8418 - val_loss: 26601.9668 - val_rmse: 26313.9141\n",
      "Epoch 26/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23120.7285 - rmse: 23120.7285\n",
      "Epoch 26: val_loss improved from 26601.96680 to 26359.80859, saving model to ./ckpt/5_class_lr005/val_rmse_26106.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23059.3652 - rmse: 23059.1777 - val_loss: 26359.8086 - val_rmse: 26105.5020\n",
      "Epoch 27/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20786.9043 - rmse: 20779.8848\n",
      "Epoch 27: val_loss improved from 26359.80859 to 25744.60156, saving model to ./ckpt/5_class_lr005/val_rmse_25492.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20786.9043 - rmse: 20779.8848 - val_loss: 25744.6016 - val_rmse: 25491.5098\n",
      "Epoch 28/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21446.5664 - rmse: 21446.5664\n",
      "Epoch 28: val_loss did not improve from 25744.60156\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21577.9746 - rmse: 21728.5859 - val_loss: 39763.9844 - val_rmse: 39506.5000\n",
      "Epoch 29/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 21792.2539 - rmse: 21792.2539\n",
      "Epoch 29: val_loss improved from 25744.60156 to 24363.70508, saving model to ./ckpt/5_class_lr005/val_rmse_24131.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21730.5312 - rmse: 21802.5352 - val_loss: 24363.7051 - val_rmse: 24130.8652\n",
      "Epoch 30/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23214.5391 - rmse: 23214.5391\n",
      "Epoch 30: val_loss did not improve from 24363.70508\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23254.0527 - rmse: 23312.6641 - val_loss: 27206.5879 - val_rmse: 27073.2461\n",
      "Epoch 31/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21039.5273 - rmse: 21039.5273\n",
      "Epoch 31: val_loss did not improve from 24363.70508\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20842.5352 - rmse: 20876.7812 - val_loss: 34677.4336 - val_rmse: 34450.6641\n",
      "Epoch 32/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20038.6309 - rmse: 20038.6309\n",
      "Epoch 32: val_loss did not improve from 24363.70508\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20385.6797 - rmse: 20415.2109 - val_loss: 24445.3262 - val_rmse: 24245.4297\n",
      "Epoch 33/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22563.4531 - rmse: 22563.4531\n",
      "Epoch 33: val_loss improved from 24363.70508 to 24321.07031, saving model to ./ckpt/5_class_lr005/val_rmse_24126.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22611.1973 - rmse: 22643.3984 - val_loss: 24321.0703 - val_rmse: 24126.0820\n",
      "Epoch 34/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22073.9570 - rmse: 22073.9570\n",
      "Epoch 34: val_loss did not improve from 24321.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22173.2559 - rmse: 22197.9922 - val_loss: 27853.4980 - val_rmse: 27747.0684\n",
      "Epoch 35/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 24999.0078 - rmse: 24999.0078\n",
      "Epoch 35: val_loss did not improve from 24321.07031\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25007.7832 - rmse: 25028.7520 - val_loss: 28495.8281 - val_rmse: 28269.3086\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21659.5938 - rmse: 21711.2988\n",
      "Epoch 36: val_loss improved from 24321.07031 to 23589.27344, saving model to ./ckpt/5_class_lr005/val_rmse_23383.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21659.5938 - rmse: 21711.2988 - val_loss: 23589.2734 - val_rmse: 23383.1113\n",
      "Epoch 37/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21562.2598 - rmse: 21562.2598\n",
      "Epoch 37: val_loss did not improve from 23589.27344\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21414.5977 - rmse: 21394.4141 - val_loss: 28952.4473 - val_rmse: 28728.4082\n",
      "Epoch 38/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19421.9883 - rmse: 19421.9883\n",
      "Epoch 38: val_loss did not improve from 23589.27344\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19629.6699 - rmse: 19627.0645 - val_loss: 23908.5645 - val_rmse: 23719.0000\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20978.3867 - rmse: 20978.3867\n",
      "Epoch 39: val_loss did not improve from 23589.27344\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21062.9785 - rmse: 21120.0312 - val_loss: 23675.8301 - val_rmse: 23512.2422\n",
      "Epoch 40/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19440.9355 - rmse: 19450.6035\n",
      "Epoch 40: val_loss did not improve from 23589.27344\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19440.9355 - rmse: 19450.6035 - val_loss: 32152.9805 - val_rmse: 31932.7441\n",
      "Epoch 41/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19117.8633 - rmse: 19117.8633\n",
      "Epoch 41: val_loss did not improve from 23589.27344\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19050.7305 - rmse: 19036.3574 - val_loss: 26964.3008 - val_rmse: 26744.6055\n",
      "Epoch 42/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21874.6777 - rmse: 21837.0508\n",
      "Epoch 42: val_loss did not improve from 23589.27344\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21874.6777 - rmse: 21837.0508 - val_loss: 24719.8066 - val_rmse: 24576.3320\n",
      "Epoch 43/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19819.9922 - rmse: 19819.9922\n",
      "Epoch 43: val_loss did not improve from 23589.27344\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19752.9980 - rmse: 19739.9434 - val_loss: 24676.4062 - val_rmse: 24473.8262\n",
      "Epoch 44/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19989.3965 - rmse: 19989.3965\n",
      "Epoch 44: val_loss did not improve from 23589.27344\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20068.1191 - rmse: 20196.1992 - val_loss: 24008.0957 - val_rmse: 23834.4277\n",
      "Epoch 45/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20560.1855 - rmse: 20560.1855\n",
      "Epoch 45: val_loss did not improve from 23589.27344\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20544.2656 - rmse: 20533.5293 - val_loss: 25840.9844 - val_rmse: 25630.7598\n",
      "Epoch 46/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19041.1602 - rmse: 19067.8301\n",
      "Epoch 46: val_loss did not improve from 23589.27344\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19041.1602 - rmse: 19067.8301 - val_loss: 24847.9902 - val_rmse: 24738.0625\n",
      "Epoch 47/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 22630.8457 - rmse: 22630.8457\n",
      "Epoch 47: val_loss did not improve from 23589.27344\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22756.8125 - rmse: 22824.5898 - val_loss: 31891.7773 - val_rmse: 31655.3789\n",
      "Epoch 48/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22005.6211 - rmse: 22005.6211\n",
      "Epoch 48: val_loss did not improve from 23589.27344\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22014.1895 - rmse: 22019.9707 - val_loss: 33188.0586 - val_rmse: 32895.6641\n",
      "Epoch 49/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19709.2578 - rmse: 19727.6504\n",
      "Epoch 49: val_loss did not improve from 23589.27344\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19709.2578 - rmse: 19727.6504 - val_loss: 24442.7715 - val_rmse: 24222.3594\n",
      "Epoch 50/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18287.9316 - rmse: 18287.9316\n",
      "Epoch 50: val_loss improved from 23589.27344 to 23097.41602, saving model to ./ckpt/5_class_lr005/val_rmse_22930.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18663.3711 - rmse: 18631.4141 - val_loss: 23097.4160 - val_rmse: 22929.7812\n",
      "Epoch 51/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18848.5254 - rmse: 18848.0840\n",
      "Epoch 51: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18848.5254 - rmse: 18848.0840 - val_loss: 25343.1758 - val_rmse: 25098.6719\n",
      "Epoch 52/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19089.6973 - rmse: 19089.6973\n",
      "Epoch 52: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19013.9863 - rmse: 19018.3281 - val_loss: 23452.8613 - val_rmse: 23295.1738\n",
      "Epoch 53/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19085.2578 - rmse: 19085.2578\n",
      "Epoch 53: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19304.0078 - rmse: 19354.5176 - val_loss: 28583.7832 - val_rmse: 28378.2930\n",
      "Epoch 54/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18979.7734 - rmse: 18979.7734\n",
      "Epoch 54: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19136.3086 - rmse: 19131.7773 - val_loss: 24074.9707 - val_rmse: 23859.9668\n",
      "Epoch 55/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19606.6543 - rmse: 19606.6543\n",
      "Epoch 55: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19589.8730 - rmse: 19562.7168 - val_loss: 26312.9062 - val_rmse: 26080.3516\n",
      "Epoch 56/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19379.7754 - rmse: 19428.8184\n",
      "Epoch 56: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 19379.7754 - rmse: 19428.8184 - val_loss: 26956.5215 - val_rmse: 26707.1367\n",
      "Epoch 57/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21687.5039 - rmse: 21703.2344\n",
      "Epoch 57: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21687.5039 - rmse: 21703.2344 - val_loss: 25473.3359 - val_rmse: 25273.8457\n",
      "Epoch 58/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18562.5684 - rmse: 18562.5684\n",
      "Epoch 58: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18569.2129 - rmse: 18572.9766 - val_loss: 25581.5762 - val_rmse: 25333.9961\n",
      "Epoch 59/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17149.3066 - rmse: 17149.3066\n",
      "Epoch 59: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17509.6504 - rmse: 17521.9961 - val_loss: 23185.8418 - val_rmse: 23012.0898\n",
      "Epoch 60/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20312.2324 - rmse: 20264.0098\n",
      "Epoch 60: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20312.2324 - rmse: 20264.0098 - val_loss: 28054.4844 - val_rmse: 27794.3438\n",
      "Epoch 61/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17560.9844 - rmse: 17560.9844\n",
      "Epoch 61: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17616.0215 - rmse: 17623.3457 - val_loss: 32155.9277 - val_rmse: 31851.6523\n",
      "Epoch 62/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21125.4297 - rmse: 21125.4297\n",
      "Epoch 62: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21069.8516 - rmse: 21032.3652 - val_loss: 32556.7285 - val_rmse: 32317.0195\n",
      "Epoch 63/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19052.9746 - rmse: 19052.9746\n",
      "Epoch 63: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19098.5137 - rmse: 19129.2266 - val_loss: 32557.1465 - val_rmse: 32295.0918\n",
      "Epoch 64/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18078.1797 - rmse: 18078.1797\n",
      "Epoch 64: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18017.4277 - rmse: 17991.2266 - val_loss: 28635.0098 - val_rmse: 28395.7402\n",
      "Epoch 65/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18047.8574 - rmse: 18047.8574\n",
      "Epoch 65: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18030.3652 - rmse: 18071.1777 - val_loss: 34594.9609 - val_rmse: 34356.8516\n",
      "Epoch 66/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17967.6348 - rmse: 17967.6348\n",
      "Epoch 66: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17939.3086 - rmse: 17920.2051 - val_loss: 25720.5117 - val_rmse: 25502.1660\n",
      "Epoch 67/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19293.7090 - rmse: 19293.7090\n",
      "Epoch 67: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19316.5273 - rmse: 19296.0332 - val_loss: 29960.4688 - val_rmse: 29709.7676\n",
      "Epoch 68/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16508.1602 - rmse: 16508.1602\n",
      "Epoch 68: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16525.1113 - rmse: 16521.2422 - val_loss: 24075.6543 - val_rmse: 23876.6777\n",
      "Epoch 69/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19180.8906 - rmse: 19160.6621\n",
      "Epoch 69: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19180.8906 - rmse: 19160.6621 - val_loss: 26483.5762 - val_rmse: 26247.0645\n",
      "Epoch 70/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18061.8086 - rmse: 18061.8086\n",
      "Epoch 70: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18043.8301 - rmse: 18031.7031 - val_loss: 23529.7715 - val_rmse: 23450.6992\n",
      "Epoch 71/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17376.5273 - rmse: 17376.5273\n",
      "Epoch 71: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17331.6523 - rmse: 17301.3867 - val_loss: 30114.6270 - val_rmse: 29868.7246\n",
      "Epoch 72/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19399.2676 - rmse: 19399.2676\n",
      "Epoch 72: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19509.2715 - rmse: 19525.4375 - val_loss: 37418.7461 - val_rmse: 37191.8398\n",
      "Epoch 73/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17087.8262 - rmse: 17087.8262\n",
      "Epoch 73: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17097.8867 - rmse: 17104.6699 - val_loss: 23847.0000 - val_rmse: 23714.8770\n",
      "Epoch 74/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17366.7402 - rmse: 17366.7402\n",
      "Epoch 74: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17343.4043 - rmse: 17327.6660 - val_loss: 24668.1504 - val_rmse: 24469.6094\n",
      "Epoch 75/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17912.8047 - rmse: 17912.8047\n",
      "Epoch 75: val_loss did not improve from 23097.41602\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17975.1270 - rmse: 18025.6133 - val_loss: 26780.9922 - val_rmse: 26560.0898\n",
      "Epoch 75: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:05:07.002444: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 104868.4453 - rmse: 104603.3906\n",
      "Epoch 1: val_loss improved from inf to 65411.21484, saving model to ./ckpt/5_class_lr005/val_rmse_65729.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 104868.4453 - rmse: 104603.3906 - val_loss: 65411.2148 - val_rmse: 65729.4609\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 63605.5312 - rmse: 63605.5312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:05:07.911122: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 52661.8281 - rmse: 52604.0820\n",
      "Epoch 2: val_loss improved from 65411.21484 to 47740.94922, saving model to ./ckpt/5_class_lr005/val_rmse_47736.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 52661.8281 - rmse: 52604.0820 - val_loss: 47740.9492 - val_rmse: 47736.4922\n",
      "Epoch 3/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 46784.0391 - rmse: 46711.5312\n",
      "Epoch 3: val_loss improved from 47740.94922 to 44796.26953, saving model to ./ckpt/5_class_lr005/val_rmse_44680.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 46784.0391 - rmse: 46711.5312 - val_loss: 44796.2695 - val_rmse: 44680.2617\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 44141.6055 - rmse: 44077.4531\n",
      "Epoch 4: val_loss improved from 44796.26953 to 43286.99609, saving model to ./ckpt/5_class_lr005/val_rmse_43071.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 44141.6055 - rmse: 44077.4531 - val_loss: 43286.9961 - val_rmse: 43071.1055\n",
      "Epoch 5/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 41622.3398 - rmse: 41622.3398\n",
      "Epoch 5: val_loss improved from 43286.99609 to 40640.67578, saving model to ./ckpt/5_class_lr005/val_rmse_40414.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 41450.2227 - rmse: 41345.7969 - val_loss: 40640.6758 - val_rmse: 40413.6953\n",
      "Epoch 6/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 39718.4961 - rmse: 39718.4961\n",
      "Epoch 6: val_loss did not improve from 40640.67578\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 39937.8242 - rmse: 39976.4023 - val_loss: 43716.4570 - val_rmse: 43420.6992\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 38088.2812 - rmse: 38173.8828\n",
      "Epoch 7: val_loss improved from 40640.67578 to 37701.26562, saving model to ./ckpt/5_class_lr005/val_rmse_37431.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 38088.2812 - rmse: 38173.8828 - val_loss: 37701.2656 - val_rmse: 37430.8008\n",
      "Epoch 8/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 36666.6484 - rmse: 36666.6484\n",
      "Epoch 8: val_loss improved from 37701.26562 to 36169.06250, saving model to ./ckpt/5_class_lr005/val_rmse_35871.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 36530.2422 - rmse: 36536.1562 - val_loss: 36169.0625 - val_rmse: 35871.0078\n",
      "Epoch 9/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 32766.3418 - rmse: 32766.3418\n",
      "Epoch 9: val_loss improved from 36169.06250 to 35060.57422, saving model to ./ckpt/5_class_lr005/val_rmse_34797.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 32621.0117 - rmse: 32648.4824 - val_loss: 35060.5742 - val_rmse: 34796.6406\n",
      "Epoch 10/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 35027.8203 - rmse: 35027.8203\n",
      "Epoch 10: val_loss improved from 35060.57422 to 33948.77344, saving model to ./ckpt/5_class_lr005/val_rmse_33713.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 34843.9844 - rmse: 34792.1406 - val_loss: 33948.7734 - val_rmse: 33712.7578\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 30486.7090 - rmse: 30486.7090\n",
      "Epoch 11: val_loss did not improve from 33948.77344\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 30410.3828 - rmse: 30376.7188 - val_loss: 36218.2344 - val_rmse: 35967.4766\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 32715.1836 - rmse: 32719.5605\n",
      "Epoch 12: val_loss did not improve from 33948.77344\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 32715.1836 - rmse: 32719.5605 - val_loss: 34347.3242 - val_rmse: 34186.9336\n",
      "Epoch 13/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 31514.5137 - rmse: 31514.5137\n",
      "Epoch 13: val_loss improved from 33948.77344 to 33322.74609, saving model to ./ckpt/5_class_lr005/val_rmse_33075.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 31027.2637 - rmse: 31062.3047 - val_loss: 33322.7461 - val_rmse: 33075.1836\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27147.4492 - rmse: 27233.0957\n",
      "Epoch 14: val_loss improved from 33322.74609 to 30781.88477, saving model to ./ckpt/5_class_lr005/val_rmse_30516.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 27147.4492 - rmse: 27233.0957 - val_loss: 30781.8848 - val_rmse: 30516.1484\n",
      "Epoch 15/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 29615.7109 - rmse: 29588.7695\n",
      "Epoch 15: val_loss did not improve from 30781.88477\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 29615.7109 - rmse: 29588.7695 - val_loss: 37872.4141 - val_rmse: 37655.0430\n",
      "Epoch 16/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 29168.6680 - rmse: 29168.6680\n",
      "Epoch 16: val_loss improved from 30781.88477 to 28920.81250, saving model to ./ckpt/5_class_lr005/val_rmse_28791.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 29094.6875 - rmse: 29050.4199 - val_loss: 28920.8125 - val_rmse: 28790.5723\n",
      "Epoch 17/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 27254.3066 - rmse: 27254.3066\n",
      "Epoch 17: val_loss did not improve from 28920.81250\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27489.1602 - rmse: 27608.0215 - val_loss: 35949.4883 - val_rmse: 35717.2852\n",
      "Epoch 18/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 28203.1719 - rmse: 28203.1719\n",
      "Epoch 18: val_loss did not improve from 28920.81250\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 28058.5820 - rmse: 28064.8613 - val_loss: 35019.4648 - val_rmse: 34752.8477\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25511.0879 - rmse: 25571.2402\n",
      "Epoch 19: val_loss did not improve from 28920.81250\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 25511.0879 - rmse: 25571.2402 - val_loss: 31925.2207 - val_rmse: 31703.1074\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24326.5605 - rmse: 24302.5117\n",
      "Epoch 20: val_loss improved from 28920.81250 to 28704.89062, saving model to ./ckpt/5_class_lr005/val_rmse_28596.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24326.5605 - rmse: 24302.5117 - val_loss: 28704.8906 - val_rmse: 28595.7383\n",
      "Epoch 21/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 27531.4551 - rmse: 27531.4551\n",
      "Epoch 21: val_loss did not improve from 28704.89062\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27462.5996 - rmse: 27522.2676 - val_loss: 29105.0918 - val_rmse: 28855.5918\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22652.2441 - rmse: 22652.2441\n",
      "Epoch 22: val_loss improved from 28704.89062 to 25659.34180, saving model to ./ckpt/5_class_lr005/val_rmse_25468.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22649.6504 - rmse: 22647.9023 - val_loss: 25659.3418 - val_rmse: 25468.1758\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23592.4453 - rmse: 23592.4453\n",
      "Epoch 23: val_loss did not improve from 25659.34180\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23435.6426 - rmse: 23420.0996 - val_loss: 25757.6758 - val_rmse: 25519.8730\n",
      "Epoch 24/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21874.5156 - rmse: 21874.5156\n",
      "Epoch 24: val_loss did not improve from 25659.34180\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21865.2715 - rmse: 21872.8887 - val_loss: 33240.9570 - val_rmse: 33004.4570\n",
      "Epoch 25/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23690.6953 - rmse: 23690.6953\n",
      "Epoch 25: val_loss did not improve from 25659.34180\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23711.9648 - rmse: 23736.6680 - val_loss: 25894.3379 - val_rmse: 25660.9922\n",
      "Epoch 26/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21241.4707 - rmse: 21241.4707\n",
      "Epoch 26: val_loss did not improve from 25659.34180\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21249.3047 - rmse: 21248.2422 - val_loss: 27156.9648 - val_rmse: 27040.7930\n",
      "Epoch 27/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21480.6094 - rmse: 21480.6094\n",
      "Epoch 27: val_loss did not improve from 25659.34180\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21621.2715 - rmse: 21662.6719 - val_loss: 26695.1934 - val_rmse: 26483.6133\n",
      "Epoch 28/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22003.6504 - rmse: 22003.6504\n",
      "Epoch 28: val_loss did not improve from 25659.34180\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22011.0938 - rmse: 22023.9219 - val_loss: 27477.1973 - val_rmse: 27457.4043\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21402.4551 - rmse: 21402.4551\n",
      "Epoch 29: val_loss improved from 25659.34180 to 24160.91992, saving model to ./ckpt/5_class_lr005/val_rmse_24013.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21374.7715 - rmse: 21356.1016 - val_loss: 24160.9199 - val_rmse: 24013.1426\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20984.4004 - rmse: 20984.4004\n",
      "Epoch 30: val_loss did not improve from 24160.91992\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21105.6328 - rmse: 21187.3984 - val_loss: 25335.4316 - val_rmse: 25095.7598\n",
      "Epoch 31/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22092.6680 - rmse: 22092.6680\n",
      "Epoch 31: val_loss did not improve from 24160.91992\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22234.2012 - rmse: 22329.6543 - val_loss: 27753.3945 - val_rmse: 27505.8965\n",
      "Epoch 32/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 25118.7812 - rmse: 25118.7812\n",
      "Epoch 32: val_loss did not improve from 24160.91992\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25013.4062 - rmse: 24994.6699 - val_loss: 25085.9590 - val_rmse: 24998.5137\n",
      "Epoch 33/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22682.5918 - rmse: 22682.5918\n",
      "Epoch 33: val_loss did not improve from 24160.91992\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22882.2773 - rmse: 23023.2969 - val_loss: 34753.6328 - val_rmse: 34556.2969\n",
      "Epoch 34/120\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 20765.4512 - rmse: 20765.4512\n",
      "Epoch 34: val_loss did not improve from 24160.91992\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21027.1016 - rmse: 21061.9922 - val_loss: 36240.0312 - val_rmse: 36025.1836\n",
      "Epoch 35/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22354.4219 - rmse: 22354.4219\n",
      "Epoch 35: val_loss did not improve from 24160.91992\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22387.9336 - rmse: 22431.6074 - val_loss: 28142.7656 - val_rmse: 28106.5859\n",
      "Epoch 36/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23814.5137 - rmse: 23814.5137\n",
      "Epoch 36: val_loss did not improve from 24160.91992\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23427.4023 - rmse: 23447.8145 - val_loss: 28121.1992 - val_rmse: 27872.6875\n",
      "Epoch 37/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21033.2207 - rmse: 21033.2207\n",
      "Epoch 37: val_loss improved from 24160.91992 to 23605.79297, saving model to ./ckpt/5_class_lr005/val_rmse_23413.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20921.1309 - rmse: 20862.2930 - val_loss: 23605.7930 - val_rmse: 23413.2109\n",
      "Epoch 38/120\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 19882.0117 - rmse: 19882.0117\n",
      "Epoch 38: val_loss did not improve from 23605.79297\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19966.9062 - rmse: 19928.8457 - val_loss: 24685.6426 - val_rmse: 24453.0215\n",
      "Epoch 39/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20201.5625 - rmse: 20201.5625\n",
      "Epoch 39: val_loss did not improve from 23605.79297\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20423.1328 - rmse: 20450.1406 - val_loss: 31321.1387 - val_rmse: 31099.6250\n",
      "Epoch 40/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21339.1895 - rmse: 21327.2832\n",
      "Epoch 40: val_loss did not improve from 23605.79297\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21339.1895 - rmse: 21327.2832 - val_loss: 25958.4492 - val_rmse: 25914.0684\n",
      "Epoch 41/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22303.7129 - rmse: 22303.7129\n",
      "Epoch 41: val_loss did not improve from 23605.79297\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22267.9473 - rmse: 22272.8711 - val_loss: 24689.8008 - val_rmse: 24466.6211\n",
      "Epoch 42/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22158.7793 - rmse: 22158.7793\n",
      "Epoch 42: val_loss did not improve from 23605.79297\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22034.3320 - rmse: 21998.4297 - val_loss: 24410.2852 - val_rmse: 24177.7305\n",
      "Epoch 43/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18721.8750 - rmse: 18721.8750\n",
      "Epoch 43: val_loss improved from 23605.79297 to 23396.53125, saving model to ./ckpt/5_class_lr005/val_rmse_23313.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18649.1973 - rmse: 18710.6016 - val_loss: 23396.5312 - val_rmse: 23313.2090\n",
      "Epoch 44/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19546.8594 - rmse: 19550.4902\n",
      "Epoch 44: val_loss did not improve from 23396.53125\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19546.8594 - rmse: 19550.4902 - val_loss: 26892.5312 - val_rmse: 26889.3086\n",
      "Epoch 45/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19964.0527 - rmse: 19964.0527\n",
      "Epoch 45: val_loss did not improve from 23396.53125\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20131.1953 - rmse: 20254.9375 - val_loss: 24631.7090 - val_rmse: 24580.7695\n",
      "Epoch 46/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21873.2539 - rmse: 21872.4629\n",
      "Epoch 46: val_loss improved from 23396.53125 to 23079.46289, saving model to ./ckpt/5_class_lr005/val_rmse_22939.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21873.2539 - rmse: 21872.4629 - val_loss: 23079.4629 - val_rmse: 22939.0781\n",
      "Epoch 47/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20049.2969 - rmse: 20049.2969\n",
      "Epoch 47: val_loss did not improve from 23079.46289\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19888.0371 - rmse: 19872.4492 - val_loss: 25992.7676 - val_rmse: 25725.7129\n",
      "Epoch 48/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19213.4492 - rmse: 19241.1484\n",
      "Epoch 48: val_loss did not improve from 23079.46289\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19213.4492 - rmse: 19241.1484 - val_loss: 24950.5020 - val_rmse: 24802.0234\n",
      "Epoch 49/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21406.4355 - rmse: 21406.4355\n",
      "Epoch 49: val_loss did not improve from 23079.46289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21495.9492 - rmse: 21517.9824 - val_loss: 25771.9707 - val_rmse: 25719.0527\n",
      "Epoch 50/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19056.5527 - rmse: 19056.5527\n",
      "Epoch 50: val_loss did not improve from 23079.46289\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19154.2715 - rmse: 19197.6602 - val_loss: 28083.0098 - val_rmse: 27815.4004\n",
      "Epoch 51/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20494.9824 - rmse: 20494.9824\n",
      "Epoch 51: val_loss did not improve from 23079.46289\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20156.8203 - rmse: 20184.5371 - val_loss: 23418.7207 - val_rmse: 23181.9570\n",
      "Epoch 52/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19184.1035 - rmse: 19184.1035\n",
      "Epoch 52: val_loss did not improve from 23079.46289\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19089.0684 - rmse: 19075.6738 - val_loss: 28814.1738 - val_rmse: 28568.1934\n",
      "Epoch 53/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17985.5957 - rmse: 17985.5957\n",
      "Epoch 53: val_loss did not improve from 23079.46289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18179.2266 - rmse: 18196.2832 - val_loss: 30696.0977 - val_rmse: 30472.1426\n",
      "Epoch 54/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19672.1328 - rmse: 19672.1328\n",
      "Epoch 54: val_loss did not improve from 23079.46289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19690.5254 - rmse: 19712.8652 - val_loss: 29240.1934 - val_rmse: 29005.7578\n",
      "Epoch 55/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18542.8945 - rmse: 18542.8945\n",
      "Epoch 55: val_loss did not improve from 23079.46289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18647.0273 - rmse: 18671.0391 - val_loss: 23745.1914 - val_rmse: 23478.5176\n",
      "Epoch 56/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20314.8828 - rmse: 20314.8828\n",
      "Epoch 56: val_loss did not improve from 23079.46289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20312.8438 - rmse: 20320.3008 - val_loss: 24036.9668 - val_rmse: 23972.7051\n",
      "Epoch 57/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20104.9980 - rmse: 20238.4492\n",
      "Epoch 57: val_loss did not improve from 23079.46289\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20104.9980 - rmse: 20238.4492 - val_loss: 30419.8613 - val_rmse: 30172.8789\n",
      "Epoch 58/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19254.1055 - rmse: 19254.1055\n",
      "Epoch 58: val_loss did not improve from 23079.46289\n",
      "70/70 [==============================] - 1s 21ms/step - loss: 19341.8457 - rmse: 19326.0957 - val_loss: 25951.7930 - val_rmse: 25731.4434\n",
      "Epoch 59/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18506.3496 - rmse: 18506.3496\n",
      "Epoch 59: val_loss improved from 23079.46289 to 22495.14062, saving model to ./ckpt/5_class_lr005/val_rmse_22353.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 18314.2168 - rmse: 18297.9590 - val_loss: 22495.1406 - val_rmse: 22352.5273\n",
      "Epoch 60/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18194.1309 - rmse: 18194.1309\n",
      "Epoch 60: val_loss did not improve from 22495.14062\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18339.1426 - rmse: 18352.1270 - val_loss: 29901.4219 - val_rmse: 29686.2754\n",
      "Epoch 61/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19014.9961 - rmse: 19071.4902\n",
      "Epoch 61: val_loss did not improve from 22495.14062\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19014.9961 - rmse: 19071.4902 - val_loss: 32232.4844 - val_rmse: 31969.2578\n",
      "Epoch 62/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18837.3730 - rmse: 18837.3730\n",
      "Epoch 62: val_loss did not improve from 22495.14062\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18767.1523 - rmse: 18784.6895 - val_loss: 24814.0977 - val_rmse: 24768.3359\n",
      "Epoch 63/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19635.0664 - rmse: 19635.0664\n",
      "Epoch 63: val_loss did not improve from 22495.14062\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19638.8281 - rmse: 19641.3633 - val_loss: 23496.7891 - val_rmse: 23253.8984\n",
      "Epoch 64/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18014.3203 - rmse: 18014.3203\n",
      "Epoch 64: val_loss did not improve from 22495.14062\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18011.6406 - rmse: 18009.8320 - val_loss: 22754.9219 - val_rmse: 22603.7910\n",
      "Epoch 65/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19347.2734 - rmse: 19347.2734\n",
      "Epoch 65: val_loss improved from 22495.14062 to 22475.32227, saving model to ./ckpt/5_class_lr005/val_rmse_22319.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19378.3047 - rmse: 19446.7832 - val_loss: 22475.3223 - val_rmse: 22318.6992\n",
      "Epoch 66/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16835.2871 - rmse: 16835.2871\n",
      "Epoch 66: val_loss did not improve from 22475.32227\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16939.8828 - rmse: 16978.0156 - val_loss: 26017.8691 - val_rmse: 25782.4180\n",
      "Epoch 67/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18902.4492 - rmse: 18902.4492\n",
      "Epoch 67: val_loss did not improve from 22475.32227\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18881.7383 - rmse: 18912.5332 - val_loss: 35627.1055 - val_rmse: 35431.0625\n",
      "Epoch 68/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18918.1250 - rmse: 19001.9141\n",
      "Epoch 68: val_loss did not improve from 22475.32227\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18918.1250 - rmse: 19001.9141 - val_loss: 29124.4551 - val_rmse: 28870.8340\n",
      "Epoch 69/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18110.4180 - rmse: 18110.4180\n",
      "Epoch 69: val_loss did not improve from 22475.32227\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18138.9453 - rmse: 18158.1855 - val_loss: 28527.7754 - val_rmse: 28262.4609\n",
      "Epoch 70/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17858.9043 - rmse: 17858.9043\n",
      "Epoch 70: val_loss did not improve from 22475.32227\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 17763.2812 - rmse: 17742.0762 - val_loss: 28681.0859 - val_rmse: 28423.7988\n",
      "Epoch 71/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18512.3145 - rmse: 18512.3145\n",
      "Epoch 71: val_loss did not improve from 22475.32227\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18446.2285 - rmse: 18455.0234 - val_loss: 23824.0508 - val_rmse: 23601.3789\n",
      "Epoch 72/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17832.0156 - rmse: 17818.4473\n",
      "Epoch 72: val_loss improved from 22475.32227 to 22202.08008, saving model to ./ckpt/5_class_lr005/val_rmse_22033.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17832.0156 - rmse: 17818.4473 - val_loss: 22202.0801 - val_rmse: 22032.7090\n",
      "Epoch 73/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18138.0215 - rmse: 18138.0215\n",
      "Epoch 73: val_loss did not improve from 22202.08008\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18184.9395 - rmse: 18179.4297 - val_loss: 22413.5762 - val_rmse: 22256.3691\n",
      "Epoch 74/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19552.6426 - rmse: 19552.6426\n",
      "Epoch 74: val_loss did not improve from 22202.08008\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19891.1816 - rmse: 19999.7598 - val_loss: 24882.8105 - val_rmse: 24639.6914\n",
      "Epoch 75/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17004.6602 - rmse: 17004.6602\n",
      "Epoch 75: val_loss did not improve from 22202.08008\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17107.4883 - rmse: 17158.7949 - val_loss: 33281.1250 - val_rmse: 33089.2148\n",
      "Epoch 76/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18990.2383 - rmse: 18990.2383\n",
      "Epoch 76: val_loss did not improve from 22202.08008\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19038.9160 - rmse: 19080.4082 - val_loss: 22903.7227 - val_rmse: 22835.5488\n",
      "Epoch 77/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16786.2363 - rmse: 16786.2363\n",
      "Epoch 77: val_loss did not improve from 22202.08008\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16864.4219 - rmse: 16854.8867 - val_loss: 22791.9453 - val_rmse: 22593.5586\n",
      "Epoch 78/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17175.0469 - rmse: 17175.0469\n",
      "Epoch 78: val_loss did not improve from 22202.08008\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17291.7949 - rmse: 17368.4844 - val_loss: 30071.1621 - val_rmse: 29850.1367\n",
      "Epoch 79/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18037.5977 - rmse: 18037.5977\n",
      "Epoch 79: val_loss did not improve from 22202.08008\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18149.1621 - rmse: 18188.9062 - val_loss: 25688.7168 - val_rmse: 25437.5215\n",
      "Epoch 80/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17267.5840 - rmse: 17267.5840\n",
      "Epoch 80: val_loss did not improve from 22202.08008\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17260.9707 - rmse: 17257.9414 - val_loss: 23352.3047 - val_rmse: 23110.9141\n",
      "Epoch 81/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16632.3301 - rmse: 16632.3301\n",
      "Epoch 81: val_loss did not improve from 22202.08008\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16699.2715 - rmse: 16743.1973 - val_loss: 22811.4316 - val_rmse: 22731.6562\n",
      "Epoch 82/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18283.6289 - rmse: 18283.6289\n",
      "Epoch 82: val_loss did not improve from 22202.08008\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18333.6523 - rmse: 18316.1621 - val_loss: 27989.8887 - val_rmse: 27795.5078\n",
      "Epoch 83/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17416.1777 - rmse: 17416.1777\n",
      "Epoch 83: val_loss did not improve from 22202.08008\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17411.5762 - rmse: 17408.4707 - val_loss: 25834.2324 - val_rmse: 25579.0801\n",
      "Epoch 84/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18782.9121 - rmse: 18782.9121\n",
      "Epoch 84: val_loss did not improve from 22202.08008\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18678.6914 - rmse: 18660.4844 - val_loss: 22651.1484 - val_rmse: 22480.2344\n",
      "Epoch 85/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16927.7070 - rmse: 16937.0059\n",
      "Epoch 85: val_loss did not improve from 22202.08008\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16927.7070 - rmse: 16937.0059 - val_loss: 22862.6387 - val_rmse: 22559.3203\n",
      "Epoch 86/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16180.8672 - rmse: 16180.8672\n",
      "Epoch 86: val_loss did not improve from 22202.08008\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16420.2246 - rmse: 16437.0645 - val_loss: 31138.3398 - val_rmse: 30884.1855\n",
      "Epoch 87/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17569.7383 - rmse: 17569.7383\n",
      "Epoch 87: val_loss did not improve from 22202.08008\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17961.3828 - rmse: 17918.5215 - val_loss: 32028.9219 - val_rmse: 31811.5605\n",
      "Epoch 88/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19194.5078 - rmse: 19194.5078\n",
      "Epoch 88: val_loss improved from 22202.08008 to 21773.50195, saving model to ./ckpt/5_class_lr005/val_rmse_21616.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19141.0078 - rmse: 19137.8438 - val_loss: 21773.5020 - val_rmse: 21615.8594\n",
      "Epoch 89/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16950.9453 - rmse: 16939.2754\n",
      "Epoch 89: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16950.9453 - rmse: 16939.2754 - val_loss: 32990.9961 - val_rmse: 32772.0977\n",
      "Epoch 90/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16273.6992 - rmse: 16273.6992\n",
      "Epoch 90: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16237.9453 - rmse: 16210.0410 - val_loss: 25583.3906 - val_rmse: 25321.6758\n",
      "Epoch 91/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16919.9316 - rmse: 16935.0352\n",
      "Epoch 91: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16919.9316 - rmse: 16935.0352 - val_loss: 22439.3223 - val_rmse: 22285.1016\n",
      "Epoch 92/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16100.4619 - rmse: 16100.4619\n",
      "Epoch 92: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16155.0723 - rmse: 16191.9033 - val_loss: 22504.2285 - val_rmse: 22324.8477\n",
      "Epoch 93/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16079.4170 - rmse: 16079.4170\n",
      "Epoch 93: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16055.5762 - rmse: 16067.3398 - val_loss: 33389.2773 - val_rmse: 33153.8281\n",
      "Epoch 94/120\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 16669.4434 - rmse: 16669.4434\n",
      "Epoch 94: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16477.4023 - rmse: 16505.3203 - val_loss: 23728.0684 - val_rmse: 23451.6855\n",
      "Epoch 95/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16009.2207 - rmse: 16009.2207\n",
      "Epoch 95: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15930.6963 - rmse: 15912.3301 - val_loss: 25908.1562 - val_rmse: 25653.3027\n",
      "Epoch 96/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16353.1670 - rmse: 16353.1670\n",
      "Epoch 96: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16396.1953 - rmse: 16409.8457 - val_loss: 23481.7812 - val_rmse: 23206.9492\n",
      "Epoch 97/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16233.4238 - rmse: 16233.4238\n",
      "Epoch 97: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16191.7207 - rmse: 16163.5947 - val_loss: 23232.5625 - val_rmse: 22982.4785\n",
      "Epoch 98/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18317.2344 - rmse: 18317.2344\n",
      "Epoch 98: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18138.7266 - rmse: 18159.6309 - val_loss: 30776.6289 - val_rmse: 30566.7500\n",
      "Epoch 99/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15851.3135 - rmse: 15851.3135\n",
      "Epoch 99: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16023.9180 - rmse: 16065.5410 - val_loss: 31694.9941 - val_rmse: 31479.7363\n",
      "Epoch 100/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17238.8086 - rmse: 17238.8086\n",
      "Epoch 100: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17370.2910 - rmse: 17450.0508 - val_loss: 22832.3535 - val_rmse: 22618.8652\n",
      "Epoch 101/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17690.6641 - rmse: 17690.6641\n",
      "Epoch 101: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17676.4570 - rmse: 17708.1172 - val_loss: 24143.2734 - val_rmse: 23874.6133\n",
      "Epoch 102/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15481.1523 - rmse: 15481.1523\n",
      "Epoch 102: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15508.9727 - rmse: 15527.7344 - val_loss: 27774.8594 - val_rmse: 27485.8262\n",
      "Epoch 103/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15112.2246 - rmse: 15112.2246\n",
      "Epoch 103: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15319.9883 - rmse: 15419.3193 - val_loss: 32583.3555 - val_rmse: 32380.3047\n",
      "Epoch 104/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18213.8945 - rmse: 18196.3418\n",
      "Epoch 104: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18213.8945 - rmse: 18196.3418 - val_loss: 29628.8027 - val_rmse: 29324.7324\n",
      "Epoch 105/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15700.3789 - rmse: 15700.3789\n",
      "Epoch 105: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15693.4082 - rmse: 15688.7070 - val_loss: 24276.5352 - val_rmse: 24003.0508\n",
      "Epoch 106/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15594.2744 - rmse: 15594.2744\n",
      "Epoch 106: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16119.0771 - rmse: 16158.9844 - val_loss: 24339.5352 - val_rmse: 24070.6055\n",
      "Epoch 107/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15947.0127 - rmse: 15947.0127\n",
      "Epoch 107: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15909.6064 - rmse: 15894.9961 - val_loss: 30924.6426 - val_rmse: 30671.4062\n",
      "Epoch 108/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16375.7686 - rmse: 16386.5566\n",
      "Epoch 108: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16375.7686 - rmse: 16386.5566 - val_loss: 26939.2285 - val_rmse: 26667.1875\n",
      "Epoch 109/120\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 14530.4053 - rmse: 14530.4053\n",
      "Epoch 109: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14644.5215 - rmse: 14670.3496 - val_loss: 23161.5234 - val_rmse: 22923.5352\n",
      "Epoch 110/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16301.2246 - rmse: 16301.2246\n",
      "Epoch 110: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16298.5850 - rmse: 16329.6943 - val_loss: 25093.1113 - val_rmse: 24819.2891\n",
      "Epoch 111/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15636.6113 - rmse: 15636.6113\n",
      "Epoch 111: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15677.9404 - rmse: 15666.0605 - val_loss: 30883.5840 - val_rmse: 30611.5098\n",
      "Epoch 112/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15295.0693 - rmse: 15295.0693\n",
      "Epoch 112: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15228.3672 - rmse: 15222.6484 - val_loss: 27093.6016 - val_rmse: 26821.0371\n",
      "Epoch 113/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15240.0723 - rmse: 15240.0723\n",
      "Epoch 113: val_loss did not improve from 21773.50195\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15287.5967 - rmse: 15319.6465 - val_loss: 39219.8594 - val_rmse: 39002.5664\n",
      "Epoch 113: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:06:26.125008: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 103386.3828 - rmse: 103060.8672\n",
      "Epoch 1: val_loss improved from inf to 59296.08203, saving model to ./ckpt/5_class_lr005/val_rmse_59494.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 103386.3828 - rmse: 103060.8672 - val_loss: 59296.0820 - val_rmse: 59493.7891\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 71340.3828 - rmse: 71340.3828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:06:27.150831: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 53042.9141 - rmse: 53042.9141\n",
      "Epoch 2: val_loss improved from 59296.08203 to 49207.90234, saving model to ./ckpt/5_class_lr005/val_rmse_49432.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 52804.0938 - rmse: 52771.3164 - val_loss: 49207.9023 - val_rmse: 49431.6406\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 46238.1328 - rmse: 46238.1328\n",
      "Epoch 3: val_loss improved from 49207.90234 to 45162.72266, saving model to ./ckpt/5_class_lr005/val_rmse_45232.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 46213.1602 - rmse: 46196.3164 - val_loss: 45162.7227 - val_rmse: 45232.2031\n",
      "Epoch 4/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 42684.3281 - rmse: 42684.3281\n",
      "Epoch 4: val_loss did not improve from 45162.72266\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 42409.7070 - rmse: 42364.1797 - val_loss: 50426.7734 - val_rmse: 50502.3945\n",
      "Epoch 5/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 41986.2656 - rmse: 41986.2656\n",
      "Epoch 5: val_loss improved from 45162.72266 to 42526.58203, saving model to ./ckpt/5_class_lr005/val_rmse_42489.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 41788.8086 - rmse: 41864.4609 - val_loss: 42526.5820 - val_rmse: 42489.0742\n",
      "Epoch 6/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 38557.4180 - rmse: 38557.4180\n",
      "Epoch 6: val_loss did not improve from 42526.58203\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 38333.1875 - rmse: 38252.9023 - val_loss: 45803.9062 - val_rmse: 45791.0938\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 37083.0156 - rmse: 37083.0156\n",
      "Epoch 7: val_loss improved from 42526.58203 to 40632.39844, saving model to ./ckpt/5_class_lr005/val_rmse_40548.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 37074.7188 - rmse: 37069.1250 - val_loss: 40632.3984 - val_rmse: 40547.7617\n",
      "Epoch 8/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 37790.9531 - rmse: 37790.9531\n",
      "Epoch 8: val_loss improved from 40632.39844 to 40033.18359, saving model to ./ckpt/5_class_lr005/val_rmse_39825.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 38013.2500 - rmse: 38210.7656 - val_loss: 40033.1836 - val_rmse: 39825.3242\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 38500.8672 - rmse: 38414.9414\n",
      "Epoch 9: val_loss improved from 40033.18359 to 36754.28516, saving model to ./ckpt/5_class_lr005/val_rmse_36631.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 38500.8672 - rmse: 38414.9414 - val_loss: 36754.2852 - val_rmse: 36631.2188\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 33593.4688 - rmse: 33531.0664\n",
      "Epoch 10: val_loss improved from 36754.28516 to 35498.68750, saving model to ./ckpt/5_class_lr005/val_rmse_35339.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 33593.4688 - rmse: 33531.0664 - val_loss: 35498.6875 - val_rmse: 35338.7305\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 33481.2305 - rmse: 33481.2305\n",
      "Epoch 11: val_loss did not improve from 35498.68750\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 33397.7852 - rmse: 33459.8984 - val_loss: 36113.6836 - val_rmse: 35975.2695\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 29836.1250 - rmse: 29756.7676\n",
      "Epoch 12: val_loss improved from 35498.68750 to 35271.30469, saving model to ./ckpt/5_class_lr005/val_rmse_35138.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 29836.1250 - rmse: 29756.7676 - val_loss: 35271.3047 - val_rmse: 35138.0586\n",
      "Epoch 13/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 29005.7324 - rmse: 29005.7324\n",
      "Epoch 13: val_loss did not improve from 35271.30469\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 29566.4531 - rmse: 29554.1250 - val_loss: 36720.2617 - val_rmse: 36697.4688\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 29549.6035 - rmse: 29506.6621\n",
      "Epoch 14: val_loss improved from 35271.30469 to 32990.67578, saving model to ./ckpt/5_class_lr005/val_rmse_32985.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 29549.6035 - rmse: 29506.6621 - val_loss: 32990.6758 - val_rmse: 32984.9766\n",
      "Epoch 15/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 27472.7188 - rmse: 27472.7188\n",
      "Epoch 15: val_loss did not improve from 32990.67578\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 27370.1992 - rmse: 27355.0371 - val_loss: 34167.6328 - val_rmse: 34209.0508\n",
      "Epoch 16/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27587.5840 - rmse: 27587.5840\n",
      "Epoch 16: val_loss did not improve from 32990.67578\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 27531.4512 - rmse: 27610.7422 - val_loss: 33957.9336 - val_rmse: 33890.8398\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 26765.9316 - rmse: 26765.9316\n",
      "Epoch 17: val_loss did not improve from 32990.67578\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 26459.0469 - rmse: 26406.8672 - val_loss: 35132.5039 - val_rmse: 35194.9805\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27435.4434 - rmse: 27598.5078\n",
      "Epoch 18: val_loss improved from 32990.67578 to 31396.99414, saving model to ./ckpt/5_class_lr005/val_rmse_31449.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27435.4434 - rmse: 27598.5078 - val_loss: 31396.9941 - val_rmse: 31448.6211\n",
      "Epoch 19/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 25381.3379 - rmse: 25381.3379\n",
      "Epoch 19: val_loss improved from 31396.99414 to 30765.67383, saving model to ./ckpt/5_class_lr005/val_rmse_30832.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 25335.5469 - rmse: 25344.6699 - val_loss: 30765.6738 - val_rmse: 30831.7148\n",
      "Epoch 20/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 24938.3242 - rmse: 24938.3242\n",
      "Epoch 20: val_loss improved from 30765.67383 to 28343.71094, saving model to ./ckpt/5_class_lr005/val_rmse_28366.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 25118.2051 - rmse: 25190.5293 - val_loss: 28343.7109 - val_rmse: 28365.5996\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23522.9785 - rmse: 23522.9785\n",
      "Epoch 21: val_loss improved from 28343.71094 to 27068.72266, saving model to ./ckpt/5_class_lr005/val_rmse_27124.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23389.9375 - rmse: 23424.2441 - val_loss: 27068.7227 - val_rmse: 27123.9668\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22939.5938 - rmse: 22939.5938\n",
      "Epoch 22: val_loss improved from 27068.72266 to 26319.85938, saving model to ./ckpt/5_class_lr005/val_rmse_26334.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22880.2676 - rmse: 22840.2578 - val_loss: 26319.8594 - val_rmse: 26333.5078\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 26375.5840 - rmse: 26375.5840\n",
      "Epoch 23: val_loss did not improve from 26319.85938\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 26383.2930 - rmse: 26369.9355 - val_loss: 28346.0898 - val_rmse: 28355.3965\n",
      "Epoch 24/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 23206.0703 - rmse: 23206.0703\n",
      "Epoch 24: val_loss did not improve from 26319.85938\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23089.9102 - rmse: 23079.6367 - val_loss: 26880.1113 - val_rmse: 26952.7129\n",
      "Epoch 25/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22674.3418 - rmse: 22674.3418\n",
      "Epoch 25: val_loss improved from 26319.85938 to 25670.85352, saving model to ./ckpt/5_class_lr005/val_rmse_25772.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22393.9414 - rmse: 22341.4023 - val_loss: 25670.8535 - val_rmse: 25771.8633\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21952.3770 - rmse: 21952.3770\n",
      "Epoch 26: val_loss did not improve from 25670.85352\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21975.7949 - rmse: 21954.8516 - val_loss: 33048.2773 - val_rmse: 33201.7578\n",
      "Epoch 27/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21599.3320 - rmse: 21549.7012\n",
      "Epoch 27: val_loss did not improve from 25670.85352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21599.3320 - rmse: 21549.7012 - val_loss: 27103.1914 - val_rmse: 27241.0410\n",
      "Epoch 28/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22919.0254 - rmse: 22919.0254\n",
      "Epoch 28: val_loss did not improve from 25670.85352\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 22805.5234 - rmse: 22797.9492 - val_loss: 34486.5430 - val_rmse: 34633.8633\n",
      "Epoch 29/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22110.6797 - rmse: 22110.6797\n",
      "Epoch 29: val_loss improved from 25670.85352 to 24936.03125, saving model to ./ckpt/5_class_lr005/val_rmse_25036.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22061.0469 - rmse: 22051.1445 - val_loss: 24936.0312 - val_rmse: 25036.3770\n",
      "Epoch 30/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22245.1953 - rmse: 22245.1953\n",
      "Epoch 30: val_loss did not improve from 24936.03125\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22181.5215 - rmse: 22170.4961 - val_loss: 25351.1426 - val_rmse: 25397.7812\n",
      "Epoch 31/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21848.4414 - rmse: 21848.4414\n",
      "Epoch 31: val_loss improved from 24936.03125 to 24521.10352, saving model to ./ckpt/5_class_lr005/val_rmse_24623.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21826.4785 - rmse: 21826.3867 - val_loss: 24521.1035 - val_rmse: 24623.2012\n",
      "Epoch 32/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23026.6953 - rmse: 23026.6953\n",
      "Epoch 32: val_loss did not improve from 24521.10352\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22932.8281 - rmse: 22932.0684 - val_loss: 25312.0840 - val_rmse: 25359.5801\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22193.1348 - rmse: 22135.9805\n",
      "Epoch 33: val_loss did not improve from 24521.10352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22193.1348 - rmse: 22135.9805 - val_loss: 25823.9746 - val_rmse: 25923.2676\n",
      "Epoch 34/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21892.4980 - rmse: 21892.4980\n",
      "Epoch 34: val_loss did not improve from 24521.10352\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21737.9922 - rmse: 21717.1875 - val_loss: 26589.7891 - val_rmse: 26714.4219\n",
      "Epoch 35/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20004.0195 - rmse: 20004.0195\n",
      "Epoch 35: val_loss did not improve from 24521.10352\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19998.8750 - rmse: 19995.4043 - val_loss: 25178.4180 - val_rmse: 25276.1504\n",
      "Epoch 36/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19007.5996 - rmse: 19007.5996\n",
      "Epoch 36: val_loss did not improve from 24521.10352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18981.7754 - rmse: 18964.3594 - val_loss: 25377.1914 - val_rmse: 25522.2715\n",
      "Epoch 37/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19945.9102 - rmse: 19945.9102\n",
      "Epoch 37: val_loss did not improve from 24521.10352\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20086.1289 - rmse: 20073.5605 - val_loss: 36737.6992 - val_rmse: 36915.3398\n",
      "Epoch 38/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21943.6504 - rmse: 21919.7246\n",
      "Epoch 38: val_loss did not improve from 24521.10352\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21943.6504 - rmse: 21919.7246 - val_loss: 28252.8613 - val_rmse: 28415.0430\n",
      "Epoch 39/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22574.1250 - rmse: 22565.3809\n",
      "Epoch 39: val_loss improved from 24521.10352 to 23372.43164, saving model to ./ckpt/5_class_lr005/val_rmse_23506.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22574.1250 - rmse: 22565.3809 - val_loss: 23372.4316 - val_rmse: 23505.8125\n",
      "Epoch 40/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22134.5664 - rmse: 22134.8457\n",
      "Epoch 40: val_loss did not improve from 23372.43164\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22134.5664 - rmse: 22134.8457 - val_loss: 23377.0684 - val_rmse: 23459.8867\n",
      "Epoch 41/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21932.7969 - rmse: 21932.7969\n",
      "Epoch 41: val_loss did not improve from 23372.43164\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21825.8633 - rmse: 21786.6035 - val_loss: 25731.7207 - val_rmse: 25871.3047\n",
      "Epoch 42/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21651.5312 - rmse: 21651.5312\n",
      "Epoch 42: val_loss did not improve from 23372.43164\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 21636.3535 - rmse: 21649.8691 - val_loss: 23989.2285 - val_rmse: 24111.3262\n",
      "Epoch 43/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20481.4434 - rmse: 20481.4434\n",
      "Epoch 43: val_loss did not improve from 23372.43164\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20551.7734 - rmse: 20535.7656 - val_loss: 26158.9336 - val_rmse: 26292.5996\n",
      "Epoch 44/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21780.0977 - rmse: 21912.6133\n",
      "Epoch 44: val_loss improved from 23372.43164 to 23148.90039, saving model to ./ckpt/5_class_lr005/val_rmse_23236.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21780.0977 - rmse: 21912.6133 - val_loss: 23148.9004 - val_rmse: 23235.9648\n",
      "Epoch 45/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20428.2910 - rmse: 20428.2910\n",
      "Epoch 45: val_loss did not improve from 23148.90039\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20459.3027 - rmse: 20480.2168 - val_loss: 32567.0117 - val_rmse: 32769.8359\n",
      "Epoch 46/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20512.5801 - rmse: 20512.5801\n",
      "Epoch 46: val_loss did not improve from 23148.90039\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20449.9590 - rmse: 20407.7227 - val_loss: 27488.0156 - val_rmse: 27614.5781\n",
      "Epoch 47/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18776.3359 - rmse: 18776.3359\n",
      "Epoch 47: val_loss did not improve from 23148.90039\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19022.1777 - rmse: 19074.0059 - val_loss: 23617.7793 - val_rmse: 23722.9082\n",
      "Epoch 48/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20106.5742 - rmse: 20100.7598\n",
      "Epoch 48: val_loss did not improve from 23148.90039\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20106.5742 - rmse: 20100.7598 - val_loss: 23247.6523 - val_rmse: 23317.2363\n",
      "Epoch 49/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19196.6191 - rmse: 19196.6191\n",
      "Epoch 49: val_loss did not improve from 23148.90039\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19256.3730 - rmse: 19296.6699 - val_loss: 26424.4824 - val_rmse: 26498.5703\n",
      "Epoch 50/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19449.7773 - rmse: 19449.7773\n",
      "Epoch 50: val_loss did not improve from 23148.90039\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19443.0156 - rmse: 19467.4473 - val_loss: 33026.3828 - val_rmse: 33216.6641\n",
      "Epoch 51/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18348.9336 - rmse: 18348.9336\n",
      "Epoch 51: val_loss did not improve from 23148.90039\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18458.6367 - rmse: 18448.6699 - val_loss: 24094.2051 - val_rmse: 24143.6465\n",
      "Epoch 52/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 22315.7266 - rmse: 22315.7266\n",
      "Epoch 52: val_loss improved from 23148.90039 to 21887.03516, saving model to ./ckpt/5_class_lr005/val_rmse_21942.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22189.2480 - rmse: 22171.3594 - val_loss: 21887.0352 - val_rmse: 21942.1582\n",
      "Epoch 53/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20234.2051 - rmse: 20201.8965\n",
      "Epoch 53: val_loss did not improve from 21887.03516\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20234.2051 - rmse: 20201.8965 - val_loss: 24123.8145 - val_rmse: 24199.7949\n",
      "Epoch 54/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18165.3652 - rmse: 18156.0977\n",
      "Epoch 54: val_loss did not improve from 21887.03516\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 18165.3652 - rmse: 18156.0977 - val_loss: 22391.3340 - val_rmse: 22455.3770\n",
      "Epoch 55/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18940.1777 - rmse: 18940.1777\n",
      "Epoch 55: val_loss did not improve from 21887.03516\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18927.4141 - rmse: 18914.9277 - val_loss: 29870.7422 - val_rmse: 29995.4648\n",
      "Epoch 56/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19092.2852 - rmse: 19092.2852\n",
      "Epoch 56: val_loss did not improve from 21887.03516\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19103.9727 - rmse: 19131.5391 - val_loss: 28960.6973 - val_rmse: 29114.4434\n",
      "Epoch 57/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18702.0605 - rmse: 18702.0605\n",
      "Epoch 57: val_loss did not improve from 21887.03516\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18680.3574 - rmse: 18729.5195 - val_loss: 28628.3867 - val_rmse: 28774.2344\n",
      "Epoch 58/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18549.9258 - rmse: 18549.9258\n",
      "Epoch 58: val_loss did not improve from 21887.03516\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18642.6660 - rmse: 18694.8027 - val_loss: 34424.9648 - val_rmse: 34610.8203\n",
      "Epoch 59/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18585.1875 - rmse: 18585.1875\n",
      "Epoch 59: val_loss did not improve from 21887.03516\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18619.5352 - rmse: 18681.0195 - val_loss: 32894.5547 - val_rmse: 32998.9219\n",
      "Epoch 60/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18581.1914 - rmse: 18581.1914\n",
      "Epoch 60: val_loss did not improve from 21887.03516\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18482.5938 - rmse: 18470.4941 - val_loss: 26693.1582 - val_rmse: 26811.9531\n",
      "Epoch 61/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17291.8066 - rmse: 17291.8066\n",
      "Epoch 61: val_loss did not improve from 21887.03516\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17291.1250 - rmse: 17290.6660 - val_loss: 22890.6270 - val_rmse: 22971.7891\n",
      "Epoch 62/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20338.1680 - rmse: 20338.1680\n",
      "Epoch 62: val_loss did not improve from 21887.03516\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20356.7930 - rmse: 20408.7422 - val_loss: 40680.0547 - val_rmse: 40802.7344\n",
      "Epoch 63/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19586.4551 - rmse: 19570.4980\n",
      "Epoch 63: val_loss did not improve from 21887.03516\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19586.4551 - rmse: 19570.4980 - val_loss: 22919.8203 - val_rmse: 22991.1719\n",
      "Epoch 64/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17564.8984 - rmse: 17564.8984\n",
      "Epoch 64: val_loss did not improve from 21887.03516\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17629.8750 - rmse: 17673.6992 - val_loss: 24412.4395 - val_rmse: 24447.3047\n",
      "Epoch 65/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18907.2324 - rmse: 18907.2324\n",
      "Epoch 65: val_loss improved from 21887.03516 to 21839.68750, saving model to ./ckpt/5_class_lr005/val_rmse_21916.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18961.5898 - rmse: 19017.1445 - val_loss: 21839.6875 - val_rmse: 21915.5410\n",
      "Epoch 66/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18232.8379 - rmse: 18232.8379\n",
      "Epoch 66: val_loss did not improve from 21839.68750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18249.8730 - rmse: 18254.5117 - val_loss: 23008.3535 - val_rmse: 23102.9492\n",
      "Epoch 67/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17525.4551 - rmse: 17525.4551\n",
      "Epoch 67: val_loss did not improve from 21839.68750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17809.6055 - rmse: 17838.6055 - val_loss: 22515.2559 - val_rmse: 22580.0625\n",
      "Epoch 68/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17161.6035 - rmse: 17161.6035\n",
      "Epoch 68: val_loss improved from 21839.68750 to 21452.43750, saving model to ./ckpt/5_class_lr005/val_rmse_21533.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17199.5273 - rmse: 17204.3184 - val_loss: 21452.4375 - val_rmse: 21533.0449\n",
      "Epoch 69/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18777.0078 - rmse: 18777.0078\n",
      "Epoch 69: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18740.4414 - rmse: 18715.7773 - val_loss: 29784.8496 - val_rmse: 29893.2930\n",
      "Epoch 70/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16930.3477 - rmse: 16930.3477\n",
      "Epoch 70: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16833.8457 - rmse: 16830.3008 - val_loss: 26771.9727 - val_rmse: 26785.8750\n",
      "Epoch 71/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18021.1191 - rmse: 18021.1191\n",
      "Epoch 71: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17979.8711 - rmse: 17972.7598 - val_loss: 23057.5078 - val_rmse: 23087.0664\n",
      "Epoch 72/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17746.9355 - rmse: 17746.9355\n",
      "Epoch 72: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 17861.6094 - rmse: 17938.9492 - val_loss: 22719.0430 - val_rmse: 22721.9961\n",
      "Epoch 73/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17353.8320 - rmse: 17353.8320\n",
      "Epoch 73: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17393.8535 - rmse: 17420.8457 - val_loss: 25592.9629 - val_rmse: 25646.9941\n",
      "Epoch 74/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18892.6504 - rmse: 18892.6504\n",
      "Epoch 74: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18950.4102 - rmse: 19053.8574 - val_loss: 24822.3281 - val_rmse: 24911.3906\n",
      "Epoch 75/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17153.1699 - rmse: 17153.1699\n",
      "Epoch 75: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17495.5762 - rmse: 17544.8164 - val_loss: 21924.8184 - val_rmse: 21946.6016\n",
      "Epoch 76/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16797.9395 - rmse: 16797.9395\n",
      "Epoch 76: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16776.9492 - rmse: 16781.3281 - val_loss: 30868.4668 - val_rmse: 30925.5156\n",
      "Epoch 77/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16781.8945 - rmse: 16781.8945\n",
      "Epoch 77: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16853.1504 - rmse: 16852.5684 - val_loss: 34603.2266 - val_rmse: 34711.1562\n",
      "Epoch 78/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18894.8848 - rmse: 18894.8848\n",
      "Epoch 78: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19147.6719 - rmse: 19180.1602 - val_loss: 25937.6250 - val_rmse: 26025.8125\n",
      "Epoch 79/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17680.6191 - rmse: 17680.6191\n",
      "Epoch 79: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17619.5293 - rmse: 17608.1562 - val_loss: 23394.4727 - val_rmse: 23469.9336\n",
      "Epoch 80/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17948.6641 - rmse: 17948.6641\n",
      "Epoch 80: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17931.1211 - rmse: 17919.2891 - val_loss: 22468.6426 - val_rmse: 22553.9863\n",
      "Epoch 81/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16148.8643 - rmse: 16148.8643\n",
      "Epoch 81: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16242.7705 - rmse: 16306.1035 - val_loss: 23585.0234 - val_rmse: 23648.3906\n",
      "Epoch 82/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16928.0215 - rmse: 16928.0215\n",
      "Epoch 82: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16922.9121 - rmse: 16919.4668 - val_loss: 27727.0195 - val_rmse: 27824.8066\n",
      "Epoch 83/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17025.4160 - rmse: 17017.8867\n",
      "Epoch 83: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17025.4160 - rmse: 17017.8867 - val_loss: 28714.0742 - val_rmse: 28796.1055\n",
      "Epoch 84/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16204.8809 - rmse: 16204.8809\n",
      "Epoch 84: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16235.0361 - rmse: 16255.3730 - val_loss: 22534.1895 - val_rmse: 22556.7852\n",
      "Epoch 85/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16657.3711 - rmse: 16657.3711\n",
      "Epoch 85: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16526.9297 - rmse: 16522.2402 - val_loss: 27243.4648 - val_rmse: 27289.7832\n",
      "Epoch 86/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16681.9531 - rmse: 16681.9531\n",
      "Epoch 86: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16485.4805 - rmse: 16471.5605 - val_loss: 26025.6973 - val_rmse: 26116.1699\n",
      "Epoch 87/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16590.3438 - rmse: 16590.3438\n",
      "Epoch 87: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16656.3809 - rmse: 16671.5566 - val_loss: 27711.1035 - val_rmse: 27785.7383\n",
      "Epoch 88/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16369.3682 - rmse: 16369.3682\n",
      "Epoch 88: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16597.3555 - rmse: 16653.3184 - val_loss: 31675.6309 - val_rmse: 31761.8320\n",
      "Epoch 89/120\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 17181.6719 - rmse: 17181.6719\n",
      "Epoch 89: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17100.8672 - rmse: 17115.9902 - val_loss: 23789.9941 - val_rmse: 23841.0391\n",
      "Epoch 90/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18018.7383 - rmse: 18018.7383\n",
      "Epoch 90: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17983.9531 - rmse: 17960.4922 - val_loss: 25485.0371 - val_rmse: 25553.5625\n",
      "Epoch 91/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16909.7500 - rmse: 16909.7500\n",
      "Epoch 91: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17037.2422 - rmse: 17035.7031 - val_loss: 28724.9180 - val_rmse: 28770.2734\n",
      "Epoch 92/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15642.5977 - rmse: 15642.5977\n",
      "Epoch 92: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15616.4580 - rmse: 15598.8291 - val_loss: 21872.9180 - val_rmse: 21880.1152\n",
      "Epoch 93/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17094.8516 - rmse: 17094.8516\n",
      "Epoch 93: val_loss did not improve from 21452.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17197.4082 - rmse: 17260.2305 - val_loss: 30780.3672 - val_rmse: 30883.3086\n",
      "Epoch 93: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:07:34.481170: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 102259.3203 - rmse: 102006.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:07:35.456879: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 55711.91406, saving model to ./ckpt/5_class_lr005/val_rmse_56819.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 102259.3203 - rmse: 102006.1250 - val_loss: 55711.9141 - val_rmse: 56818.8281\n",
      "Epoch 2/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 51427.1992 - rmse: 51427.1992\n",
      "Epoch 2: val_loss improved from 55711.91406 to 44818.43359, saving model to ./ckpt/5_class_lr005/val_rmse_45541.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 51943.0703 - rmse: 52066.5820 - val_loss: 44818.4336 - val_rmse: 45540.9453\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 46509.1992 - rmse: 46509.1992\n",
      "Epoch 3: val_loss improved from 44818.43359 to 41720.85156, saving model to ./ckpt/5_class_lr005/val_rmse_42186.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 46285.9141 - rmse: 46226.3672 - val_loss: 41720.8516 - val_rmse: 42185.7109\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 44838.5938 - rmse: 44895.1445\n",
      "Epoch 4: val_loss improved from 41720.85156 to 38816.77344, saving model to ./ckpt/5_class_lr005/val_rmse_39215.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 44838.5938 - rmse: 44895.1445 - val_loss: 38816.7734 - val_rmse: 39215.4141\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 42281.9961 - rmse: 42296.3164\n",
      "Epoch 5: val_loss did not improve from 38816.77344\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 42281.9961 - rmse: 42296.3164 - val_loss: 40027.8594 - val_rmse: 40317.3164\n",
      "Epoch 6/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 39492.2695 - rmse: 39492.2695\n",
      "Epoch 6: val_loss improved from 38816.77344 to 35414.67578, saving model to ./ckpt/5_class_lr005/val_rmse_35628.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 39055.0664 - rmse: 38965.6094 - val_loss: 35414.6758 - val_rmse: 35627.5234\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 39982.6250 - rmse: 39915.8516\n",
      "Epoch 7: val_loss did not improve from 35414.67578\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 39982.6250 - rmse: 39915.8516 - val_loss: 39413.3281 - val_rmse: 39602.3789\n",
      "Epoch 8/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 37490.1836 - rmse: 37490.1836\n",
      "Epoch 8: val_loss did not improve from 35414.67578\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 37572.2656 - rmse: 37527.7188 - val_loss: 38450.6992 - val_rmse: 38632.9492\n",
      "Epoch 9/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 35510.5938 - rmse: 35510.5938\n",
      "Epoch 9: val_loss improved from 35414.67578 to 31397.36133, saving model to ./ckpt/5_class_lr005/val_rmse_31447.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 35300.2617 - rmse: 35252.4258 - val_loss: 31397.3613 - val_rmse: 31447.0000\n",
      "Epoch 10/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 37553.8477 - rmse: 37553.8477\n",
      "Epoch 10: val_loss improved from 31397.36133 to 29466.69727, saving model to ./ckpt/5_class_lr005/val_rmse_29493.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 37447.3594 - rmse: 37384.3984 - val_loss: 29466.6973 - val_rmse: 29493.4082\n",
      "Epoch 11/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 33731.7305 - rmse: 33731.7305\n",
      "Epoch 11: val_loss improved from 29466.69727 to 28522.96289, saving model to ./ckpt/5_class_lr005/val_rmse_28474.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 34037.1641 - rmse: 34160.5586 - val_loss: 28522.9629 - val_rmse: 28474.4785\n",
      "Epoch 12/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 31587.1934 - rmse: 31587.1934\n",
      "Epoch 12: val_loss improved from 28522.96289 to 26527.07031, saving model to ./ckpt/5_class_lr005/val_rmse_26461.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 31442.0078 - rmse: 31436.6543 - val_loss: 26527.0703 - val_rmse: 26461.3965\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 34484.6172 - rmse: 34440.2227\n",
      "Epoch 13: val_loss improved from 26527.07031 to 25981.21680, saving model to ./ckpt/5_class_lr005/val_rmse_25877.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 34484.6172 - rmse: 34440.2227 - val_loss: 25981.2168 - val_rmse: 25877.1016\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 29082.8633 - rmse: 29108.3535\n",
      "Epoch 14: val_loss did not improve from 25981.21680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 29082.8633 - rmse: 29108.3535 - val_loss: 30960.1328 - val_rmse: 30939.3105\n",
      "Epoch 15/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 28765.7441 - rmse: 28765.7441\n",
      "Epoch 15: val_loss improved from 25981.21680 to 24371.59180, saving model to ./ckpt/5_class_lr005/val_rmse_24245.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 28741.2617 - rmse: 28809.9219 - val_loss: 24371.5918 - val_rmse: 24244.7188\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 32514.4746 - rmse: 32514.4746\n",
      "Epoch 16: val_loss did not improve from 24371.59180\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 32387.6777 - rmse: 32312.6855 - val_loss: 27243.5215 - val_rmse: 27121.8926\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 27959.2500 - rmse: 27959.2500\n",
      "Epoch 17: val_loss improved from 24371.59180 to 23878.23828, saving model to ./ckpt/5_class_lr005/val_rmse_23751.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27972.0449 - rmse: 27979.6133 - val_loss: 23878.2383 - val_rmse: 23751.1094\n",
      "Epoch 18/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 29562.8242 - rmse: 29562.8242\n",
      "Epoch 18: val_loss did not improve from 23878.23828\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 29476.8535 - rmse: 29529.8047 - val_loss: 29157.7207 - val_rmse: 29181.2598\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27587.1172 - rmse: 27587.1172\n",
      "Epoch 19: val_loss did not improve from 23878.23828\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 27164.9590 - rmse: 27202.8887 - val_loss: 24013.3477 - val_rmse: 23921.2227\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24958.1992 - rmse: 24985.7852\n",
      "Epoch 20: val_loss did not improve from 23878.23828\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 24958.1992 - rmse: 24985.7852 - val_loss: 28154.4062 - val_rmse: 28077.7500\n",
      "Epoch 21/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27042.4121 - rmse: 27042.4121\n",
      "Epoch 21: val_loss improved from 23878.23828 to 22756.80078, saving model to ./ckpt/5_class_lr005/val_rmse_22606.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27294.3828 - rmse: 27287.6895 - val_loss: 22756.8008 - val_rmse: 22606.4922\n",
      "Epoch 22/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 26016.5742 - rmse: 26016.5742\n",
      "Epoch 22: val_loss improved from 22756.80078 to 22171.74023, saving model to ./ckpt/5_class_lr005/val_rmse_21982.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26162.3242 - rmse: 26221.9316 - val_loss: 22171.7402 - val_rmse: 21981.6250\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 24597.9609 - rmse: 24597.9609\n",
      "Epoch 23: val_loss did not improve from 22171.74023\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 24436.9453 - rmse: 24424.9629 - val_loss: 24338.5508 - val_rmse: 24225.6797\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23510.7793 - rmse: 23510.7793\n",
      "Epoch 24: val_loss improved from 22171.74023 to 20438.67773, saving model to ./ckpt/5_class_lr005/val_rmse_20262.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23577.8887 - rmse: 23617.5781 - val_loss: 20438.6777 - val_rmse: 20261.6406\n",
      "Epoch 25/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22856.1484 - rmse: 22856.1484\n",
      "Epoch 25: val_loss improved from 20438.67773 to 20101.34180, saving model to ./ckpt/5_class_lr005/val_rmse_19938.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22942.7441 - rmse: 22960.9023 - val_loss: 20101.3418 - val_rmse: 19938.0957\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22569.3242 - rmse: 22569.3242\n",
      "Epoch 26: val_loss improved from 20101.34180 to 19224.44922, saving model to ./ckpt/5_class_lr005/val_rmse_18964.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22701.4863 - rmse: 22779.6523 - val_loss: 19224.4492 - val_rmse: 18963.7500\n",
      "Epoch 27/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25566.0352 - rmse: 25626.5371\n",
      "Epoch 27: val_loss did not improve from 19224.44922\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25566.0352 - rmse: 25626.5371 - val_loss: 21080.4492 - val_rmse: 20894.1055\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22644.0176 - rmse: 22644.0176\n",
      "Epoch 28: val_loss did not improve from 19224.44922\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22712.1328 - rmse: 22752.4160 - val_loss: 20675.2734 - val_rmse: 20614.1484\n",
      "Epoch 29/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22602.1973 - rmse: 22602.1973\n",
      "Epoch 29: val_loss did not improve from 19224.44922\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22696.0176 - rmse: 22697.1777 - val_loss: 20852.7734 - val_rmse: 20698.0117\n",
      "Epoch 30/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20322.0781 - rmse: 20322.0781\n",
      "Epoch 30: val_loss did not improve from 19224.44922\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21162.1680 - rmse: 21185.3730 - val_loss: 26391.3828 - val_rmse: 26461.1914\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27037.2441 - rmse: 27063.5547\n",
      "Epoch 31: val_loss did not improve from 19224.44922\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 27037.2441 - rmse: 27063.5547 - val_loss: 36477.5117 - val_rmse: 36629.7031\n",
      "Epoch 32/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23752.9551 - rmse: 23752.9551\n",
      "Epoch 32: val_loss did not improve from 19224.44922\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23539.5840 - rmse: 23511.1309 - val_loss: 21748.2734 - val_rmse: 21573.9102\n",
      "Epoch 33/120\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 22395.1035 - rmse: 22395.1035\n",
      "Epoch 33: val_loss did not improve from 19224.44922\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22303.9277 - rmse: 22447.5742 - val_loss: 24283.5703 - val_rmse: 24370.2266\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21621.3086 - rmse: 21632.0820\n",
      "Epoch 34: val_loss did not improve from 19224.44922\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 21621.3086 - rmse: 21632.0820 - val_loss: 23905.0371 - val_rmse: 23875.8789\n",
      "Epoch 35/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21699.1992 - rmse: 21699.1992\n",
      "Epoch 35: val_loss improved from 19224.44922 to 19087.33203, saving model to ./ckpt/5_class_lr005/val_rmse_18784.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21581.1426 - rmse: 21568.5254 - val_loss: 19087.3320 - val_rmse: 18783.7266\n",
      "Epoch 36/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20619.3574 - rmse: 20619.3574\n",
      "Epoch 36: val_loss improved from 19087.33203 to 18495.12891, saving model to ./ckpt/5_class_lr005/val_rmse_18224.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20457.1875 - rmse: 20428.6582 - val_loss: 18495.1289 - val_rmse: 18223.9844\n",
      "Epoch 37/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20149.5938 - rmse: 20149.5938\n",
      "Epoch 37: val_loss improved from 18495.12891 to 17753.08398, saving model to ./ckpt/5_class_lr005/val_rmse_17507.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20229.4297 - rmse: 20276.6465 - val_loss: 17753.0840 - val_rmse: 17507.3770\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21142.3848 - rmse: 21142.3848\n",
      "Epoch 38: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 21168.2754 - rmse: 21183.5859 - val_loss: 18359.5469 - val_rmse: 18056.8672\n",
      "Epoch 39/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21968.1602 - rmse: 22010.9941\n",
      "Epoch 39: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21968.1602 - rmse: 22010.9941 - val_loss: 23875.3535 - val_rmse: 23730.7520\n",
      "Epoch 40/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21354.6348 - rmse: 21354.6348\n",
      "Epoch 40: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21419.2266 - rmse: 21443.3965 - val_loss: 20020.5762 - val_rmse: 19856.8086\n",
      "Epoch 41/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21210.4590 - rmse: 21210.4590\n",
      "Epoch 41: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21131.6875 - rmse: 21203.5684 - val_loss: 18426.4922 - val_rmse: 18193.9082\n",
      "Epoch 42/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21044.8086 - rmse: 21044.8086\n",
      "Epoch 42: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21263.9297 - rmse: 21263.5703 - val_loss: 17803.8281 - val_rmse: 17568.1289\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23302.2891 - rmse: 23302.2891\n",
      "Epoch 43: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23369.9707 - rmse: 23409.9961 - val_loss: 20955.3223 - val_rmse: 20873.9199\n",
      "Epoch 44/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22899.0586 - rmse: 22899.0586\n",
      "Epoch 44: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22686.1309 - rmse: 22692.1953 - val_loss: 25314.8555 - val_rmse: 25304.0820\n",
      "Epoch 45/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18894.7129 - rmse: 18978.5840\n",
      "Epoch 45: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18894.7129 - rmse: 18978.5840 - val_loss: 19140.6309 - val_rmse: 18981.6855\n",
      "Epoch 46/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20793.4688 - rmse: 20793.4688\n",
      "Epoch 46: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20475.6836 - rmse: 20432.7461 - val_loss: 18926.4941 - val_rmse: 18809.8828\n",
      "Epoch 47/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20443.4258 - rmse: 20443.4258\n",
      "Epoch 47: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20405.2617 - rmse: 20409.7949 - val_loss: 18971.4727 - val_rmse: 18716.6230\n",
      "Epoch 48/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19427.8379 - rmse: 19424.5293\n",
      "Epoch 48: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19427.8379 - rmse: 19424.5293 - val_loss: 23617.3379 - val_rmse: 23780.2578\n",
      "Epoch 49/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20559.4434 - rmse: 20559.4434\n",
      "Epoch 49: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20553.0020 - rmse: 20569.9492 - val_loss: 18019.1406 - val_rmse: 17757.9316\n",
      "Epoch 50/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20671.1484 - rmse: 20671.1484\n",
      "Epoch 50: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20759.3418 - rmse: 20763.5645 - val_loss: 21153.0391 - val_rmse: 21136.6621\n",
      "Epoch 51/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21751.1562 - rmse: 21751.1562\n",
      "Epoch 51: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21689.9219 - rmse: 21653.7051 - val_loss: 26330.1641 - val_rmse: 26556.0469\n",
      "Epoch 52/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20350.5801 - rmse: 20350.5801\n",
      "Epoch 52: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20216.2090 - rmse: 20224.8574 - val_loss: 21261.0039 - val_rmse: 21291.2383\n",
      "Epoch 53/120\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 19273.6699 - rmse: 19273.6699\n",
      "Epoch 53: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19541.6543 - rmse: 19538.3438 - val_loss: 19840.1602 - val_rmse: 19852.7930\n",
      "Epoch 54/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19018.2500 - rmse: 19018.2500\n",
      "Epoch 54: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19037.1309 - rmse: 19022.5586 - val_loss: 22502.8828 - val_rmse: 22502.6191\n",
      "Epoch 55/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20648.5293 - rmse: 20647.9023\n",
      "Epoch 55: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20648.5293 - rmse: 20647.9023 - val_loss: 19317.5352 - val_rmse: 19126.8105\n",
      "Epoch 56/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24171.7520 - rmse: 24193.4102\n",
      "Epoch 56: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24171.7520 - rmse: 24193.4102 - val_loss: 40342.4141 - val_rmse: 40797.1953\n",
      "Epoch 57/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23465.8594 - rmse: 23465.8594\n",
      "Epoch 57: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23360.2246 - rmse: 23328.9277 - val_loss: 26469.7090 - val_rmse: 26697.4629\n",
      "Epoch 58/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18776.4297 - rmse: 18776.4297\n",
      "Epoch 58: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18801.9668 - rmse: 18809.8145 - val_loss: 31029.8242 - val_rmse: 31358.3691\n",
      "Epoch 59/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20933.7852 - rmse: 20933.7852\n",
      "Epoch 59: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20872.3613 - rmse: 20871.6152 - val_loss: 24084.4512 - val_rmse: 24312.4609\n",
      "Epoch 60/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19925.5508 - rmse: 19934.9492\n",
      "Epoch 60: val_loss did not improve from 17753.08398\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19925.5508 - rmse: 19934.9492 - val_loss: 20293.5645 - val_rmse: 20084.3906\n",
      "Epoch 61/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21622.5332 - rmse: 21622.5332\n",
      "Epoch 61: val_loss improved from 17753.08398 to 17539.41211, saving model to ./ckpt/5_class_lr005/val_rmse_17333.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21512.5723 - rmse: 21474.8457 - val_loss: 17539.4121 - val_rmse: 17332.8262\n",
      "Epoch 62/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18690.7676 - rmse: 18690.7676\n",
      "Epoch 62: val_loss improved from 17539.41211 to 17446.03711, saving model to ./ckpt/5_class_lr005/val_rmse_17208.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 18709.3379 - rmse: 18755.2832 - val_loss: 17446.0371 - val_rmse: 17208.2246\n",
      "Epoch 63/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18517.5430 - rmse: 18521.6289\n",
      "Epoch 63: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18517.5430 - rmse: 18521.6289 - val_loss: 23010.5664 - val_rmse: 23057.5605\n",
      "Epoch 64/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18183.5098 - rmse: 18183.5098\n",
      "Epoch 64: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18255.3516 - rmse: 18242.5723 - val_loss: 20629.8555 - val_rmse: 20573.5938\n",
      "Epoch 65/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19701.3770 - rmse: 19701.3770\n",
      "Epoch 65: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19899.3633 - rmse: 19908.4922 - val_loss: 32776.7344 - val_rmse: 33119.3633\n",
      "Epoch 66/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18820.5625 - rmse: 18820.5625\n",
      "Epoch 66: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19000.9414 - rmse: 19039.2656 - val_loss: 37389.1094 - val_rmse: 37749.2422\n",
      "Epoch 67/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21439.8477 - rmse: 21439.8477\n",
      "Epoch 67: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21667.5820 - rmse: 21713.7109 - val_loss: 17964.1602 - val_rmse: 17784.7500\n",
      "Epoch 68/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18528.8418 - rmse: 18528.8418\n",
      "Epoch 68: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18519.0781 - rmse: 18513.3027 - val_loss: 17861.5078 - val_rmse: 17593.4883\n",
      "Epoch 69/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18264.4922 - rmse: 18264.4922\n",
      "Epoch 69: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18133.3770 - rmse: 18106.1699 - val_loss: 18461.5469 - val_rmse: 18241.1621\n",
      "Epoch 70/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18205.0371 - rmse: 18205.0371\n",
      "Epoch 70: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18170.5723 - rmse: 18186.2617 - val_loss: 18529.1367 - val_rmse: 18277.9297\n",
      "Epoch 71/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19393.0469 - rmse: 19393.0469\n",
      "Epoch 71: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19477.2637 - rmse: 19551.5469 - val_loss: 27401.6328 - val_rmse: 27369.0430\n",
      "Epoch 72/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19246.3652 - rmse: 19246.3652\n",
      "Epoch 72: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19176.2676 - rmse: 19164.2402 - val_loss: 21270.7734 - val_rmse: 21362.6465\n",
      "Epoch 73/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19115.9160 - rmse: 19159.2930\n",
      "Epoch 73: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19115.9160 - rmse: 19159.2930 - val_loss: 22441.1445 - val_rmse: 22290.8125\n",
      "Epoch 74/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18321.9727 - rmse: 18321.9727\n",
      "Epoch 74: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18405.2656 - rmse: 18373.5391 - val_loss: 18090.0586 - val_rmse: 17877.6191\n",
      "Epoch 75/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17992.0371 - rmse: 17992.0371\n",
      "Epoch 75: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17937.1914 - rmse: 17928.0762 - val_loss: 21231.0762 - val_rmse: 21234.4004\n",
      "Epoch 76/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17177.6816 - rmse: 17177.6816\n",
      "Epoch 76: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17796.6816 - rmse: 17818.3379 - val_loss: 21085.9785 - val_rmse: 20933.7734\n",
      "Epoch 77/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20445.3691 - rmse: 20437.2324\n",
      "Epoch 77: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20445.3691 - rmse: 20437.2324 - val_loss: 23945.4980 - val_rmse: 24038.4219\n",
      "Epoch 78/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17680.8262 - rmse: 17680.8262\n",
      "Epoch 78: val_loss did not improve from 17446.03711\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17736.6016 - rmse: 17784.4746 - val_loss: 24790.3184 - val_rmse: 24909.2812\n",
      "Epoch 79/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17196.9863 - rmse: 17192.9004\n",
      "Epoch 79: val_loss improved from 17446.03711 to 17208.07031, saving model to ./ckpt/5_class_lr005/val_rmse_17012.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17196.9863 - rmse: 17192.9004 - val_loss: 17208.0703 - val_rmse: 17012.2891\n",
      "Epoch 80/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17331.5742 - rmse: 17337.1934\n",
      "Epoch 80: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17331.5742 - rmse: 17337.1934 - val_loss: 24871.5645 - val_rmse: 25058.0703\n",
      "Epoch 81/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17415.8262 - rmse: 17415.8262\n",
      "Epoch 81: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17210.2891 - rmse: 17204.8359 - val_loss: 28041.6230 - val_rmse: 28294.2461\n",
      "Epoch 82/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16693.8633 - rmse: 16693.8633\n",
      "Epoch 82: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16696.8086 - rmse: 16689.5801 - val_loss: 20150.1543 - val_rmse: 20140.4746\n",
      "Epoch 83/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16390.9219 - rmse: 16390.9219\n",
      "Epoch 83: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16335.0400 - rmse: 16354.2129 - val_loss: 25182.4219 - val_rmse: 25291.3809\n",
      "Epoch 84/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16156.8975 - rmse: 16156.8975\n",
      "Epoch 84: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16198.4834 - rmse: 16227.2646 - val_loss: 23050.5137 - val_rmse: 23184.3027\n",
      "Epoch 85/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17790.6484 - rmse: 17790.6484\n",
      "Epoch 85: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17851.1719 - rmse: 17887.4355 - val_loss: 21624.1738 - val_rmse: 21617.7109\n",
      "Epoch 86/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18919.7402 - rmse: 18919.7402\n",
      "Epoch 86: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18968.0117 - rmse: 18996.5586 - val_loss: 23568.5039 - val_rmse: 23615.9863\n",
      "Epoch 87/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18254.1055 - rmse: 18254.1055\n",
      "Epoch 87: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18429.1758 - rmse: 18424.5625 - val_loss: 17616.2949 - val_rmse: 17408.5234\n",
      "Epoch 88/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16907.2246 - rmse: 16907.2246\n",
      "Epoch 88: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16872.9434 - rmse: 16852.6660 - val_loss: 17525.7422 - val_rmse: 17314.7832\n",
      "Epoch 89/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16931.0254 - rmse: 16965.8242\n",
      "Epoch 89: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16931.0254 - rmse: 16965.8242 - val_loss: 20313.6211 - val_rmse: 20301.1211\n",
      "Epoch 90/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16878.3223 - rmse: 16878.3223\n",
      "Epoch 90: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17052.1621 - rmse: 17080.5996 - val_loss: 19924.8223 - val_rmse: 19844.6172\n",
      "Epoch 91/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18504.9609 - rmse: 18504.9609\n",
      "Epoch 91: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18727.3105 - rmse: 18743.2285 - val_loss: 18611.5781 - val_rmse: 18420.7637\n",
      "Epoch 92/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18473.1699 - rmse: 18444.2520\n",
      "Epoch 92: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18473.1699 - rmse: 18444.2520 - val_loss: 18427.7383 - val_rmse: 18415.5508\n",
      "Epoch 93/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18325.1367 - rmse: 18325.1367\n",
      "Epoch 93: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18219.3008 - rmse: 18216.5527 - val_loss: 22156.3281 - val_rmse: 22260.3184\n",
      "Epoch 94/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16250.7852 - rmse: 16250.7852\n",
      "Epoch 94: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16366.3213 - rmse: 16463.8027 - val_loss: 26044.2617 - val_rmse: 26154.7148\n",
      "Epoch 95/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16159.2197 - rmse: 16159.2197\n",
      "Epoch 95: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16154.7344 - rmse: 16152.0820 - val_loss: 19498.6719 - val_rmse: 19495.1875\n",
      "Epoch 96/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16082.7754 - rmse: 16082.7754\n",
      "Epoch 96: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16116.7148 - rmse: 16136.7871 - val_loss: 22726.0078 - val_rmse: 22840.3320\n",
      "Epoch 97/120\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 15829.8350 - rmse: 15829.8350\n",
      "Epoch 97: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16106.1494 - rmse: 16166.9785 - val_loss: 21409.4629 - val_rmse: 21471.1094\n",
      "Epoch 98/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17876.2461 - rmse: 17876.2461\n",
      "Epoch 98: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17942.2266 - rmse: 17981.2500 - val_loss: 24614.4922 - val_rmse: 24773.9512\n",
      "Epoch 99/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18534.0410 - rmse: 18534.0410\n",
      "Epoch 99: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18617.3477 - rmse: 18601.3184 - val_loss: 21087.9512 - val_rmse: 21054.0801\n",
      "Epoch 100/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16237.1250 - rmse: 16237.1250\n",
      "Epoch 100: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16427.7422 - rmse: 16454.4238 - val_loss: 22690.8203 - val_rmse: 22835.3066\n",
      "Epoch 101/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16711.3516 - rmse: 16711.3516\n",
      "Epoch 101: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16776.0723 - rmse: 16799.6914 - val_loss: 20185.0352 - val_rmse: 20089.6094\n",
      "Epoch 102/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17275.2695 - rmse: 17344.4102\n",
      "Epoch 102: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17275.2695 - rmse: 17344.4102 - val_loss: 20341.5352 - val_rmse: 20262.4219\n",
      "Epoch 103/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16052.0391 - rmse: 16052.0391\n",
      "Epoch 103: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16164.9033 - rmse: 16162.0605 - val_loss: 23438.3340 - val_rmse: 23546.4727\n",
      "Epoch 104/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16492.9766 - rmse: 16492.9766\n",
      "Epoch 104: val_loss did not improve from 17208.07031\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16533.9043 - rmse: 16552.3965 - val_loss: 17341.3516 - val_rmse: 17116.3223\n",
      "Epoch 104: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:08:48.247138: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 101436.9922 - rmse: 101596.6875\n",
      "Epoch 1: val_loss improved from inf to 53297.80469, saving model to ./ckpt/5_class_lr005/val_rmse_52962.hdf5\n",
      "70/70 [==============================] - 2s 15ms/step - loss: 101436.9922 - rmse: 101596.6875 - val_loss: 53297.8047 - val_rmse: 52962.0703\n",
      "Epoch 2/120\n",
      " 6/70 [=>............................] - ETA: 0s - loss: 59707.6641 - rmse: 59707.6641"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:08:49.386066: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/70 [==========================>...] - ETA: 0s - loss: 54147.1094 - rmse: 54147.1094\n",
      "Epoch 2: val_loss improved from 53297.80469 to 40897.62500, saving model to ./ckpt/5_class_lr005/val_rmse_40833.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 53073.1211 - rmse: 53031.3633 - val_loss: 40897.6250 - val_rmse: 40832.6758\n",
      "Epoch 3/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 48350.9375 - rmse: 48350.9375\n",
      "Epoch 3: val_loss did not improve from 40897.62500\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 48002.1094 - rmse: 47897.7734 - val_loss: 43347.4219 - val_rmse: 43480.0547\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 43526.7305 - rmse: 43501.0664\n",
      "Epoch 4: val_loss improved from 40897.62500 to 37777.48438, saving model to ./ckpt/5_class_lr005/val_rmse_37538.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 43526.7305 - rmse: 43501.0664 - val_loss: 37777.4844 - val_rmse: 37538.0469\n",
      "Epoch 5/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 42868.3516 - rmse: 42868.3516\n",
      "Epoch 5: val_loss improved from 37777.48438 to 32751.18555, saving model to ./ckpt/5_class_lr005/val_rmse_32645.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 42128.3555 - rmse: 42066.8008 - val_loss: 32751.1855 - val_rmse: 32644.8086\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 40127.9141 - rmse: 40127.9141\n",
      "Epoch 6: val_loss improved from 32751.18555 to 31724.96094, saving model to ./ckpt/5_class_lr005/val_rmse_31631.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 40018.8711 - rmse: 40030.1211 - val_loss: 31724.9609 - val_rmse: 31631.1934\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 37880.6562 - rmse: 37798.2734\n",
      "Epoch 7: val_loss improved from 31724.96094 to 29600.08398, saving model to ./ckpt/5_class_lr005/val_rmse_29652.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 37880.6562 - rmse: 37798.2734 - val_loss: 29600.0840 - val_rmse: 29651.7617\n",
      "Epoch 8/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 38289.9023 - rmse: 38289.9023\n",
      "Epoch 8: val_loss did not improve from 29600.08398\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 37578.5234 - rmse: 37519.9375 - val_loss: 31727.9277 - val_rmse: 31727.4375\n",
      "Epoch 9/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 34984.6797 - rmse: 34984.6797\n",
      "Epoch 9: val_loss did not improve from 29600.08398\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 34806.8438 - rmse: 34791.0195 - val_loss: 30192.2148 - val_rmse: 30268.6055\n",
      "Epoch 10/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 34347.7734 - rmse: 34347.7734\n",
      "Epoch 10: val_loss did not improve from 29600.08398\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 34743.3750 - rmse: 34772.4062 - val_loss: 37157.1680 - val_rmse: 37253.2461\n",
      "Epoch 11/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 32300.3691 - rmse: 32300.3691\n",
      "Epoch 11: val_loss improved from 29600.08398 to 29182.89062, saving model to ./ckpt/5_class_lr005/val_rmse_29324.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 32488.6074 - rmse: 32566.6797 - val_loss: 29182.8906 - val_rmse: 29324.2148\n",
      "Epoch 12/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 35438.5391 - rmse: 35438.5391\n",
      "Epoch 12: val_loss did not improve from 29182.89062\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 35298.4531 - rmse: 35322.4727 - val_loss: 43007.7578 - val_rmse: 43103.2734\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 34729.6562 - rmse: 34729.6562\n",
      "Epoch 13: val_loss did not improve from 29182.89062\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 34711.0664 - rmse: 34700.0703 - val_loss: 38556.4883 - val_rmse: 38716.3398\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 31416.4180 - rmse: 31467.6152\n",
      "Epoch 14: val_loss improved from 29182.89062 to 20931.02344, saving model to ./ckpt/5_class_lr005/val_rmse_21128.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 31416.4180 - rmse: 31467.6152 - val_loss: 20931.0234 - val_rmse: 21127.5449\n",
      "Epoch 15/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 31146.0586 - rmse: 31104.3496\n",
      "Epoch 15: val_loss improved from 20931.02344 to 20438.18555, saving model to ./ckpt/5_class_lr005/val_rmse_20683.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 31146.0586 - rmse: 31104.3496 - val_loss: 20438.1855 - val_rmse: 20683.0469\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 28458.2969 - rmse: 28458.2969\n",
      "Epoch 16: val_loss did not improve from 20438.18555\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 28441.8691 - rmse: 28432.1543 - val_loss: 20816.3398 - val_rmse: 21019.8633\n",
      "Epoch 17/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 27662.6074 - rmse: 27662.6074\n",
      "Epoch 17: val_loss improved from 20438.18555 to 18858.52734, saving model to ./ckpt/5_class_lr005/val_rmse_19085.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 27513.0391 - rmse: 27518.9336 - val_loss: 18858.5273 - val_rmse: 19085.1973\n",
      "Epoch 18/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25677.6328 - rmse: 25677.6328\n",
      "Epoch 18: val_loss did not improve from 18858.52734\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 25970.4629 - rmse: 25994.8945 - val_loss: 19368.4961 - val_rmse: 19578.3809\n",
      "Epoch 19/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27160.5352 - rmse: 27160.5352\n",
      "Epoch 19: val_loss did not improve from 18858.52734\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27225.1172 - rmse: 27218.8867 - val_loss: 30489.7012 - val_rmse: 30578.0586\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27665.7500 - rmse: 27665.7500\n",
      "Epoch 20: val_loss did not improve from 18858.52734\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27582.1250 - rmse: 27580.5430 - val_loss: 22870.0215 - val_rmse: 23063.8516\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25225.3359 - rmse: 25225.3359\n",
      "Epoch 21: val_loss did not improve from 18858.52734\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 25317.6035 - rmse: 25362.2422 - val_loss: 25276.1309 - val_rmse: 25304.6211\n",
      "Epoch 22/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 26961.2422 - rmse: 26961.2422\n",
      "Epoch 22: val_loss did not improve from 18858.52734\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27342.9160 - rmse: 27433.1172 - val_loss: 22922.5723 - val_rmse: 23020.4551\n",
      "Epoch 23/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 24186.7559 - rmse: 24186.7559\n",
      "Epoch 23: val_loss improved from 18858.52734 to 17940.91797, saving model to ./ckpt/5_class_lr005/val_rmse_18101.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23993.4199 - rmse: 24018.8984 - val_loss: 17940.9180 - val_rmse: 18101.0586\n",
      "Epoch 24/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23737.8633 - rmse: 23737.8633\n",
      "Epoch 24: val_loss improved from 17940.91797 to 16143.03809, saving model to ./ckpt/5_class_lr005/val_rmse_16339.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23989.5078 - rmse: 24021.4082 - val_loss: 16143.0381 - val_rmse: 16339.0430\n",
      "Epoch 25/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25765.8691 - rmse: 25765.8691\n",
      "Epoch 25: val_loss did not improve from 16143.03809\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25932.6074 - rmse: 25897.0312 - val_loss: 20677.6758 - val_rmse: 20888.8926\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23579.8398 - rmse: 23647.3184\n",
      "Epoch 26: val_loss improved from 16143.03809 to 15845.41504, saving model to ./ckpt/5_class_lr005/val_rmse_16084.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23579.8398 - rmse: 23647.3184 - val_loss: 15845.4150 - val_rmse: 16083.8320\n",
      "Epoch 27/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 24024.8984 - rmse: 24024.8984\n",
      "Epoch 27: val_loss did not improve from 15845.41504\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23957.8945 - rmse: 23937.1367 - val_loss: 19416.6133 - val_rmse: 19492.2070\n",
      "Epoch 28/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 24238.4707 - rmse: 24238.4707\n",
      "Epoch 28: val_loss did not improve from 15845.41504\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24481.5078 - rmse: 24535.7188 - val_loss: 28820.7285 - val_rmse: 28975.2031\n",
      "Epoch 29/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24975.4004 - rmse: 25041.9023\n",
      "Epoch 29: val_loss did not improve from 15845.41504\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 24975.4004 - rmse: 25041.9023 - val_loss: 27918.5391 - val_rmse: 28102.4473\n",
      "Epoch 30/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22345.1191 - rmse: 22345.1191\n",
      "Epoch 30: val_loss improved from 15845.41504 to 14060.30371, saving model to ./ckpt/5_class_lr005/val_rmse_14262.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22324.5605 - rmse: 22305.4492 - val_loss: 14060.3037 - val_rmse: 14261.5117\n",
      "Epoch 31/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 22085.6699 - rmse: 22085.6699\n",
      "Epoch 31: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22369.6777 - rmse: 22340.0254 - val_loss: 16845.3828 - val_rmse: 17036.9336\n",
      "Epoch 32/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23683.2051 - rmse: 23683.2051\n",
      "Epoch 32: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23633.7266 - rmse: 23639.5957 - val_loss: 29061.2676 - val_rmse: 29157.1523\n",
      "Epoch 33/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 23603.0977 - rmse: 23603.0977\n",
      "Epoch 33: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23573.0957 - rmse: 23539.6895 - val_loss: 18498.4141 - val_rmse: 18625.7676\n",
      "Epoch 34/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 22176.4863 - rmse: 22176.4863\n",
      "Epoch 34: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22171.6348 - rmse: 22172.8594 - val_loss: 17140.5820 - val_rmse: 17280.0234\n",
      "Epoch 35/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23099.7988 - rmse: 23099.7988\n",
      "Epoch 35: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23138.5469 - rmse: 23161.4609 - val_loss: 22343.5000 - val_rmse: 22535.1680\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20826.9043 - rmse: 20815.3223\n",
      "Epoch 36: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20826.9043 - rmse: 20815.3223 - val_loss: 18298.0020 - val_rmse: 18480.3574\n",
      "Epoch 37/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22807.2363 - rmse: 22807.2363\n",
      "Epoch 37: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23124.7207 - rmse: 23289.5801 - val_loss: 22913.6797 - val_rmse: 22975.4395\n",
      "Epoch 38/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22657.5762 - rmse: 22656.3457\n",
      "Epoch 38: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22657.5762 - rmse: 22656.3457 - val_loss: 16676.7324 - val_rmse: 16891.4297\n",
      "Epoch 39/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20819.5781 - rmse: 20819.5781\n",
      "Epoch 39: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20772.5898 - rmse: 20728.7188 - val_loss: 15594.2695 - val_rmse: 15682.4785\n",
      "Epoch 40/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21306.5410 - rmse: 21306.5410\n",
      "Epoch 40: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21404.3574 - rmse: 21411.3691 - val_loss: 15443.2920 - val_rmse: 15518.9492\n",
      "Epoch 41/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21593.3555 - rmse: 21593.3555\n",
      "Epoch 41: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21606.7617 - rmse: 21614.6914 - val_loss: 25176.7578 - val_rmse: 25215.8301\n",
      "Epoch 42/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21475.8066 - rmse: 21475.8066\n",
      "Epoch 42: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21570.6699 - rmse: 21587.3457 - val_loss: 20954.8789 - val_rmse: 21106.7324\n",
      "Epoch 43/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23543.3242 - rmse: 23543.3242\n",
      "Epoch 43: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23593.6094 - rmse: 23549.7930 - val_loss: 23616.5137 - val_rmse: 23770.5898\n",
      "Epoch 44/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22041.0273 - rmse: 22041.0273\n",
      "Epoch 44: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21889.0254 - rmse: 21886.7109 - val_loss: 16016.3027 - val_rmse: 16224.4336\n",
      "Epoch 45/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20220.5449 - rmse: 20220.5449\n",
      "Epoch 45: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20259.0293 - rmse: 20281.7910 - val_loss: 24417.7598 - val_rmse: 24558.5723\n",
      "Epoch 46/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22415.2539 - rmse: 22466.0469\n",
      "Epoch 46: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22415.2539 - rmse: 22466.0469 - val_loss: 14743.7529 - val_rmse: 14932.8115\n",
      "Epoch 47/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21625.4492 - rmse: 21636.4023\n",
      "Epoch 47: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21625.4492 - rmse: 21636.4023 - val_loss: 16748.7578 - val_rmse: 16989.6465\n",
      "Epoch 48/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20100.3926 - rmse: 20100.3926\n",
      "Epoch 48: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20013.6426 - rmse: 20017.8184 - val_loss: 15643.2793 - val_rmse: 15775.2422\n",
      "Epoch 49/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18838.9434 - rmse: 18838.9434\n",
      "Epoch 49: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19104.1562 - rmse: 19129.3281 - val_loss: 16808.1426 - val_rmse: 16946.1230\n",
      "Epoch 50/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19843.2441 - rmse: 19843.2441\n",
      "Epoch 50: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19840.0898 - rmse: 19838.2246 - val_loss: 16009.6240 - val_rmse: 16170.2900\n",
      "Epoch 51/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19798.0391 - rmse: 19798.0391\n",
      "Epoch 51: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19856.2168 - rmse: 19903.7852 - val_loss: 19100.2754 - val_rmse: 19185.3203\n",
      "Epoch 52/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19507.0957 - rmse: 19507.0957\n",
      "Epoch 52: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19482.3145 - rmse: 19508.1719 - val_loss: 16479.0645 - val_rmse: 16566.1738\n",
      "Epoch 53/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19995.7383 - rmse: 19995.7383\n",
      "Epoch 53: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20218.6777 - rmse: 20226.8672 - val_loss: 23283.6738 - val_rmse: 23362.8359\n",
      "Epoch 54/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21756.2988 - rmse: 21756.2988\n",
      "Epoch 54: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21779.8262 - rmse: 21906.7305 - val_loss: 14431.9883 - val_rmse: 14556.9648\n",
      "Epoch 55/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20204.5723 - rmse: 20186.2676\n",
      "Epoch 55: val_loss did not improve from 14060.30371\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20204.5723 - rmse: 20186.2676 - val_loss: 17034.3457 - val_rmse: 17215.1719\n",
      "Epoch 55: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:09:27.960100: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 100631.2188 - rmse: 100361.6484\n",
      "Epoch 1: val_loss improved from inf to 53638.03516, saving model to ./ckpt/5_class_lr005/val_rmse_56067.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 100631.2188 - rmse: 100361.6484 - val_loss: 53638.0352 - val_rmse: 56067.4062\n",
      "Epoch 2/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:09:29.052533: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 52982.0117 - rmse: 52937.7891\n",
      "Epoch 2: val_loss improved from 53638.03516 to 39879.98828, saving model to ./ckpt/5_class_lr005/val_rmse_41710.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 52982.0117 - rmse: 52937.7891 - val_loss: 39879.9883 - val_rmse: 41709.6758\n",
      "Epoch 3/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 46302.3086 - rmse: 46302.3086\n",
      "Epoch 3: val_loss improved from 39879.98828 to 35649.89062, saving model to ./ckpt/5_class_lr005/val_rmse_37695.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 47218.0039 - rmse: 47165.3242 - val_loss: 35649.8906 - val_rmse: 37695.0234\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 45246.7852 - rmse: 45246.7852\n",
      "Epoch 4: val_loss did not improve from 35649.89062\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 45020.9375 - rmse: 44992.9492 - val_loss: 39358.4961 - val_rmse: 41362.5000\n",
      "Epoch 5/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 43784.4258 - rmse: 43784.4258\n",
      "Epoch 5: val_loss improved from 35649.89062 to 34510.55078, saving model to ./ckpt/5_class_lr005/val_rmse_36320.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 43456.6914 - rmse: 43434.9688 - val_loss: 34510.5508 - val_rmse: 36320.0664\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 40893.2305 - rmse: 40893.2305\n",
      "Epoch 6: val_loss improved from 34510.55078 to 27345.76758, saving model to ./ckpt/5_class_lr005/val_rmse_28636.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 41033.3086 - rmse: 40979.5664 - val_loss: 27345.7676 - val_rmse: 28636.2598\n",
      "Epoch 7/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 38685.5156 - rmse: 38685.5156\n",
      "Epoch 7: val_loss improved from 27345.76758 to 26068.85352, saving model to ./ckpt/5_class_lr005/val_rmse_27141.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 38888.9336 - rmse: 38925.4805 - val_loss: 26068.8535 - val_rmse: 27141.3594\n",
      "Epoch 8/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 40719.4023 - rmse: 40719.4023\n",
      "Epoch 8: val_loss did not improve from 26068.85352\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 40533.9961 - rmse: 40499.2578 - val_loss: 30008.3379 - val_rmse: 31296.3398\n",
      "Epoch 9/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 37373.2188 - rmse: 37373.2188\n",
      "Epoch 9: val_loss improved from 26068.85352 to 22628.67773, saving model to ./ckpt/5_class_lr005/val_rmse_23540.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 37178.9922 - rmse: 37144.8438 - val_loss: 22628.6777 - val_rmse: 23539.8750\n",
      "Epoch 10/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 34547.2812 - rmse: 34547.2812\n",
      "Epoch 10: val_loss did not improve from 22628.67773\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 34463.6094 - rmse: 34458.7969 - val_loss: 27154.5293 - val_rmse: 27754.3320\n",
      "Epoch 11/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 31034.8164 - rmse: 31034.8164\n",
      "Epoch 11: val_loss did not improve from 22628.67773\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 31215.2500 - rmse: 31203.3926 - val_loss: 22981.8184 - val_rmse: 23483.6230\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 32588.3770 - rmse: 32565.5293\n",
      "Epoch 12: val_loss improved from 22628.67773 to 19568.65820, saving model to ./ckpt/5_class_lr005/val_rmse_20016.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 32588.3770 - rmse: 32565.5293 - val_loss: 19568.6582 - val_rmse: 20015.9492\n",
      "Epoch 13/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 31591.7793 - rmse: 31591.7793\n",
      "Epoch 13: val_loss did not improve from 19568.65820\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 31554.4883 - rmse: 31601.5117 - val_loss: 21918.6914 - val_rmse: 22264.6777\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 30217.1289 - rmse: 30307.2285\n",
      "Epoch 14: val_loss did not improve from 19568.65820\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 30217.1289 - rmse: 30307.2285 - val_loss: 40174.5117 - val_rmse: 41166.4375\n",
      "Epoch 15/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 29544.5137 - rmse: 29544.5137\n",
      "Epoch 15: val_loss did not improve from 19568.65820\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 29401.8477 - rmse: 29391.5176 - val_loss: 23722.9336 - val_rmse: 24200.8535\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 26907.5645 - rmse: 26907.5645\n",
      "Epoch 16: val_loss improved from 19568.65820 to 18208.30859, saving model to ./ckpt/5_class_lr005/val_rmse_18430.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 26932.8652 - rmse: 26938.1621 - val_loss: 18208.3086 - val_rmse: 18430.4961\n",
      "Epoch 17/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 27937.0078 - rmse: 27937.0078\n",
      "Epoch 17: val_loss did not improve from 18208.30859\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 28067.4277 - rmse: 28039.6895 - val_loss: 26235.6504 - val_rmse: 26848.6328\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 26756.8047 - rmse: 26756.8047\n",
      "Epoch 18: val_loss did not improve from 18208.30859\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 26589.6445 - rmse: 26543.2617 - val_loss: 21141.9844 - val_rmse: 21791.6797\n",
      "Epoch 19/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25325.4980 - rmse: 25325.4980\n",
      "Epoch 19: val_loss did not improve from 18208.30859\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25336.8027 - rmse: 25328.4375 - val_loss: 28118.2285 - val_rmse: 28637.1074\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 26081.5977 - rmse: 26160.9238\n",
      "Epoch 20: val_loss improved from 18208.30859 to 17365.44141, saving model to ./ckpt/5_class_lr005/val_rmse_17538.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 26081.5977 - rmse: 26160.9238 - val_loss: 17365.4414 - val_rmse: 17538.1191\n",
      "Epoch 21/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 25686.5508 - rmse: 25686.5508\n",
      "Epoch 21: val_loss improved from 17365.44141 to 17151.71680, saving model to ./ckpt/5_class_lr005/val_rmse_17219.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 25548.0742 - rmse: 25547.9922 - val_loss: 17151.7168 - val_rmse: 17219.0098\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24252.0547 - rmse: 24209.6855\n",
      "Epoch 22: val_loss improved from 17151.71680 to 16767.48047, saving model to ./ckpt/5_class_lr005/val_rmse_16831.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 24252.0547 - rmse: 24209.6855 - val_loss: 16767.4805 - val_rmse: 16831.2754\n",
      "Epoch 23/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 25664.5234 - rmse: 25664.5234\n",
      "Epoch 23: val_loss did not improve from 16767.48047\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25659.6113 - rmse: 25669.9922 - val_loss: 20686.1836 - val_rmse: 20704.4473\n",
      "Epoch 24/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24656.1172 - rmse: 24623.6582\n",
      "Epoch 24: val_loss did not improve from 16767.48047\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 24656.1172 - rmse: 24623.6582 - val_loss: 20804.7852 - val_rmse: 21461.8750\n",
      "Epoch 25/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23695.7031 - rmse: 23695.7031\n",
      "Epoch 25: val_loss improved from 16767.48047 to 14425.18848, saving model to ./ckpt/5_class_lr005/val_rmse_14502.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23676.3516 - rmse: 23664.9043 - val_loss: 14425.1885 - val_rmse: 14501.9805\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22248.5801 - rmse: 22264.4160\n",
      "Epoch 26: val_loss did not improve from 14425.18848\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22248.5801 - rmse: 22264.4160 - val_loss: 15992.8096 - val_rmse: 16451.6348\n",
      "Epoch 27/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22385.7539 - rmse: 22385.7539\n",
      "Epoch 27: val_loss did not improve from 14425.18848\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22525.3535 - rmse: 22501.7676 - val_loss: 15955.0186 - val_rmse: 16177.5703\n",
      "Epoch 28/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 23629.5273 - rmse: 23629.5273\n",
      "Epoch 28: val_loss did not improve from 14425.18848\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23818.0723 - rmse: 23813.3633 - val_loss: 21703.9727 - val_rmse: 22130.5059\n",
      "Epoch 29/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21446.1777 - rmse: 21446.1777\n",
      "Epoch 29: val_loss improved from 14425.18848 to 13636.94434, saving model to ./ckpt/5_class_lr005/val_rmse_13836.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21527.5957 - rmse: 21539.3555 - val_loss: 13636.9443 - val_rmse: 13835.5098\n",
      "Epoch 30/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24874.4570 - rmse: 24874.4570\n",
      "Epoch 30: val_loss did not improve from 13636.94434\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 24859.1758 - rmse: 24852.0234 - val_loss: 15602.0645 - val_rmse: 15967.5273\n",
      "Epoch 31/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21870.2520 - rmse: 21870.2520\n",
      "Epoch 31: val_loss did not improve from 13636.94434\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22054.9824 - rmse: 22058.8242 - val_loss: 18323.6875 - val_rmse: 18303.6191\n",
      "Epoch 32/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 22275.9570 - rmse: 22275.9570\n",
      "Epoch 32: val_loss did not improve from 13636.94434\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22134.9180 - rmse: 22154.2910 - val_loss: 13942.2598 - val_rmse: 14215.4990\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23735.5039 - rmse: 23755.8945\n",
      "Epoch 33: val_loss did not improve from 13636.94434\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23735.5039 - rmse: 23755.8945 - val_loss: 15290.7646 - val_rmse: 15458.8184\n",
      "Epoch 34/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22385.4238 - rmse: 22385.4238\n",
      "Epoch 34: val_loss did not improve from 13636.94434\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22398.6484 - rmse: 22406.4707 - val_loss: 14774.4746 - val_rmse: 15060.1885\n",
      "Epoch 35/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21043.5859 - rmse: 21043.5859\n",
      "Epoch 35: val_loss did not improve from 13636.94434\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21206.4062 - rmse: 21202.3105 - val_loss: 21273.8828 - val_rmse: 21844.5137\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20718.0918 - rmse: 20725.2793\n",
      "Epoch 36: val_loss did not improve from 13636.94434\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20718.0918 - rmse: 20725.2793 - val_loss: 17748.3105 - val_rmse: 18263.1738\n",
      "Epoch 37/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22832.1289 - rmse: 22816.4277\n",
      "Epoch 37: val_loss did not improve from 13636.94434\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22832.1289 - rmse: 22816.4277 - val_loss: 20667.7246 - val_rmse: 21284.7812\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21344.9414 - rmse: 21344.9414\n",
      "Epoch 38: val_loss did not improve from 13636.94434\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21410.4551 - rmse: 21449.2012 - val_loss: 21987.6719 - val_rmse: 22735.8711\n",
      "Epoch 39/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20788.8555 - rmse: 20788.8555\n",
      "Epoch 39: val_loss did not improve from 13636.94434\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21209.1758 - rmse: 21255.4355 - val_loss: 33144.0156 - val_rmse: 34344.8125\n",
      "Epoch 40/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23204.4082 - rmse: 23204.4082\n",
      "Epoch 40: val_loss did not improve from 13636.94434\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23040.1836 - rmse: 23122.0566 - val_loss: 16929.2910 - val_rmse: 17541.7598\n",
      "Epoch 41/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22943.2949 - rmse: 22943.2949\n",
      "Epoch 41: val_loss improved from 13636.94434 to 13152.62500, saving model to ./ckpt/5_class_lr005/val_rmse_13256.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22757.9258 - rmse: 22704.1289 - val_loss: 13152.6250 - val_rmse: 13256.4971\n",
      "Epoch 42/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20130.1133 - rmse: 20130.1133\n",
      "Epoch 42: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20100.9785 - rmse: 20129.6055 - val_loss: 20931.0059 - val_rmse: 21404.1973\n",
      "Epoch 43/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20486.6406 - rmse: 20486.6406\n",
      "Epoch 43: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20490.3125 - rmse: 20499.0078 - val_loss: 16636.5684 - val_rmse: 17282.9609\n",
      "Epoch 44/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20516.5430 - rmse: 20516.5430\n",
      "Epoch 44: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20711.4980 - rmse: 20826.7988 - val_loss: 17608.8457 - val_rmse: 17901.0605\n",
      "Epoch 45/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21746.3359 - rmse: 21746.3359\n",
      "Epoch 45: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22046.3066 - rmse: 22088.7266 - val_loss: 13950.4102 - val_rmse: 14109.9424\n",
      "Epoch 46/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20989.4414 - rmse: 20989.4414\n",
      "Epoch 46: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20884.8691 - rmse: 20883.6367 - val_loss: 13714.9521 - val_rmse: 13894.9873\n",
      "Epoch 47/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 21759.7422 - rmse: 21759.7422\n",
      "Epoch 47: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21878.6055 - rmse: 21843.1562 - val_loss: 13995.0156 - val_rmse: 14041.6162\n",
      "Epoch 48/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23463.6094 - rmse: 23463.6094\n",
      "Epoch 48: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23472.1309 - rmse: 23477.1719 - val_loss: 16726.3262 - val_rmse: 17146.5410\n",
      "Epoch 49/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20293.9785 - rmse: 20293.9785\n",
      "Epoch 49: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20264.4961 - rmse: 20244.0586 - val_loss: 18096.6074 - val_rmse: 18526.2930\n",
      "Epoch 50/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18846.6816 - rmse: 18846.6816\n",
      "Epoch 50: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19080.5098 - rmse: 19218.8027 - val_loss: 14570.6562 - val_rmse: 14945.1045\n",
      "Epoch 51/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21691.4336 - rmse: 21691.4336\n",
      "Epoch 51: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21772.6445 - rmse: 21767.4180 - val_loss: 13320.6562 - val_rmse: 13435.7656\n",
      "Epoch 52/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21579.0781 - rmse: 21579.0781\n",
      "Epoch 52: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21700.1270 - rmse: 21771.7188 - val_loss: 25478.6875 - val_rmse: 26206.5430\n",
      "Epoch 53/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18766.3906 - rmse: 18766.3906\n",
      "Epoch 53: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19009.1523 - rmse: 19063.7969 - val_loss: 22326.0996 - val_rmse: 22894.8711\n",
      "Epoch 54/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22271.8809 - rmse: 22271.8809\n",
      "Epoch 54: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22461.6855 - rmse: 22520.7773 - val_loss: 14178.3975 - val_rmse: 14450.7422\n",
      "Epoch 55/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18502.4395 - rmse: 18502.4395\n",
      "Epoch 55: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18736.6895 - rmse: 18853.4258 - val_loss: 25673.8301 - val_rmse: 26321.0645\n",
      "Epoch 56/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18790.5371 - rmse: 18790.5371\n",
      "Epoch 56: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18820.5723 - rmse: 18796.5156 - val_loss: 24114.0781 - val_rmse: 24921.3633\n",
      "Epoch 57/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19433.7207 - rmse: 19433.7207\n",
      "Epoch 57: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19475.1191 - rmse: 19473.0312 - val_loss: 15166.2285 - val_rmse: 15193.6465\n",
      "Epoch 58/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19250.9824 - rmse: 19250.9824\n",
      "Epoch 58: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19194.5137 - rmse: 19187.8984 - val_loss: 15285.9111 - val_rmse: 15784.4600\n",
      "Epoch 59/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18289.3809 - rmse: 18289.3809\n",
      "Epoch 59: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18214.7773 - rmse: 18200.9062 - val_loss: 15262.9209 - val_rmse: 15519.0215\n",
      "Epoch 60/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18340.8125 - rmse: 18340.8125\n",
      "Epoch 60: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18393.1953 - rmse: 18397.2754 - val_loss: 15002.4805 - val_rmse: 15295.5000\n",
      "Epoch 61/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19794.5820 - rmse: 19794.5820\n",
      "Epoch 61: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19684.4688 - rmse: 19728.6895 - val_loss: 20844.1699 - val_rmse: 21496.9570\n",
      "Epoch 62/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18284.4824 - rmse: 18284.4824\n",
      "Epoch 62: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18364.4844 - rmse: 18366.1074 - val_loss: 15555.6758 - val_rmse: 15595.6025\n",
      "Epoch 63/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20151.1426 - rmse: 20151.1426\n",
      "Epoch 63: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20176.6836 - rmse: 20228.5801 - val_loss: 23680.1465 - val_rmse: 24110.9453\n",
      "Epoch 64/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20200.6172 - rmse: 20207.1016\n",
      "Epoch 64: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20200.6172 - rmse: 20207.1016 - val_loss: 14754.8994 - val_rmse: 14846.3232\n",
      "Epoch 65/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18438.3477 - rmse: 18438.3477\n",
      "Epoch 65: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18447.8516 - rmse: 18465.5820 - val_loss: 14844.9521 - val_rmse: 15116.1377\n",
      "Epoch 66/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20521.2539 - rmse: 20521.2539\n",
      "Epoch 66: val_loss did not improve from 13152.62500\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20560.9688 - rmse: 20617.7617 - val_loss: 25718.5625 - val_rmse: 26219.1699\n",
      "Epoch 66: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:10:15.665049: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 106009.5234 - rmse: 105738.6953\n",
      "Epoch 1: val_loss improved from inf to 43910.97656, saving model to ./ckpt/5_class_lr005/val_rmse_44142.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 106009.5234 - rmse: 105738.6953 - val_loss: 43910.9766 - val_rmse: 44141.8242\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 58519.4766 - rmse: 58519.4766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:10:16.732338: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/70 [============================>.] - ETA: 0s - loss: 54881.5078 - rmse: 54881.5078\n",
      "Epoch 2: val_loss did not improve from 43910.97656\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 55432.7617 - rmse: 55703.2422 - val_loss: 51572.0625 - val_rmse: 51808.2969\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 51251.5781 - rmse: 51251.5781\n",
      "Epoch 3: val_loss improved from 43910.97656 to 33892.00000, saving model to ./ckpt/5_class_lr005/val_rmse_33549.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 51196.4219 - rmse: 51163.7969 - val_loss: 33892.0000 - val_rmse: 33548.6016\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 44966.2500 - rmse: 44986.6484\n",
      "Epoch 4: val_loss improved from 33892.00000 to 28774.96680, saving model to ./ckpt/5_class_lr005/val_rmse_28572.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 44966.2500 - rmse: 44986.6484 - val_loss: 28774.9668 - val_rmse: 28572.4805\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 42213.7891 - rmse: 42214.9453\n",
      "Epoch 5: val_loss improved from 28774.96680 to 28259.45508, saving model to ./ckpt/5_class_lr005/val_rmse_27855.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 42213.7891 - rmse: 42214.9453 - val_loss: 28259.4551 - val_rmse: 27854.6387\n",
      "Epoch 6/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 40304.9961 - rmse: 40304.9961\n",
      "Epoch 6: val_loss did not improve from 28259.45508\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 40196.1797 - rmse: 40128.0078 - val_loss: 30450.0293 - val_rmse: 29931.6992\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 39836.9453 - rmse: 39836.9453\n",
      "Epoch 7: val_loss did not improve from 28259.45508\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 39941.8594 - rmse: 39867.3281 - val_loss: 30068.8555 - val_rmse: 29554.3496\n",
      "Epoch 8/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 39905.8789 - rmse: 39905.8789\n",
      "Epoch 8: val_loss did not improve from 28259.45508\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 40219.0859 - rmse: 40271.1562 - val_loss: 36208.9570 - val_rmse: 35704.9844\n",
      "Epoch 9/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 37591.5273 - rmse: 37591.5273\n",
      "Epoch 9: val_loss improved from 28259.45508 to 25620.96484, saving model to ./ckpt/5_class_lr005/val_rmse_25266.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 37335.9922 - rmse: 37377.8555 - val_loss: 25620.9648 - val_rmse: 25265.8828\n",
      "Epoch 10/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 36295.7812 - rmse: 36295.7812\n",
      "Epoch 10: val_loss did not improve from 25620.96484\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 36181.5430 - rmse: 36160.2539 - val_loss: 28857.1777 - val_rmse: 28461.8359\n",
      "Epoch 11/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 32495.0410 - rmse: 32495.0410\n",
      "Epoch 11: val_loss improved from 25620.96484 to 22295.16211, saving model to ./ckpt/5_class_lr005/val_rmse_22436.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 33010.6016 - rmse: 32945.2305 - val_loss: 22295.1621 - val_rmse: 22435.6289\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 31368.4023 - rmse: 31395.0781\n",
      "Epoch 12: val_loss did not improve from 22295.16211\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 31368.4023 - rmse: 31395.0781 - val_loss: 22637.6973 - val_rmse: 22564.2070\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 31281.8223 - rmse: 31281.8223\n",
      "Epoch 13: val_loss improved from 22295.16211 to 21327.74805, saving model to ./ckpt/5_class_lr005/val_rmse_21346.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 31104.9961 - rmse: 31000.4141 - val_loss: 21327.7480 - val_rmse: 21346.3281\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 31866.5215 - rmse: 31877.3438\n",
      "Epoch 14: val_loss did not improve from 21327.74805\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 31866.5215 - rmse: 31877.3438 - val_loss: 22232.6562 - val_rmse: 22130.8008\n",
      "Epoch 15/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 32847.4648 - rmse: 32847.4648\n",
      "Epoch 15: val_loss did not improve from 21327.74805\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 32807.3672 - rmse: 32814.5898 - val_loss: 23007.4766 - val_rmse: 23336.8887\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 28813.4648 - rmse: 28813.4648\n",
      "Epoch 16: val_loss improved from 21327.74805 to 20256.55859, saving model to ./ckpt/5_class_lr005/val_rmse_20277.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 28703.6797 - rmse: 28708.1738 - val_loss: 20256.5586 - val_rmse: 20277.2930\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27027.8125 - rmse: 27057.7715\n",
      "Epoch 17: val_loss improved from 20256.55859 to 19508.81836, saving model to ./ckpt/5_class_lr005/val_rmse_19973.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27027.8125 - rmse: 27057.7715 - val_loss: 19508.8184 - val_rmse: 19973.0176\n",
      "Epoch 18/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 27917.0449 - rmse: 27917.0449\n",
      "Epoch 18: val_loss did not improve from 19508.81836\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 27812.3457 - rmse: 27790.3105 - val_loss: 22580.2969 - val_rmse: 22495.0527\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25414.9785 - rmse: 25374.5020\n",
      "Epoch 19: val_loss improved from 19508.81836 to 19228.62109, saving model to ./ckpt/5_class_lr005/val_rmse_19242.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 25414.9785 - rmse: 25374.5020 - val_loss: 19228.6211 - val_rmse: 19242.2285\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 29428.1289 - rmse: 29516.2188\n",
      "Epoch 20: val_loss improved from 19228.62109 to 18862.66211, saving model to ./ckpt/5_class_lr005/val_rmse_19323.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 29428.1289 - rmse: 29516.2188 - val_loss: 18862.6621 - val_rmse: 19323.4082\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25379.2109 - rmse: 25379.2109\n",
      "Epoch 21: val_loss did not improve from 18862.66211\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 25547.9883 - rmse: 25526.7715 - val_loss: 20775.7617 - val_rmse: 20696.5156\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 26432.7539 - rmse: 26432.7539\n",
      "Epoch 22: val_loss did not improve from 18862.66211\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26815.8281 - rmse: 26901.9141 - val_loss: 26891.4004 - val_rmse: 26702.2422\n",
      "Epoch 23/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25646.5215 - rmse: 25646.5215\n",
      "Epoch 23: val_loss did not improve from 18862.66211\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 25626.3965 - rmse: 25654.8711 - val_loss: 21572.1855 - val_rmse: 22039.3887\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25516.0527 - rmse: 25516.0527\n",
      "Epoch 24: val_loss improved from 18862.66211 to 17796.57227, saving model to ./ckpt/5_class_lr005/val_rmse_17868.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 25592.5371 - rmse: 25587.3984 - val_loss: 17796.5723 - val_rmse: 17867.8574\n",
      "Epoch 25/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23336.9688 - rmse: 23336.9688\n",
      "Epoch 25: val_loss did not improve from 17796.57227\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23510.2812 - rmse: 23518.4492 - val_loss: 24739.5137 - val_rmse: 24595.4648\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24760.0352 - rmse: 24760.0352\n",
      "Epoch 26: val_loss improved from 17796.57227 to 16375.84375, saving model to ./ckpt/5_class_lr005/val_rmse_16564.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24603.6055 - rmse: 24591.5840 - val_loss: 16375.8438 - val_rmse: 16563.6602\n",
      "Epoch 27/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22385.8574 - rmse: 22385.8574\n",
      "Epoch 27: val_loss did not improve from 16375.84375\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22482.1621 - rmse: 22503.7852 - val_loss: 22364.0449 - val_rmse: 22289.9961\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23427.1484 - rmse: 23427.1484\n",
      "Epoch 28: val_loss improved from 16375.84375 to 14632.36816, saving model to ./ckpt/5_class_lr005/val_rmse_14905.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23447.2207 - rmse: 23459.0938 - val_loss: 14632.3682 - val_rmse: 14904.5977\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 24586.4180 - rmse: 24586.4180\n",
      "Epoch 29: val_loss did not improve from 14632.36816\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 24793.5684 - rmse: 24916.0820 - val_loss: 18060.8438 - val_rmse: 17989.8496\n",
      "Epoch 30/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 25298.4121 - rmse: 25298.4121\n",
      "Epoch 30: val_loss did not improve from 14632.36816\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 25198.8809 - rmse: 25179.9590 - val_loss: 18666.6660 - val_rmse: 18634.9492\n",
      "Epoch 31/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22629.2578 - rmse: 22629.2578\n",
      "Epoch 31: val_loss did not improve from 14632.36816\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22663.8203 - rmse: 22684.2617 - val_loss: 15287.2803 - val_rmse: 15485.5352\n",
      "Epoch 32/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22687.9707 - rmse: 22687.9707\n",
      "Epoch 32: val_loss did not improve from 14632.36816\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 22815.8164 - rmse: 22802.2637 - val_loss: 14895.9395 - val_rmse: 15084.6895\n",
      "Epoch 33/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21966.8574 - rmse: 21966.8574\n",
      "Epoch 33: val_loss did not improve from 14632.36816\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21888.3066 - rmse: 21874.9980 - val_loss: 19644.5449 - val_rmse: 19642.7793\n",
      "Epoch 34/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20653.5723 - rmse: 20653.5723\n",
      "Epoch 34: val_loss did not improve from 14632.36816\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20677.5430 - rmse: 20678.5898 - val_loss: 17280.1738 - val_rmse: 17220.1758\n",
      "Epoch 35/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19845.8887 - rmse: 19845.8887\n",
      "Epoch 35: val_loss did not improve from 14632.36816\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19958.8516 - rmse: 19928.6133 - val_loss: 16858.0273 - val_rmse: 16799.1172\n",
      "Epoch 36/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21632.7383 - rmse: 21632.7383\n",
      "Epoch 36: val_loss did not improve from 14632.36816\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21922.0195 - rmse: 21935.7754 - val_loss: 15552.5996 - val_rmse: 15632.7285\n",
      "Epoch 37/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20497.5938 - rmse: 20497.5938\n",
      "Epoch 37: val_loss did not improve from 14632.36816\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20561.6523 - rmse: 20680.8867 - val_loss: 15399.1582 - val_rmse: 15667.6855\n",
      "Epoch 38/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20779.8574 - rmse: 20779.8574\n",
      "Epoch 38: val_loss did not improve from 14632.36816\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20830.0918 - rmse: 20815.0801 - val_loss: 15369.7783 - val_rmse: 15333.2363\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21273.8105 - rmse: 21273.8105\n",
      "Epoch 39: val_loss did not improve from 14632.36816\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21235.8711 - rmse: 21213.4316 - val_loss: 21772.6465 - val_rmse: 21635.8398\n",
      "Epoch 40/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20981.5859 - rmse: 20981.5859\n",
      "Epoch 40: val_loss improved from 14632.36816 to 14113.91309, saving model to ./ckpt/5_class_lr005/val_rmse_14303.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21377.8496 - rmse: 21380.3652 - val_loss: 14113.9131 - val_rmse: 14302.7949\n",
      "Epoch 41/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20322.6523 - rmse: 20322.6523\n",
      "Epoch 41: val_loss did not improve from 14113.91309\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20331.7676 - rmse: 20360.8750 - val_loss: 14830.5020 - val_rmse: 14818.4443\n",
      "Epoch 42/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21959.5469 - rmse: 21981.2363\n",
      "Epoch 42: val_loss did not improve from 14113.91309\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21959.5469 - rmse: 21981.2363 - val_loss: 15288.5605 - val_rmse: 15534.5488\n",
      "Epoch 43/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20649.8379 - rmse: 20649.8379\n",
      "Epoch 43: val_loss did not improve from 14113.91309\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20764.1914 - rmse: 20748.9648 - val_loss: 18879.9395 - val_rmse: 18734.6289\n",
      "Epoch 44/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20077.0488 - rmse: 20077.0488\n",
      "Epoch 44: val_loss did not improve from 14113.91309\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20224.6172 - rmse: 20233.3164 - val_loss: 22879.9727 - val_rmse: 22735.9863\n",
      "Epoch 45/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21714.1309 - rmse: 21720.0215\n",
      "Epoch 45: val_loss did not improve from 14113.91309\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21714.1309 - rmse: 21720.0215 - val_loss: 14491.7305 - val_rmse: 14733.9375\n",
      "Epoch 46/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20683.4922 - rmse: 20683.4922\n",
      "Epoch 46: val_loss did not improve from 14113.91309\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20729.6152 - rmse: 20701.2109 - val_loss: 15682.9785 - val_rmse: 15912.0303\n",
      "Epoch 47/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21275.2090 - rmse: 21275.2090\n",
      "Epoch 47: val_loss did not improve from 14113.91309\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21249.7285 - rmse: 21227.1094 - val_loss: 21496.2969 - val_rmse: 21376.0781\n",
      "Epoch 48/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24052.0879 - rmse: 24052.0879\n",
      "Epoch 48: val_loss improved from 14113.91309 to 13611.16895, saving model to ./ckpt/5_class_lr005/val_rmse_13752.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23717.9785 - rmse: 23666.3887 - val_loss: 13611.1689 - val_rmse: 13752.2988\n",
      "Epoch 49/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19698.7207 - rmse: 19698.7207\n",
      "Epoch 49: val_loss did not improve from 13611.16895\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19571.6094 - rmse: 19624.8145 - val_loss: 17319.8418 - val_rmse: 17250.1387\n",
      "Epoch 50/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21903.0371 - rmse: 21914.6328\n",
      "Epoch 50: val_loss did not improve from 13611.16895\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21903.0371 - rmse: 21914.6328 - val_loss: 20573.9336 - val_rmse: 20426.1133\n",
      "Epoch 51/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19640.5449 - rmse: 19785.6387\n",
      "Epoch 51: val_loss did not improve from 13611.16895\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19640.5449 - rmse: 19785.6387 - val_loss: 14616.4414 - val_rmse: 14578.5576\n",
      "Epoch 52/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19960.2070 - rmse: 19960.2070\n",
      "Epoch 52: val_loss improved from 13611.16895 to 13441.15234, saving model to ./ckpt/5_class_lr005/val_rmse_13534.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20231.9141 - rmse: 20231.3965 - val_loss: 13441.1523 - val_rmse: 13533.5195\n",
      "Epoch 53/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19969.7578 - rmse: 19969.7578\n",
      "Epoch 53: val_loss did not improve from 13441.15234\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20728.1953 - rmse: 20699.1152 - val_loss: 33527.6992 - val_rmse: 33857.5508\n",
      "Epoch 54/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21761.7734 - rmse: 21761.7734\n",
      "Epoch 54: val_loss improved from 13441.15234 to 13432.39746, saving model to ./ckpt/5_class_lr005/val_rmse_13488.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21752.1777 - rmse: 21783.4219 - val_loss: 13432.3975 - val_rmse: 13488.1221\n",
      "Epoch 55/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19262.4531 - rmse: 19262.4531\n",
      "Epoch 55: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19306.8340 - rmse: 19346.1367 - val_loss: 14097.5215 - val_rmse: 14160.9502\n",
      "Epoch 56/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21747.4648 - rmse: 21747.4648\n",
      "Epoch 56: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21717.6641 - rmse: 21758.2305 - val_loss: 14638.7197 - val_rmse: 14634.8730\n",
      "Epoch 57/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18951.9805 - rmse: 18951.9805\n",
      "Epoch 57: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18794.9512 - rmse: 18813.2266 - val_loss: 13687.6182 - val_rmse: 13778.7002\n",
      "Epoch 58/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20186.1328 - rmse: 20186.1328\n",
      "Epoch 58: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20505.3047 - rmse: 20612.5430 - val_loss: 13826.4922 - val_rmse: 13899.2324\n",
      "Epoch 59/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 22166.5156 - rmse: 22166.5156\n",
      "Epoch 59: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22458.4941 - rmse: 22471.4043 - val_loss: 13856.1533 - val_rmse: 13879.5527\n",
      "Epoch 60/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18590.4453 - rmse: 18590.4453\n",
      "Epoch 60: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18548.0605 - rmse: 18544.1797 - val_loss: 14329.4746 - val_rmse: 14343.1455\n",
      "Epoch 61/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19650.1953 - rmse: 19650.1953\n",
      "Epoch 61: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19684.9277 - rmse: 19694.1387 - val_loss: 17344.9512 - val_rmse: 17242.0859\n",
      "Epoch 62/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18466.0879 - rmse: 18466.0879\n",
      "Epoch 62: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18684.0996 - rmse: 18712.9258 - val_loss: 14058.1006 - val_rmse: 14153.1621\n",
      "Epoch 63/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19664.0625 - rmse: 19650.3691\n",
      "Epoch 63: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19664.0625 - rmse: 19650.3691 - val_loss: 16697.9473 - val_rmse: 16625.1445\n",
      "Epoch 64/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19434.2305 - rmse: 19434.2305\n",
      "Epoch 64: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19470.6055 - rmse: 19464.6855 - val_loss: 13927.2666 - val_rmse: 13914.3799\n",
      "Epoch 65/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18813.4766 - rmse: 18813.4766\n",
      "Epoch 65: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18771.1289 - rmse: 18781.4746 - val_loss: 13587.9082 - val_rmse: 13742.9844\n",
      "Epoch 66/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18995.6699 - rmse: 18995.6699\n",
      "Epoch 66: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18996.2754 - rmse: 18996.6328 - val_loss: 21223.0664 - val_rmse: 21040.9297\n",
      "Epoch 67/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19469.7461 - rmse: 19448.3887\n",
      "Epoch 67: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19469.7461 - rmse: 19448.3887 - val_loss: 14963.9385 - val_rmse: 15149.6904\n",
      "Epoch 68/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19091.1543 - rmse: 19091.1543\n",
      "Epoch 68: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19079.9961 - rmse: 19079.7148 - val_loss: 22901.3691 - val_rmse: 23234.1738\n",
      "Epoch 69/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19842.9336 - rmse: 19878.9512\n",
      "Epoch 69: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19842.9336 - rmse: 19878.9512 - val_loss: 13642.2959 - val_rmse: 13829.7480\n",
      "Epoch 70/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18984.3066 - rmse: 18984.3066\n",
      "Epoch 70: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19010.7969 - rmse: 19076.1035 - val_loss: 13637.5859 - val_rmse: 13777.1680\n",
      "Epoch 71/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20074.0840 - rmse: 20074.0840\n",
      "Epoch 71: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20371.2285 - rmse: 20546.9707 - val_loss: 16188.1094 - val_rmse: 16107.4648\n",
      "Epoch 72/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19394.3027 - rmse: 19394.3027\n",
      "Epoch 72: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19534.2832 - rmse: 19520.7324 - val_loss: 23331.7402 - val_rmse: 23184.8203\n",
      "Epoch 73/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18799.4590 - rmse: 18799.4590\n",
      "Epoch 73: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18998.4316 - rmse: 18999.4863 - val_loss: 14128.4697 - val_rmse: 14292.1738\n",
      "Epoch 74/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18429.4531 - rmse: 18429.4531\n",
      "Epoch 74: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18622.1465 - rmse: 18649.1738 - val_loss: 16829.8457 - val_rmse: 16770.6836\n",
      "Epoch 75/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18743.2480 - rmse: 18743.2480\n",
      "Epoch 75: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18835.2363 - rmse: 18900.9688 - val_loss: 21857.2656 - val_rmse: 21702.4570\n",
      "Epoch 76/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18149.4180 - rmse: 18149.4180\n",
      "Epoch 76: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18392.4238 - rmse: 18407.2832 - val_loss: 16037.0371 - val_rmse: 16224.8857\n",
      "Epoch 77/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18188.9941 - rmse: 18188.9941\n",
      "Epoch 77: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18050.6934 - rmse: 18001.6641 - val_loss: 17514.9844 - val_rmse: 17401.3438\n",
      "Epoch 78/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17872.6250 - rmse: 17872.6250\n",
      "Epoch 78: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17924.3770 - rmse: 17930.5508 - val_loss: 13554.1143 - val_rmse: 13595.9297\n",
      "Epoch 79/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19947.2480 - rmse: 19947.2480\n",
      "Epoch 79: val_loss did not improve from 13432.39746\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19804.0039 - rmse: 19787.4160 - val_loss: 17543.5508 - val_rmse: 17461.1699\n",
      "Epoch 79: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:11:12.224327: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 100935.4141 - rmse: 100639.5312\n",
      "Epoch 1: val_loss improved from inf to 60587.43359, saving model to ./ckpt/5_class_lr005/val_rmse_59514.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 100935.4141 - rmse: 100639.5312 - val_loss: 60587.4336 - val_rmse: 59514.4180\n",
      "Epoch 2/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:11:13.272153: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 52399.1680 - rmse: 52399.1680\n",
      "Epoch 2: val_loss improved from 60587.43359 to 44495.48828, saving model to ./ckpt/5_class_lr005/val_rmse_43729.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 51922.3008 - rmse: 51890.1211 - val_loss: 44495.4883 - val_rmse: 43729.2461\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 46842.6016 - rmse: 46842.6016\n",
      "Epoch 3: val_loss did not improve from 44495.48828\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 46232.5703 - rmse: 46138.0273 - val_loss: 46621.3086 - val_rmse: 46083.5859\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 43897.0469 - rmse: 43897.0469\n",
      "Epoch 4: val_loss improved from 44495.48828 to 38995.86719, saving model to ./ckpt/5_class_lr005/val_rmse_38498.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 43838.0977 - rmse: 43803.2305 - val_loss: 38995.8672 - val_rmse: 38497.6055\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 42430.4023 - rmse: 42389.9336\n",
      "Epoch 5: val_loss did not improve from 38995.86719\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 42430.4023 - rmse: 42389.9336 - val_loss: 41638.3477 - val_rmse: 40981.1797\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 39691.1289 - rmse: 39691.1289\n",
      "Epoch 6: val_loss improved from 38995.86719 to 34174.57422, saving model to ./ckpt/5_class_lr005/val_rmse_33808.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 39956.1914 - rmse: 39996.3945 - val_loss: 34174.5742 - val_rmse: 33807.7461\n",
      "Epoch 7/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 37525.3477 - rmse: 37525.3477\n",
      "Epoch 7: val_loss did not improve from 34174.57422\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 37128.7695 - rmse: 37165.1914 - val_loss: 35602.1055 - val_rmse: 35404.1680\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 37865.2617 - rmse: 37865.2617\n",
      "Epoch 8: val_loss improved from 34174.57422 to 31028.43164, saving model to ./ckpt/5_class_lr005/val_rmse_30825.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 37901.5742 - rmse: 37923.0469 - val_loss: 31028.4316 - val_rmse: 30825.1152\n",
      "Epoch 9/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 33264.8711 - rmse: 33264.8711\n",
      "Epoch 9: val_loss improved from 31028.43164 to 27868.85742, saving model to ./ckpt/5_class_lr005/val_rmse_27809.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 33040.8633 - rmse: 33004.7930 - val_loss: 27868.8574 - val_rmse: 27808.7891\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 33820.4375 - rmse: 33827.4414\n",
      "Epoch 10: val_loss did not improve from 27868.85742\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 33820.4375 - rmse: 33827.4414 - val_loss: 33957.8359 - val_rmse: 33725.6797\n",
      "Epoch 11/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 35974.8906 - rmse: 35974.8906\n",
      "Epoch 11: val_loss improved from 27868.85742 to 26114.79492, saving model to ./ckpt/5_class_lr005/val_rmse_26052.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 36120.0352 - rmse: 36131.5391 - val_loss: 26114.7949 - val_rmse: 26051.5195\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 29493.6367 - rmse: 29493.6367\n",
      "Epoch 12: val_loss improved from 26114.79492 to 25830.21484, saving model to ./ckpt/5_class_lr005/val_rmse_25818.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 30013.5332 - rmse: 29962.6855 - val_loss: 25830.2148 - val_rmse: 25818.4980\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 28161.3066 - rmse: 28161.3066\n",
      "Epoch 13: val_loss improved from 25830.21484 to 24501.76953, saving model to ./ckpt/5_class_lr005/val_rmse_24460.hdf5\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 28121.9004 - rmse: 28098.5938 - val_loss: 24501.7695 - val_rmse: 24459.5566\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 28369.0645 - rmse: 28445.7754\n",
      "Epoch 14: val_loss improved from 24501.76953 to 23651.84180, saving model to ./ckpt/5_class_lr005/val_rmse_23603.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 28369.0645 - rmse: 28445.7754 - val_loss: 23651.8418 - val_rmse: 23602.9980\n",
      "Epoch 15/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 29049.8145 - rmse: 29049.8145\n",
      "Epoch 15: val_loss did not improve from 23651.84180\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 28818.3594 - rmse: 28798.4629 - val_loss: 24140.0977 - val_rmse: 24117.7422\n",
      "Epoch 16/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 29260.6094 - rmse: 29260.6094\n",
      "Epoch 16: val_loss did not improve from 23651.84180\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 29386.0703 - rmse: 29415.0957 - val_loss: 24275.3184 - val_rmse: 24103.1523\n",
      "Epoch 17/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 27177.6895 - rmse: 27177.6895\n",
      "Epoch 17: val_loss did not improve from 23651.84180\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27117.9629 - rmse: 27079.3066 - val_loss: 24813.9531 - val_rmse: 24680.1191\n",
      "Epoch 18/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 29574.6758 - rmse: 29574.6758\n",
      "Epoch 18: val_loss improved from 23651.84180 to 22370.53125, saving model to ./ckpt/5_class_lr005/val_rmse_22403.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 29786.5820 - rmse: 29846.7402 - val_loss: 22370.5312 - val_rmse: 22402.8535\n",
      "Epoch 19/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 25640.7832 - rmse: 25640.7832\n",
      "Epoch 19: val_loss did not improve from 22370.53125\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 25586.8418 - rmse: 25554.9355 - val_loss: 43264.0078 - val_rmse: 42836.4961\n",
      "Epoch 20/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 27937.4648 - rmse: 27937.4648\n",
      "Epoch 20: val_loss improved from 22370.53125 to 19161.82227, saving model to ./ckpt/5_class_lr005/val_rmse_19122.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 28364.5820 - rmse: 28364.3242 - val_loss: 19161.8223 - val_rmse: 19121.5371\n",
      "Epoch 21/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 25462.5977 - rmse: 25462.5977\n",
      "Epoch 21: val_loss did not improve from 19161.82227\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 25587.4648 - rmse: 25661.3145 - val_loss: 27134.1895 - val_rmse: 27120.7930\n",
      "Epoch 22/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 25873.3691 - rmse: 25873.3691\n",
      "Epoch 22: val_loss did not improve from 19161.82227\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 25705.9844 - rmse: 25683.7090 - val_loss: 21323.4609 - val_rmse: 21161.3398\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 26151.1289 - rmse: 26151.1289\n",
      "Epoch 23: val_loss did not improve from 19161.82227\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 26118.3770 - rmse: 26099.0078 - val_loss: 20538.0488 - val_rmse: 20537.0820\n",
      "Epoch 24/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23695.5117 - rmse: 23695.5117\n",
      "Epoch 24: val_loss did not improve from 19161.82227\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23606.3965 - rmse: 23624.1191 - val_loss: 19896.9199 - val_rmse: 19834.2793\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23179.1094 - rmse: 23143.4688\n",
      "Epoch 25: val_loss did not improve from 19161.82227\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23179.1094 - rmse: 23143.4688 - val_loss: 20711.7891 - val_rmse: 20590.2871\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23015.3477 - rmse: 23015.3477\n",
      "Epoch 26: val_loss improved from 19161.82227 to 17487.29102, saving model to ./ckpt/5_class_lr005/val_rmse_17508.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23020.8301 - rmse: 23042.1543 - val_loss: 17487.2910 - val_rmse: 17507.5137\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23145.8828 - rmse: 23145.8828\n",
      "Epoch 27: val_loss did not improve from 17487.29102\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22972.6211 - rmse: 22946.9531 - val_loss: 17801.1777 - val_rmse: 17754.2031\n",
      "Epoch 28/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23761.9629 - rmse: 23761.9629\n",
      "Epoch 28: val_loss did not improve from 17487.29102\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23733.7715 - rmse: 23734.7422 - val_loss: 17910.6914 - val_rmse: 17935.1758\n",
      "Epoch 29/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23798.2969 - rmse: 23798.2969\n",
      "Epoch 29: val_loss improved from 17487.29102 to 17272.31836, saving model to ./ckpt/5_class_lr005/val_rmse_17250.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 23671.4062 - rmse: 23626.6016 - val_loss: 17272.3184 - val_rmse: 17249.6035\n",
      "Epoch 30/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21441.9961 - rmse: 21529.4863\n",
      "Epoch 30: val_loss did not improve from 17272.31836\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21441.9961 - rmse: 21529.4863 - val_loss: 28257.2031 - val_rmse: 27848.6094\n",
      "Epoch 31/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 23756.1953 - rmse: 23756.1953\n",
      "Epoch 31: val_loss did not improve from 17272.31836\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23710.3965 - rmse: 23673.0137 - val_loss: 19302.3574 - val_rmse: 19177.1855\n",
      "Epoch 32/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21877.1934 - rmse: 21877.1934\n",
      "Epoch 32: val_loss improved from 17272.31836 to 16902.65039, saving model to ./ckpt/5_class_lr005/val_rmse_16898.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21861.9121 - rmse: 21852.8750 - val_loss: 16902.6504 - val_rmse: 16898.3672\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22601.3105 - rmse: 22720.2520\n",
      "Epoch 33: val_loss did not improve from 16902.65039\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22601.3105 - rmse: 22720.2520 - val_loss: 25147.0801 - val_rmse: 25045.2051\n",
      "Epoch 34/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23342.3184 - rmse: 23342.3184\n",
      "Epoch 34: val_loss did not improve from 16902.65039\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 23323.1328 - rmse: 23311.7852 - val_loss: 17940.1992 - val_rmse: 17867.6289\n",
      "Epoch 35/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21222.9238 - rmse: 21222.9238\n",
      "Epoch 35: val_loss did not improve from 16902.65039\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21159.6309 - rmse: 21161.2461 - val_loss: 27810.4980 - val_rmse: 27687.7051\n",
      "Epoch 36/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22018.4453 - rmse: 22018.4453\n",
      "Epoch 36: val_loss did not improve from 16902.65039\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21942.4199 - rmse: 21941.9492 - val_loss: 17094.1230 - val_rmse: 17130.9277\n",
      "Epoch 37/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21634.1660 - rmse: 21634.1660\n",
      "Epoch 37: val_loss did not improve from 16902.65039\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21529.3145 - rmse: 21524.7578 - val_loss: 20871.5820 - val_rmse: 20892.5605\n",
      "Epoch 38/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21937.1367 - rmse: 21951.3809\n",
      "Epoch 38: val_loss did not improve from 16902.65039\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21937.1367 - rmse: 21951.3809 - val_loss: 29270.9980 - val_rmse: 29044.7695\n",
      "Epoch 39/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 21150.5293 - rmse: 21150.5293\n",
      "Epoch 39: val_loss did not improve from 16902.65039\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21139.7637 - rmse: 21159.8242 - val_loss: 19474.4434 - val_rmse: 19482.8379\n",
      "Epoch 40/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20877.0449 - rmse: 20877.0449\n",
      "Epoch 40: val_loss did not improve from 16902.65039\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20691.0137 - rmse: 20669.7676 - val_loss: 21835.7227 - val_rmse: 21751.7031\n",
      "Epoch 41/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21089.6836 - rmse: 21089.6836\n",
      "Epoch 41: val_loss did not improve from 16902.65039\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21274.1074 - rmse: 21397.4883 - val_loss: 17036.0684 - val_rmse: 17036.1602\n",
      "Epoch 42/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20727.6133 - rmse: 20738.3867\n",
      "Epoch 42: val_loss did not improve from 16902.65039\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20727.6133 - rmse: 20738.3867 - val_loss: 21477.2090 - val_rmse: 21541.5625\n",
      "Epoch 43/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21144.6426 - rmse: 21144.6426\n",
      "Epoch 43: val_loss did not improve from 16902.65039\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21334.4688 - rmse: 21409.0605 - val_loss: 23414.5469 - val_rmse: 23390.0000\n",
      "Epoch 44/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20913.6855 - rmse: 20919.5137\n",
      "Epoch 44: val_loss did not improve from 16902.65039\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20913.6855 - rmse: 20919.5137 - val_loss: 24844.8867 - val_rmse: 24504.9277\n",
      "Epoch 45/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 23231.8516 - rmse: 23231.8516\n",
      "Epoch 45: val_loss improved from 16902.65039 to 16080.60547, saving model to ./ckpt/5_class_lr005/val_rmse_16086.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23154.9180 - rmse: 23198.0156 - val_loss: 16080.6055 - val_rmse: 16086.2041\n",
      "Epoch 46/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20202.9570 - rmse: 20202.9570\n",
      "Epoch 46: val_loss improved from 16080.60547 to 14718.71680, saving model to ./ckpt/5_class_lr005/val_rmse_14719.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20266.7793 - rmse: 20316.2246 - val_loss: 14718.7168 - val_rmse: 14718.6318\n",
      "Epoch 47/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21010.9258 - rmse: 21010.9258\n",
      "Epoch 47: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20937.4414 - rmse: 20998.5527 - val_loss: 19690.7246 - val_rmse: 19512.5801\n",
      "Epoch 48/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21403.6973 - rmse: 21401.5820\n",
      "Epoch 48: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21403.6973 - rmse: 21401.5820 - val_loss: 16231.7354 - val_rmse: 16221.7285\n",
      "Epoch 49/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20450.3672 - rmse: 20450.3672\n",
      "Epoch 49: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 20596.1523 - rmse: 20663.2930 - val_loss: 18267.8984 - val_rmse: 18209.0840\n",
      "Epoch 50/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19657.3711 - rmse: 19657.3711\n",
      "Epoch 50: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19646.4844 - rmse: 19674.4805 - val_loss: 23503.7969 - val_rmse: 23326.1562\n",
      "Epoch 51/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22042.9336 - rmse: 22042.9336\n",
      "Epoch 51: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22074.7207 - rmse: 22076.7012 - val_loss: 25916.8711 - val_rmse: 25747.2051\n",
      "Epoch 52/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19721.0566 - rmse: 19721.0566\n",
      "Epoch 52: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19749.6621 - rmse: 19766.5781 - val_loss: 21026.8418 - val_rmse: 20977.0273\n",
      "Epoch 53/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18983.0000 - rmse: 18983.0000\n",
      "Epoch 53: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19017.3047 - rmse: 19048.7734 - val_loss: 15524.8252 - val_rmse: 15521.5566\n",
      "Epoch 54/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19458.0898 - rmse: 19458.0898\n",
      "Epoch 54: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19475.9863 - rmse: 19486.5723 - val_loss: 18014.4258 - val_rmse: 18065.9824\n",
      "Epoch 55/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18924.4648 - rmse: 18924.4648\n",
      "Epoch 55: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18934.5156 - rmse: 19013.0801 - val_loss: 15045.9473 - val_rmse: 15024.8369\n",
      "Epoch 56/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19474.9863 - rmse: 19474.9863\n",
      "Epoch 56: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 19250.3047 - rmse: 19215.1816 - val_loss: 21408.9316 - val_rmse: 21329.4512\n",
      "Epoch 57/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19458.3574 - rmse: 19458.3574\n",
      "Epoch 57: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19484.7129 - rmse: 19473.1250 - val_loss: 16409.0762 - val_rmse: 16351.8838\n",
      "Epoch 58/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20367.5938 - rmse: 20361.8906\n",
      "Epoch 58: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20367.5938 - rmse: 20361.8906 - val_loss: 15587.3467 - val_rmse: 15534.3467\n",
      "Epoch 59/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18550.3594 - rmse: 18550.3594\n",
      "Epoch 59: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18477.3496 - rmse: 18475.7773 - val_loss: 15437.6221 - val_rmse: 15414.7891\n",
      "Epoch 60/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18527.9316 - rmse: 18527.9316\n",
      "Epoch 60: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18613.3789 - rmse: 18634.1445 - val_loss: 21970.9531 - val_rmse: 21939.5449\n",
      "Epoch 61/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20015.2227 - rmse: 20015.2227\n",
      "Epoch 61: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20181.6309 - rmse: 20188.7891 - val_loss: 30796.3652 - val_rmse: 30593.6758\n",
      "Epoch 62/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20938.0879 - rmse: 20953.2617\n",
      "Epoch 62: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 20938.0879 - rmse: 20953.2617 - val_loss: 19398.5430 - val_rmse: 19372.7598\n",
      "Epoch 63/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19180.3027 - rmse: 19180.3027\n",
      "Epoch 63: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19298.4043 - rmse: 19289.2207 - val_loss: 18233.4297 - val_rmse: 18262.5918\n",
      "Epoch 64/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18349.6523 - rmse: 18349.6523\n",
      "Epoch 64: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18847.6230 - rmse: 18922.6758 - val_loss: 17648.9766 - val_rmse: 17698.8359\n",
      "Epoch 65/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19323.0996 - rmse: 19323.0996\n",
      "Epoch 65: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19848.3555 - rmse: 19909.6152 - val_loss: 22165.8809 - val_rmse: 22203.7422\n",
      "Epoch 66/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 24353.3867 - rmse: 24353.3867\n",
      "Epoch 66: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24235.2988 - rmse: 24193.8457 - val_loss: 21084.2812 - val_rmse: 21094.6035\n",
      "Epoch 67/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21282.1953 - rmse: 21282.1953\n",
      "Epoch 67: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 21291.6445 - rmse: 21305.0332 - val_loss: 19659.7539 - val_rmse: 19642.4355\n",
      "Epoch 68/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18614.8477 - rmse: 18614.8477\n",
      "Epoch 68: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18673.7949 - rmse: 18708.6582 - val_loss: 23184.3906 - val_rmse: 23135.2832\n",
      "Epoch 69/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17221.1426 - rmse: 17221.1426\n",
      "Epoch 69: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17285.2383 - rmse: 17291.2656 - val_loss: 26251.3047 - val_rmse: 26054.5977\n",
      "Epoch 70/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19577.9805 - rmse: 19577.9805\n",
      "Epoch 70: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19695.3203 - rmse: 19703.1738 - val_loss: 31763.5723 - val_rmse: 31549.4766\n",
      "Epoch 71/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20552.0332 - rmse: 20552.0332\n",
      "Epoch 71: val_loss did not improve from 14718.71680\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20642.2656 - rmse: 20636.6719 - val_loss: 22939.9648 - val_rmse: 22920.9082\n",
      "Epoch 71: early stopping\n",
      "Finish 10-fold cross validation\n",
      "Best performing model has 51572.0625 validation loss (RMSE)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# modify to save ckpt for each test\n",
    "ckpt = os.path.join(ckpt_path, \"val_rmse_{val_rmse:.0f}.hdf5\")\n",
    "\n",
    "# training params\n",
    "epochs = epochs\n",
    "lr = lr\n",
    "\n",
    "# the k for k fold CV\n",
    "n_split = 10\n",
    "\n",
    "# for recording best performance\n",
    "min_loss = np.inf\n",
    "best_history = None\n",
    "\n",
    "'''\n",
    "k-fold cross validation\n",
    "Save the best model using validation accuracy as metric\n",
    "Print the global best performace when finished\n",
    "'''\n",
    "for train_index, test_index in KFold(n_split).split(train_examples):\n",
    "\n",
    "    x_train, x_vad = train_examples[train_index], train_examples[test_index]\n",
    "    y_train, y_vad = train_labels[train_index], train_labels[test_index]\n",
    "\n",
    "    model=create_reg_model(lr)\n",
    "  \n",
    "    # callbacks\n",
    "    checkpoint_filepath = ckpt\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='auto',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=25,\n",
    "        verbose=1,\n",
    "        mode='auto',\n",
    "    )\n",
    "\n",
    "    # Model weights are saved at the end of every epoch, if it's the best seen so far.\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_vad, y_vad),\n",
    "                        callbacks=[model_checkpoint_callback, early_stopping_callback])\n",
    "\n",
    "    val_loss = max(history.history['val_loss'])\n",
    "    \n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        best_history = history\n",
    "        # print('Best acc so far. Saving params...\\n')\n",
    "\n",
    "print('Finish {}-fold cross validation'.format(n_split))\n",
    "print('Best performing model has {} validation loss (RMSE)'.format(min_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:16:17.795823: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x169b41df0>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = best_history\n",
    "\n",
    "train_rmse = history.history['rmse']\n",
    "val_rmse = history.history['val_rmse']\n",
    "\n",
    "epochs_range = range(len(history.history['loss']))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_rmse, label='Training RMSE')\n",
    "plt.plot(epochs_range, val_rmse, label='Validation RMSE')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "\n",
    "'''How far are predictions from real values?'''\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def format_tick_labels(x, pos):\n",
    "    return '{:.0f}k'.format(x/1000)\n",
    "\n",
    "predictions = model.predict(test_examples)\n",
    "\n",
    "xlims = [0, max(test_labels)*1.1]\n",
    "ylims = [0, max(predictions)*1.1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(test_labels, predictions)\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "ax.set_xlim(xlims)\n",
    "ax.set_ylim(ylims)\n",
    "ax.set_xlabel('Actual Price')\n",
    "ax.set_ylabel('Predicted Price')\n",
    "\n",
    "ax.plot(xlims, ylims, 'r-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/14 [=>............................] - ETA: 2s - loss: 10692.5234 - rmse: 10692.5234"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 21:17:10.902746: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 16ms/step - loss: 14123.4756 - rmse: 14148.6104\n",
      "\n",
      "evaluation on test set:\n",
      "loss (RMSE) = 14123.47559\n"
     ]
    }
   ],
   "source": [
    "model = create_reg_model()\n",
    "model.load_weights(os.path.join(ckpt_path, \"val_rmse_13256.hdf5\"))\n",
    "\n",
    "loss, acc = model.evaluate(test_examples, test_labels)\n",
    "\n",
    "print('\\nevaluation on test set:\\nloss (RMSE) = {:.5f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29cc21816e506614f017a9125cfdb1f5dc655865e499c52ef5f5406a40d25695"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
