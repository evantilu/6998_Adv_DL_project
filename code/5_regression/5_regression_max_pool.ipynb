{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and env settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.min_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables/parameters used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../../data/home_sale_data_324_features_raw_price.csv'\n",
    "\n",
    "ckpt_path = \"./ckpt/reg_lr005/\"\n",
    "os.makedirs(ckpt_path, exist_ok=True)\n",
    "\n",
    "lr = 0.005\n",
    "epochs = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>TotalSF</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>Total_Bathrooms</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>YrBltAndRemod</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>Foundation_PConc</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>hasfireplace</th>\n",
       "      <th>ExterQual_Gd</th>\n",
       "      <th>BsmtQual_Ex</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>HeatingQC_Ex</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>Total_porch_sf</th>\n",
       "      <th>BsmtFinType1_GLQ</th>\n",
       "      <th>KitchenQual_Ex</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>GarageFinish_Fin</th>\n",
       "      <th>...</th>\n",
       "      <th>BsmtExposure_No</th>\n",
       "      <th>Neighborhood_OldTown</th>\n",
       "      <th>Foundation_BrkTil</th>\n",
       "      <th>GarageFinish_None</th>\n",
       "      <th>GarageCond_None</th>\n",
       "      <th>GarageQual_None</th>\n",
       "      <th>GarageType_None</th>\n",
       "      <th>MSSubClass_30</th>\n",
       "      <th>LotShape_Reg</th>\n",
       "      <th>PavedDrive_N</th>\n",
       "      <th>Foundation_CBlock</th>\n",
       "      <th>MSZoning_RM</th>\n",
       "      <th>HeatingQC_TA</th>\n",
       "      <th>CentralAir_N</th>\n",
       "      <th>GarageType_Detchd</th>\n",
       "      <th>MasVnrType_None</th>\n",
       "      <th>GarageFinish_Unf</th>\n",
       "      <th>BsmtQual_TA</th>\n",
       "      <th>FireplaceQu_None</th>\n",
       "      <th>KitchenQual_TA</th>\n",
       "      <th>ExterQual_TA</th>\n",
       "      <th>dummy_1</th>\n",
       "      <th>dummy_2</th>\n",
       "      <th>dummy_3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.394045</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.187149</td>\n",
       "      <td>0.239834</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37900.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.455198</td>\n",
       "      <td>0.229732</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.163872</td>\n",
       "      <td>0.258064</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.354336</td>\n",
       "      <td>0.501653</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.461628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.503533</td>\n",
       "      <td>0.276200</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.163872</td>\n",
       "      <td>0.188172</td>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.194635</td>\n",
       "      <td>0.281376</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.378065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133284.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.586532</td>\n",
       "      <td>0.319898</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.327743</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.493450</td>\n",
       "      <td>0.640997</td>\n",
       "      <td>0.862319</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155234.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.334376</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.263441</td>\n",
       "      <td>0.694737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.371392</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107500.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OverallQual  GrLivArea   TotalSF  GarageCars  Total_Bathrooms  \\\n",
       "963      0.222222   0.394045  0.208333         0.0         0.001104   \n",
       "669      0.555556   0.455198  0.229732         0.2         0.163872   \n",
       "1811     0.555556   0.503533  0.276200         0.2         0.163872   \n",
       "1912     0.444444   0.586532  0.319898         0.4         0.327743   \n",
       "528      0.444444   0.334376  0.000184         0.2         0.001104   \n",
       "\n",
       "      GarageArea  YrBltAndRemod  TotalBsmtSF  1stFlrSF  YearBuilt  FullBath  \\\n",
       "963     0.000000       0.157895     0.187149  0.239834   0.275362      0.25   \n",
       "669     0.258064       0.526316     0.354336  0.501653   0.673913      0.25   \n",
       "1811    0.188172       0.494737     0.194635  0.281376   0.326087      0.25   \n",
       "1912    0.451613       0.800000     0.493450  0.640997   0.862319      0.50   \n",
       "528     0.263441       0.694737     0.000000  0.371392   0.601449      0.25   \n",
       "\n",
       "      YearRemodAdd  Foundation_PConc  TotRmsAbvGrd  hasfireplace  \\\n",
       "963       0.000000               0.0      0.516936           0.0   \n",
       "669       0.250000               0.0      0.421336           1.0   \n",
       "1811      0.950000               0.0      0.421336           1.0   \n",
       "1912      0.683333               1.0      0.421336           0.0   \n",
       "528       0.950000               0.0      0.421336           1.0   \n",
       "\n",
       "      ExterQual_Gd  BsmtQual_Ex  Fireplaces  HeatingQC_Ex  MasVnrArea  \\\n",
       "963            0.0          0.0    0.000000           0.0         0.0   \n",
       "669            0.0          0.0    0.293793           0.0         0.0   \n",
       "1811           0.0          0.0    0.293793           1.0         0.0   \n",
       "1912           0.0          0.0    0.000000           0.0         0.0   \n",
       "528            0.0          0.0    0.293793           1.0         0.0   \n",
       "\n",
       "      Total_porch_sf  BsmtFinType1_GLQ  KitchenQual_Ex  OpenPorchSF  \\\n",
       "963         0.000000               0.0             0.0     0.000000   \n",
       "669         0.461628               0.0             0.0     0.000000   \n",
       "1811        0.103898               0.0             1.0     0.378065   \n",
       "1912        0.000000               0.0             0.0     0.000000   \n",
       "528         0.000000               0.0             0.0     0.000000   \n",
       "\n",
       "      GarageFinish_Fin  ...  BsmtExposure_No  Neighborhood_OldTown  \\\n",
       "963                0.0  ...              1.0                   1.0   \n",
       "669                0.0  ...              1.0                   0.0   \n",
       "1811               0.0  ...              1.0                   1.0   \n",
       "1912               0.0  ...              1.0                   0.0   \n",
       "528                0.0  ...              0.0                   0.0   \n",
       "\n",
       "      Foundation_BrkTil  GarageFinish_None  GarageCond_None  GarageQual_None  \\\n",
       "963                 1.0                1.0              1.0              1.0   \n",
       "669                 0.0                0.0              0.0              0.0   \n",
       "1811                1.0                0.0              0.0              0.0   \n",
       "1912                0.0                0.0              0.0              0.0   \n",
       "528                 0.0                0.0              0.0              0.0   \n",
       "\n",
       "      GarageType_None  MSSubClass_30  LotShape_Reg  PavedDrive_N  \\\n",
       "963               1.0            0.0           1.0           0.0   \n",
       "669               0.0            0.0           1.0           0.0   \n",
       "1811              0.0            0.0           1.0           1.0   \n",
       "1912              0.0            0.0           1.0           0.0   \n",
       "528               0.0            0.0           1.0           0.0   \n",
       "\n",
       "      Foundation_CBlock  MSZoning_RM  HeatingQC_TA  CentralAir_N  \\\n",
       "963                 0.0          1.0           0.0           1.0   \n",
       "669                 1.0          0.0           1.0           0.0   \n",
       "1811                0.0          1.0           0.0           0.0   \n",
       "1912                0.0          0.0           1.0           0.0   \n",
       "528                 0.0          0.0           0.0           0.0   \n",
       "\n",
       "      GarageType_Detchd  MasVnrType_None  GarageFinish_Unf  BsmtQual_TA  \\\n",
       "963                 0.0              1.0               0.0          1.0   \n",
       "669                 0.0              1.0               0.0          1.0   \n",
       "1811                0.0              1.0               1.0          1.0   \n",
       "1912                1.0              1.0               1.0          0.0   \n",
       "528                 1.0              1.0               1.0          0.0   \n",
       "\n",
       "      FireplaceQu_None  KitchenQual_TA  ExterQual_TA  dummy_1  dummy_2  \\\n",
       "963                1.0             1.0           1.0      0.0      0.0   \n",
       "669                0.0             1.0           1.0      0.0      0.0   \n",
       "1811               0.0             0.0           1.0      0.0      0.0   \n",
       "1912               1.0             1.0           1.0      0.0      0.0   \n",
       "528                0.0             1.0           1.0      0.0      0.0   \n",
       "\n",
       "      dummy_3      label  \n",
       "963       0.0   37900.00  \n",
       "669       0.0  140000.00  \n",
       "1811      0.0  133284.77  \n",
       "1912      0.0  155234.50  \n",
       "528       0.0  107500.00  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(file)\n",
    "\n",
    "'''suffle rows randomly'''\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "labels = data['label']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>TotalSF</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>Total_Bathrooms</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>YrBltAndRemod</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>Foundation_PConc</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>hasfireplace</th>\n",
       "      <th>ExterQual_Gd</th>\n",
       "      <th>BsmtQual_Ex</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>HeatingQC_Ex</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>Total_porch_sf</th>\n",
       "      <th>BsmtFinType1_GLQ</th>\n",
       "      <th>KitchenQual_Ex</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>GarageFinish_Fin</th>\n",
       "      <th>...</th>\n",
       "      <th>Neighborhood_IDOTRR</th>\n",
       "      <th>BsmtExposure_No</th>\n",
       "      <th>Neighborhood_OldTown</th>\n",
       "      <th>Foundation_BrkTil</th>\n",
       "      <th>GarageFinish_None</th>\n",
       "      <th>GarageCond_None</th>\n",
       "      <th>GarageQual_None</th>\n",
       "      <th>GarageType_None</th>\n",
       "      <th>MSSubClass_30</th>\n",
       "      <th>LotShape_Reg</th>\n",
       "      <th>PavedDrive_N</th>\n",
       "      <th>Foundation_CBlock</th>\n",
       "      <th>MSZoning_RM</th>\n",
       "      <th>HeatingQC_TA</th>\n",
       "      <th>CentralAir_N</th>\n",
       "      <th>GarageType_Detchd</th>\n",
       "      <th>MasVnrType_None</th>\n",
       "      <th>GarageFinish_Unf</th>\n",
       "      <th>BsmtQual_TA</th>\n",
       "      <th>FireplaceQu_None</th>\n",
       "      <th>KitchenQual_TA</th>\n",
       "      <th>ExterQual_TA</th>\n",
       "      <th>dummy_1</th>\n",
       "      <th>dummy_2</th>\n",
       "      <th>dummy_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>2911.0</td>\n",
       "      <td>2911.0</td>\n",
       "      <td>2911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.565136</td>\n",
       "      <td>0.542763</td>\n",
       "      <td>0.293431</td>\n",
       "      <td>0.353143</td>\n",
       "      <td>0.200863</td>\n",
       "      <td>0.317285</td>\n",
       "      <td>0.660811</td>\n",
       "      <td>0.326706</td>\n",
       "      <td>0.487996</td>\n",
       "      <td>0.719569</td>\n",
       "      <td>0.391876</td>\n",
       "      <td>0.570892</td>\n",
       "      <td>0.447956</td>\n",
       "      <td>0.542786</td>\n",
       "      <td>0.512882</td>\n",
       "      <td>0.335967</td>\n",
       "      <td>0.087255</td>\n",
       "      <td>0.171718</td>\n",
       "      <td>0.511508</td>\n",
       "      <td>0.159954</td>\n",
       "      <td>0.203170</td>\n",
       "      <td>0.290622</td>\n",
       "      <td>0.069392</td>\n",
       "      <td>0.176542</td>\n",
       "      <td>0.245620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031261</td>\n",
       "      <td>0.652697</td>\n",
       "      <td>0.082102</td>\n",
       "      <td>0.106493</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.053590</td>\n",
       "      <td>0.047750</td>\n",
       "      <td>0.637582</td>\n",
       "      <td>0.073514</td>\n",
       "      <td>0.423222</td>\n",
       "      <td>0.158708</td>\n",
       "      <td>0.293370</td>\n",
       "      <td>0.066644</td>\n",
       "      <td>0.266919</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.421848</td>\n",
       "      <td>0.439368</td>\n",
       "      <td>0.487118</td>\n",
       "      <td>0.511852</td>\n",
       "      <td>0.616627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.155988</td>\n",
       "      <td>0.125193</td>\n",
       "      <td>0.119772</td>\n",
       "      <td>0.152244</td>\n",
       "      <td>0.133043</td>\n",
       "      <td>0.143976</td>\n",
       "      <td>0.242686</td>\n",
       "      <td>0.131930</td>\n",
       "      <td>0.131369</td>\n",
       "      <td>0.219351</td>\n",
       "      <td>0.138140</td>\n",
       "      <td>0.348189</td>\n",
       "      <td>0.497369</td>\n",
       "      <td>0.128554</td>\n",
       "      <td>0.499920</td>\n",
       "      <td>0.472409</td>\n",
       "      <td>0.282257</td>\n",
       "      <td>0.181727</td>\n",
       "      <td>0.499953</td>\n",
       "      <td>0.218350</td>\n",
       "      <td>0.163524</td>\n",
       "      <td>0.454127</td>\n",
       "      <td>0.254163</td>\n",
       "      <td>0.183129</td>\n",
       "      <td>0.430528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174051</td>\n",
       "      <td>0.476195</td>\n",
       "      <td>0.274568</td>\n",
       "      <td>0.308520</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.225245</td>\n",
       "      <td>0.213273</td>\n",
       "      <td>0.480781</td>\n",
       "      <td>0.261024</td>\n",
       "      <td>0.494155</td>\n",
       "      <td>0.365467</td>\n",
       "      <td>0.455385</td>\n",
       "      <td>0.249447</td>\n",
       "      <td>0.442425</td>\n",
       "      <td>0.488798</td>\n",
       "      <td>0.493939</td>\n",
       "      <td>0.496395</td>\n",
       "      <td>0.499920</td>\n",
       "      <td>0.499945</td>\n",
       "      <td>0.486292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.451636</td>\n",
       "      <td>0.213557</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.088659</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.247193</td>\n",
       "      <td>0.395003</td>\n",
       "      <td>0.590580</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.547807</td>\n",
       "      <td>0.282326</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.164976</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.652632</td>\n",
       "      <td>0.308172</td>\n",
       "      <td>0.481664</td>\n",
       "      <td>0.731884</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516936</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.621247</td>\n",
       "      <td>0.356447</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.252530</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.405490</td>\n",
       "      <td>0.582574</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.353218</td>\n",
       "      <td>0.327089</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 324 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       OverallQual    GrLivArea      TotalSF   GarageCars  Total_Bathrooms  \\\n",
       "count  2911.000000  2911.000000  2911.000000  2911.000000      2911.000000   \n",
       "mean      0.565136     0.542763     0.293431     0.353143         0.200863   \n",
       "std       0.155988     0.125193     0.119772     0.152244         0.133043   \n",
       "min       0.000000     0.000000     0.000000     0.000000         0.000000   \n",
       "25%       0.444444     0.451636     0.213557     0.200000         0.088659   \n",
       "50%       0.555556     0.547807     0.282326     0.400000         0.164976   \n",
       "75%       0.666667     0.621247     0.356447     0.400000         0.252530   \n",
       "max       1.000000     1.000000     1.000000     1.000000         1.000000   \n",
       "\n",
       "        GarageArea  YrBltAndRemod  TotalBsmtSF     1stFlrSF    YearBuilt  \\\n",
       "count  2911.000000    2911.000000  2911.000000  2911.000000  2911.000000   \n",
       "mean      0.317285       0.660811     0.326706     0.487996     0.719569   \n",
       "std       0.143976       0.242686     0.131930     0.131369     0.219351   \n",
       "min       0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.215054       0.473684     0.247193     0.395003     0.590580   \n",
       "50%       0.322581       0.652632     0.308172     0.481664     0.731884   \n",
       "75%       0.387097       0.905263     0.405490     0.582574     0.934783   \n",
       "max       1.000000       1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          FullBath  YearRemodAdd  Foundation_PConc  TotRmsAbvGrd  \\\n",
       "count  2911.000000   2911.000000       2911.000000   2911.000000   \n",
       "mean      0.391876      0.570892          0.447956      0.542786   \n",
       "std       0.138140      0.348189          0.497369      0.128554   \n",
       "min       0.000000      0.000000          0.000000      0.000000   \n",
       "25%       0.250000      0.250000          0.000000      0.421336   \n",
       "50%       0.500000      0.716667          0.000000      0.516936   \n",
       "75%       0.500000      0.900000          1.000000      0.600315   \n",
       "max       1.000000      1.000000          1.000000      1.000000   \n",
       "\n",
       "       hasfireplace  ExterQual_Gd  BsmtQual_Ex   Fireplaces  HeatingQC_Ex  \\\n",
       "count   2911.000000   2911.000000  2911.000000  2911.000000   2911.000000   \n",
       "mean       0.512882      0.335967     0.087255     0.171718      0.511508   \n",
       "std        0.499920      0.472409     0.282257     0.181727      0.499953   \n",
       "min        0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "25%        0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "50%        1.000000      0.000000     0.000000     0.293793      1.000000   \n",
       "75%        1.000000      1.000000     0.000000     0.293793      1.000000   \n",
       "max        1.000000      1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "        MasVnrArea  Total_porch_sf  BsmtFinType1_GLQ  KitchenQual_Ex  \\\n",
       "count  2911.000000     2911.000000       2911.000000     2911.000000   \n",
       "mean      0.159954        0.203170          0.290622        0.069392   \n",
       "std       0.218350        0.163524          0.454127        0.254163   \n",
       "min       0.000000        0.000000          0.000000        0.000000   \n",
       "25%       0.000000        0.066964          0.000000        0.000000   \n",
       "50%       0.000000        0.179849          0.000000        0.000000   \n",
       "75%       0.353218        0.327089          1.000000        0.000000   \n",
       "max       1.000000        1.000000          1.000000        1.000000   \n",
       "\n",
       "       OpenPorchSF  GarageFinish_Fin  ...  Neighborhood_IDOTRR  \\\n",
       "count  2911.000000       2911.000000  ...          2911.000000   \n",
       "mean      0.176542          0.245620  ...             0.031261   \n",
       "std       0.183129          0.430528  ...             0.174051   \n",
       "min       0.000000          0.000000  ...             0.000000   \n",
       "25%       0.000000          0.000000  ...             0.000000   \n",
       "50%       0.180865          0.000000  ...             0.000000   \n",
       "75%       0.309510          0.000000  ...             0.000000   \n",
       "max       1.000000          1.000000  ...             1.000000   \n",
       "\n",
       "       BsmtExposure_No  Neighborhood_OldTown  Foundation_BrkTil  \\\n",
       "count      2911.000000           2911.000000        2911.000000   \n",
       "mean          0.652697              0.082102           0.106493   \n",
       "std           0.476195              0.274568           0.308520   \n",
       "min           0.000000              0.000000           0.000000   \n",
       "25%           0.000000              0.000000           0.000000   \n",
       "50%           1.000000              0.000000           0.000000   \n",
       "75%           1.000000              0.000000           0.000000   \n",
       "max           1.000000              1.000000           1.000000   \n",
       "\n",
       "       GarageFinish_None  GarageCond_None  GarageQual_None  GarageType_None  \\\n",
       "count        2911.000000      2911.000000      2911.000000      2911.000000   \n",
       "mean            0.054277         0.054277         0.054277         0.053590   \n",
       "std             0.226602         0.226602         0.226602         0.225245   \n",
       "min             0.000000         0.000000         0.000000         0.000000   \n",
       "25%             0.000000         0.000000         0.000000         0.000000   \n",
       "50%             0.000000         0.000000         0.000000         0.000000   \n",
       "75%             0.000000         0.000000         0.000000         0.000000   \n",
       "max             1.000000         1.000000         1.000000         1.000000   \n",
       "\n",
       "       MSSubClass_30  LotShape_Reg  PavedDrive_N  Foundation_CBlock  \\\n",
       "count    2911.000000   2911.000000   2911.000000        2911.000000   \n",
       "mean        0.047750      0.637582      0.073514           0.423222   \n",
       "std         0.213273      0.480781      0.261024           0.494155   \n",
       "min         0.000000      0.000000      0.000000           0.000000   \n",
       "25%         0.000000      0.000000      0.000000           0.000000   \n",
       "50%         0.000000      1.000000      0.000000           0.000000   \n",
       "75%         0.000000      1.000000      0.000000           1.000000   \n",
       "max         1.000000      1.000000      1.000000           1.000000   \n",
       "\n",
       "       MSZoning_RM  HeatingQC_TA  CentralAir_N  GarageType_Detchd  \\\n",
       "count  2911.000000   2911.000000   2911.000000        2911.000000   \n",
       "mean      0.158708      0.293370      0.066644           0.266919   \n",
       "std       0.365467      0.455385      0.249447           0.442425   \n",
       "min       0.000000      0.000000      0.000000           0.000000   \n",
       "25%       0.000000      0.000000      0.000000           0.000000   \n",
       "50%       0.000000      0.000000      0.000000           0.000000   \n",
       "75%       0.000000      1.000000      0.000000           1.000000   \n",
       "max       1.000000      1.000000      1.000000           1.000000   \n",
       "\n",
       "       MasVnrType_None  GarageFinish_Unf  BsmtQual_TA  FireplaceQu_None  \\\n",
       "count      2911.000000       2911.000000  2911.000000       2911.000000   \n",
       "mean          0.605634          0.421848     0.439368          0.487118   \n",
       "std           0.488798          0.493939     0.496395          0.499920   \n",
       "min           0.000000          0.000000     0.000000          0.000000   \n",
       "25%           0.000000          0.000000     0.000000          0.000000   \n",
       "50%           1.000000          0.000000     0.000000          0.000000   \n",
       "75%           1.000000          1.000000     1.000000          1.000000   \n",
       "max           1.000000          1.000000     1.000000          1.000000   \n",
       "\n",
       "       KitchenQual_TA  ExterQual_TA  dummy_1  dummy_2  dummy_3  \n",
       "count     2911.000000   2911.000000   2911.0   2911.0   2911.0  \n",
       "mean         0.511852      0.616627      0.0      0.0      0.0  \n",
       "std          0.499945      0.486292      0.0      0.0      0.0  \n",
       "min          0.000000      0.000000      0.0      0.0      0.0  \n",
       "25%          0.000000      0.000000      0.0      0.0      0.0  \n",
       "50%          1.000000      1.000000      0.0      0.0      0.0  \n",
       "75%          1.000000      1.000000      0.0      0.0      0.0  \n",
       "max          1.000000      1.000000      0.0      0.0      0.0  \n",
       "\n",
       "[8 rows x 324 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop label column\n",
    "data.drop(['label'], axis=1, inplace=True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      2911.000000\n",
       "mean     180161.314822\n",
       "std       78538.134736\n",
       "min       34900.000000\n",
       "25%      129082.035000\n",
       "50%      160200.000000\n",
       "75%      212000.000000\n",
       "max      755000.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 18, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_np = data.to_numpy()\n",
    "data_np.shape\n",
    "data_np = data_np.reshape(len(data), 18, 18)\n",
    "data_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data_np[0].shape\n",
    "\n",
    "nArray = np.array(data_np[99])\n",
    "\n",
    "\n",
    "a11=nArray.reshape(18,18)\n",
    "plt.imshow(a11)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''because we don't use one-hot encoding in reggresion, need to change labels from pandas series to np array'''\n",
    "labels = labels.to_numpy()\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 18, 18)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_examples = data_np\n",
    "all_examples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train test splitting\n",
    "- hold out 15% for testing\n",
    "- use 85% to train model with K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_samples = all_examples.shape[0] \n",
    "test_ratio = 0.15\n",
    "test_samples = int(test_ratio * all_examples.shape[0])\n",
    "\n",
    "train_examples = all_examples[:-1*test_samples]\n",
    "test_examples = all_examples[-1*test_samples:]\n",
    "train_labels = labels[:-1*test_samples]\n",
    "test_labels = labels[-1*test_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (2475, 18, 18)\n",
      "test:  (436, 18, 18)\n",
      "train label:  (2475,)\n",
      "test label:  (436,)\n"
     ]
    }
   ],
   "source": [
    "print('train: ', train_examples.shape)\n",
    "print('test: ', test_examples.shape)\n",
    "print('train label: ', train_labels.shape)\n",
    "print('test label: ', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def create_reg_model(lr=0.005):\n",
    "\n",
    "\t# Working\n",
    "\twith tf.device('/cpu:0'):\n",
    "\t\tdata_augmentation = tf.keras.Sequential([ \n",
    "\t\t\ttf.keras.layers.RandomFlip(\"horizontal\", input_shape=(18, 18, 1)),\n",
    "\t  \t\ttf.keras.layers.RandomRotation(0.1),\n",
    "\t\t    tf.keras.layers.RandomZoom(0.1)\n",
    "\t\t\t])\n",
    "\n",
    "\n",
    "\tmodel = tf.keras.Sequential([\n",
    "\t\t# data_augmentation,\n",
    "\t  \t# tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(18, 18, 1)),\n",
    "\t\ttf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.MaxPooling2D((2,2)),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t  \ttf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "\t  \ttf.keras.layers.MaxPooling2D(),\n",
    "\t\ttf.keras.layers.Dropout(0.1),\n",
    "\t\ttf.keras.layers.Flatten(),\n",
    "\t\ttf.keras.layers.Dense(128, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(64, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(32, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "\t])\n",
    "\n",
    "\t# opt = tf.keras.optimizers.SGD(lr=0.005, momentum=0.9)\n",
    "\topt = tf.keras.optimizers.Adam(lr=lr)\n",
    "\tmodel.compile(optimizer=opt, loss=rmse, metrics=[rmse])\n",
    "\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trainging & visualizing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_reg_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(train_examples, train_labels, epochs=epochs, validation_data=(test_examples, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_rmse = history.history['rmse']\n",
    "# val_rmse = history.history['val_rmse']\n",
    "\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# epochs_range = range(epochs)\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(epochs_range, train_rmse, label='Training RMSE')\n",
    "# plt.plot(epochs_range, val_rmse, label='Validation RMSE')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "\n",
    "\n",
    "# '''How far are predictions from real values?'''\n",
    "# from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# def format_tick_labels(x, pos):\n",
    "#     return '{:.0f}k'.format(x/1000)\n",
    "\n",
    "# predictions = model.predict(test_examples)\n",
    "\n",
    "# xlims = [0, max(test_labels)*1.1]\n",
    "# ylims = [0, max(predictions)*1.1]\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.set_aspect('equal')\n",
    "# ax.scatter(test_labels, predictions)\n",
    "# ax.xaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "# ax.yaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "# ax.set_xlim(xlims)\n",
    "# ax.set_ylim(ylims)\n",
    "# ax.set_xlabel('Actual Price')\n",
    "# ax.set_ylabel('Predicted Price')\n",
    "\n",
    "# ax.plot(xlims, ylims, 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold CV Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:42:03.871773: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-30 13:42:03.871877: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/Users/evantilu/miniforge3/envs/6998_DL_tf/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:42:04.101359: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-04-30 13:42:04.425780: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 109945.1172 - rmse: 109841.2969"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:42:05.700307: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 58174.08594, saving model to ./ckpt/reg_lr005/val_rmse_57952.hdf5\n",
      "70/70 [==============================] - 2s 21ms/step - loss: 109945.1172 - rmse: 109841.2969 - val_loss: 58174.0859 - val_rmse: 57951.6250\n",
      "Epoch 2/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 57281.0938 - rmse: 57281.0938\n",
      "Epoch 2: val_loss improved from 58174.08594 to 42655.10547, saving model to ./ckpt/reg_lr005/val_rmse_42552.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 57147.3828 - rmse: 57057.1953 - val_loss: 42655.1055 - val_rmse: 42551.6367\n",
      "Epoch 3/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 49642.2344 - rmse: 49642.2344\n",
      "Epoch 3: val_loss improved from 42655.10547 to 39756.19922, saving model to ./ckpt/reg_lr005/val_rmse_39711.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 49414.0586 - rmse: 49350.2383 - val_loss: 39756.1992 - val_rmse: 39710.7969\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 45232.8125 - rmse: 45221.8438\n",
      "Epoch 4: val_loss improved from 39756.19922 to 36647.24219, saving model to ./ckpt/reg_lr005/val_rmse_36701.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 45232.8125 - rmse: 45221.8438 - val_loss: 36647.2422 - val_rmse: 36701.2422\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 43599.7500 - rmse: 43552.7578\n",
      "Epoch 5: val_loss did not improve from 36647.24219\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 43599.7500 - rmse: 43552.7578 - val_loss: 44164.4258 - val_rmse: 44061.0703\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 42642.0234 - rmse: 42642.0234\n",
      "Epoch 6: val_loss did not improve from 36647.24219\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 42676.3711 - rmse: 42699.5312 - val_loss: 40402.0859 - val_rmse: 40512.1250\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 43011.1328 - rmse: 43011.1328\n",
      "Epoch 7: val_loss improved from 36647.24219 to 34953.91406, saving model to ./ckpt/reg_lr005/val_rmse_35032.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 42940.8867 - rmse: 42939.9805 - val_loss: 34953.9141 - val_rmse: 35032.2656\n",
      "Epoch 8/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 39130.1133 - rmse: 39130.1133\n",
      "Epoch 8: val_loss improved from 34953.91406 to 34674.72266, saving model to ./ckpt/reg_lr005/val_rmse_34738.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 39510.1680 - rmse: 39611.0664 - val_loss: 34674.7227 - val_rmse: 34738.3281\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 38346.0547 - rmse: 38293.7305\n",
      "Epoch 9: val_loss improved from 34674.72266 to 29886.01953, saving model to ./ckpt/reg_lr005/val_rmse_29877.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 38346.0547 - rmse: 38293.7305 - val_loss: 29886.0195 - val_rmse: 29877.3184\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 36041.6484 - rmse: 35996.4453\n",
      "Epoch 10: val_loss improved from 29886.01953 to 29851.08789, saving model to ./ckpt/reg_lr005/val_rmse_29832.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 36041.6484 - rmse: 35996.4453 - val_loss: 29851.0879 - val_rmse: 29831.8164\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 35314.9219 - rmse: 35314.9219\n",
      "Epoch 11: val_loss did not improve from 29851.08789\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 35454.2383 - rmse: 35558.6094 - val_loss: 30574.7617 - val_rmse: 30474.8379\n",
      "Epoch 12/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 34257.9883 - rmse: 34257.9883\n",
      "Epoch 12: val_loss improved from 29851.08789 to 26950.39648, saving model to ./ckpt/reg_lr005/val_rmse_26868.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 33818.3672 - rmse: 33851.1797 - val_loss: 26950.3965 - val_rmse: 26867.9395\n",
      "Epoch 13/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 31829.4219 - rmse: 31829.4219\n",
      "Epoch 13: val_loss did not improve from 26950.39648\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 31913.0938 - rmse: 31895.7422 - val_loss: 30381.9746 - val_rmse: 30254.3594\n",
      "Epoch 14/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 31577.1309 - rmse: 31577.1309\n",
      "Epoch 14: val_loss did not improve from 26950.39648\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 31589.8164 - rmse: 31659.8789 - val_loss: 33675.1250 - val_rmse: 33560.8945\n",
      "Epoch 15/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 32742.3730 - rmse: 32742.3730\n",
      "Epoch 15: val_loss did not improve from 26950.39648\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 32794.7969 - rmse: 32813.2500 - val_loss: 28348.7344 - val_rmse: 28212.7637\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 29579.5195 - rmse: 29579.5195\n",
      "Epoch 16: val_loss improved from 26950.39648 to 25274.10742, saving model to ./ckpt/reg_lr005/val_rmse_25130.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 29738.1484 - rmse: 29668.4961 - val_loss: 25274.1074 - val_rmse: 25130.3242\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 28574.4844 - rmse: 28574.4844\n",
      "Epoch 17: val_loss did not improve from 25274.10742\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 28155.7754 - rmse: 28135.8340 - val_loss: 35452.1953 - val_rmse: 35342.4688\n",
      "Epoch 18/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 28728.4668 - rmse: 28728.4668\n",
      "Epoch 18: val_loss improved from 25274.10742 to 22128.56641, saving model to ./ckpt/reg_lr005/val_rmse_21969.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 28438.9082 - rmse: 28405.1152 - val_loss: 22128.5664 - val_rmse: 21968.6406\n",
      "Epoch 19/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 28040.1680 - rmse: 28040.1680\n",
      "Epoch 19: val_loss did not improve from 22128.56641\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 27943.6504 - rmse: 27943.2402 - val_loss: 25710.0781 - val_rmse: 25582.9141\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 27183.4570 - rmse: 27183.4570\n",
      "Epoch 20: val_loss improved from 22128.56641 to 20926.00000, saving model to ./ckpt/reg_lr005/val_rmse_20758.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 27161.5996 - rmse: 27146.8574 - val_loss: 20926.0000 - val_rmse: 20758.0645\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 27248.0137 - rmse: 27248.0137\n",
      "Epoch 21: val_loss did not improve from 20926.00000\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 27001.4824 - rmse: 26999.5352 - val_loss: 38798.8828 - val_rmse: 38681.2148\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27914.7734 - rmse: 27848.7461\n",
      "Epoch 22: val_loss improved from 20926.00000 to 20181.94336, saving model to ./ckpt/reg_lr005/val_rmse_20038.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 27914.7734 - rmse: 27848.7461 - val_loss: 20181.9434 - val_rmse: 20038.2344\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 25549.3359 - rmse: 25549.3359\n",
      "Epoch 23: val_loss did not improve from 20181.94336\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 25510.3242 - rmse: 25484.0137 - val_loss: 21432.2148 - val_rmse: 21323.0566\n",
      "Epoch 24/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 25830.3809 - rmse: 25830.3809\n",
      "Epoch 24: val_loss did not improve from 20181.94336\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 25553.6641 - rmse: 25498.5293 - val_loss: 23280.6855 - val_rmse: 23146.9688\n",
      "Epoch 25/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 23532.5156 - rmse: 23532.5156\n",
      "Epoch 25: val_loss improved from 20181.94336 to 18591.54492, saving model to ./ckpt/reg_lr005/val_rmse_18435.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23565.5410 - rmse: 23616.4785 - val_loss: 18591.5449 - val_rmse: 18434.7168\n",
      "Epoch 26/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22229.4277 - rmse: 22229.4277\n",
      "Epoch 26: val_loss did not improve from 18591.54492\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22110.7812 - rmse: 22109.9922 - val_loss: 23061.0117 - val_rmse: 22936.2461\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22820.5898 - rmse: 22820.5898\n",
      "Epoch 27: val_loss improved from 18591.54492 to 18423.67773, saving model to ./ckpt/reg_lr005/val_rmse_18278.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22922.9727 - rmse: 22894.1152 - val_loss: 18423.6777 - val_rmse: 18278.0430\n",
      "Epoch 28/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 24178.5078 - rmse: 24178.5078\n",
      "Epoch 28: val_loss did not improve from 18423.67773\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 24080.9609 - rmse: 24085.9434 - val_loss: 20459.7324 - val_rmse: 20360.8926\n",
      "Epoch 29/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22129.8262 - rmse: 22129.8262\n",
      "Epoch 29: val_loss improved from 18423.67773 to 18271.70117, saving model to ./ckpt/reg_lr005/val_rmse_18123.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22000.7207 - rmse: 21999.4082 - val_loss: 18271.7012 - val_rmse: 18123.1523\n",
      "Epoch 30/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23340.1797 - rmse: 23385.4121\n",
      "Epoch 30: val_loss improved from 18271.70117 to 17907.96680, saving model to ./ckpt/reg_lr005/val_rmse_17767.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23340.1797 - rmse: 23385.4121 - val_loss: 17907.9668 - val_rmse: 17766.6738\n",
      "Epoch 31/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22567.5410 - rmse: 22567.5410\n",
      "Epoch 31: val_loss did not improve from 17907.96680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22524.8711 - rmse: 22521.0098 - val_loss: 28641.5977 - val_rmse: 28500.4844\n",
      "Epoch 32/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 24204.3145 - rmse: 24204.3145\n",
      "Epoch 32: val_loss did not improve from 17907.96680\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 24200.1367 - rmse: 24197.3184 - val_loss: 19354.7383 - val_rmse: 19210.3320\n",
      "Epoch 33/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21217.4863 - rmse: 21217.4863\n",
      "Epoch 33: val_loss did not improve from 17907.96680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21405.2070 - rmse: 21389.7129 - val_loss: 24066.5625 - val_rmse: 23942.0684\n",
      "Epoch 34/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20485.4238 - rmse: 20485.4238\n",
      "Epoch 34: val_loss did not improve from 17907.96680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20497.1367 - rmse: 20498.0703 - val_loss: 18951.6309 - val_rmse: 18841.3086\n",
      "Epoch 35/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 19821.9746 - rmse: 19821.9746\n",
      "Epoch 35: val_loss did not improve from 17907.96680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19880.5020 - rmse: 19875.4785 - val_loss: 26474.4160 - val_rmse: 26355.8730\n",
      "Epoch 36/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21189.2305 - rmse: 21189.2305\n",
      "Epoch 36: val_loss did not improve from 17907.96680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21102.5156 - rmse: 21116.5703 - val_loss: 19743.1758 - val_rmse: 19639.8828\n",
      "Epoch 37/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21761.2754 - rmse: 21761.2754\n",
      "Epoch 37: val_loss did not improve from 17907.96680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21826.2520 - rmse: 21897.1992 - val_loss: 20548.5586 - val_rmse: 20430.1797\n",
      "Epoch 38/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20415.9746 - rmse: 20433.4062\n",
      "Epoch 38: val_loss did not improve from 17907.96680\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20415.9746 - rmse: 20433.4062 - val_loss: 24192.6074 - val_rmse: 24109.6504\n",
      "Epoch 39/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21895.7402 - rmse: 21895.7402\n",
      "Epoch 39: val_loss did not improve from 17907.96680\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 21888.8516 - rmse: 22078.1582 - val_loss: 20417.1016 - val_rmse: 20294.9023\n",
      "Epoch 40/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20558.2832 - rmse: 20558.2832\n",
      "Epoch 40: val_loss did not improve from 17907.96680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20694.3008 - rmse: 20737.8945 - val_loss: 25454.6055 - val_rmse: 25350.0645\n",
      "Epoch 41/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19250.5020 - rmse: 19250.5020\n",
      "Epoch 41: val_loss did not improve from 17907.96680\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19432.0352 - rmse: 19462.8027 - val_loss: 18268.3906 - val_rmse: 18124.5273\n",
      "Epoch 42/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21785.7617 - rmse: 21785.7617\n",
      "Epoch 42: val_loss improved from 17907.96680 to 17576.73242, saving model to ./ckpt/reg_lr005/val_rmse_17465.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21631.2324 - rmse: 21609.9141 - val_loss: 17576.7324 - val_rmse: 17464.7148\n",
      "Epoch 43/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20772.4824 - rmse: 20772.4824\n",
      "Epoch 43: val_loss did not improve from 17576.73242\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20692.4355 - rmse: 20702.3301 - val_loss: 17981.4258 - val_rmse: 17863.1855\n",
      "Epoch 44/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18600.9961 - rmse: 18600.9961\n",
      "Epoch 44: val_loss did not improve from 17576.73242\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 18866.0605 - rmse: 18914.3477 - val_loss: 17955.7461 - val_rmse: 17859.8809\n",
      "Epoch 45/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21124.4922 - rmse: 21124.4922\n",
      "Epoch 45: val_loss did not improve from 17576.73242\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21102.0352 - rmse: 21049.8066 - val_loss: 17639.8809 - val_rmse: 17555.7715\n",
      "Epoch 46/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20053.1953 - rmse: 20053.1953\n",
      "Epoch 46: val_loss improved from 17576.73242 to 17439.46289, saving model to ./ckpt/reg_lr005/val_rmse_17328.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19924.1191 - rmse: 19887.7461 - val_loss: 17439.4629 - val_rmse: 17327.7344\n",
      "Epoch 47/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18327.1172 - rmse: 18327.1172\n",
      "Epoch 47: val_loss improved from 17439.46289 to 17120.07812, saving model to ./ckpt/reg_lr005/val_rmse_17008.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18290.1992 - rmse: 18327.5996 - val_loss: 17120.0781 - val_rmse: 17008.2793\n",
      "Epoch 48/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19569.9160 - rmse: 19569.9160\n",
      "Epoch 48: val_loss did not improve from 17120.07812\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19595.3027 - rmse: 19592.6699 - val_loss: 22554.4629 - val_rmse: 22505.3809\n",
      "Epoch 49/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18529.4590 - rmse: 18529.4590\n",
      "Epoch 49: val_loss did not improve from 17120.07812\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 18654.5820 - rmse: 18649.8711 - val_loss: 18349.4375 - val_rmse: 18241.6641\n",
      "Epoch 50/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19274.6367 - rmse: 19274.6367\n",
      "Epoch 50: val_loss improved from 17120.07812 to 17076.98633, saving model to ./ckpt/reg_lr005/val_rmse_16970.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19295.3477 - rmse: 19337.3418 - val_loss: 17076.9863 - val_rmse: 16970.4863\n",
      "Epoch 51/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19222.6641 - rmse: 19222.6641\n",
      "Epoch 51: val_loss did not improve from 17076.98633\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19226.1543 - rmse: 19263.2559 - val_loss: 18676.4922 - val_rmse: 18555.5117\n",
      "Epoch 52/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18411.3203 - rmse: 18411.3203\n",
      "Epoch 52: val_loss did not improve from 17076.98633\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18448.9395 - rmse: 18474.3125 - val_loss: 17971.2949 - val_rmse: 17869.4297\n",
      "Epoch 53/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19644.0801 - rmse: 19644.0801\n",
      "Epoch 53: val_loss did not improve from 17076.98633\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19642.0449 - rmse: 19640.6719 - val_loss: 27512.0059 - val_rmse: 27416.4062\n",
      "Epoch 54/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18599.2617 - rmse: 18599.2617\n",
      "Epoch 54: val_loss improved from 17076.98633 to 17038.16602, saving model to ./ckpt/reg_lr005/val_rmse_16934.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18665.7676 - rmse: 18706.8262 - val_loss: 17038.1660 - val_rmse: 16933.5098\n",
      "Epoch 55/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18484.5312 - rmse: 18492.5840\n",
      "Epoch 55: val_loss did not improve from 17038.16602\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18484.5312 - rmse: 18492.5840 - val_loss: 18347.5586 - val_rmse: 18264.8125\n",
      "Epoch 56/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18005.7012 - rmse: 18005.7012\n",
      "Epoch 56: val_loss did not improve from 17038.16602\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 17996.2129 - rmse: 18019.1875 - val_loss: 17378.1504 - val_rmse: 17286.8281\n",
      "Epoch 57/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17612.5879 - rmse: 17612.5879\n",
      "Epoch 57: val_loss did not improve from 17038.16602\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17523.6016 - rmse: 17523.8398 - val_loss: 25886.7812 - val_rmse: 25772.6172\n",
      "Epoch 58/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17824.5762 - rmse: 17824.5762\n",
      "Epoch 58: val_loss did not improve from 17038.16602\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17820.6211 - rmse: 17817.9531 - val_loss: 17360.0000 - val_rmse: 17249.0762\n",
      "Epoch 59/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20259.0195 - rmse: 20259.0195\n",
      "Epoch 59: val_loss improved from 17038.16602 to 16771.72656, saving model to ./ckpt/reg_lr005/val_rmse_16704.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20223.8398 - rmse: 20198.7402 - val_loss: 16771.7266 - val_rmse: 16703.8242\n",
      "Epoch 60/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18155.5332 - rmse: 18155.5332\n",
      "Epoch 60: val_loss did not improve from 16771.72656\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18337.9180 - rmse: 18471.6992 - val_loss: 17256.8477 - val_rmse: 17184.8965\n",
      "Epoch 61/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19176.4941 - rmse: 19176.4941\n",
      "Epoch 61: val_loss did not improve from 16771.72656\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18949.6973 - rmse: 18916.2773 - val_loss: 24675.4512 - val_rmse: 24569.7148\n",
      "Epoch 62/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19367.0156 - rmse: 19367.0156\n",
      "Epoch 62: val_loss did not improve from 16771.72656\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19243.2676 - rmse: 19214.7598 - val_loss: 19826.5000 - val_rmse: 19697.2930\n",
      "Epoch 63/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18482.7988 - rmse: 18482.7988\n",
      "Epoch 63: val_loss did not improve from 16771.72656\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18536.7129 - rmse: 18573.0742 - val_loss: 18538.3223 - val_rmse: 18445.2129\n",
      "Epoch 64/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17175.7070 - rmse: 17175.7070\n",
      "Epoch 64: val_loss did not improve from 16771.72656\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17135.7188 - rmse: 17186.9629 - val_loss: 17394.1816 - val_rmse: 17312.5449\n",
      "Epoch 65/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16942.8926 - rmse: 16942.8926\n",
      "Epoch 65: val_loss did not improve from 16771.72656\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16930.3457 - rmse: 16911.1211 - val_loss: 17217.5898 - val_rmse: 17174.0020\n",
      "Epoch 66/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16460.0957 - rmse: 16460.0957\n",
      "Epoch 66: val_loss improved from 16771.72656 to 16626.66797, saving model to ./ckpt/reg_lr005/val_rmse_16564.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16506.1914 - rmse: 16503.1797 - val_loss: 16626.6680 - val_rmse: 16563.8340\n",
      "Epoch 67/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17102.0996 - rmse: 17102.0996\n",
      "Epoch 67: val_loss did not improve from 16626.66797\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17227.9375 - rmse: 17294.1816 - val_loss: 16632.2734 - val_rmse: 16566.9453\n",
      "Epoch 68/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17772.8398 - rmse: 17772.8398\n",
      "Epoch 68: val_loss did not improve from 16626.66797\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17707.6621 - rmse: 17726.6191 - val_loss: 17609.1797 - val_rmse: 17521.2969\n",
      "Epoch 69/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17908.7520 - rmse: 17908.7520\n",
      "Epoch 69: val_loss improved from 16626.66797 to 16475.61719, saving model to ./ckpt/reg_lr005/val_rmse_16398.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17895.0898 - rmse: 17871.2207 - val_loss: 16475.6172 - val_rmse: 16398.3926\n",
      "Epoch 70/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16857.8164 - rmse: 16857.8164\n",
      "Epoch 70: val_loss did not improve from 16475.61719\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16991.5430 - rmse: 16989.7051 - val_loss: 17524.7500 - val_rmse: 17436.6680\n",
      "Epoch 71/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19815.0879 - rmse: 19815.0879\n",
      "Epoch 71: val_loss did not improve from 16475.61719\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19577.3008 - rmse: 19612.6387 - val_loss: 21491.3047 - val_rmse: 21397.2734\n",
      "Epoch 72/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16843.6465 - rmse: 16843.6465\n",
      "Epoch 72: val_loss improved from 16475.61719 to 16410.46094, saving model to ./ckpt/reg_lr005/val_rmse_16325.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16806.9688 - rmse: 16823.7773 - val_loss: 16410.4609 - val_rmse: 16325.2656\n",
      "Epoch 73/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18013.9902 - rmse: 18013.9902\n",
      "Epoch 73: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17933.6523 - rmse: 17944.4746 - val_loss: 22184.5781 - val_rmse: 22102.3379\n",
      "Epoch 74/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17354.5137 - rmse: 17354.5137\n",
      "Epoch 74: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17429.7656 - rmse: 17492.9473 - val_loss: 17003.3633 - val_rmse: 16940.7637\n",
      "Epoch 75/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16580.2422 - rmse: 16580.2422\n",
      "Epoch 75: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16638.4238 - rmse: 16626.5410 - val_loss: 17188.3223 - val_rmse: 17119.3105\n",
      "Epoch 76/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16602.5371 - rmse: 16602.5371\n",
      "Epoch 76: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16568.1094 - rmse: 16602.7480 - val_loss: 24785.7559 - val_rmse: 24690.9648\n",
      "Epoch 77/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17779.4121 - rmse: 17779.4121\n",
      "Epoch 77: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17748.5625 - rmse: 17727.7559 - val_loss: 21044.6484 - val_rmse: 20946.6055\n",
      "Epoch 78/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15748.2852 - rmse: 15748.2852\n",
      "Epoch 78: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15691.4727 - rmse: 15687.0469 - val_loss: 20664.1348 - val_rmse: 20574.0117\n",
      "Epoch 79/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16405.2031 - rmse: 16405.2031\n",
      "Epoch 79: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16554.6641 - rmse: 16621.5332 - val_loss: 30195.8730 - val_rmse: 30110.3457\n",
      "Epoch 80/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15962.4170 - rmse: 15962.4170\n",
      "Epoch 80: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15794.6230 - rmse: 15812.8604 - val_loss: 17896.9492 - val_rmse: 17801.1992\n",
      "Epoch 81/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17041.6035 - rmse: 17041.6035\n",
      "Epoch 81: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17011.2832 - rmse: 17026.5508 - val_loss: 26625.5234 - val_rmse: 26512.1992\n",
      "Epoch 82/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16569.9492 - rmse: 16569.9492\n",
      "Epoch 82: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16512.6484 - rmse: 16530.7910 - val_loss: 19779.1387 - val_rmse: 19686.2988\n",
      "Epoch 83/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16407.4941 - rmse: 16407.4941\n",
      "Epoch 83: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16302.2012 - rmse: 16311.2783 - val_loss: 22412.2324 - val_rmse: 22302.2031\n",
      "Epoch 84/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15297.1221 - rmse: 15297.1221\n",
      "Epoch 84: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15320.5684 - rmse: 15325.8291 - val_loss: 17829.8086 - val_rmse: 17713.2832\n",
      "Epoch 85/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16289.0049 - rmse: 16289.0049\n",
      "Epoch 85: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16137.6514 - rmse: 16116.6836 - val_loss: 19539.4316 - val_rmse: 19428.1660\n",
      "Epoch 86/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15732.5225 - rmse: 15732.5225\n",
      "Epoch 86: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15689.5342 - rmse: 15720.9463 - val_loss: 22200.4434 - val_rmse: 22114.0996\n",
      "Epoch 87/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15778.7158 - rmse: 15778.7158\n",
      "Epoch 87: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15922.9766 - rmse: 15921.8516 - val_loss: 16695.4922 - val_rmse: 16603.6582\n",
      "Epoch 88/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15848.5928 - rmse: 15848.5928\n",
      "Epoch 88: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15966.0303 - rmse: 16014.5430 - val_loss: 23900.8418 - val_rmse: 23838.1309\n",
      "Epoch 89/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16122.4824 - rmse: 16122.4824\n",
      "Epoch 89: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16114.4395 - rmse: 16128.2861 - val_loss: 22593.2422 - val_rmse: 22497.4980\n",
      "Epoch 90/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17150.5215 - rmse: 17150.5215\n",
      "Epoch 90: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17188.8906 - rmse: 17209.3711 - val_loss: 26917.1211 - val_rmse: 26808.4863\n",
      "Epoch 91/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16510.9629 - rmse: 16510.9629\n",
      "Epoch 91: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16468.7090 - rmse: 16453.5820 - val_loss: 19743.2539 - val_rmse: 19659.2148\n",
      "Epoch 92/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15804.1455 - rmse: 15804.1455\n",
      "Epoch 92: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15995.4600 - rmse: 15979.7676 - val_loss: 18519.4277 - val_rmse: 18419.1680\n",
      "Epoch 93/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16137.1963 - rmse: 16137.1963\n",
      "Epoch 93: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16186.8457 - rmse: 16178.0088 - val_loss: 17081.8145 - val_rmse: 17015.6328\n",
      "Epoch 94/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 14610.9580 - rmse: 14610.9580\n",
      "Epoch 94: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 14593.8525 - rmse: 14599.1143 - val_loss: 17943.9531 - val_rmse: 17869.0957\n",
      "Epoch 95/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16668.4141 - rmse: 16668.4141\n",
      "Epoch 95: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16732.7012 - rmse: 16734.2832 - val_loss: 16549.4277 - val_rmse: 16470.9883\n",
      "Epoch 96/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17807.5879 - rmse: 17807.5879\n",
      "Epoch 96: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17579.0449 - rmse: 17556.0898 - val_loss: 20629.2617 - val_rmse: 20542.0508\n",
      "Epoch 97/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 14415.6445 - rmse: 14415.6445\n",
      "Epoch 97: val_loss did not improve from 16410.46094\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14612.8486 - rmse: 14623.0273 - val_loss: 24948.2480 - val_rmse: 24837.0723\n",
      "Epoch 97: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 29s - loss: 209541.2344 - rmse: 209541.2344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:43:16.400756: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 111305.4844 - rmse: 110982.6953\n",
      "Epoch 1: val_loss improved from inf to 61463.19531, saving model to ./ckpt/reg_lr005/val_rmse_61084.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 111305.4844 - rmse: 110982.6953 - val_loss: 61463.1953 - val_rmse: 61083.6758\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 57001.0312 - rmse: 57001.0312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:43:17.410674: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 57438.4336 - rmse: 57438.4336\n",
      "Epoch 2: val_loss improved from 61463.19531 to 48898.94141, saving model to ./ckpt/reg_lr005/val_rmse_48820.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 57142.1406 - rmse: 57215.1680 - val_loss: 48898.9414 - val_rmse: 48819.8516\n",
      "Epoch 3/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 47129.1016 - rmse: 47129.1016\n",
      "Epoch 3: val_loss did not improve from 48898.94141\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 46814.2891 - rmse: 46780.0078 - val_loss: 51682.4570 - val_rmse: 51741.2734\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 44277.5273 - rmse: 44277.5273\n",
      "Epoch 4: val_loss improved from 48898.94141 to 44376.42188, saving model to ./ckpt/reg_lr005/val_rmse_44355.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 44112.1523 - rmse: 43976.6992 - val_loss: 44376.4219 - val_rmse: 44354.6641\n",
      "Epoch 5/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 43817.3711 - rmse: 43817.3711\n",
      "Epoch 5: val_loss did not improve from 44376.42188\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 43548.2344 - rmse: 43536.9219 - val_loss: 51950.8398 - val_rmse: 51995.5000\n",
      "Epoch 6/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 44025.7812 - rmse: 44025.7812\n",
      "Epoch 6: val_loss improved from 44376.42188 to 40658.75391, saving model to ./ckpt/reg_lr005/val_rmse_40588.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 43520.4648 - rmse: 43436.7031 - val_loss: 40658.7539 - val_rmse: 40587.6289\n",
      "Epoch 7/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 38266.8789 - rmse: 38266.8789\n",
      "Epoch 7: val_loss did not improve from 40658.75391\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 38277.1367 - rmse: 38359.8320 - val_loss: 46304.0938 - val_rmse: 46126.3438\n",
      "Epoch 8/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 37105.5898 - rmse: 37065.9062\n",
      "Epoch 8: val_loss improved from 40658.75391 to 39942.69141, saving model to ./ckpt/reg_lr005/val_rmse_39804.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 37105.5898 - rmse: 37065.9062 - val_loss: 39942.6914 - val_rmse: 39803.6953\n",
      "Epoch 9/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 34970.8398 - rmse: 34970.8398\n",
      "Epoch 9: val_loss improved from 39942.69141 to 38341.93359, saving model to ./ckpt/reg_lr005/val_rmse_38327.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 35266.7617 - rmse: 35466.3398 - val_loss: 38341.9336 - val_rmse: 38327.3828\n",
      "Epoch 10/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 37781.3203 - rmse: 37781.3203\n",
      "Epoch 10: val_loss improved from 38341.93359 to 34963.87500, saving model to ./ckpt/reg_lr005/val_rmse_34955.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 37305.6953 - rmse: 37312.0039 - val_loss: 34963.8750 - val_rmse: 34954.9609\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 34100.9531 - rmse: 34124.2500\n",
      "Epoch 11: val_loss did not improve from 34963.87500\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 34100.9531 - rmse: 34124.2500 - val_loss: 44944.4336 - val_rmse: 44738.6055\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 32352.2676 - rmse: 32438.0332\n",
      "Epoch 12: val_loss improved from 34963.87500 to 34502.69922, saving model to ./ckpt/reg_lr005/val_rmse_34517.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 32352.2676 - rmse: 32438.0332 - val_loss: 34502.6992 - val_rmse: 34516.8047\n",
      "Epoch 13/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 34510.3672 - rmse: 34510.3672\n",
      "Epoch 13: val_loss improved from 34502.69922 to 32828.79297, saving model to ./ckpt/reg_lr005/val_rmse_32742.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 34060.5547 - rmse: 33999.6523 - val_loss: 32828.7930 - val_rmse: 32741.8535\n",
      "Epoch 14/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 32087.0352 - rmse: 32087.0352\n",
      "Epoch 14: val_loss did not improve from 32828.79297\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 32001.3652 - rmse: 31947.2617 - val_loss: 39387.0000 - val_rmse: 39225.5391\n",
      "Epoch 15/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 29497.5703 - rmse: 29489.6602\n",
      "Epoch 15: val_loss improved from 32828.79297 to 28064.76367, saving model to ./ckpt/reg_lr005/val_rmse_27994.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 29497.5703 - rmse: 29489.6602 - val_loss: 28064.7637 - val_rmse: 27993.6602\n",
      "Epoch 16/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 28176.7480 - rmse: 28176.7480\n",
      "Epoch 16: val_loss improved from 28064.76367 to 27284.72852, saving model to ./ckpt/reg_lr005/val_rmse_27230.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 28371.2637 - rmse: 28422.0078 - val_loss: 27284.7285 - val_rmse: 27229.8906\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 27255.8105 - rmse: 27255.8105\n",
      "Epoch 17: val_loss improved from 27284.72852 to 25932.57422, saving model to ./ckpt/reg_lr005/val_rmse_25856.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 27268.1992 - rmse: 27276.5547 - val_loss: 25932.5742 - val_rmse: 25855.6992\n",
      "Epoch 18/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 27613.8125 - rmse: 27613.8125\n",
      "Epoch 18: val_loss did not improve from 25932.57422\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 27515.1094 - rmse: 27521.3398 - val_loss: 26840.7812 - val_rmse: 26731.7266\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 26276.0254 - rmse: 26277.5547\n",
      "Epoch 19: val_loss did not improve from 25932.57422\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 26276.0254 - rmse: 26277.5547 - val_loss: 26529.4980 - val_rmse: 26439.0020\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 25090.9609 - rmse: 25090.9609\n",
      "Epoch 20: val_loss improved from 25932.57422 to 25395.26758, saving model to ./ckpt/reg_lr005/val_rmse_25233.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24913.3555 - rmse: 24927.1094 - val_loss: 25395.2676 - val_rmse: 25232.8262\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23687.6660 - rmse: 23744.1152\n",
      "Epoch 21: val_loss did not improve from 25395.26758\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23687.6660 - rmse: 23744.1152 - val_loss: 27175.8730 - val_rmse: 27110.9141\n",
      "Epoch 22/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 26190.6738 - rmse: 26190.6738\n",
      "Epoch 22: val_loss did not improve from 25395.26758\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 26437.9375 - rmse: 26417.6133 - val_loss: 26132.7930 - val_rmse: 25986.6758\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 24670.2500 - rmse: 24670.2500\n",
      "Epoch 23: val_loss did not improve from 25395.26758\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 24696.4805 - rmse: 24714.1699 - val_loss: 36974.7578 - val_rmse: 36783.8164\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 24366.5508 - rmse: 24366.5508\n",
      "Epoch 24: val_loss improved from 25395.26758 to 21968.22461, saving model to ./ckpt/reg_lr005/val_rmse_21824.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 24400.9355 - rmse: 24424.1250 - val_loss: 21968.2246 - val_rmse: 21824.0488\n",
      "Epoch 25/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23972.3008 - rmse: 23972.3008\n",
      "Epoch 25: val_loss did not improve from 21968.22461\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23912.1172 - rmse: 23871.5273 - val_loss: 24580.1777 - val_rmse: 24498.9512\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23103.2559 - rmse: 23089.9531\n",
      "Epoch 26: val_loss did not improve from 21968.22461\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23103.2559 - rmse: 23089.9531 - val_loss: 22710.7969 - val_rmse: 22589.6445\n",
      "Epoch 27/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21674.0234 - rmse: 21664.6875\n",
      "Epoch 27: val_loss improved from 21968.22461 to 20636.75195, saving model to ./ckpt/reg_lr005/val_rmse_20486.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21674.0234 - rmse: 21664.6875 - val_loss: 20636.7520 - val_rmse: 20486.0625\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22410.3984 - rmse: 22410.3984\n",
      "Epoch 28: val_loss did not improve from 20636.75195\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22391.8184 - rmse: 22379.2852 - val_loss: 25116.2441 - val_rmse: 24958.4727\n",
      "Epoch 29/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21468.7852 - rmse: 21468.7852\n",
      "Epoch 29: val_loss did not improve from 20636.75195\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 21486.6953 - rmse: 21506.1914 - val_loss: 22573.3984 - val_rmse: 22478.2773\n",
      "Epoch 30/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20994.0176 - rmse: 20994.0176\n",
      "Epoch 30: val_loss did not improve from 20636.75195\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 20965.5762 - rmse: 20952.2422 - val_loss: 21982.1582 - val_rmse: 21842.5898\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20668.6797 - rmse: 20685.2285\n",
      "Epoch 31: val_loss did not improve from 20636.75195\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20668.6797 - rmse: 20685.2285 - val_loss: 23865.0742 - val_rmse: 23734.5723\n",
      "Epoch 32/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20603.9492 - rmse: 20603.9492\n",
      "Epoch 32: val_loss did not improve from 20636.75195\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20687.6309 - rmse: 20660.0938 - val_loss: 21009.0801 - val_rmse: 20900.3203\n",
      "Epoch 33/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23509.9492 - rmse: 23509.9492\n",
      "Epoch 33: val_loss improved from 20636.75195 to 19920.02734, saving model to ./ckpt/reg_lr005/val_rmse_19788.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23397.4824 - rmse: 23484.6582 - val_loss: 19920.0273 - val_rmse: 19788.1074\n",
      "Epoch 34/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21522.3789 - rmse: 21522.3789\n",
      "Epoch 34: val_loss did not improve from 19920.02734\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21544.3027 - rmse: 21568.7480 - val_loss: 21705.7969 - val_rmse: 21582.2773\n",
      "Epoch 35/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19859.4688 - rmse: 19859.4688\n",
      "Epoch 35: val_loss improved from 19920.02734 to 19129.57227, saving model to ./ckpt/reg_lr005/val_rmse_19007.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19822.6465 - rmse: 19778.8652 - val_loss: 19129.5723 - val_rmse: 19006.8789\n",
      "Epoch 36/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20549.7383 - rmse: 20549.7383\n",
      "Epoch 36: val_loss did not improve from 19129.57227\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20542.0137 - rmse: 20536.8047 - val_loss: 21092.1270 - val_rmse: 20956.0527\n",
      "Epoch 37/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19445.6914 - rmse: 19445.6914\n",
      "Epoch 37: val_loss did not improve from 19129.57227\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19678.4980 - rmse: 19719.6367 - val_loss: 29879.9180 - val_rmse: 29756.0176\n",
      "Epoch 38/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19785.9648 - rmse: 19785.9648\n",
      "Epoch 38: val_loss did not improve from 19129.57227\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19799.4453 - rmse: 19812.0273 - val_loss: 22093.3516 - val_rmse: 21957.7148\n",
      "Epoch 39/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19773.1680 - rmse: 19773.1680\n",
      "Epoch 39: val_loss did not improve from 19129.57227\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19626.7754 - rmse: 19602.7012 - val_loss: 19244.7441 - val_rmse: 19133.5586\n",
      "Epoch 40/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20715.4512 - rmse: 20715.4512\n",
      "Epoch 40: val_loss did not improve from 19129.57227\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20737.3652 - rmse: 20752.1445 - val_loss: 19712.3984 - val_rmse: 19595.9043\n",
      "Epoch 41/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21507.0020 - rmse: 21507.0020\n",
      "Epoch 41: val_loss improved from 19129.57227 to 18729.80273, saving model to ./ckpt/reg_lr005/val_rmse_18590.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21854.0605 - rmse: 21909.0488 - val_loss: 18729.8027 - val_rmse: 18590.4023\n",
      "Epoch 42/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21562.4238 - rmse: 21562.4238\n",
      "Epoch 42: val_loss did not improve from 18729.80273\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 21463.2793 - rmse: 21432.2324 - val_loss: 19459.4316 - val_rmse: 19334.6152\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19839.7734 - rmse: 19839.7734\n",
      "Epoch 43: val_loss did not improve from 18729.80273\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 19843.5156 - rmse: 19846.0391 - val_loss: 22995.5059 - val_rmse: 22859.4922\n",
      "Epoch 44/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19989.4082 - rmse: 19989.4082\n",
      "Epoch 44: val_loss did not improve from 18729.80273\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20322.0996 - rmse: 20412.2969 - val_loss: 27734.4453 - val_rmse: 27572.2754\n",
      "Epoch 45/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19288.9473 - rmse: 19288.9473\n",
      "Epoch 45: val_loss did not improve from 18729.80273\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19442.3066 - rmse: 19437.3633 - val_loss: 20735.4004 - val_rmse: 20601.5059\n",
      "Epoch 46/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18663.9102 - rmse: 18663.9102\n",
      "Epoch 46: val_loss did not improve from 18729.80273\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18657.0059 - rmse: 18652.3477 - val_loss: 21057.1484 - val_rmse: 20905.3340\n",
      "Epoch 47/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18528.5684 - rmse: 18528.5684\n",
      "Epoch 47: val_loss did not improve from 18729.80273\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18665.0449 - rmse: 18743.7129 - val_loss: 18983.3320 - val_rmse: 18841.5918\n",
      "Epoch 48/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18357.6172 - rmse: 18357.6172\n",
      "Epoch 48: val_loss did not improve from 18729.80273\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18329.9844 - rmse: 18311.3477 - val_loss: 23621.2051 - val_rmse: 23451.3164\n",
      "Epoch 49/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18694.7363 - rmse: 18694.7363\n",
      "Epoch 49: val_loss did not improve from 18729.80273\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18699.0723 - rmse: 18701.9961 - val_loss: 23340.0879 - val_rmse: 23181.4102\n",
      "Epoch 50/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18452.4160 - rmse: 18452.4160\n",
      "Epoch 50: val_loss did not improve from 18729.80273\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18536.7305 - rmse: 18521.3086 - val_loss: 19339.6914 - val_rmse: 19218.5391\n",
      "Epoch 51/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19307.4004 - rmse: 19307.4004\n",
      "Epoch 51: val_loss did not improve from 18729.80273\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19317.9023 - rmse: 19342.8203 - val_loss: 30608.8789 - val_rmse: 30478.1836\n",
      "Epoch 52/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19212.5332 - rmse: 19212.5332\n",
      "Epoch 52: val_loss did not improve from 18729.80273\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19224.6660 - rmse: 19267.7324 - val_loss: 20806.0391 - val_rmse: 20663.1758\n",
      "Epoch 53/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17889.3652 - rmse: 17889.3652\n",
      "Epoch 53: val_loss improved from 18729.80273 to 18063.46875, saving model to ./ckpt/reg_lr005/val_rmse_17954.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17884.1152 - rmse: 17880.5742 - val_loss: 18063.4688 - val_rmse: 17953.9082\n",
      "Epoch 54/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18134.5566 - rmse: 18134.5566\n",
      "Epoch 54: val_loss did not improve from 18063.46875\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18206.4551 - rmse: 18254.9473 - val_loss: 18330.8594 - val_rmse: 18220.0664\n",
      "Epoch 55/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18289.2715 - rmse: 18289.2715\n",
      "Epoch 55: val_loss did not improve from 18063.46875\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18258.2461 - rmse: 18285.1094 - val_loss: 18512.9629 - val_rmse: 18368.2832\n",
      "Epoch 56/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18604.7930 - rmse: 18617.5684\n",
      "Epoch 56: val_loss did not improve from 18063.46875\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18604.7930 - rmse: 18617.5684 - val_loss: 20257.9551 - val_rmse: 20154.6680\n",
      "Epoch 57/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17592.5859 - rmse: 17592.5859\n",
      "Epoch 57: val_loss did not improve from 18063.46875\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17499.0527 - rmse: 17468.6523 - val_loss: 18539.1660 - val_rmse: 18402.6543\n",
      "Epoch 58/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19034.5000 - rmse: 19034.5000\n",
      "Epoch 58: val_loss did not improve from 18063.46875\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19067.0410 - rmse: 19066.7871 - val_loss: 20495.5273 - val_rmse: 20338.9141\n",
      "Epoch 59/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17567.2598 - rmse: 17542.9512\n",
      "Epoch 59: val_loss did not improve from 18063.46875\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 17567.2598 - rmse: 17542.9512 - val_loss: 23286.7891 - val_rmse: 23126.8809\n",
      "Epoch 60/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18958.7305 - rmse: 18958.7305\n",
      "Epoch 60: val_loss did not improve from 18063.46875\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19049.3594 - rmse: 19110.4805 - val_loss: 30329.4941 - val_rmse: 30192.1699\n",
      "Epoch 61/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18377.6992 - rmse: 18377.6992\n",
      "Epoch 61: val_loss did not improve from 18063.46875\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18418.3574 - rmse: 18414.7559 - val_loss: 25184.1660 - val_rmse: 25032.5586\n",
      "Epoch 62/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16339.0068 - rmse: 16339.0068\n",
      "Epoch 62: val_loss did not improve from 18063.46875\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16472.4746 - rmse: 16509.6992 - val_loss: 18229.8926 - val_rmse: 18106.7031\n",
      "Epoch 63/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17649.4062 - rmse: 17649.4062\n",
      "Epoch 63: val_loss did not improve from 18063.46875\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17502.2246 - rmse: 17483.5020 - val_loss: 19246.3750 - val_rmse: 19130.7617\n",
      "Epoch 64/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16524.3672 - rmse: 16524.3672\n",
      "Epoch 64: val_loss did not improve from 18063.46875\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16508.7148 - rmse: 16519.9883 - val_loss: 20165.9023 - val_rmse: 20029.1328\n",
      "Epoch 65/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16930.6582 - rmse: 16930.6582\n",
      "Epoch 65: val_loss improved from 18063.46875 to 17636.29883, saving model to ./ckpt/reg_lr005/val_rmse_17529.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16859.8887 - rmse: 16874.1523 - val_loss: 17636.2988 - val_rmse: 17528.5039\n",
      "Epoch 66/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16872.8223 - rmse: 16872.8223\n",
      "Epoch 66: val_loss did not improve from 17636.29883\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16947.4453 - rmse: 17004.1133 - val_loss: 18329.3066 - val_rmse: 18203.2754\n",
      "Epoch 67/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18576.7715 - rmse: 18576.7715\n",
      "Epoch 67: val_loss did not improve from 17636.29883\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18421.6270 - rmse: 18396.7227 - val_loss: 17981.6914 - val_rmse: 17861.3340\n",
      "Epoch 68/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16715.0117 - rmse: 16715.0117\n",
      "Epoch 68: val_loss did not improve from 17636.29883\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16754.0684 - rmse: 16805.1406 - val_loss: 21075.5879 - val_rmse: 20983.0898\n",
      "Epoch 69/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19634.6172 - rmse: 19641.0977\n",
      "Epoch 69: val_loss did not improve from 17636.29883\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19634.6172 - rmse: 19641.0977 - val_loss: 32965.9531 - val_rmse: 32837.3242\n",
      "Epoch 70/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18394.7383 - rmse: 18394.7383\n",
      "Epoch 70: val_loss did not improve from 17636.29883\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18362.8066 - rmse: 18344.8906 - val_loss: 18583.2109 - val_rmse: 18506.4824\n",
      "Epoch 71/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18224.2402 - rmse: 18224.2402\n",
      "Epoch 71: val_loss improved from 17636.29883 to 17581.39453, saving model to ./ckpt/reg_lr005/val_rmse_17459.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18014.1973 - rmse: 18000.8262 - val_loss: 17581.3945 - val_rmse: 17459.1992\n",
      "Epoch 72/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18075.3086 - rmse: 18075.3086\n",
      "Epoch 72: val_loss did not improve from 17581.39453\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18156.2637 - rmse: 18137.9375 - val_loss: 20682.5430 - val_rmse: 20543.1992\n",
      "Epoch 73/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15486.0537 - rmse: 15486.0537\n",
      "Epoch 73: val_loss did not improve from 17581.39453\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15598.3711 - rmse: 15611.5000 - val_loss: 22611.0586 - val_rmse: 22482.8086\n",
      "Epoch 74/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16412.4160 - rmse: 16412.4160\n",
      "Epoch 74: val_loss did not improve from 17581.39453\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16363.5332 - rmse: 16368.2715 - val_loss: 20138.4512 - val_rmse: 20036.7148\n",
      "Epoch 75/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16154.1006 - rmse: 16154.1006\n",
      "Epoch 75: val_loss did not improve from 17581.39453\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16099.0967 - rmse: 16087.0449 - val_loss: 20462.6426 - val_rmse: 20325.7520\n",
      "Epoch 76/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16080.0801 - rmse: 16080.0801\n",
      "Epoch 76: val_loss improved from 17581.39453 to 17141.43750, saving model to ./ckpt/reg_lr005/val_rmse_17019.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16032.0898 - rmse: 16023.8291 - val_loss: 17141.4375 - val_rmse: 17019.2637\n",
      "Epoch 77/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16862.8223 - rmse: 16862.8223\n",
      "Epoch 77: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16806.5352 - rmse: 16795.8906 - val_loss: 18873.7812 - val_rmse: 18749.3945\n",
      "Epoch 78/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16016.0850 - rmse: 16016.0850\n",
      "Epoch 78: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16023.2266 - rmse: 16028.0410 - val_loss: 19076.6348 - val_rmse: 18958.7871\n",
      "Epoch 79/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16522.5117 - rmse: 16522.5117\n",
      "Epoch 79: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16413.0020 - rmse: 16395.0234 - val_loss: 21173.3906 - val_rmse: 21030.4023\n",
      "Epoch 80/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15581.5088 - rmse: 15581.5088\n",
      "Epoch 80: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15651.6289 - rmse: 15708.9805 - val_loss: 20603.7598 - val_rmse: 20422.7402\n",
      "Epoch 81/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15712.7979 - rmse: 15712.7979\n",
      "Epoch 81: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15691.8223 - rmse: 15682.7588 - val_loss: 19547.9727 - val_rmse: 19466.2188\n",
      "Epoch 82/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17510.5273 - rmse: 17510.5273\n",
      "Epoch 82: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17498.4297 - rmse: 17491.4297 - val_loss: 19132.0996 - val_rmse: 18977.0352\n",
      "Epoch 83/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16012.1211 - rmse: 15991.3535\n",
      "Epoch 83: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16012.1211 - rmse: 15991.3535 - val_loss: 18636.9121 - val_rmse: 18501.6953\n",
      "Epoch 84/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15718.4307 - rmse: 15718.4307\n",
      "Epoch 84: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15803.1133 - rmse: 15781.4609 - val_loss: 17933.4395 - val_rmse: 17805.4922\n",
      "Epoch 85/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15765.5508 - rmse: 15765.5508\n",
      "Epoch 85: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16024.0615 - rmse: 16055.2783 - val_loss: 17782.5117 - val_rmse: 17644.3750\n",
      "Epoch 86/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15696.5410 - rmse: 15688.0928\n",
      "Epoch 86: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 15696.5410 - rmse: 15688.0928 - val_loss: 20326.5000 - val_rmse: 20177.2031\n",
      "Epoch 87/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16582.6035 - rmse: 16582.6035\n",
      "Epoch 87: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 16613.6602 - rmse: 16658.2539 - val_loss: 19901.8906 - val_rmse: 19789.8125\n",
      "Epoch 88/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15645.7227 - rmse: 15645.7227\n",
      "Epoch 88: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15712.5918 - rmse: 15765.5605 - val_loss: 18782.2695 - val_rmse: 18684.6113\n",
      "Epoch 89/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15040.2783 - rmse: 15040.2783\n",
      "Epoch 89: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15065.7051 - rmse: 15054.4844 - val_loss: 18993.4297 - val_rmse: 18860.4023\n",
      "Epoch 90/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17730.1855 - rmse: 17730.1855\n",
      "Epoch 90: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17738.5430 - rmse: 17764.2793 - val_loss: 18802.3594 - val_rmse: 18698.7637\n",
      "Epoch 91/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15838.6396 - rmse: 15838.6396\n",
      "Epoch 91: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15801.2812 - rmse: 15813.4199 - val_loss: 17318.0801 - val_rmse: 17177.0742\n",
      "Epoch 92/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 14651.0996 - rmse: 14651.0996\n",
      "Epoch 92: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 14757.1504 - rmse: 14848.0078 - val_loss: 19338.6172 - val_rmse: 19210.4141\n",
      "Epoch 93/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16460.5117 - rmse: 16460.5117\n",
      "Epoch 93: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16601.9863 - rmse: 16651.4961 - val_loss: 30605.4414 - val_rmse: 30447.2070\n",
      "Epoch 94/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16114.6025 - rmse: 16114.6025\n",
      "Epoch 94: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16079.9971 - rmse: 16067.5518 - val_loss: 20706.3906 - val_rmse: 20572.8887\n",
      "Epoch 95/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15035.5410 - rmse: 15035.5410\n",
      "Epoch 95: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15050.6943 - rmse: 15034.1836 - val_loss: 24924.9844 - val_rmse: 24800.6270\n",
      "Epoch 96/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16080.0215 - rmse: 16080.0215\n",
      "Epoch 96: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16166.6602 - rmse: 16179.0361 - val_loss: 17551.3496 - val_rmse: 17427.9844\n",
      "Epoch 97/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15080.0098 - rmse: 15090.9326\n",
      "Epoch 97: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15080.0098 - rmse: 15090.9326 - val_loss: 17933.7637 - val_rmse: 17853.3574\n",
      "Epoch 98/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15215.8682 - rmse: 15215.8682\n",
      "Epoch 98: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15328.9180 - rmse: 15372.2393 - val_loss: 17935.6113 - val_rmse: 17848.8633\n",
      "Epoch 99/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18816.3438 - rmse: 18816.3438\n",
      "Epoch 99: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18746.6621 - rmse: 18730.9609 - val_loss: 27286.2617 - val_rmse: 27169.5527\n",
      "Epoch 100/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 14428.6064 - rmse: 14428.6064\n",
      "Epoch 100: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 14515.4912 - rmse: 14522.6934 - val_loss: 17548.7520 - val_rmse: 17425.6758\n",
      "Epoch 101/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15753.5088 - rmse: 15753.5088\n",
      "Epoch 101: val_loss did not improve from 17141.43750\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 15727.2031 - rmse: 15743.0684 - val_loss: 22538.9141 - val_rmse: 22376.9492\n",
      "Epoch 101: early stopping\n",
      "Epoch 1/120\n",
      " 1/70 [..............................] - ETA: 31s - loss: 184701.2656 - rmse: 184701.2656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:44:31.816237: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 110440.2500 - rmse: 110067.0781\n",
      "Epoch 1: val_loss improved from inf to 71325.56250, saving model to ./ckpt/reg_lr005/val_rmse_71019.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 110440.2500 - rmse: 110067.0781 - val_loss: 71325.5625 - val_rmse: 71018.5781\n",
      "Epoch 2/120\n",
      " 7/70 [==>...........................] - ETA: 0s - loss: 67919.9297 - rmse: 67919.9297"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:44:32.794394: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 57074.7930 - rmse: 56997.3594\n",
      "Epoch 2: val_loss improved from 71325.56250 to 57478.50391, saving model to ./ckpt/reg_lr005/val_rmse_57114.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 57074.7930 - rmse: 56997.3594 - val_loss: 57478.5039 - val_rmse: 57113.6133\n",
      "Epoch 3/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 47021.4492 - rmse: 47021.4492\n",
      "Epoch 3: val_loss improved from 57478.50391 to 50640.06250, saving model to ./ckpt/reg_lr005/val_rmse_50282.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 47075.9961 - rmse: 47235.6172 - val_loss: 50640.0625 - val_rmse: 50282.4219\n",
      "Epoch 4/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 45493.9609 - rmse: 45493.9609\n",
      "Epoch 4: val_loss improved from 50640.06250 to 49443.35156, saving model to ./ckpt/reg_lr005/val_rmse_49093.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 45357.4844 - rmse: 45330.6523 - val_loss: 49443.3516 - val_rmse: 49093.3203\n",
      "Epoch 5/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 42807.6758 - rmse: 42807.6758\n",
      "Epoch 5: val_loss improved from 49443.35156 to 47134.60938, saving model to ./ckpt/reg_lr005/val_rmse_46774.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 43034.7148 - rmse: 43033.1211 - val_loss: 47134.6094 - val_rmse: 46773.6641\n",
      "Epoch 6/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 40745.7969 - rmse: 40745.7969\n",
      "Epoch 6: val_loss improved from 47134.60938 to 44947.83594, saving model to ./ckpt/reg_lr005/val_rmse_44596.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 40634.0742 - rmse: 40808.6953 - val_loss: 44947.8359 - val_rmse: 44596.3242\n",
      "Epoch 7/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 38489.8398 - rmse: 38489.8398\n",
      "Epoch 7: val_loss improved from 44947.83594 to 43055.49219, saving model to ./ckpt/reg_lr005/val_rmse_42727.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 38681.8711 - rmse: 38728.6562 - val_loss: 43055.4922 - val_rmse: 42726.5508\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 37239.1602 - rmse: 37239.1602\n",
      "Epoch 8: val_loss improved from 43055.49219 to 41733.23438, saving model to ./ckpt/reg_lr005/val_rmse_41424.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 37330.7852 - rmse: 37392.5781 - val_loss: 41733.2344 - val_rmse: 41423.5000\n",
      "Epoch 9/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 35877.2852 - rmse: 35877.2852\n",
      "Epoch 9: val_loss improved from 41733.23438 to 39732.73828, saving model to ./ckpt/reg_lr005/val_rmse_39424.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 35739.1875 - rmse: 35679.7656 - val_loss: 39732.7383 - val_rmse: 39423.6289\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 33877.7695 - rmse: 33888.8555\n",
      "Epoch 10: val_loss did not improve from 39732.73828\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 33877.7695 - rmse: 33888.8555 - val_loss: 53397.8281 - val_rmse: 53057.8828\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 35258.7305 - rmse: 35378.1484\n",
      "Epoch 11: val_loss did not improve from 39732.73828\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 35258.7305 - rmse: 35378.1484 - val_loss: 39980.4648 - val_rmse: 39637.1445\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 32502.3125 - rmse: 32502.3125\n",
      "Epoch 12: val_loss did not improve from 39732.73828\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 32444.9004 - rmse: 32427.6387 - val_loss: 44376.3203 - val_rmse: 44205.5195\n",
      "Epoch 13/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 30880.2031 - rmse: 30880.2031\n",
      "Epoch 13: val_loss improved from 39732.73828 to 37671.97656, saving model to ./ckpt/reg_lr005/val_rmse_37344.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 30911.8965 - rmse: 30981.6035 - val_loss: 37671.9766 - val_rmse: 37343.7188\n",
      "Epoch 14/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 28802.4160 - rmse: 28802.4160\n",
      "Epoch 14: val_loss improved from 37671.97656 to 36632.92188, saving model to ./ckpt/reg_lr005/val_rmse_36327.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 28846.9961 - rmse: 28928.4492 - val_loss: 36632.9219 - val_rmse: 36327.1016\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 29714.3887 - rmse: 29714.3887\n",
      "Epoch 15: val_loss improved from 36632.92188 to 36203.08594, saving model to ./ckpt/reg_lr005/val_rmse_36075.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 29685.0898 - rmse: 29665.3262 - val_loss: 36203.0859 - val_rmse: 36074.5898\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 30274.1855 - rmse: 30274.1855\n",
      "Epoch 16: val_loss improved from 36203.08594 to 30652.71484, saving model to ./ckpt/reg_lr005/val_rmse_30413.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 30272.0586 - rmse: 30270.6211 - val_loss: 30652.7148 - val_rmse: 30413.3984\n",
      "Epoch 17/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27065.6641 - rmse: 27065.6641\n",
      "Epoch 17: val_loss improved from 30652.71484 to 29895.25781, saving model to ./ckpt/reg_lr005/val_rmse_29738.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 27227.9941 - rmse: 27332.5547 - val_loss: 29895.2578 - val_rmse: 29737.9844\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 25660.3438 - rmse: 25660.3438\n",
      "Epoch 18: val_loss did not improve from 29895.25781\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 25925.6895 - rmse: 25960.0840 - val_loss: 49070.2188 - val_rmse: 49023.7969\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 28136.7266 - rmse: 28194.2539\n",
      "Epoch 19: val_loss did not improve from 29895.25781\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 28136.7266 - rmse: 28194.2539 - val_loss: 30008.7812 - val_rmse: 29815.1680\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25604.5273 - rmse: 25571.5918\n",
      "Epoch 20: val_loss did not improve from 29895.25781\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 25604.5273 - rmse: 25571.5918 - val_loss: 32380.1973 - val_rmse: 32307.3789\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25131.7812 - rmse: 25131.7812\n",
      "Epoch 21: val_loss improved from 29895.25781 to 25993.71094, saving model to ./ckpt/reg_lr005/val_rmse_25848.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24911.0664 - rmse: 24858.8984 - val_loss: 25993.7109 - val_rmse: 25848.2129\n",
      "Epoch 22/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 24225.1738 - rmse: 24225.1738\n",
      "Epoch 22: val_loss did not improve from 25993.71094\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 24322.9102 - rmse: 24388.8281 - val_loss: 38210.9688 - val_rmse: 38194.3164\n",
      "Epoch 23/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 25507.4727 - rmse: 25507.4727\n",
      "Epoch 23: val_loss improved from 25993.71094 to 25084.12305, saving model to ./ckpt/reg_lr005/val_rmse_24981.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25416.9512 - rmse: 25456.0703 - val_loss: 25084.1230 - val_rmse: 24980.5215\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 24245.6367 - rmse: 24245.6367\n",
      "Epoch 24: val_loss improved from 25084.12305 to 24946.33594, saving model to ./ckpt/reg_lr005/val_rmse_24842.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 24204.0371 - rmse: 24175.9805 - val_loss: 24946.3359 - val_rmse: 24842.2109\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21769.9199 - rmse: 21788.0742\n",
      "Epoch 25: val_loss did not improve from 24946.33594\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21769.9199 - rmse: 21788.0742 - val_loss: 25123.9668 - val_rmse: 25047.2480\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23150.9355 - rmse: 23150.9355\n",
      "Epoch 26: val_loss did not improve from 24946.33594\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 23164.2539 - rmse: 23173.2363 - val_loss: 31938.8691 - val_rmse: 31948.0664\n",
      "Epoch 27/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23740.4258 - rmse: 23740.4258\n",
      "Epoch 27: val_loss did not improve from 24946.33594\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23694.0098 - rmse: 23674.9180 - val_loss: 28543.5254 - val_rmse: 28534.5371\n",
      "Epoch 28/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21996.9688 - rmse: 21996.9688\n",
      "Epoch 28: val_loss improved from 24946.33594 to 24551.73047, saving model to ./ckpt/reg_lr005/val_rmse_24509.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21951.6836 - rmse: 21955.7871 - val_loss: 24551.7305 - val_rmse: 24509.2637\n",
      "Epoch 29/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21519.7930 - rmse: 21519.7930\n",
      "Epoch 29: val_loss did not improve from 24551.73047\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21408.4434 - rmse: 21428.9316 - val_loss: 29639.2637 - val_rmse: 29504.3672\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22859.7930 - rmse: 22859.7930\n",
      "Epoch 30: val_loss improved from 24551.73047 to 24066.20508, saving model to ./ckpt/reg_lr005/val_rmse_24043.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22824.6230 - rmse: 22800.9004 - val_loss: 24066.2051 - val_rmse: 24042.8242\n",
      "Epoch 31/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 21056.1523 - rmse: 21056.1523\n",
      "Epoch 31: val_loss improved from 24066.20508 to 23271.35156, saving model to ./ckpt/reg_lr005/val_rmse_23294.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20844.4805 - rmse: 20830.1172 - val_loss: 23271.3516 - val_rmse: 23294.3652\n",
      "Epoch 32/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20434.2109 - rmse: 20434.2109\n",
      "Epoch 32: val_loss improved from 23271.35156 to 22858.75781, saving model to ./ckpt/reg_lr005/val_rmse_22806.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20371.7285 - rmse: 20406.5527 - val_loss: 22858.7578 - val_rmse: 22805.9023\n",
      "Epoch 33/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20866.3145 - rmse: 20866.3145\n",
      "Epoch 33: val_loss did not improve from 22858.75781\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21153.8906 - rmse: 21137.5879 - val_loss: 39702.2734 - val_rmse: 39769.2461\n",
      "Epoch 34/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21931.1250 - rmse: 21931.1250\n",
      "Epoch 34: val_loss did not improve from 22858.75781\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21945.7461 - rmse: 21939.1602 - val_loss: 24080.4609 - val_rmse: 24118.4609\n",
      "Epoch 35/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22800.1289 - rmse: 22800.1289\n",
      "Epoch 35: val_loss did not improve from 22858.75781\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22782.4707 - rmse: 22717.3242 - val_loss: 22980.0391 - val_rmse: 22920.2949\n",
      "Epoch 36/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21395.1602 - rmse: 21395.1602\n",
      "Epoch 36: val_loss improved from 22858.75781 to 22502.28711, saving model to ./ckpt/reg_lr005/val_rmse_22520.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21379.6230 - rmse: 21402.5293 - val_loss: 22502.2871 - val_rmse: 22520.3809\n",
      "Epoch 37/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20678.5000 - rmse: 20678.4004\n",
      "Epoch 37: val_loss did not improve from 22502.28711\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20678.5000 - rmse: 20678.4004 - val_loss: 23369.4023 - val_rmse: 23310.5234\n",
      "Epoch 38/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21511.9941 - rmse: 21511.9941\n",
      "Epoch 38: val_loss improved from 22502.28711 to 22015.75977, saving model to ./ckpt/reg_lr005/val_rmse_22043.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 21367.7402 - rmse: 21411.5137 - val_loss: 22015.7598 - val_rmse: 22042.5820\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19837.5645 - rmse: 19837.5645\n",
      "Epoch 39: val_loss did not improve from 22015.75977\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 19858.4707 - rmse: 19872.5703 - val_loss: 22981.6738 - val_rmse: 22928.5195\n",
      "Epoch 40/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19527.0176 - rmse: 19527.0176\n",
      "Epoch 40: val_loss did not improve from 22015.75977\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19688.3086 - rmse: 19677.9102 - val_loss: 22643.0918 - val_rmse: 22612.5039\n",
      "Epoch 41/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19567.0039 - rmse: 19567.0039\n",
      "Epoch 41: val_loss improved from 22015.75977 to 21272.83789, saving model to ./ckpt/reg_lr005/val_rmse_21309.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19687.6836 - rmse: 19769.0742 - val_loss: 21272.8379 - val_rmse: 21309.0742\n",
      "Epoch 42/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20150.5586 - rmse: 20150.5586\n",
      "Epoch 42: val_loss did not improve from 21272.83789\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20114.6484 - rmse: 20108.9082 - val_loss: 21682.2012 - val_rmse: 21667.9258\n",
      "Epoch 43/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18878.2539 - rmse: 18878.2539\n",
      "Epoch 43: val_loss did not improve from 21272.83789\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18834.9238 - rmse: 18800.3926 - val_loss: 28932.2715 - val_rmse: 29003.8633\n",
      "Epoch 44/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19662.3652 - rmse: 19704.2773\n",
      "Epoch 44: val_loss did not improve from 21272.83789\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19662.3652 - rmse: 19704.2773 - val_loss: 21996.9746 - val_rmse: 22068.3398\n",
      "Epoch 45/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19216.6152 - rmse: 19209.1992\n",
      "Epoch 45: val_loss did not improve from 21272.83789\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19216.6152 - rmse: 19209.1992 - val_loss: 21939.4824 - val_rmse: 22004.8125\n",
      "Epoch 46/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18838.5781 - rmse: 18838.5781\n",
      "Epoch 46: val_loss improved from 21272.83789 to 21182.95117, saving model to ./ckpt/reg_lr005/val_rmse_21188.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19025.8945 - rmse: 19072.5176 - val_loss: 21182.9512 - val_rmse: 21187.9844\n",
      "Epoch 47/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18795.7070 - rmse: 18795.7070\n",
      "Epoch 47: val_loss did not improve from 21182.95117\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18842.9785 - rmse: 18874.8594 - val_loss: 22239.2090 - val_rmse: 22277.3047\n",
      "Epoch 48/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20380.7949 - rmse: 20380.7949\n",
      "Epoch 48: val_loss did not improve from 21182.95117\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20414.0566 - rmse: 20428.4961 - val_loss: 28711.5352 - val_rmse: 28795.6797\n",
      "Epoch 49/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18654.0664 - rmse: 18654.0664\n",
      "Epoch 49: val_loss did not improve from 21182.95117\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18559.0117 - rmse: 18560.0527 - val_loss: 25848.2715 - val_rmse: 25926.3535\n",
      "Epoch 50/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19541.0645 - rmse: 19505.5332\n",
      "Epoch 50: val_loss did not improve from 21182.95117\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19541.0645 - rmse: 19505.5332 - val_loss: 23243.5234 - val_rmse: 23334.9609\n",
      "Epoch 51/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17327.1211 - rmse: 17327.1211\n",
      "Epoch 51: val_loss improved from 21182.95117 to 20549.68359, saving model to ./ckpt/reg_lr005/val_rmse_20564.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 17446.3301 - rmse: 17421.5879 - val_loss: 20549.6836 - val_rmse: 20564.0488\n",
      "Epoch 52/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19893.5938 - rmse: 19893.5938\n",
      "Epoch 52: val_loss did not improve from 20549.68359\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19766.7188 - rmse: 19771.6270 - val_loss: 24489.7227 - val_rmse: 24561.9648\n",
      "Epoch 53/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17704.1953 - rmse: 17704.1953\n",
      "Epoch 53: val_loss did not improve from 20549.68359\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17691.5820 - rmse: 17691.9746 - val_loss: 21709.0625 - val_rmse: 21747.8457\n",
      "Epoch 54/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18687.9727 - rmse: 18784.2930\n",
      "Epoch 54: val_loss did not improve from 20549.68359\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18687.9727 - rmse: 18784.2930 - val_loss: 27908.7402 - val_rmse: 27977.1484\n",
      "Epoch 55/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19088.4355 - rmse: 19088.4355\n",
      "Epoch 55: val_loss did not improve from 20549.68359\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18964.7266 - rmse: 18963.3125 - val_loss: 22530.3887 - val_rmse: 22573.0059\n",
      "Epoch 56/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18889.7852 - rmse: 18903.8652\n",
      "Epoch 56: val_loss did not improve from 20549.68359\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18889.7852 - rmse: 18903.8652 - val_loss: 21623.3418 - val_rmse: 21676.7520\n",
      "Epoch 57/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18899.8242 - rmse: 18899.1738\n",
      "Epoch 57: val_loss did not improve from 20549.68359\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18899.8242 - rmse: 18899.1738 - val_loss: 26450.7812 - val_rmse: 26552.3438\n",
      "Epoch 58/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17383.8027 - rmse: 17383.8027\n",
      "Epoch 58: val_loss improved from 20549.68359 to 20393.47852, saving model to ./ckpt/reg_lr005/val_rmse_20443.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17413.6895 - rmse: 17442.3457 - val_loss: 20393.4785 - val_rmse: 20443.2793\n",
      "Epoch 59/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17548.0137 - rmse: 17548.0137\n",
      "Epoch 59: val_loss did not improve from 20393.47852\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17640.8320 - rmse: 17608.1191 - val_loss: 24362.6777 - val_rmse: 24434.9648\n",
      "Epoch 60/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19115.1816 - rmse: 19115.1816\n",
      "Epoch 60: val_loss did not improve from 20393.47852\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19258.2266 - rmse: 19354.6992 - val_loss: 22756.5840 - val_rmse: 22806.1484\n",
      "Epoch 61/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17538.2344 - rmse: 17516.1035\n",
      "Epoch 61: val_loss did not improve from 20393.47852\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 17538.2344 - rmse: 17516.1035 - val_loss: 25260.4453 - val_rmse: 25327.7617\n",
      "Epoch 62/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17786.5312 - rmse: 17786.5312\n",
      "Epoch 62: val_loss did not improve from 20393.47852\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17800.7969 - rmse: 17810.4160 - val_loss: 27271.2871 - val_rmse: 27351.5684\n",
      "Epoch 63/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17661.5547 - rmse: 17661.5547\n",
      "Epoch 63: val_loss did not improve from 20393.47852\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17750.5879 - rmse: 17808.4023 - val_loss: 22923.9609 - val_rmse: 22998.2148\n",
      "Epoch 64/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17362.4980 - rmse: 17362.4980\n",
      "Epoch 64: val_loss did not improve from 20393.47852\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 17402.6484 - rmse: 17408.6445 - val_loss: 25360.8750 - val_rmse: 25430.8164\n",
      "Epoch 65/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16995.3457 - rmse: 16995.3457\n",
      "Epoch 65: val_loss did not improve from 20393.47852\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16751.1113 - rmse: 16733.0137 - val_loss: 20758.9746 - val_rmse: 20778.2207\n",
      "Epoch 66/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16695.8633 - rmse: 16695.8633\n",
      "Epoch 66: val_loss did not improve from 20393.47852\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16916.7422 - rmse: 16994.1211 - val_loss: 21211.3105 - val_rmse: 21133.9824\n",
      "Epoch 67/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17370.1758 - rmse: 17370.1758\n",
      "Epoch 67: val_loss did not improve from 20393.47852\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17300.1973 - rmse: 17312.6035 - val_loss: 21788.7051 - val_rmse: 21811.2012\n",
      "Epoch 68/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19412.2402 - rmse: 19434.6973\n",
      "Epoch 68: val_loss did not improve from 20393.47852\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19412.2402 - rmse: 19434.6973 - val_loss: 21255.4355 - val_rmse: 21228.7305\n",
      "Epoch 69/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16748.9551 - rmse: 16748.9551\n",
      "Epoch 69: val_loss improved from 20393.47852 to 20178.94922, saving model to ./ckpt/reg_lr005/val_rmse_20159.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16736.8359 - rmse: 16728.6621 - val_loss: 20178.9492 - val_rmse: 20158.9453\n",
      "Epoch 70/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17115.5859 - rmse: 17115.5859\n",
      "Epoch 70: val_loss did not improve from 20178.94922\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17036.6855 - rmse: 17020.8867 - val_loss: 20258.8086 - val_rmse: 20304.0879\n",
      "Epoch 71/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18439.4785 - rmse: 18439.4785\n",
      "Epoch 71: val_loss improved from 20178.94922 to 19776.88477, saving model to ./ckpt/reg_lr005/val_rmse_19813.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18314.3496 - rmse: 18294.2910 - val_loss: 19776.8848 - val_rmse: 19812.9219\n",
      "Epoch 72/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17391.4004 - rmse: 17383.7402\n",
      "Epoch 72: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17391.4004 - rmse: 17383.7402 - val_loss: 30513.1914 - val_rmse: 30560.3516\n",
      "Epoch 73/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17193.8457 - rmse: 17193.8457\n",
      "Epoch 73: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17189.1895 - rmse: 17186.0508 - val_loss: 24955.7637 - val_rmse: 25019.0000\n",
      "Epoch 74/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17210.5625 - rmse: 17217.6582\n",
      "Epoch 74: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17210.5625 - rmse: 17217.6582 - val_loss: 20542.2559 - val_rmse: 20550.6836\n",
      "Epoch 75/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16855.2891 - rmse: 16855.2891\n",
      "Epoch 75: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16996.7910 - rmse: 17022.8828 - val_loss: 27934.2305 - val_rmse: 27995.8887\n",
      "Epoch 76/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18048.9961 - rmse: 18014.2617\n",
      "Epoch 76: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18048.9961 - rmse: 18014.2617 - val_loss: 19784.2246 - val_rmse: 19777.6367\n",
      "Epoch 77/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16676.2852 - rmse: 16681.4062\n",
      "Epoch 77: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16676.2852 - rmse: 16681.4062 - val_loss: 24029.8906 - val_rmse: 24055.5625\n",
      "Epoch 78/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18103.6250 - rmse: 18103.6250\n",
      "Epoch 78: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18377.9375 - rmse: 18353.4199 - val_loss: 23036.9375 - val_rmse: 22969.0469\n",
      "Epoch 79/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19940.4492 - rmse: 19923.3965\n",
      "Epoch 79: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19940.4492 - rmse: 19923.3965 - val_loss: 25339.0293 - val_rmse: 25351.8301\n",
      "Epoch 80/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17165.2363 - rmse: 17165.2363\n",
      "Epoch 80: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17483.8301 - rmse: 17510.0938 - val_loss: 24573.5859 - val_rmse: 24596.6133\n",
      "Epoch 81/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17116.4863 - rmse: 17116.4863\n",
      "Epoch 81: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17148.1875 - rmse: 17163.3789 - val_loss: 21972.6504 - val_rmse: 21976.8633\n",
      "Epoch 82/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16530.5020 - rmse: 16530.5020\n",
      "Epoch 82: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16305.9639 - rmse: 16312.5928 - val_loss: 23389.1562 - val_rmse: 23419.8652\n",
      "Epoch 83/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16008.5537 - rmse: 16008.5537\n",
      "Epoch 83: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15959.0303 - rmse: 15961.1787 - val_loss: 21854.4395 - val_rmse: 21877.5254\n",
      "Epoch 84/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16609.1836 - rmse: 16609.1836\n",
      "Epoch 84: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16858.0938 - rmse: 16926.3281 - val_loss: 22057.1523 - val_rmse: 22027.5391\n",
      "Epoch 85/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17329.3145 - rmse: 17329.3145\n",
      "Epoch 85: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17272.5215 - rmse: 17365.8867 - val_loss: 22265.5234 - val_rmse: 22266.3516\n",
      "Epoch 86/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16049.7686 - rmse: 16049.7686\n",
      "Epoch 86: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15898.5332 - rmse: 15886.2246 - val_loss: 20981.3867 - val_rmse: 20979.5605\n",
      "Epoch 87/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16472.4629 - rmse: 16472.4629\n",
      "Epoch 87: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16333.2324 - rmse: 16301.1729 - val_loss: 19869.7676 - val_rmse: 19877.3477\n",
      "Epoch 88/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16432.3750 - rmse: 16469.6797\n",
      "Epoch 88: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16432.3750 - rmse: 16469.6797 - val_loss: 20261.6680 - val_rmse: 20241.0469\n",
      "Epoch 89/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16241.4316 - rmse: 16207.3057\n",
      "Epoch 89: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16241.4316 - rmse: 16207.3057 - val_loss: 20511.5176 - val_rmse: 20539.3945\n",
      "Epoch 90/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16030.5039 - rmse: 16030.5039\n",
      "Epoch 90: val_loss did not improve from 19776.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15937.3965 - rmse: 15984.1572 - val_loss: 22587.3145 - val_rmse: 22633.8281\n",
      "Epoch 91/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16583.4512 - rmse: 16583.4512\n",
      "Epoch 91: val_loss improved from 19776.88477 to 19724.27539, saving model to ./ckpt/reg_lr005/val_rmse_19717.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16504.8691 - rmse: 16516.9883 - val_loss: 19724.2754 - val_rmse: 19716.5332\n",
      "Epoch 92/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15656.5020 - rmse: 15656.5020\n",
      "Epoch 92: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15589.1895 - rmse: 15588.4854 - val_loss: 20135.0723 - val_rmse: 20113.0645\n",
      "Epoch 93/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18150.3105 - rmse: 18200.6172\n",
      "Epoch 93: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18150.3105 - rmse: 18200.6172 - val_loss: 29675.6641 - val_rmse: 29703.7227\n",
      "Epoch 94/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16431.8672 - rmse: 16431.8672\n",
      "Epoch 94: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16653.3809 - rmse: 16723.0234 - val_loss: 33369.2891 - val_rmse: 33367.8555\n",
      "Epoch 95/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16938.8828 - rmse: 16938.8828\n",
      "Epoch 95: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16837.1836 - rmse: 16871.2402 - val_loss: 21095.3574 - val_rmse: 21104.1230\n",
      "Epoch 96/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16020.0049 - rmse: 16020.0049\n",
      "Epoch 96: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16111.0596 - rmse: 16157.5234 - val_loss: 20915.4766 - val_rmse: 20907.1484\n",
      "Epoch 97/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15062.2441 - rmse: 15062.2441\n",
      "Epoch 97: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15119.4473 - rmse: 15153.7461 - val_loss: 20876.5293 - val_rmse: 20881.3555\n",
      "Epoch 98/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15118.8486 - rmse: 15118.8486\n",
      "Epoch 98: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15182.9609 - rmse: 15218.2607 - val_loss: 23685.1934 - val_rmse: 23696.0762\n",
      "Epoch 99/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15834.5557 - rmse: 15834.5557\n",
      "Epoch 99: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15864.0527 - rmse: 15875.5840 - val_loss: 19884.1523 - val_rmse: 19867.3262\n",
      "Epoch 100/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 15030.3848 - rmse: 15030.3848\n",
      "Epoch 100: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15078.3174 - rmse: 15079.6094 - val_loss: 19756.8652 - val_rmse: 19715.1602\n",
      "Epoch 101/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16467.3027 - rmse: 16467.3027\n",
      "Epoch 101: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16549.7812 - rmse: 16540.3672 - val_loss: 21386.8594 - val_rmse: 21390.0176\n",
      "Epoch 102/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15513.8896 - rmse: 15513.8896\n",
      "Epoch 102: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 15414.1230 - rmse: 15416.2930 - val_loss: 21194.5469 - val_rmse: 21138.9375\n",
      "Epoch 103/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 14960.1143 - rmse: 14960.1143\n",
      "Epoch 103: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15452.5537 - rmse: 15515.8184 - val_loss: 33591.3594 - val_rmse: 33646.1680\n",
      "Epoch 104/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15450.3809 - rmse: 15450.3809\n",
      "Epoch 104: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15491.2100 - rmse: 15566.5283 - val_loss: 19818.8633 - val_rmse: 19762.2930\n",
      "Epoch 105/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16564.2617 - rmse: 16564.2617\n",
      "Epoch 105: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16524.7988 - rmse: 16527.3105 - val_loss: 25550.1836 - val_rmse: 25570.1133\n",
      "Epoch 106/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14504.3428 - rmse: 14487.9121\n",
      "Epoch 106: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 14504.3428 - rmse: 14487.9121 - val_loss: 19986.8301 - val_rmse: 19946.5117\n",
      "Epoch 107/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15987.7559 - rmse: 15993.2041\n",
      "Epoch 107: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15987.7559 - rmse: 15993.2041 - val_loss: 21117.0703 - val_rmse: 21117.6426\n",
      "Epoch 108/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15659.3281 - rmse: 15659.3281\n",
      "Epoch 108: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15662.5459 - rmse: 15664.7148 - val_loss: 23493.7012 - val_rmse: 23518.3066\n",
      "Epoch 109/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 14340.8867 - rmse: 14340.8867\n",
      "Epoch 109: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 14596.6484 - rmse: 14631.0381 - val_loss: 23716.6816 - val_rmse: 23684.2109\n",
      "Epoch 110/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14215.6670 - rmse: 14265.6035\n",
      "Epoch 110: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 14215.6670 - rmse: 14265.6035 - val_loss: 23592.1992 - val_rmse: 23605.1992\n",
      "Epoch 111/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 14815.8857 - rmse: 14815.8857\n",
      "Epoch 111: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 14912.4971 - rmse: 14945.8486 - val_loss: 24772.7871 - val_rmse: 24770.5273\n",
      "Epoch 112/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 14178.3311 - rmse: 14178.3311\n",
      "Epoch 112: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 14212.0703 - rmse: 14200.2773 - val_loss: 21571.4375 - val_rmse: 21534.9746\n",
      "Epoch 113/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15333.4355 - rmse: 15374.1318\n",
      "Epoch 113: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15333.4355 - rmse: 15374.1318 - val_loss: 22074.8613 - val_rmse: 22098.7266\n",
      "Epoch 114/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15813.4453 - rmse: 15813.4453\n",
      "Epoch 114: val_loss did not improve from 19724.27539\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15848.3350 - rmse: 15866.9521 - val_loss: 19812.6602 - val_rmse: 19782.2031\n",
      "Epoch 115/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16045.4453 - rmse: 16045.4453\n",
      "Epoch 115: val_loss improved from 19724.27539 to 19664.66211, saving model to ./ckpt/reg_lr005/val_rmse_19633.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16063.6523 - rmse: 16065.0020 - val_loss: 19664.6621 - val_rmse: 19632.7324\n",
      "Epoch 116/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16053.3418 - rmse: 16053.3418\n",
      "Epoch 116: val_loss did not improve from 19664.66211\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 16148.0576 - rmse: 16170.5342 - val_loss: 27696.8867 - val_rmse: 27719.6934\n",
      "Epoch 117/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15156.4824 - rmse: 15156.4824\n",
      "Epoch 117: val_loss did not improve from 19664.66211\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15103.4619 - rmse: 15091.6611 - val_loss: 26373.4688 - val_rmse: 26361.7754\n",
      "Epoch 118/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 14279.0352 - rmse: 14279.0352\n",
      "Epoch 118: val_loss improved from 19664.66211 to 19637.53320, saving model to ./ckpt/reg_lr005/val_rmse_19570.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 14299.2588 - rmse: 14312.8975 - val_loss: 19637.5332 - val_rmse: 19569.7461\n",
      "Epoch 119/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 14735.0469 - rmse: 14735.0469\n",
      "Epoch 119: val_loss did not improve from 19637.53320\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 15030.5732 - rmse: 15035.2148 - val_loss: 21048.7891 - val_rmse: 20991.8008\n",
      "Epoch 120/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 14776.0361 - rmse: 14776.0361\n",
      "Epoch 120: val_loss did not improve from 19637.53320\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 14813.6348 - rmse: 14815.0732 - val_loss: 22927.5293 - val_rmse: 22937.7754\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:46:02.758519: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 110421.7812 - rmse: 110113.7969\n",
      "Epoch 1: val_loss improved from inf to 70914.51562, saving model to ./ckpt/reg_lr005/val_rmse_70315.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 110421.7812 - rmse: 110113.7969 - val_loss: 70914.5156 - val_rmse: 70314.5312\n",
      "Epoch 2/120\n",
      " 6/70 [=>............................] - ETA: 0s - loss: 60426.5547 - rmse: 60426.5547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:46:03.832698: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 57718.1211 - rmse: 57716.2266\n",
      "Epoch 2: val_loss improved from 70914.51562 to 56452.44922, saving model to ./ckpt/reg_lr005/val_rmse_55864.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 57718.1211 - rmse: 57716.2266 - val_loss: 56452.4492 - val_rmse: 55863.8398\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 46189.8711 - rmse: 46189.8711\n",
      "Epoch 3: val_loss improved from 56452.44922 to 49770.78906, saving model to ./ckpt/reg_lr005/val_rmse_49342.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 46235.4727 - rmse: 46266.2305 - val_loss: 49770.7891 - val_rmse: 49341.9609\n",
      "Epoch 4/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 46078.9453 - rmse: 46078.9453\n",
      "Epoch 4: val_loss did not improve from 49770.78906\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 45417.5547 - rmse: 45419.8047 - val_loss: 51443.5352 - val_rmse: 50902.5508\n",
      "Epoch 5/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 45699.4492 - rmse: 45699.4492\n",
      "Epoch 5: val_loss improved from 49770.78906 to 48971.87109, saving model to ./ckpt/reg_lr005/val_rmse_48431.hdf5\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 45356.8320 - rmse: 45229.7500 - val_loss: 48971.8711 - val_rmse: 48430.8320\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 41813.3242 - rmse: 41813.3242\n",
      "Epoch 6: val_loss improved from 48971.87109 to 47526.37891, saving model to ./ckpt/reg_lr005/val_rmse_46993.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 41800.2109 - rmse: 41977.5391 - val_loss: 47526.3789 - val_rmse: 46992.9258\n",
      "Epoch 7/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 37483.9766 - rmse: 37483.9766\n",
      "Epoch 7: val_loss improved from 47526.37891 to 41997.27734, saving model to ./ckpt/reg_lr005/val_rmse_41517.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 38226.8359 - rmse: 38193.3750 - val_loss: 41997.2773 - val_rmse: 41517.1602\n",
      "Epoch 8/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 37910.7695 - rmse: 37910.7695\n",
      "Epoch 8: val_loss improved from 41997.27734 to 41487.43750, saving model to ./ckpt/reg_lr005/val_rmse_41012.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 38025.5117 - rmse: 38091.6367 - val_loss: 41487.4375 - val_rmse: 41012.4102\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 39708.2812 - rmse: 39829.1250\n",
      "Epoch 9: val_loss improved from 41487.43750 to 40867.82422, saving model to ./ckpt/reg_lr005/val_rmse_40558.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 39708.2812 - rmse: 39829.1250 - val_loss: 40867.8242 - val_rmse: 40558.2656\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 35224.3906 - rmse: 35224.3906\n",
      "Epoch 10: val_loss improved from 40867.82422 to 37552.48438, saving model to ./ckpt/reg_lr005/val_rmse_37211.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 35258.9922 - rmse: 35282.3281 - val_loss: 37552.4844 - val_rmse: 37210.6211\n",
      "Epoch 11/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 35259.5117 - rmse: 35259.5117\n",
      "Epoch 11: val_loss did not improve from 37552.48438\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 35760.9414 - rmse: 35873.7500 - val_loss: 47731.1875 - val_rmse: 47584.0391\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 34723.9180 - rmse: 34817.2500\n",
      "Epoch 12: val_loss improved from 37552.48438 to 37394.71875, saving model to ./ckpt/reg_lr005/val_rmse_37012.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 34723.9180 - rmse: 34817.2500 - val_loss: 37394.7188 - val_rmse: 37011.7695\n",
      "Epoch 13/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 33398.2656 - rmse: 33398.2656\n",
      "Epoch 13: val_loss improved from 37394.71875 to 34968.37109, saving model to ./ckpt/reg_lr005/val_rmse_34618.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 33688.0039 - rmse: 33721.5547 - val_loss: 34968.3711 - val_rmse: 34617.9844\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 31561.4746 - rmse: 31509.1211\n",
      "Epoch 14: val_loss improved from 34968.37109 to 32127.87695, saving model to ./ckpt/reg_lr005/val_rmse_31808.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 31561.4746 - rmse: 31509.1211 - val_loss: 32127.8770 - val_rmse: 31808.0449\n",
      "Epoch 15/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 32019.4023 - rmse: 32019.4023\n",
      "Epoch 15: val_loss did not improve from 32127.87695\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 31859.1582 - rmse: 31880.6465 - val_loss: 44560.5586 - val_rmse: 44259.1641\n",
      "Epoch 16/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 28762.9551 - rmse: 28703.9297\n",
      "Epoch 16: val_loss improved from 32127.87695 to 29687.52344, saving model to ./ckpt/reg_lr005/val_rmse_29460.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 28762.9551 - rmse: 28703.9297 - val_loss: 29687.5234 - val_rmse: 29460.4980\n",
      "Epoch 17/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 27352.9336 - rmse: 27352.9336\n",
      "Epoch 17: val_loss improved from 29687.52344 to 28385.88477, saving model to ./ckpt/reg_lr005/val_rmse_28165.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27604.4395 - rmse: 27611.4473 - val_loss: 28385.8848 - val_rmse: 28164.9551\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27499.6855 - rmse: 27499.6855\n",
      "Epoch 18: val_loss did not improve from 28385.88477\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27513.7949 - rmse: 27519.7344 - val_loss: 29900.6562 - val_rmse: 29650.0820\n",
      "Epoch 19/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 24961.0703 - rmse: 24961.0703\n",
      "Epoch 19: val_loss improved from 28385.88477 to 25688.05859, saving model to ./ckpt/reg_lr005/val_rmse_25483.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 25027.5781 - rmse: 25087.0742 - val_loss: 25688.0586 - val_rmse: 25483.4980\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 26230.3965 - rmse: 26176.7832\n",
      "Epoch 20: val_loss did not improve from 25688.05859\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 26230.3965 - rmse: 26176.7832 - val_loss: 29499.4922 - val_rmse: 29247.1035\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25599.9961 - rmse: 25586.0879\n",
      "Epoch 21: val_loss did not improve from 25688.05859\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 25599.9961 - rmse: 25586.0879 - val_loss: 33885.2617 - val_rmse: 33820.0977\n",
      "Epoch 22/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 24105.7793 - rmse: 24105.7793\n",
      "Epoch 22: val_loss improved from 25688.05859 to 23151.60742, saving model to ./ckpt/reg_lr005/val_rmse_22991.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 24015.9199 - rmse: 23978.7207 - val_loss: 23151.6074 - val_rmse: 22990.5059\n",
      "Epoch 23/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23289.2500 - rmse: 23289.2500\n",
      "Epoch 23: val_loss did not improve from 23151.60742\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23308.7109 - rmse: 23335.0801 - val_loss: 24280.1426 - val_rmse: 24166.3164\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21844.7090 - rmse: 21844.7090\n",
      "Epoch 24: val_loss did not improve from 23151.60742\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 21962.4043 - rmse: 22060.6582 - val_loss: 26055.6953 - val_rmse: 26037.0625\n",
      "Epoch 25/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21662.1914 - rmse: 21662.1914\n",
      "Epoch 25: val_loss did not improve from 23151.60742\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21819.9062 - rmse: 21820.5879 - val_loss: 23953.5762 - val_rmse: 23850.6152\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21257.7227 - rmse: 21257.7227\n",
      "Epoch 26: val_loss improved from 23151.60742 to 22381.45898, saving model to ./ckpt/reg_lr005/val_rmse_22273.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 21293.5352 - rmse: 21317.6855 - val_loss: 22381.4590 - val_rmse: 22272.5977\n",
      "Epoch 27/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21473.7305 - rmse: 21476.8750\n",
      "Epoch 27: val_loss did not improve from 22381.45898\n",
      "70/70 [==============================] - 1s 19ms/step - loss: 21473.7305 - rmse: 21476.8750 - val_loss: 22801.1055 - val_rmse: 22634.1953\n",
      "Epoch 28/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21364.5137 - rmse: 21386.3770\n",
      "Epoch 28: val_loss improved from 22381.45898 to 21552.19141, saving model to ./ckpt/reg_lr005/val_rmse_21466.hdf5\n",
      "70/70 [==============================] - 5s 71ms/step - loss: 21364.5137 - rmse: 21386.3770 - val_loss: 21552.1914 - val_rmse: 21466.1133\n",
      "Epoch 29/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22144.3711 - rmse: 22144.3711\n",
      "Epoch 29: val_loss did not improve from 21552.19141\n",
      "70/70 [==============================] - 4s 61ms/step - loss: 22217.8047 - rmse: 22267.3262 - val_loss: 27175.5195 - val_rmse: 27068.9648\n",
      "Epoch 30/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 22564.7461 - rmse: 22564.7461\n",
      "Epoch 30: val_loss improved from 21552.19141 to 20793.83789, saving model to ./ckpt/reg_lr005/val_rmse_20688.hdf5\n",
      "70/70 [==============================] - 3s 48ms/step - loss: 22568.7344 - rmse: 22596.3848 - val_loss: 20793.8379 - val_rmse: 20688.2539\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22903.6855 - rmse: 22936.8301\n",
      "Epoch 31: val_loss did not improve from 20793.83789\n",
      "70/70 [==============================] - 4s 57ms/step - loss: 22903.6855 - rmse: 22936.8301 - val_loss: 35404.0742 - val_rmse: 35235.6055\n",
      "Epoch 32/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22072.8867 - rmse: 22140.3457\n",
      "Epoch 32: val_loss did not improve from 20793.83789\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 22072.8867 - rmse: 22140.3457 - val_loss: 21226.6621 - val_rmse: 21165.1211\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22925.0781 - rmse: 22936.8750\n",
      "Epoch 33: val_loss did not improve from 20793.83789\n",
      "70/70 [==============================] - 4s 53ms/step - loss: 22925.0781 - rmse: 22936.8750 - val_loss: 20908.3613 - val_rmse: 20852.8574\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20944.4824 - rmse: 20911.0215\n",
      "Epoch 34: val_loss did not improve from 20793.83789\n",
      "70/70 [==============================] - 3s 48ms/step - loss: 20944.4824 - rmse: 20911.0215 - val_loss: 24483.4551 - val_rmse: 24500.8398\n",
      "Epoch 35/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20775.5957 - rmse: 20775.5957\n",
      "Epoch 35: val_loss did not improve from 20793.83789\n",
      "70/70 [==============================] - 4s 57ms/step - loss: 20613.1582 - rmse: 20566.8906 - val_loss: 33237.7383 - val_rmse: 33118.3516\n",
      "Epoch 36/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21223.8008 - rmse: 21223.8008\n",
      "Epoch 36: val_loss did not improve from 20793.83789\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 21182.9492 - rmse: 21155.3965 - val_loss: 24929.5527 - val_rmse: 24829.8262\n",
      "Epoch 37/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19753.0371 - rmse: 19776.9023\n",
      "Epoch 37: val_loss improved from 20793.83789 to 19551.10742, saving model to ./ckpt/reg_lr005/val_rmse_19542.hdf5\n",
      "70/70 [==============================] - 5s 71ms/step - loss: 19753.0371 - rmse: 19776.9023 - val_loss: 19551.1074 - val_rmse: 19541.7422\n",
      "Epoch 38/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19919.0645 - rmse: 19930.3242\n",
      "Epoch 38: val_loss did not improve from 19551.10742\n",
      "70/70 [==============================] - 8s 111ms/step - loss: 19919.0645 - rmse: 19930.3242 - val_loss: 22587.4023 - val_rmse: 22508.7793\n",
      "Epoch 39/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20469.6582 - rmse: 20428.8691\n",
      "Epoch 39: val_loss did not improve from 19551.10742\n",
      "70/70 [==============================] - 7s 92ms/step - loss: 20469.6582 - rmse: 20428.8691 - val_loss: 19984.2324 - val_rmse: 19983.7305\n",
      "Epoch 40/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20065.8398 - rmse: 20080.2832\n",
      "Epoch 40: val_loss did not improve from 19551.10742\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 20065.8398 - rmse: 20080.2832 - val_loss: 20936.2734 - val_rmse: 20846.7383\n",
      "Epoch 41/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19929.3320 - rmse: 19982.1543\n",
      "Epoch 41: val_loss did not improve from 19551.10742\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 19929.3320 - rmse: 19982.1543 - val_loss: 26210.9727 - val_rmse: 26035.6895\n",
      "Epoch 42/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19251.9316 - rmse: 19251.9316\n",
      "Epoch 42: val_loss did not improve from 19551.10742\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 19264.3340 - rmse: 19272.6992 - val_loss: 20693.1426 - val_rmse: 20595.8203\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18796.9551 - rmse: 18796.9551\n",
      "Epoch 43: val_loss did not improve from 19551.10742\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 18782.1465 - rmse: 18772.1582 - val_loss: 26329.1523 - val_rmse: 26195.4297\n",
      "Epoch 44/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19481.5332 - rmse: 19481.5332\n",
      "Epoch 44: val_loss did not improve from 19551.10742\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 19177.7578 - rmse: 19165.2832 - val_loss: 21957.7363 - val_rmse: 21849.4863\n",
      "Epoch 45/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18876.4824 - rmse: 18876.4824\n",
      "Epoch 45: val_loss did not improve from 19551.10742\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18921.2480 - rmse: 18951.4414 - val_loss: 24135.1992 - val_rmse: 24053.5352\n",
      "Epoch 46/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18667.3125 - rmse: 18667.3125\n",
      "Epoch 46: val_loss did not improve from 19551.10742\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 18610.3633 - rmse: 18583.8848 - val_loss: 23257.4512 - val_rmse: 23134.3164\n",
      "Epoch 47/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18548.4902 - rmse: 18548.4902\n",
      "Epoch 47: val_loss did not improve from 19551.10742\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 18604.7734 - rmse: 18678.1992 - val_loss: 20439.7148 - val_rmse: 20395.5840\n",
      "Epoch 48/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19475.6191 - rmse: 19475.6191\n",
      "Epoch 48: val_loss did not improve from 19551.10742\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 19426.9414 - rmse: 19447.4746 - val_loss: 25106.3535 - val_rmse: 25125.5156\n",
      "Epoch 49/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20648.0742 - rmse: 20648.0742\n",
      "Epoch 49: val_loss did not improve from 19551.10742\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20960.3691 - rmse: 21005.0195 - val_loss: 25804.5508 - val_rmse: 25882.3984\n",
      "Epoch 50/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20200.5078 - rmse: 20200.5078\n",
      "Epoch 50: val_loss did not improve from 19551.10742\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19923.0000 - rmse: 19891.2324 - val_loss: 19884.8262 - val_rmse: 19825.4121\n",
      "Epoch 51/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18313.3418 - rmse: 18313.3418\n",
      "Epoch 51: val_loss improved from 19551.10742 to 18746.05469, saving model to ./ckpt/reg_lr005/val_rmse_18707.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18391.7695 - rmse: 18444.6621 - val_loss: 18746.0547 - val_rmse: 18706.9219\n",
      "Epoch 52/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19252.6113 - rmse: 19252.6113\n",
      "Epoch 52: val_loss did not improve from 18746.05469\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19423.9785 - rmse: 19556.4414 - val_loss: 21049.4023 - val_rmse: 20953.8535\n",
      "Epoch 53/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17729.2871 - rmse: 17705.1582\n",
      "Epoch 53: val_loss did not improve from 18746.05469\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17729.2871 - rmse: 17705.1582 - val_loss: 20388.2969 - val_rmse: 20284.4219\n",
      "Epoch 54/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17502.3711 - rmse: 17502.3711\n",
      "Epoch 54: val_loss did not improve from 18746.05469\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17557.5703 - rmse: 17530.2773 - val_loss: 19956.1758 - val_rmse: 19931.0137\n",
      "Epoch 55/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18028.0215 - rmse: 18028.0215\n",
      "Epoch 55: val_loss did not improve from 18746.05469\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 17849.0449 - rmse: 17816.6152 - val_loss: 19659.3535 - val_rmse: 19607.3750\n",
      "Epoch 56/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18429.0059 - rmse: 18429.0059\n",
      "Epoch 56: val_loss did not improve from 18746.05469\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18335.4043 - rmse: 18316.4961 - val_loss: 20873.6309 - val_rmse: 20773.7461\n",
      "Epoch 57/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19790.3027 - rmse: 19790.3027\n",
      "Epoch 57: val_loss did not improve from 18746.05469\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19717.0918 - rmse: 19714.0547 - val_loss: 24258.7832 - val_rmse: 24253.9160\n",
      "Epoch 58/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19274.8418 - rmse: 19274.8418\n",
      "Epoch 58: val_loss did not improve from 18746.05469\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 19262.7520 - rmse: 19254.5957 - val_loss: 22638.2578 - val_rmse: 22517.5684\n",
      "Epoch 59/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18754.0195 - rmse: 18754.0195\n",
      "Epoch 59: val_loss improved from 18746.05469 to 18065.15430, saving model to ./ckpt/reg_lr005/val_rmse_18055.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18715.4863 - rmse: 18714.0703 - val_loss: 18065.1543 - val_rmse: 18054.5918\n",
      "Epoch 60/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19634.1367 - rmse: 19634.1367\n",
      "Epoch 60: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19729.0625 - rmse: 19870.6680 - val_loss: 26355.5215 - val_rmse: 26229.1543\n",
      "Epoch 61/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18922.6309 - rmse: 18922.6309\n",
      "Epoch 61: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18794.9141 - rmse: 18758.9902 - val_loss: 20319.0000 - val_rmse: 20256.8359\n",
      "Epoch 62/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17742.9219 - rmse: 17742.9219\n",
      "Epoch 62: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 17823.0176 - rmse: 17896.2578 - val_loss: 28176.5312 - val_rmse: 28044.4062\n",
      "Epoch 63/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17697.9609 - rmse: 17717.2441\n",
      "Epoch 63: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17697.9609 - rmse: 17717.2441 - val_loss: 20066.9414 - val_rmse: 20056.4258\n",
      "Epoch 64/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17691.8125 - rmse: 17691.8125\n",
      "Epoch 64: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17882.4824 - rmse: 17882.7051 - val_loss: 19900.1934 - val_rmse: 19798.9941\n",
      "Epoch 65/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17207.2793 - rmse: 17207.2793\n",
      "Epoch 65: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 17192.0020 - rmse: 17181.6992 - val_loss: 22105.2441 - val_rmse: 22033.4531\n",
      "Epoch 66/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17599.1191 - rmse: 17599.1191\n",
      "Epoch 66: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17472.5469 - rmse: 17488.8301 - val_loss: 18347.8984 - val_rmse: 18289.9043\n",
      "Epoch 67/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16775.1934 - rmse: 16775.1934\n",
      "Epoch 67: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16976.3262 - rmse: 17140.2734 - val_loss: 19616.9570 - val_rmse: 19612.2695\n",
      "Epoch 68/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18350.7070 - rmse: 18350.7070\n",
      "Epoch 68: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18096.3984 - rmse: 18077.7402 - val_loss: 23664.9609 - val_rmse: 23605.4473\n",
      "Epoch 69/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17554.8223 - rmse: 17554.8223\n",
      "Epoch 69: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17803.6602 - rmse: 17828.6211 - val_loss: 33144.1797 - val_rmse: 32999.1875\n",
      "Epoch 70/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18989.7695 - rmse: 18989.7695\n",
      "Epoch 70: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19072.6426 - rmse: 19166.6113 - val_loss: 29274.8555 - val_rmse: 29155.9980\n",
      "Epoch 71/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17339.1172 - rmse: 17339.1172\n",
      "Epoch 71: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17429.0957 - rmse: 17457.7969 - val_loss: 26132.8379 - val_rmse: 26014.1621\n",
      "Epoch 72/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17643.6641 - rmse: 17643.6641\n",
      "Epoch 72: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17832.0098 - rmse: 17871.1602 - val_loss: 19340.1426 - val_rmse: 19337.1172\n",
      "Epoch 73/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16790.4863 - rmse: 16790.4863\n",
      "Epoch 73: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16795.5723 - rmse: 16795.3672 - val_loss: 22499.6270 - val_rmse: 22392.2188\n",
      "Epoch 74/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17985.3242 - rmse: 17985.3242\n",
      "Epoch 74: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17855.7324 - rmse: 17886.6152 - val_loss: 24509.6289 - val_rmse: 24364.5723\n",
      "Epoch 75/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16553.3301 - rmse: 16553.3301\n",
      "Epoch 75: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16378.7920 - rmse: 16370.5967 - val_loss: 19878.4121 - val_rmse: 19807.2422\n",
      "Epoch 76/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16311.8584 - rmse: 16311.8584\n",
      "Epoch 76: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16626.8027 - rmse: 16633.9863 - val_loss: 21454.3066 - val_rmse: 21413.5645\n",
      "Epoch 77/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18207.2910 - rmse: 18207.2910\n",
      "Epoch 77: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18117.8496 - rmse: 18119.4199 - val_loss: 18230.4395 - val_rmse: 18197.0410\n",
      "Epoch 78/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 16010.9922 - rmse: 16010.9922\n",
      "Epoch 78: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16106.4648 - rmse: 16092.4023 - val_loss: 22933.9648 - val_rmse: 22838.3613\n",
      "Epoch 79/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 16979.8457 - rmse: 16979.8457\n",
      "Epoch 79: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16821.6562 - rmse: 16804.7246 - val_loss: 18834.0703 - val_rmse: 18769.0820\n",
      "Epoch 80/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16902.3730 - rmse: 16884.2520\n",
      "Epoch 80: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16902.3730 - rmse: 16884.2520 - val_loss: 24460.9902 - val_rmse: 24347.7832\n",
      "Epoch 81/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15792.8594 - rmse: 15792.8594\n",
      "Epoch 81: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 15810.2959 - rmse: 15805.3057 - val_loss: 19576.3027 - val_rmse: 19502.7617\n",
      "Epoch 82/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16161.3916 - rmse: 16161.3916\n",
      "Epoch 82: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 16263.7275 - rmse: 16283.9199 - val_loss: 24236.1406 - val_rmse: 24098.5176\n",
      "Epoch 83/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17730.4805 - rmse: 17688.8867\n",
      "Epoch 83: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17730.4805 - rmse: 17688.8867 - val_loss: 26487.3633 - val_rmse: 26339.9297\n",
      "Epoch 84/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16540.2812 - rmse: 16621.7109\n",
      "Epoch 84: val_loss did not improve from 18065.15430\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 16540.2812 - rmse: 16621.7109 - val_loss: 19101.0605 - val_rmse: 19026.2344\n",
      "Epoch 84: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:48:00.744962: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 110498.4219 - rmse: 110166.4688\n",
      "Epoch 1: val_loss improved from inf to 78513.38281, saving model to ./ckpt/reg_lr005/val_rmse_78606.hdf5\n",
      "70/70 [==============================] - 2s 14ms/step - loss: 110498.4219 - rmse: 110166.4688 - val_loss: 78513.3828 - val_rmse: 78605.5547\n",
      "Epoch 2/120\n",
      " 1/70 [..............................] - ETA: 0s - loss: 63333.1367 - rmse: 63333.1367"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:48:01.845405: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/70 [============================>.] - ETA: 0s - loss: 58027.6602 - rmse: 58027.6602\n",
      "Epoch 2: val_loss improved from 78513.38281 to 69063.08594, saving model to ./ckpt/reg_lr005/val_rmse_69059.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 58119.1484 - rmse: 58179.2266 - val_loss: 69063.0859 - val_rmse: 69059.0938\n",
      "Epoch 3/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 47603.6016 - rmse: 47603.6016\n",
      "Epoch 3: val_loss improved from 69063.08594 to 55694.24609, saving model to ./ckpt/reg_lr005/val_rmse_55653.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 47702.9570 - rmse: 47951.9180 - val_loss: 55694.2461 - val_rmse: 55652.5078\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 44218.7188 - rmse: 44202.0430\n",
      "Epoch 4: val_loss improved from 55694.24609 to 47907.19141, saving model to ./ckpt/reg_lr005/val_rmse_47791.hdf5\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 44218.7188 - rmse: 44202.0430 - val_loss: 47907.1914 - val_rmse: 47790.8594\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 43430.7070 - rmse: 43388.6602\n",
      "Epoch 5: val_loss improved from 47907.19141 to 44894.50391, saving model to ./ckpt/reg_lr005/val_rmse_44688.hdf5\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 43430.7070 - rmse: 43388.6602 - val_loss: 44894.5039 - val_rmse: 44688.4336\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 39748.1172 - rmse: 39856.7695\n",
      "Epoch 6: val_loss did not improve from 44894.50391\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 39748.1172 - rmse: 39856.7695 - val_loss: 49936.5547 - val_rmse: 49653.4141\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 36963.1445 - rmse: 36896.9961\n",
      "Epoch 7: val_loss improved from 44894.50391 to 40101.35156, saving model to ./ckpt/reg_lr005/val_rmse_39761.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 36963.1445 - rmse: 36896.9961 - val_loss: 40101.3516 - val_rmse: 39761.2383\n",
      "Epoch 8/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 36279.4023 - rmse: 36279.4023\n",
      "Epoch 8: val_loss did not improve from 40101.35156\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 36101.3086 - rmse: 36224.4414 - val_loss: 41273.8711 - val_rmse: 40865.8984\n",
      "Epoch 9/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 38284.2109 - rmse: 38284.2109\n",
      "Epoch 9: val_loss did not improve from 40101.35156\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 38217.8633 - rmse: 38182.5000 - val_loss: 44915.7734 - val_rmse: 44545.9375\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 35351.6250 - rmse: 35340.4727\n",
      "Epoch 10: val_loss did not improve from 40101.35156\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 35351.6250 - rmse: 35340.4727 - val_loss: 43579.7617 - val_rmse: 43166.7148\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 32773.3906 - rmse: 32773.3906\n",
      "Epoch 11: val_loss improved from 40101.35156 to 37828.60547, saving model to ./ckpt/reg_lr005/val_rmse_37420.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 32761.2148 - rmse: 32742.5859 - val_loss: 37828.6055 - val_rmse: 37420.2148\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 34335.8320 - rmse: 34335.8320\n",
      "Epoch 12: val_loss improved from 37828.60547 to 35601.87891, saving model to ./ckpt/reg_lr005/val_rmse_35218.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 34225.0078 - rmse: 34150.2578 - val_loss: 35601.8789 - val_rmse: 35217.5898\n",
      "Epoch 13/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 30515.4277 - rmse: 30515.4277\n",
      "Epoch 13: val_loss improved from 35601.87891 to 32438.76758, saving model to ./ckpt/reg_lr005/val_rmse_32105.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 30527.8965 - rmse: 30536.3066 - val_loss: 32438.7676 - val_rmse: 32104.7480\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 28657.2031 - rmse: 28700.1211\n",
      "Epoch 14: val_loss did not improve from 32438.76758\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 28657.2031 - rmse: 28700.1211 - val_loss: 36856.0703 - val_rmse: 36459.4883\n",
      "Epoch 15/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 29358.7188 - rmse: 29301.0234\n",
      "Epoch 15: val_loss did not improve from 32438.76758\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 29358.7188 - rmse: 29301.0234 - val_loss: 38428.4062 - val_rmse: 38046.2695\n",
      "Epoch 16/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27201.0781 - rmse: 27185.5645\n",
      "Epoch 16: val_loss improved from 32438.76758 to 30692.78125, saving model to ./ckpt/reg_lr005/val_rmse_30310.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 27201.0781 - rmse: 27185.5645 - val_loss: 30692.7812 - val_rmse: 30309.8555\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 27944.0332 - rmse: 27944.0332\n",
      "Epoch 17: val_loss did not improve from 30692.78125\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 28022.8848 - rmse: 28076.0645 - val_loss: 42390.1055 - val_rmse: 42044.9570\n",
      "Epoch 18/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 27865.7461 - rmse: 27865.7461\n",
      "Epoch 18: val_loss improved from 30692.78125 to 30449.62891, saving model to ./ckpt/reg_lr005/val_rmse_30188.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 27968.7773 - rmse: 27996.9883 - val_loss: 30449.6289 - val_rmse: 30187.7559\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23484.4395 - rmse: 23529.0625\n",
      "Epoch 19: val_loss improved from 30449.62891 to 29093.45898, saving model to ./ckpt/reg_lr005/val_rmse_28758.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 23484.4395 - rmse: 23529.0625 - val_loss: 29093.4590 - val_rmse: 28758.0742\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23552.4785 - rmse: 23552.4785\n",
      "Epoch 20: val_loss did not improve from 29093.45898\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23582.1582 - rmse: 23602.1719 - val_loss: 29964.8223 - val_rmse: 29705.2617\n",
      "Epoch 21/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22342.6504 - rmse: 22342.6504\n",
      "Epoch 21: val_loss improved from 29093.45898 to 28011.82617, saving model to ./ckpt/reg_lr005/val_rmse_27649.hdf5\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 22735.6758 - rmse: 22919.2129 - val_loss: 28011.8262 - val_rmse: 27648.7930\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22255.7656 - rmse: 22262.9277\n",
      "Epoch 22: val_loss did not improve from 28011.82617\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 22255.7656 - rmse: 22262.9277 - val_loss: 36702.9336 - val_rmse: 36394.4414\n",
      "Epoch 23/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 24099.5547 - rmse: 24099.5547\n",
      "Epoch 23: val_loss did not improve from 28011.82617\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 23914.2715 - rmse: 23884.7402 - val_loss: 34704.8711 - val_rmse: 34393.9062\n",
      "Epoch 24/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22726.2520 - rmse: 22726.2520\n",
      "Epoch 24: val_loss did not improve from 28011.82617\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22707.4316 - rmse: 22655.4746 - val_loss: 38733.8320 - val_rmse: 38435.3477\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22381.4199 - rmse: 22336.6504\n",
      "Epoch 25: val_loss did not improve from 28011.82617\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 22381.4199 - rmse: 22336.6504 - val_loss: 35728.3281 - val_rmse: 35518.4922\n",
      "Epoch 26/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 25827.7441 - rmse: 25827.7441\n",
      "Epoch 26: val_loss improved from 28011.82617 to 26571.50586, saving model to ./ckpt/reg_lr005/val_rmse_26183.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 25563.5176 - rmse: 25607.5332 - val_loss: 26571.5059 - val_rmse: 26182.8223\n",
      "Epoch 27/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20527.0625 - rmse: 20527.0625\n",
      "Epoch 27: val_loss did not improve from 26571.50586\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20456.5938 - rmse: 20457.4688 - val_loss: 28740.6699 - val_rmse: 28336.7441\n",
      "Epoch 28/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 21298.1172 - rmse: 21298.1172\n",
      "Epoch 28: val_loss improved from 26571.50586 to 26003.16016, saving model to ./ckpt/reg_lr005/val_rmse_25563.hdf5\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 21110.7207 - rmse: 21104.7539 - val_loss: 26003.1602 - val_rmse: 25563.3516\n",
      "Epoch 29/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20932.7480 - rmse: 20932.7480\n",
      "Epoch 29: val_loss did not improve from 26003.16016\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 20892.6445 - rmse: 20892.9863 - val_loss: 32405.5723 - val_rmse: 32064.4824\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20053.3047 - rmse: 20053.3047\n",
      "Epoch 30: val_loss did not improve from 26003.16016\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 20118.6172 - rmse: 20162.6660 - val_loss: 26490.8105 - val_rmse: 26105.1934\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20420.3906 - rmse: 20563.0605\n",
      "Epoch 31: val_loss did not improve from 26003.16016\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20420.3906 - rmse: 20563.0605 - val_loss: 27406.2637 - val_rmse: 27068.5762\n",
      "Epoch 32/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20421.8457 - rmse: 20407.1816\n",
      "Epoch 32: val_loss did not improve from 26003.16016\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20421.8457 - rmse: 20407.1816 - val_loss: 26361.3379 - val_rmse: 26011.5000\n",
      "Epoch 33/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20582.7773 - rmse: 20582.7773\n",
      "Epoch 33: val_loss did not improve from 26003.16016\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20497.8301 - rmse: 20469.4961 - val_loss: 26933.0254 - val_rmse: 26487.0684\n",
      "Epoch 34/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18772.7305 - rmse: 18772.7305\n",
      "Epoch 34: val_loss did not improve from 26003.16016\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18808.2930 - rmse: 18832.2793 - val_loss: 26036.5820 - val_rmse: 25675.4961\n",
      "Epoch 35/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19102.3867 - rmse: 19102.3867\n",
      "Epoch 35: val_loss did not improve from 26003.16016\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19435.3535 - rmse: 19454.9082 - val_loss: 38261.8398 - val_rmse: 37979.0703\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20344.5977 - rmse: 20338.8711\n",
      "Epoch 36: val_loss improved from 26003.16016 to 24607.75977, saving model to ./ckpt/reg_lr005/val_rmse_24215.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20344.5977 - rmse: 20338.8711 - val_loss: 24607.7598 - val_rmse: 24215.1797\n",
      "Epoch 37/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19592.6562 - rmse: 19592.6562\n",
      "Epoch 37: val_loss did not improve from 24607.75977\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19605.7480 - rmse: 19614.5762 - val_loss: 27279.7773 - val_rmse: 26960.7656\n",
      "Epoch 38/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20635.1719 - rmse: 20635.1719\n",
      "Epoch 38: val_loss did not improve from 24607.75977\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20500.9980 - rmse: 20511.9785 - val_loss: 27604.7148 - val_rmse: 27266.0430\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19283.8828 - rmse: 19283.8828\n",
      "Epoch 39: val_loss did not improve from 24607.75977\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 19321.2109 - rmse: 19346.3848 - val_loss: 24978.1465 - val_rmse: 24603.6836\n",
      "Epoch 40/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20242.3711 - rmse: 20242.3711\n",
      "Epoch 40: val_loss did not improve from 24607.75977\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 20373.7012 - rmse: 20404.7754 - val_loss: 33417.6719 - val_rmse: 33145.8438\n",
      "Epoch 41/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17573.8184 - rmse: 17573.8184\n",
      "Epoch 41: val_loss did not improve from 24607.75977\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17787.6465 - rmse: 17846.7793 - val_loss: 24811.4727 - val_rmse: 24439.4492\n",
      "Epoch 42/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 19228.1953 - rmse: 19228.1953\n",
      "Epoch 42: val_loss improved from 24607.75977 to 24306.65820, saving model to ./ckpt/reg_lr005/val_rmse_23878.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 19249.8301 - rmse: 19238.4004 - val_loss: 24306.6582 - val_rmse: 23878.0703\n",
      "Epoch 43/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17518.7129 - rmse: 17518.7129\n",
      "Epoch 43: val_loss improved from 24306.65820 to 24223.08594, saving model to ./ckpt/reg_lr005/val_rmse_23847.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17739.7500 - rmse: 17799.5957 - val_loss: 24223.0859 - val_rmse: 23847.4648\n",
      "Epoch 44/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 18805.7578 - rmse: 18805.7578\n",
      "Epoch 44: val_loss did not improve from 24223.08594\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18764.3105 - rmse: 18736.6484 - val_loss: 27383.4219 - val_rmse: 27021.0703\n",
      "Epoch 45/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 18608.3340 - rmse: 18608.3340\n",
      "Epoch 45: val_loss did not improve from 24223.08594\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18528.7012 - rmse: 18523.6074 - val_loss: 27028.0820 - val_rmse: 26716.4609\n",
      "Epoch 46/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 21041.8691 - rmse: 21041.8691\n",
      "Epoch 46: val_loss did not improve from 24223.08594\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 20700.2109 - rmse: 20679.4707 - val_loss: 24510.3750 - val_rmse: 24127.5312\n",
      "Epoch 47/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18397.0254 - rmse: 18397.0254\n",
      "Epoch 47: val_loss did not improve from 24223.08594\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 18339.9180 - rmse: 18378.7539 - val_loss: 32319.2207 - val_rmse: 32012.6836\n",
      "Epoch 48/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17561.3125 - rmse: 17561.3125\n",
      "Epoch 48: val_loss improved from 24223.08594 to 23867.37891, saving model to ./ckpt/reg_lr005/val_rmse_23460.hdf5\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17512.4863 - rmse: 17489.1523 - val_loss: 23867.3789 - val_rmse: 23459.5176\n",
      "Epoch 49/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18384.9062 - rmse: 18384.9062\n",
      "Epoch 49: val_loss did not improve from 23867.37891\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 18342.8555 - rmse: 18353.0391 - val_loss: 24199.2734 - val_rmse: 23772.3086\n",
      "Epoch 50/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17896.9922 - rmse: 17914.3906\n",
      "Epoch 50: val_loss did not improve from 23867.37891\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 17896.9922 - rmse: 17914.3906 - val_loss: 25189.8477 - val_rmse: 24801.4922\n",
      "Epoch 51/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17170.3906 - rmse: 17201.2539\n",
      "Epoch 51: val_loss improved from 23867.37891 to 23590.49414, saving model to ./ckpt/reg_lr005/val_rmse_23205.hdf5\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 17170.3906 - rmse: 17201.2539 - val_loss: 23590.4941 - val_rmse: 23205.3867\n",
      "Epoch 52/120\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 17103.7734 - rmse: 17103.7734\n",
      "Epoch 52: val_loss did not improve from 23590.49414\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 17076.9512 - rmse: 17071.6992 - val_loss: 30326.4766 - val_rmse: 30097.4492\n",
      "Epoch 53/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17816.9902 - rmse: 17878.0566\n",
      "Epoch 53: val_loss did not improve from 23590.49414\n",
      "70/70 [==============================] - 2s 36ms/step - loss: 17816.9902 - rmse: 17878.0566 - val_loss: 24136.9766 - val_rmse: 23783.1328\n",
      "Epoch 54/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17506.1738 - rmse: 17506.1738\n",
      "Epoch 54: val_loss did not improve from 23590.49414\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 17683.8047 - rmse: 17745.1211 - val_loss: 24151.9551 - val_rmse: 23710.8652\n",
      "Epoch 55/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18538.0156 - rmse: 18561.6992\n",
      "Epoch 55: val_loss did not improve from 23590.49414\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 18538.0156 - rmse: 18561.6992 - val_loss: 29333.5293 - val_rmse: 29082.5078\n",
      "Epoch 56/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16295.3447 - rmse: 16294.1611\n",
      "Epoch 56: val_loss did not improve from 23590.49414\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 16295.3447 - rmse: 16294.1611 - val_loss: 24409.7832 - val_rmse: 23959.1875\n",
      "Epoch 57/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17159.3555 - rmse: 17159.3555\n",
      "Epoch 57: val_loss did not improve from 23590.49414\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 17213.4746 - rmse: 17213.6992 - val_loss: 25780.8105 - val_rmse: 25460.9219\n",
      "Epoch 58/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18270.1523 - rmse: 18271.7461\n",
      "Epoch 58: val_loss did not improve from 23590.49414\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 18270.1523 - rmse: 18271.7461 - val_loss: 23605.6660 - val_rmse: 23157.6992\n",
      "Epoch 59/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18062.7910 - rmse: 18062.7910\n",
      "Epoch 59: val_loss did not improve from 23590.49414\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 18061.7090 - rmse: 18060.9785 - val_loss: 25181.7305 - val_rmse: 24858.5000\n",
      "Epoch 60/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17494.9727 - rmse: 17494.9727\n",
      "Epoch 60: val_loss did not improve from 23590.49414\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 17474.9902 - rmse: 17480.6797 - val_loss: 26268.1016 - val_rmse: 25949.7266\n",
      "Epoch 61/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16649.8145 - rmse: 16696.1230\n",
      "Epoch 61: val_loss improved from 23590.49414 to 23559.36133, saving model to ./ckpt/reg_lr005/val_rmse_23149.hdf5\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 16649.8145 - rmse: 16696.1230 - val_loss: 23559.3613 - val_rmse: 23149.1484\n",
      "Epoch 62/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16609.6973 - rmse: 16609.6973\n",
      "Epoch 62: val_loss did not improve from 23559.36133\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 16591.3711 - rmse: 16579.0117 - val_loss: 23861.3555 - val_rmse: 23544.6074\n",
      "Epoch 63/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16474.5000 - rmse: 16474.5000\n",
      "Epoch 63: val_loss did not improve from 23559.36133\n",
      "70/70 [==============================] - 2s 23ms/step - loss: 16464.7402 - rmse: 16435.4922 - val_loss: 24910.1230 - val_rmse: 24506.5430\n",
      "Epoch 64/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16662.3711 - rmse: 16648.6191\n",
      "Epoch 64: val_loss improved from 23559.36133 to 23236.95508, saving model to ./ckpt/reg_lr005/val_rmse_22778.hdf5\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 16662.3711 - rmse: 16648.6191 - val_loss: 23236.9551 - val_rmse: 22778.4863\n",
      "Epoch 65/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16855.4434 - rmse: 16860.8867\n",
      "Epoch 65: val_loss did not improve from 23236.95508\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 16855.4434 - rmse: 16860.8867 - val_loss: 25068.6699 - val_rmse: 24675.8984\n",
      "Epoch 66/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18474.8750 - rmse: 18466.6367\n",
      "Epoch 66: val_loss did not improve from 23236.95508\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 18474.8750 - rmse: 18466.6367 - val_loss: 35471.8633 - val_rmse: 35276.4609\n",
      "Epoch 67/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17240.5723 - rmse: 17240.5723\n",
      "Epoch 67: val_loss did not improve from 23236.95508\n",
      "70/70 [==============================] - 1s 21ms/step - loss: 17267.8086 - rmse: 17285.2812 - val_loss: 29935.2441 - val_rmse: 29716.2422\n",
      "Epoch 68/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17786.6621 - rmse: 17786.6621\n",
      "Epoch 68: val_loss did not improve from 23236.95508\n",
      "70/70 [==============================] - 1s 21ms/step - loss: 17740.1387 - rmse: 17723.8906 - val_loss: 24418.6641 - val_rmse: 24014.6406\n",
      "Epoch 69/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16620.4355 - rmse: 16620.4355\n",
      "Epoch 69: val_loss did not improve from 23236.95508\n",
      "70/70 [==============================] - 2s 23ms/step - loss: 16793.2188 - rmse: 16896.9863 - val_loss: 30158.5430 - val_rmse: 29938.4414\n",
      "Epoch 70/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16223.2217 - rmse: 16223.2217\n",
      "Epoch 70: val_loss did not improve from 23236.95508\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 16130.6602 - rmse: 16121.4766 - val_loss: 23290.1836 - val_rmse: 22936.0781\n",
      "Epoch 71/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15344.1787 - rmse: 15344.1787\n",
      "Epoch 71: val_loss did not improve from 23236.95508\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 15322.4219 - rmse: 15349.3662 - val_loss: 27059.0195 - val_rmse: 26819.2949\n",
      "Epoch 72/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15518.0498 - rmse: 15566.0879\n",
      "Epoch 72: val_loss did not improve from 23236.95508\n",
      "70/70 [==============================] - 2s 23ms/step - loss: 15518.0498 - rmse: 15566.0879 - val_loss: 30567.9219 - val_rmse: 30360.2754\n",
      "Epoch 73/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16727.8828 - rmse: 16712.0215\n",
      "Epoch 73: val_loss did not improve from 23236.95508\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 16727.8828 - rmse: 16712.0215 - val_loss: 28655.0801 - val_rmse: 28391.8184\n",
      "Epoch 74/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15906.0557 - rmse: 15906.0557\n",
      "Epoch 74: val_loss did not improve from 23236.95508\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 15889.4502 - rmse: 15907.2822 - val_loss: 24882.8652 - val_rmse: 24618.6523\n",
      "Epoch 75/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15627.7988 - rmse: 15627.7988\n",
      "Epoch 75: val_loss did not improve from 23236.95508\n",
      "70/70 [==============================] - 2s 23ms/step - loss: 15627.6211 - rmse: 15627.5000 - val_loss: 25318.1152 - val_rmse: 25043.8535\n",
      "Epoch 76/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16172.5186 - rmse: 16172.5186\n",
      "Epoch 76: val_loss did not improve from 23236.95508\n",
      "70/70 [==============================] - 2s 23ms/step - loss: 16226.1328 - rmse: 16244.9678 - val_loss: 24924.8516 - val_rmse: 24678.2070\n",
      "Epoch 77/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15533.8555 - rmse: 15531.6592\n",
      "Epoch 77: val_loss did not improve from 23236.95508\n",
      "70/70 [==============================] - 2s 23ms/step - loss: 15533.8555 - rmse: 15531.6592 - val_loss: 25782.9043 - val_rmse: 25559.7812\n",
      "Epoch 78/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15446.4141 - rmse: 15416.5273\n",
      "Epoch 78: val_loss improved from 23236.95508 to 22901.76172, saving model to ./ckpt/reg_lr005/val_rmse_22479.hdf5\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 15446.4141 - rmse: 15416.5273 - val_loss: 22901.7617 - val_rmse: 22478.5605\n",
      "Epoch 79/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15210.8975 - rmse: 15210.8975\n",
      "Epoch 79: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 15227.3740 - rmse: 15238.4844 - val_loss: 35457.2344 - val_rmse: 35304.7500\n",
      "Epoch 80/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15989.9453 - rmse: 15989.9453\n",
      "Epoch 80: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 16047.8174 - rmse: 16154.5732 - val_loss: 24866.9375 - val_rmse: 24604.5078\n",
      "Epoch 81/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17427.9648 - rmse: 17427.9648\n",
      "Epoch 81: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 17491.7285 - rmse: 17534.7344 - val_loss: 24564.4531 - val_rmse: 24238.3047\n",
      "Epoch 82/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16278.2227 - rmse: 16301.4717\n",
      "Epoch 82: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 16278.2227 - rmse: 16301.4717 - val_loss: 26728.4336 - val_rmse: 26460.5781\n",
      "Epoch 83/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 14689.0352 - rmse: 14689.0352\n",
      "Epoch 83: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 14689.9980 - rmse: 14690.6475 - val_loss: 26094.5117 - val_rmse: 25847.6660\n",
      "Epoch 84/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15305.2373 - rmse: 15305.2373\n",
      "Epoch 84: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 22ms/step - loss: 15301.5742 - rmse: 15299.1016 - val_loss: 24774.4531 - val_rmse: 24294.4570\n",
      "Epoch 85/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16892.1289 - rmse: 16890.0098\n",
      "Epoch 85: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 22ms/step - loss: 16892.1289 - rmse: 16890.0098 - val_loss: 23324.4355 - val_rmse: 22957.4941\n",
      "Epoch 86/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16376.3457 - rmse: 16394.8789\n",
      "Epoch 86: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 23ms/step - loss: 16376.3457 - rmse: 16394.8789 - val_loss: 25189.9961 - val_rmse: 24899.7559\n",
      "Epoch 87/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15700.0674 - rmse: 15742.1162\n",
      "Epoch 87: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 15700.0674 - rmse: 15742.1162 - val_loss: 26579.4082 - val_rmse: 26296.3828\n",
      "Epoch 88/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 14537.9043 - rmse: 14537.9043\n",
      "Epoch 88: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 14578.3086 - rmse: 14605.5586 - val_loss: 25273.5723 - val_rmse: 25022.9668\n",
      "Epoch 89/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 14580.9756 - rmse: 14580.9756\n",
      "Epoch 89: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 14604.1885 - rmse: 14619.8428 - val_loss: 23424.6660 - val_rmse: 23072.7637\n",
      "Epoch 90/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16323.2158 - rmse: 16323.2158\n",
      "Epoch 90: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 16380.8701 - rmse: 16419.7539 - val_loss: 33054.5625 - val_rmse: 32861.5977\n",
      "Epoch 91/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 14667.1826 - rmse: 14667.1826\n",
      "Epoch 91: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 14934.0752 - rmse: 15011.8730 - val_loss: 23104.7324 - val_rmse: 22690.3672\n",
      "Epoch 92/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16426.0605 - rmse: 16426.0605\n",
      "Epoch 92: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 16396.0508 - rmse: 16375.8086 - val_loss: 37411.5352 - val_rmse: 37229.5078\n",
      "Epoch 93/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16040.4912 - rmse: 16040.4912\n",
      "Epoch 93: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 22ms/step - loss: 15991.5859 - rmse: 16012.4355 - val_loss: 26654.9023 - val_rmse: 26454.6230\n",
      "Epoch 94/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15088.4736 - rmse: 15088.4736\n",
      "Epoch 94: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 23ms/step - loss: 15070.0557 - rmse: 15057.6318 - val_loss: 26260.4492 - val_rmse: 26034.9219\n",
      "Epoch 95/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 14690.8174 - rmse: 14690.8174\n",
      "Epoch 95: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 14791.6016 - rmse: 14830.0459 - val_loss: 34306.2227 - val_rmse: 34159.7344\n",
      "Epoch 96/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 14592.8086 - rmse: 14592.8086\n",
      "Epoch 96: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 14706.8721 - rmse: 14804.2256 - val_loss: 28940.7129 - val_rmse: 28776.7402\n",
      "Epoch 97/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15146.6572 - rmse: 15146.6572\n",
      "Epoch 97: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 15285.5850 - rmse: 15401.2822 - val_loss: 25530.2051 - val_rmse: 25281.7383\n",
      "Epoch 98/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15159.5420 - rmse: 15187.5928\n",
      "Epoch 98: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 15159.5420 - rmse: 15187.5928 - val_loss: 33394.8789 - val_rmse: 33227.1602\n",
      "Epoch 99/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15013.0166 - rmse: 15003.0391\n",
      "Epoch 99: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 22ms/step - loss: 15013.0166 - rmse: 15003.0391 - val_loss: 24278.9492 - val_rmse: 23976.9570\n",
      "Epoch 100/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 14029.5508 - rmse: 14029.5508\n",
      "Epoch 100: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 23ms/step - loss: 14061.8350 - rmse: 14039.2207 - val_loss: 24237.7051 - val_rmse: 23974.8301\n",
      "Epoch 101/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14244.5840 - rmse: 14237.9531\n",
      "Epoch 101: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 14244.5840 - rmse: 14237.9531 - val_loss: 36505.0430 - val_rmse: 36351.2227\n",
      "Epoch 102/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 14788.6875 - rmse: 14788.6875\n",
      "Epoch 102: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 2s 22ms/step - loss: 14878.7383 - rmse: 14892.6475 - val_loss: 37971.9453 - val_rmse: 37878.5312\n",
      "Epoch 103/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15347.3818 - rmse: 15347.3818\n",
      "Epoch 103: val_loss did not improve from 22901.76172\n",
      "70/70 [==============================] - 1s 21ms/step - loss: 15314.9746 - rmse: 15307.9307 - val_loss: 29281.3535 - val_rmse: 29102.0840\n",
      "Epoch 103: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:50:18.013869: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 109067.1016 - rmse: 108912.1406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:50:20.634234: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 60773.07812, saving model to ./ckpt/reg_lr005/val_rmse_61365.hdf5\n",
      "70/70 [==============================] - 5s 40ms/step - loss: 109067.1016 - rmse: 108912.1406 - val_loss: 60773.0781 - val_rmse: 61364.5938\n",
      "Epoch 2/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 57362.0469 - rmse: 57362.0469\n",
      "Epoch 2: val_loss improved from 60773.07812 to 49440.53906, saving model to ./ckpt/reg_lr005/val_rmse_50092.hdf5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 57541.8164 - rmse: 57648.1328 - val_loss: 49440.5391 - val_rmse: 50092.3594\n",
      "Epoch 3/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 47266.0352 - rmse: 47191.1914\n",
      "Epoch 3: val_loss improved from 49440.53906 to 42367.37109, saving model to ./ckpt/reg_lr005/val_rmse_42659.hdf5\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 47266.0352 - rmse: 47191.1914 - val_loss: 42367.3711 - val_rmse: 42658.5703\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 43791.6484 - rmse: 43791.6484\n",
      "Epoch 4: val_loss improved from 42367.37109 to 39816.83203, saving model to ./ckpt/reg_lr005/val_rmse_40051.hdf5\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 43780.4414 - rmse: 43773.8125 - val_loss: 39816.8320 - val_rmse: 40050.5898\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 43472.5938 - rmse: 43648.0664\n",
      "Epoch 5: val_loss improved from 39816.83203 to 37161.17969, saving model to ./ckpt/reg_lr005/val_rmse_37390.hdf5\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 43472.5938 - rmse: 43648.0664 - val_loss: 37161.1797 - val_rmse: 37389.9570\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 38900.6914 - rmse: 38900.6914\n",
      "Epoch 6: val_loss did not improve from 37161.17969\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 39460.0781 - rmse: 39790.9102 - val_loss: 37751.5664 - val_rmse: 37905.6875\n",
      "Epoch 7/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 38566.9727 - rmse: 38566.9727\n",
      "Epoch 7: val_loss improved from 37161.17969 to 33187.50781, saving model to ./ckpt/reg_lr005/val_rmse_33392.hdf5\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 38864.2383 - rmse: 39040.0508 - val_loss: 33187.5078 - val_rmse: 33391.9609\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 37243.9492 - rmse: 37243.9492\n",
      "Epoch 8: val_loss improved from 33187.50781 to 31611.59766, saving model to ./ckpt/reg_lr005/val_rmse_31803.hdf5\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 37154.0781 - rmse: 37100.9297 - val_loss: 31611.5977 - val_rmse: 31803.4922\n",
      "Epoch 9/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 35314.2578 - rmse: 35314.2578\n",
      "Epoch 9: val_loss improved from 31611.59766 to 30056.76172, saving model to ./ckpt/reg_lr005/val_rmse_30236.hdf5\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 35591.9648 - rmse: 35582.3125 - val_loss: 30056.7617 - val_rmse: 30235.5898\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 34360.4336 - rmse: 34426.3750\n",
      "Epoch 10: val_loss did not improve from 30056.76172\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 34360.4336 - rmse: 34426.3750 - val_loss: 36858.0391 - val_rmse: 37051.9453\n",
      "Epoch 11/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 32469.4629 - rmse: 32394.7207\n",
      "Epoch 11: val_loss improved from 30056.76172 to 28325.43945, saving model to ./ckpt/reg_lr005/val_rmse_28524.hdf5\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 32469.4629 - rmse: 32394.7207 - val_loss: 28325.4395 - val_rmse: 28524.4160\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 33992.3398 - rmse: 33992.3398\n",
      "Epoch 12: val_loss did not improve from 28325.43945\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 34000.9961 - rmse: 34006.1133 - val_loss: 31206.1719 - val_rmse: 31395.2148\n",
      "Epoch 13/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 30571.6543 - rmse: 30571.6543\n",
      "Epoch 13: val_loss did not improve from 28325.43945\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 30460.8164 - rmse: 30404.6641 - val_loss: 40634.5898 - val_rmse: 40870.0117\n",
      "Epoch 14/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 30547.2441 - rmse: 30547.2441\n",
      "Epoch 14: val_loss did not improve from 28325.43945\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 30459.2246 - rmse: 30457.5723 - val_loss: 48881.6484 - val_rmse: 49235.5898\n",
      "Epoch 15/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 29240.4570 - rmse: 29401.3535\n",
      "Epoch 15: val_loss improved from 28325.43945 to 26040.30273, saving model to ./ckpt/reg_lr005/val_rmse_26171.hdf5\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 29240.4570 - rmse: 29401.3535 - val_loss: 26040.3027 - val_rmse: 26171.1836\n",
      "Epoch 16/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 27741.2910 - rmse: 27741.2910\n",
      "Epoch 16: val_loss improved from 26040.30273 to 23246.61328, saving model to ./ckpt/reg_lr005/val_rmse_23465.hdf5\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 27543.4277 - rmse: 27503.3223 - val_loss: 23246.6133 - val_rmse: 23465.4980\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27065.3066 - rmse: 27042.3242\n",
      "Epoch 17: val_loss improved from 23246.61328 to 21824.89453, saving model to ./ckpt/reg_lr005/val_rmse_22006.hdf5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 27065.3066 - rmse: 27042.3242 - val_loss: 21824.8945 - val_rmse: 22006.2441\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25434.8906 - rmse: 25388.1582\n",
      "Epoch 18: val_loss did not improve from 21824.89453\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 25434.8906 - rmse: 25388.1582 - val_loss: 22370.0254 - val_rmse: 22589.4688\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24600.0020 - rmse: 24638.9316\n",
      "Epoch 19: val_loss improved from 21824.89453 to 19728.88281, saving model to ./ckpt/reg_lr005/val_rmse_19803.hdf5\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 24600.0020 - rmse: 24638.9316 - val_loss: 19728.8828 - val_rmse: 19803.0449\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23872.6113 - rmse: 23835.6855\n",
      "Epoch 20: val_loss did not improve from 19728.88281\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 23872.6113 - rmse: 23835.6855 - val_loss: 20175.2227 - val_rmse: 20210.7090\n",
      "Epoch 21/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 25572.0020 - rmse: 25572.0020\n",
      "Epoch 21: val_loss did not improve from 19728.88281\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 25538.3516 - rmse: 25571.9316 - val_loss: 26769.7207 - val_rmse: 26929.3867\n",
      "Epoch 22/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23550.0176 - rmse: 23550.0176\n",
      "Epoch 22: val_loss did not improve from 19728.88281\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 23687.1934 - rmse: 23748.5039 - val_loss: 23212.8984 - val_rmse: 23361.1895\n",
      "Epoch 23/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22266.7656 - rmse: 22281.7773\n",
      "Epoch 23: val_loss improved from 19728.88281 to 19435.25977, saving model to ./ckpt/reg_lr005/val_rmse_19398.hdf5\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 22266.7656 - rmse: 22281.7773 - val_loss: 19435.2598 - val_rmse: 19397.9375\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23726.3301 - rmse: 23726.3301\n",
      "Epoch 24: val_loss did not improve from 19435.25977\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 23887.1562 - rmse: 23982.2734 - val_loss: 26105.5918 - val_rmse: 26162.3652\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24950.9199 - rmse: 24929.8672\n",
      "Epoch 25: val_loss improved from 19435.25977 to 17704.76562, saving model to ./ckpt/reg_lr005/val_rmse_17734.hdf5\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 24950.9199 - rmse: 24929.8672 - val_loss: 17704.7656 - val_rmse: 17733.9590\n",
      "Epoch 26/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20709.3848 - rmse: 20709.3848\n",
      "Epoch 26: val_loss did not improve from 17704.76562\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 20694.5312 - rmse: 20701.7617 - val_loss: 19390.8965 - val_rmse: 19427.3438\n",
      "Epoch 27/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23263.3340 - rmse: 23263.3340\n",
      "Epoch 27: val_loss improved from 17704.76562 to 17523.36523, saving model to ./ckpt/reg_lr005/val_rmse_17554.hdf5\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 23092.6055 - rmse: 23091.4707 - val_loss: 17523.3652 - val_rmse: 17553.5352\n",
      "Epoch 28/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21463.5293 - rmse: 21434.4492\n",
      "Epoch 28: val_loss did not improve from 17523.36523\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 21463.5293 - rmse: 21434.4492 - val_loss: 20235.2148 - val_rmse: 20376.6719\n",
      "Epoch 29/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23185.8398 - rmse: 23193.9043\n",
      "Epoch 29: val_loss did not improve from 17523.36523\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 23185.8398 - rmse: 23193.9043 - val_loss: 17942.1328 - val_rmse: 17987.0742\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20786.5508 - rmse: 20786.5508\n",
      "Epoch 30: val_loss improved from 17523.36523 to 17049.41406, saving model to ./ckpt/reg_lr005/val_rmse_17045.hdf5\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 20807.6152 - rmse: 20820.0723 - val_loss: 17049.4141 - val_rmse: 17045.4277\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21313.4980 - rmse: 21385.2363\n",
      "Epoch 31: val_loss did not improve from 17049.41406\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 21313.4980 - rmse: 21385.2363 - val_loss: 17747.3242 - val_rmse: 17768.5781\n",
      "Epoch 32/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22382.6992 - rmse: 22395.7871\n",
      "Epoch 32: val_loss did not improve from 17049.41406\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 22382.6992 - rmse: 22395.7871 - val_loss: 17957.8789 - val_rmse: 17940.1797\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21028.9434 - rmse: 21110.5781\n",
      "Epoch 33: val_loss did not improve from 17049.41406\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 21028.9434 - rmse: 21110.5781 - val_loss: 25696.9785 - val_rmse: 25721.8867\n",
      "Epoch 34/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21535.2988 - rmse: 21535.2988\n",
      "Epoch 34: val_loss did not improve from 17049.41406\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 21531.5039 - rmse: 21529.2578 - val_loss: 17080.5449 - val_rmse: 17059.7949\n",
      "Epoch 35/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19841.1699 - rmse: 19848.5352\n",
      "Epoch 35: val_loss did not improve from 17049.41406\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 19841.1699 - rmse: 19848.5352 - val_loss: 20780.4688 - val_rmse: 20820.0742\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19751.3555 - rmse: 19892.2344\n",
      "Epoch 36: val_loss improved from 17049.41406 to 17017.40820, saving model to ./ckpt/reg_lr005/val_rmse_17003.hdf5\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 19751.3555 - rmse: 19892.2344 - val_loss: 17017.4082 - val_rmse: 17003.1191\n",
      "Epoch 37/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19661.1055 - rmse: 19661.1055\n",
      "Epoch 37: val_loss improved from 17017.40820 to 16790.62500, saving model to ./ckpt/reg_lr005/val_rmse_16811.hdf5\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 19660.0586 - rmse: 19659.4375 - val_loss: 16790.6250 - val_rmse: 16810.7266\n",
      "Epoch 38/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20688.7832 - rmse: 20688.7832\n",
      "Epoch 38: val_loss improved from 16790.62500 to 16345.96387, saving model to ./ckpt/reg_lr005/val_rmse_16357.hdf5\n",
      "70/70 [==============================] - 3s 47ms/step - loss: 20682.0430 - rmse: 20704.1152 - val_loss: 16345.9639 - val_rmse: 16356.6504\n",
      "Epoch 39/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21364.5195 - rmse: 21364.5195\n",
      "Epoch 39: val_loss did not improve from 16345.96387\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 21259.9238 - rmse: 21254.4434 - val_loss: 18118.5996 - val_rmse: 18093.8164\n",
      "Epoch 40/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18624.8867 - rmse: 18624.8867\n",
      "Epoch 40: val_loss did not improve from 16345.96387\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 18652.3867 - rmse: 18638.3340 - val_loss: 16622.2734 - val_rmse: 16565.6328\n",
      "Epoch 41/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18977.7656 - rmse: 18976.1641\n",
      "Epoch 41: val_loss did not improve from 16345.96387\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 18977.7656 - rmse: 18976.1641 - val_loss: 18392.3594 - val_rmse: 18353.5977\n",
      "Epoch 42/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19741.7793 - rmse: 19722.2383\n",
      "Epoch 42: val_loss did not improve from 16345.96387\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 19741.7793 - rmse: 19722.2383 - val_loss: 28946.0566 - val_rmse: 29108.9707\n",
      "Epoch 43/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 21244.6758 - rmse: 21244.6758\n",
      "Epoch 43: val_loss did not improve from 16345.96387\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 21307.5332 - rmse: 21339.8672 - val_loss: 19172.2734 - val_rmse: 19226.3809\n",
      "Epoch 44/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21267.5762 - rmse: 21256.1699\n",
      "Epoch 44: val_loss did not improve from 16345.96387\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 21267.5762 - rmse: 21256.1699 - val_loss: 17555.2559 - val_rmse: 17562.1133\n",
      "Epoch 45/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20406.8418 - rmse: 20406.8418\n",
      "Epoch 45: val_loss did not improve from 16345.96387\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 20327.1934 - rmse: 20327.8477 - val_loss: 18923.3223 - val_rmse: 18932.7988\n",
      "Epoch 46/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19695.1309 - rmse: 19678.2891\n",
      "Epoch 46: val_loss did not improve from 16345.96387\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 19695.1309 - rmse: 19678.2891 - val_loss: 16374.4600 - val_rmse: 16344.2178\n",
      "Epoch 47/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18928.0605 - rmse: 18928.0605\n",
      "Epoch 47: val_loss did not improve from 16345.96387\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 18889.0293 - rmse: 18876.0078 - val_loss: 17173.7578 - val_rmse: 17142.9375\n",
      "Epoch 48/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19278.2441 - rmse: 19348.9395\n",
      "Epoch 48: val_loss did not improve from 16345.96387\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 19278.2441 - rmse: 19348.9395 - val_loss: 17341.5547 - val_rmse: 17298.6328\n",
      "Epoch 49/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18464.3477 - rmse: 18464.3477\n",
      "Epoch 49: val_loss did not improve from 16345.96387\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 18490.0605 - rmse: 18505.2676 - val_loss: 16610.1426 - val_rmse: 16505.4375\n",
      "Epoch 50/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17283.2383 - rmse: 17311.9219\n",
      "Epoch 50: val_loss did not improve from 16345.96387\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 17283.2383 - rmse: 17311.9219 - val_loss: 17161.5938 - val_rmse: 17102.5488\n",
      "Epoch 51/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 20464.3164 - rmse: 20464.3164\n",
      "Epoch 51: val_loss did not improve from 16345.96387\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 20687.8281 - rmse: 20669.5176 - val_loss: 38866.7266 - val_rmse: 38944.9258\n",
      "Epoch 52/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21571.6367 - rmse: 21590.7109\n",
      "Epoch 52: val_loss improved from 16345.96387 to 15807.91406, saving model to ./ckpt/reg_lr005/val_rmse_15787.hdf5\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 21571.6367 - rmse: 21590.7109 - val_loss: 15807.9141 - val_rmse: 15786.9141\n",
      "Epoch 53/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18400.2891 - rmse: 18414.8496\n",
      "Epoch 53: val_loss did not improve from 15807.91406\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 18400.2891 - rmse: 18414.8496 - val_loss: 16682.8516 - val_rmse: 16656.3320\n",
      "Epoch 54/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18418.8906 - rmse: 18459.8457\n",
      "Epoch 54: val_loss did not improve from 15807.91406\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 18418.8906 - rmse: 18459.8457 - val_loss: 16007.8564 - val_rmse: 15982.8535\n",
      "Epoch 55/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18829.5195 - rmse: 18829.5195\n",
      "Epoch 55: val_loss improved from 15807.91406 to 15707.10645, saving model to ./ckpt/reg_lr005/val_rmse_15686.hdf5\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 18914.5039 - rmse: 18993.2715 - val_loss: 15707.1064 - val_rmse: 15685.5830\n",
      "Epoch 56/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17334.6582 - rmse: 17334.6582\n",
      "Epoch 56: val_loss did not improve from 15707.10645\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 17409.7012 - rmse: 17429.8418 - val_loss: 16622.5586 - val_rmse: 16536.5996\n",
      "Epoch 57/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17590.1094 - rmse: 17590.1094\n",
      "Epoch 57: val_loss improved from 15707.10645 to 15350.49023, saving model to ./ckpt/reg_lr005/val_rmse_15319.hdf5\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 17804.1270 - rmse: 17835.4180 - val_loss: 15350.4902 - val_rmse: 15319.0293\n",
      "Epoch 58/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17548.8164 - rmse: 17548.8164\n",
      "Epoch 58: val_loss did not improve from 15350.49023\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 17566.4199 - rmse: 17576.8301 - val_loss: 31761.2520 - val_rmse: 31830.6758\n",
      "Epoch 59/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20181.4238 - rmse: 20181.4238\n",
      "Epoch 59: val_loss did not improve from 15350.49023\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 20109.4570 - rmse: 20107.6426 - val_loss: 23655.1797 - val_rmse: 23715.1348\n",
      "Epoch 60/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17328.1016 - rmse: 17328.1016\n",
      "Epoch 60: val_loss did not improve from 15350.49023\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 17338.9941 - rmse: 17345.4375 - val_loss: 27694.3672 - val_rmse: 27683.9980\n",
      "Epoch 61/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19132.7793 - rmse: 19114.7090\n",
      "Epoch 61: val_loss did not improve from 15350.49023\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 19132.7793 - rmse: 19114.7090 - val_loss: 17988.6973 - val_rmse: 17919.5156\n",
      "Epoch 62/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17691.5215 - rmse: 17800.8555\n",
      "Epoch 62: val_loss did not improve from 15350.49023\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 17691.5215 - rmse: 17800.8555 - val_loss: 15737.1758 - val_rmse: 15657.7656\n",
      "Epoch 63/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16856.5527 - rmse: 16856.5527\n",
      "Epoch 63: val_loss did not improve from 15350.49023\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 16844.3086 - rmse: 16837.0684 - val_loss: 15712.0996 - val_rmse: 15660.0088\n",
      "Epoch 64/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17576.0176 - rmse: 17576.0176\n",
      "Epoch 64: val_loss did not improve from 15350.49023\n",
      "70/70 [==============================] - 2s 22ms/step - loss: 17576.9629 - rmse: 17579.2891 - val_loss: 15515.2188 - val_rmse: 15451.3301\n",
      "Epoch 65/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17441.6016 - rmse: 17441.6016\n",
      "Epoch 65: val_loss did not improve from 15350.49023\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 17485.5410 - rmse: 17511.5273 - val_loss: 16224.9971 - val_rmse: 16234.2119\n",
      "Epoch 66/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17442.1426 - rmse: 17442.1426\n",
      "Epoch 66: val_loss improved from 15350.49023 to 14990.66797, saving model to ./ckpt/reg_lr005/val_rmse_14940.hdf5\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 17577.5273 - rmse: 17571.7227 - val_loss: 14990.6680 - val_rmse: 14940.1914\n",
      "Epoch 67/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16241.5068 - rmse: 16262.8359\n",
      "Epoch 67: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 16241.5068 - rmse: 16262.8359 - val_loss: 15425.5713 - val_rmse: 15428.6973\n",
      "Epoch 68/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16564.5762 - rmse: 16568.6367\n",
      "Epoch 68: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 16564.5762 - rmse: 16568.6367 - val_loss: 22274.8223 - val_rmse: 22217.8711\n",
      "Epoch 69/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16495.8867 - rmse: 16494.3242\n",
      "Epoch 69: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 16495.8867 - rmse: 16494.3242 - val_loss: 18986.5566 - val_rmse: 19031.3164\n",
      "Epoch 70/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16980.7266 - rmse: 16980.7266\n",
      "Epoch 70: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 17023.4980 - rmse: 17014.7598 - val_loss: 19054.9941 - val_rmse: 18964.7812\n",
      "Epoch 71/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19729.6875 - rmse: 19728.0117\n",
      "Epoch 71: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 19729.6875 - rmse: 19728.0117 - val_loss: 15564.4678 - val_rmse: 15504.9268\n",
      "Epoch 72/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17683.4727 - rmse: 17683.4727\n",
      "Epoch 72: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 17707.7773 - rmse: 17722.1523 - val_loss: 16352.9756 - val_rmse: 16353.4941\n",
      "Epoch 73/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16451.7539 - rmse: 16503.7988\n",
      "Epoch 73: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 16451.7539 - rmse: 16503.7988 - val_loss: 17654.2168 - val_rmse: 17524.5176\n",
      "Epoch 74/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16071.4795 - rmse: 16071.4795\n",
      "Epoch 74: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 16022.3057 - rmse: 16036.5088 - val_loss: 16551.6562 - val_rmse: 16454.3789\n",
      "Epoch 75/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16885.5664 - rmse: 16888.0703\n",
      "Epoch 75: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 16885.5664 - rmse: 16888.0703 - val_loss: 23601.7930 - val_rmse: 23577.5977\n",
      "Epoch 76/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17204.7617 - rmse: 17204.7617\n",
      "Epoch 76: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 17185.9941 - rmse: 17215.4961 - val_loss: 27132.4902 - val_rmse: 27104.2188\n",
      "Epoch 77/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16619.2656 - rmse: 16619.2656\n",
      "Epoch 77: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 16563.2930 - rmse: 16566.5859 - val_loss: 20404.5586 - val_rmse: 20370.3770\n",
      "Epoch 78/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16413.0352 - rmse: 16426.2812\n",
      "Epoch 78: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 16413.0352 - rmse: 16426.2812 - val_loss: 18108.3613 - val_rmse: 18117.1523\n",
      "Epoch 79/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17956.9648 - rmse: 18023.2637\n",
      "Epoch 79: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 17956.9648 - rmse: 18023.2637 - val_loss: 16572.7773 - val_rmse: 16532.9160\n",
      "Epoch 80/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16298.3330 - rmse: 16296.8193\n",
      "Epoch 80: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 16298.3330 - rmse: 16296.8193 - val_loss: 17365.0215 - val_rmse: 17376.1152\n",
      "Epoch 81/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16751.7480 - rmse: 16751.7480\n",
      "Epoch 81: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 16955.1035 - rmse: 17023.2812 - val_loss: 15522.0410 - val_rmse: 15534.5811\n",
      "Epoch 82/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16860.5996 - rmse: 16860.5996\n",
      "Epoch 82: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 16795.9238 - rmse: 16780.1992 - val_loss: 22020.0508 - val_rmse: 22026.2500\n",
      "Epoch 83/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17897.4844 - rmse: 17897.4844\n",
      "Epoch 83: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 18011.4453 - rmse: 18089.8613 - val_loss: 22560.2109 - val_rmse: 22584.0430\n",
      "Epoch 84/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15718.1377 - rmse: 15718.1377\n",
      "Epoch 84: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 15774.2266 - rmse: 15807.4004 - val_loss: 15826.9805 - val_rmse: 15811.0859\n",
      "Epoch 85/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18089.3887 - rmse: 18089.3887\n",
      "Epoch 85: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 18109.3711 - rmse: 18121.1895 - val_loss: 15960.9629 - val_rmse: 15965.2705\n",
      "Epoch 86/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15332.8438 - rmse: 15332.8438\n",
      "Epoch 86: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 15416.5879 - rmse: 15450.2842 - val_loss: 16309.5059 - val_rmse: 16232.0371\n",
      "Epoch 87/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16846.5938 - rmse: 16848.9316\n",
      "Epoch 87: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 16846.5938 - rmse: 16848.9316 - val_loss: 17175.8789 - val_rmse: 17222.1816\n",
      "Epoch 88/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16005.7979 - rmse: 16002.8359\n",
      "Epoch 88: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 16005.7979 - rmse: 16002.8359 - val_loss: 20776.5586 - val_rmse: 20792.9121\n",
      "Epoch 89/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16176.1318 - rmse: 16189.3984\n",
      "Epoch 89: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 16176.1318 - rmse: 16189.3984 - val_loss: 15052.0361 - val_rmse: 15020.6582\n",
      "Epoch 90/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15185.0957 - rmse: 15227.5645\n",
      "Epoch 90: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 15185.0957 - rmse: 15227.5645 - val_loss: 21696.9629 - val_rmse: 21741.1230\n",
      "Epoch 91/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15967.8730 - rmse: 15967.8730\n",
      "Epoch 91: val_loss did not improve from 14990.66797\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 16099.8213 - rmse: 16108.2021 - val_loss: 25082.6035 - val_rmse: 25140.0898\n",
      "Epoch 91: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:53:33.300462: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 110992.3438 - rmse: 110757.8125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:53:35.885895: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 69137.73438, saving model to ./ckpt/reg_lr005/val_rmse_68535.hdf5\n",
      "70/70 [==============================] - 3s 35ms/step - loss: 110992.3438 - rmse: 110757.8125 - val_loss: 69137.7344 - val_rmse: 68534.7422\n",
      "Epoch 2/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 57925.5000 - rmse: 57925.5000\n",
      "Epoch 2: val_loss improved from 69137.73438 to 47267.86719, saving model to ./ckpt/reg_lr005/val_rmse_46607.hdf5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 58264.7422 - rmse: 58465.3711 - val_loss: 47267.8672 - val_rmse: 46606.9023\n",
      "Epoch 3/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 47299.6445 - rmse: 47242.4414\n",
      "Epoch 3: val_loss improved from 47267.86719 to 42581.42969, saving model to ./ckpt/reg_lr005/val_rmse_42128.hdf5\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 47299.6445 - rmse: 47242.4414 - val_loss: 42581.4297 - val_rmse: 42128.0391\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 44883.5859 - rmse: 44883.5859\n",
      "Epoch 4: val_loss improved from 42581.42969 to 39980.85547, saving model to ./ckpt/reg_lr005/val_rmse_39604.hdf5\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 44863.9570 - rmse: 44852.3477 - val_loss: 39980.8555 - val_rmse: 39604.0625\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 42887.7109 - rmse: 42998.2539\n",
      "Epoch 5: val_loss improved from 39980.85547 to 37991.70703, saving model to ./ckpt/reg_lr005/val_rmse_37647.hdf5\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 42887.7109 - rmse: 42998.2539 - val_loss: 37991.7070 - val_rmse: 37647.2578\n",
      "Epoch 6/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 40632.1875 - rmse: 40632.1875\n",
      "Epoch 6: val_loss improved from 37991.70703 to 37755.91406, saving model to ./ckpt/reg_lr005/val_rmse_37552.hdf5\n",
      "70/70 [==============================] - 3s 48ms/step - loss: 40770.1250 - rmse: 40762.3242 - val_loss: 37755.9141 - val_rmse: 37552.2148\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 39259.7266 - rmse: 39337.4805\n",
      "Epoch 7: val_loss improved from 37755.91406 to 34008.11328, saving model to ./ckpt/reg_lr005/val_rmse_33754.hdf5\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 39259.7266 - rmse: 39337.4805 - val_loss: 34008.1133 - val_rmse: 33753.8750\n",
      "Epoch 8/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 37316.4531 - rmse: 37316.4531\n",
      "Epoch 8: val_loss improved from 34008.11328 to 32252.82422, saving model to ./ckpt/reg_lr005/val_rmse_32040.hdf5\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 37437.0781 - rmse: 37412.2812 - val_loss: 32252.8242 - val_rmse: 32039.7070\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 37203.5039 - rmse: 37275.3125\n",
      "Epoch 9: val_loss did not improve from 32252.82422\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 37203.5039 - rmse: 37275.3125 - val_loss: 34931.2734 - val_rmse: 34875.8008\n",
      "Epoch 10/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 33853.6992 - rmse: 33853.6992\n",
      "Epoch 10: val_loss improved from 32252.82422 to 29211.39844, saving model to ./ckpt/reg_lr005/val_rmse_29096.hdf5\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 33727.7578 - rmse: 33681.6836 - val_loss: 29211.3984 - val_rmse: 29096.3164\n",
      "Epoch 11/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 32575.9473 - rmse: 32575.9473\n",
      "Epoch 11: val_loss did not improve from 29211.39844\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 32479.8438 - rmse: 32527.0938 - val_loss: 29484.9316 - val_rmse: 29467.7480\n",
      "Epoch 12/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 31314.0508 - rmse: 31314.0508\n",
      "Epoch 12: val_loss improved from 29211.39844 to 28874.73633, saving model to ./ckpt/reg_lr005/val_rmse_28750.hdf5\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 31281.6484 - rmse: 31301.7109 - val_loss: 28874.7363 - val_rmse: 28750.2109\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 30007.0312 - rmse: 29993.6172\n",
      "Epoch 13: val_loss improved from 28874.73633 to 25775.02734, saving model to ./ckpt/reg_lr005/val_rmse_25788.hdf5\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 30007.0312 - rmse: 29993.6172 - val_loss: 25775.0273 - val_rmse: 25787.7461\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 28705.9062 - rmse: 29020.6250\n",
      "Epoch 14: val_loss did not improve from 25775.02734\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 28705.9062 - rmse: 29020.6250 - val_loss: 26299.0938 - val_rmse: 26230.3320\n",
      "Epoch 15/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 28039.9414 - rmse: 28209.1289\n",
      "Epoch 15: val_loss improved from 25775.02734 to 24204.48242, saving model to ./ckpt/reg_lr005/val_rmse_24190.hdf5\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 28039.9414 - rmse: 28209.1289 - val_loss: 24204.4824 - val_rmse: 24190.1172\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 29138.5293 - rmse: 29138.5293\n",
      "Epoch 16: val_loss did not improve from 24204.48242\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 29077.2383 - rmse: 29040.9863 - val_loss: 34826.2852 - val_rmse: 34682.7500\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 28197.8906 - rmse: 28195.2871\n",
      "Epoch 17: val_loss did not improve from 24204.48242\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 28197.8906 - rmse: 28195.2871 - val_loss: 27275.2188 - val_rmse: 27311.2520\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25473.9746 - rmse: 25455.6035\n",
      "Epoch 18: val_loss improved from 24204.48242 to 20877.64258, saving model to ./ckpt/reg_lr005/val_rmse_20785.hdf5\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 25473.9746 - rmse: 25455.6035 - val_loss: 20877.6426 - val_rmse: 20784.7363\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25035.0332 - rmse: 25000.6699\n",
      "Epoch 19: val_loss did not improve from 20877.64258\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 25035.0332 - rmse: 25000.6699 - val_loss: 29588.1465 - val_rmse: 29485.9844\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 26357.2344 - rmse: 26357.2344\n",
      "Epoch 20: val_loss did not improve from 20877.64258\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 26347.4062 - rmse: 26341.5938 - val_loss: 30286.6152 - val_rmse: 30182.1328\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 23456.5898 - rmse: 23448.9570\n",
      "Epoch 21: val_loss did not improve from 20877.64258\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 23456.5898 - rmse: 23448.9570 - val_loss: 30560.2012 - val_rmse: 30475.5527\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24589.1855 - rmse: 24546.9922\n",
      "Epoch 22: val_loss improved from 20877.64258 to 20244.35156, saving model to ./ckpt/reg_lr005/val_rmse_20106.hdf5\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 24589.1855 - rmse: 24546.9922 - val_loss: 20244.3516 - val_rmse: 20106.0176\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 24917.2441 - rmse: 24917.2441\n",
      "Epoch 23: val_loss did not improve from 20244.35156\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 24915.0176 - rmse: 24913.6992 - val_loss: 30735.6152 - val_rmse: 30610.1562\n",
      "Epoch 24/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23361.7930 - rmse: 23361.7930\n",
      "Epoch 24: val_loss improved from 20244.35156 to 19040.04883, saving model to ./ckpt/reg_lr005/val_rmse_18918.hdf5\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 23234.6426 - rmse: 23238.7129 - val_loss: 19040.0488 - val_rmse: 18918.4062\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22610.6797 - rmse: 22671.9062\n",
      "Epoch 25: val_loss did not improve from 19040.04883\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 22610.6797 - rmse: 22671.9062 - val_loss: 20535.0996 - val_rmse: 20483.8613\n",
      "Epoch 26/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 26871.3965 - rmse: 26871.3965\n",
      "Epoch 26: val_loss did not improve from 19040.04883\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 26824.1953 - rmse: 26796.2754 - val_loss: 20236.1094 - val_rmse: 20087.3301\n",
      "Epoch 27/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21918.6484 - rmse: 21918.6484\n",
      "Epoch 27: val_loss improved from 19040.04883 to 18277.29102, saving model to ./ckpt/reg_lr005/val_rmse_18180.hdf5\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 21906.5371 - rmse: 21899.3730 - val_loss: 18277.2910 - val_rmse: 18180.2363\n",
      "Epoch 28/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21597.5508 - rmse: 21597.5508\n",
      "Epoch 28: val_loss did not improve from 18277.29102\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 21608.3125 - rmse: 21614.6777 - val_loss: 23579.0742 - val_rmse: 23472.3945\n",
      "Epoch 29/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21114.2480 - rmse: 21272.6426\n",
      "Epoch 29: val_loss improved from 18277.29102 to 17805.60547, saving model to ./ckpt/reg_lr005/val_rmse_17704.hdf5\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 21114.2480 - rmse: 21272.6426 - val_loss: 17805.6055 - val_rmse: 17703.7402\n",
      "Epoch 30/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20290.7578 - rmse: 20378.6211\n",
      "Epoch 30: val_loss did not improve from 17805.60547\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 20290.7578 - rmse: 20378.6211 - val_loss: 24726.3301 - val_rmse: 24580.1660\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20839.5293 - rmse: 20841.5059\n",
      "Epoch 31: val_loss did not improve from 17805.60547\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 20839.5293 - rmse: 20841.5059 - val_loss: 19704.9199 - val_rmse: 19573.9414\n",
      "Epoch 32/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19731.8613 - rmse: 19731.8613\n",
      "Epoch 32: val_loss did not improve from 17805.60547\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 19699.4883 - rmse: 19680.3418 - val_loss: 21034.8984 - val_rmse: 20987.0273\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19995.4883 - rmse: 20018.2500\n",
      "Epoch 33: val_loss did not improve from 17805.60547\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 19995.4883 - rmse: 20018.2500 - val_loss: 21315.4648 - val_rmse: 21194.1602\n",
      "Epoch 34/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20846.1660 - rmse: 20846.1660\n",
      "Epoch 34: val_loss did not improve from 17805.60547\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 20837.2324 - rmse: 20831.9492 - val_loss: 18521.3613 - val_rmse: 18437.0645\n",
      "Epoch 35/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19837.2578 - rmse: 19827.0586\n",
      "Epoch 35: val_loss did not improve from 17805.60547\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 19837.2578 - rmse: 19827.0586 - val_loss: 18245.1250 - val_rmse: 18161.5840\n",
      "Epoch 36/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20043.2695 - rmse: 20043.2695\n",
      "Epoch 36: val_loss improved from 17805.60547 to 17208.02344, saving model to ./ckpt/reg_lr005/val_rmse_17094.hdf5\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 20175.0430 - rmse: 20252.9766 - val_loss: 17208.0234 - val_rmse: 17093.5566\n",
      "Epoch 37/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20331.8594 - rmse: 20331.8594\n",
      "Epoch 37: val_loss did not improve from 17208.02344\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 20246.9062 - rmse: 20251.8594 - val_loss: 17330.1855 - val_rmse: 17205.6348\n",
      "Epoch 38/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20168.8008 - rmse: 20144.6992\n",
      "Epoch 38: val_loss did not improve from 17208.02344\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 20168.8008 - rmse: 20144.6992 - val_loss: 17806.2344 - val_rmse: 17682.0234\n",
      "Epoch 39/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18827.5801 - rmse: 18827.5801\n",
      "Epoch 39: val_loss did not improve from 17208.02344\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 19039.5645 - rmse: 19069.5352 - val_loss: 18924.1523 - val_rmse: 18786.1230\n",
      "Epoch 40/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19432.3086 - rmse: 19471.5176\n",
      "Epoch 40: val_loss did not improve from 17208.02344\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 19432.3086 - rmse: 19471.5176 - val_loss: 26829.4062 - val_rmse: 26731.0898\n",
      "Epoch 41/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19855.0273 - rmse: 19860.7910\n",
      "Epoch 41: val_loss did not improve from 17208.02344\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 19855.0273 - rmse: 19860.7910 - val_loss: 17362.8457 - val_rmse: 17236.2598\n",
      "Epoch 42/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19368.9453 - rmse: 19368.9453\n",
      "Epoch 42: val_loss did not improve from 17208.02344\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 19447.3652 - rmse: 19476.8516 - val_loss: 32592.4688 - val_rmse: 32480.5723\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21241.3340 - rmse: 21241.3340\n",
      "Epoch 43: val_loss did not improve from 17208.02344\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 21317.0938 - rmse: 21361.9023 - val_loss: 18182.1973 - val_rmse: 18081.7168\n",
      "Epoch 44/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19135.9707 - rmse: 19135.8789\n",
      "Epoch 44: val_loss improved from 17208.02344 to 17082.71289, saving model to ./ckpt/reg_lr005/val_rmse_16940.hdf5\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 19135.9707 - rmse: 19135.8789 - val_loss: 17082.7129 - val_rmse: 16939.9629\n",
      "Epoch 45/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17965.8926 - rmse: 17965.8926\n",
      "Epoch 45: val_loss did not improve from 17082.71289\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 18046.2676 - rmse: 18093.8027 - val_loss: 17677.6562 - val_rmse: 17544.1387\n",
      "Epoch 46/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19662.2852 - rmse: 19662.2852\n",
      "Epoch 46: val_loss improved from 17082.71289 to 16941.64844, saving model to ./ckpt/reg_lr005/val_rmse_16834.hdf5\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 19610.2812 - rmse: 19588.7715 - val_loss: 16941.6484 - val_rmse: 16834.4121\n",
      "Epoch 47/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18268.8379 - rmse: 18257.8906\n",
      "Epoch 47: val_loss did not improve from 16941.64844\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 18268.8379 - rmse: 18257.8906 - val_loss: 23186.5293 - val_rmse: 23042.4336\n",
      "Epoch 48/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17862.8379 - rmse: 17871.9668\n",
      "Epoch 48: val_loss did not improve from 16941.64844\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 17862.8379 - rmse: 17871.9668 - val_loss: 17516.1035 - val_rmse: 17387.7441\n",
      "Epoch 49/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17901.0840 - rmse: 17895.7871\n",
      "Epoch 49: val_loss did not improve from 16941.64844\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 17901.0840 - rmse: 17895.7871 - val_loss: 20504.5273 - val_rmse: 20331.6016\n",
      "Epoch 50/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17844.6738 - rmse: 17844.6738\n",
      "Epoch 50: val_loss did not improve from 16941.64844\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 17870.1621 - rmse: 17885.2383 - val_loss: 18211.8164 - val_rmse: 18096.1719\n",
      "Epoch 51/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18553.8086 - rmse: 18555.8008\n",
      "Epoch 51: val_loss did not improve from 16941.64844\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 18553.8086 - rmse: 18555.8008 - val_loss: 17824.5762 - val_rmse: 17653.0977\n",
      "Epoch 52/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18075.1270 - rmse: 18130.8438\n",
      "Epoch 52: val_loss did not improve from 16941.64844\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 18075.1270 - rmse: 18130.8438 - val_loss: 22486.7383 - val_rmse: 22430.8281\n",
      "Epoch 53/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19129.6367 - rmse: 19123.0059\n",
      "Epoch 53: val_loss did not improve from 16941.64844\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 19129.6367 - rmse: 19123.0059 - val_loss: 21987.8340 - val_rmse: 21862.6797\n",
      "Epoch 54/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17697.8555 - rmse: 17689.1738\n",
      "Epoch 54: val_loss did not improve from 16941.64844\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 17697.8555 - rmse: 17689.1738 - val_loss: 16996.7285 - val_rmse: 16830.2578\n",
      "Epoch 55/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17332.5215 - rmse: 17332.5215\n",
      "Epoch 55: val_loss did not improve from 16941.64844\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 17403.2773 - rmse: 17445.1230 - val_loss: 18106.4883 - val_rmse: 17967.3203\n",
      "Epoch 56/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19633.0527 - rmse: 19633.0527\n",
      "Epoch 56: val_loss did not improve from 16941.64844\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 19490.0352 - rmse: 19477.9609 - val_loss: 20263.8613 - val_rmse: 20098.8750\n",
      "Epoch 57/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17479.1172 - rmse: 17484.2617\n",
      "Epoch 57: val_loss improved from 16941.64844 to 16822.82422, saving model to ./ckpt/reg_lr005/val_rmse_16652.hdf5\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 17479.1172 - rmse: 17484.2617 - val_loss: 16822.8242 - val_rmse: 16652.0566\n",
      "Epoch 58/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 19469.0918 - rmse: 19469.0918\n",
      "Epoch 58: val_loss did not improve from 16822.82422\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 19501.8887 - rmse: 19541.5918 - val_loss: 22342.1699 - val_rmse: 22163.5488\n",
      "Epoch 59/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18283.3730 - rmse: 18304.0664\n",
      "Epoch 59: val_loss did not improve from 16822.82422\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 18283.3730 - rmse: 18304.0664 - val_loss: 21610.7207 - val_rmse: 21426.8086\n",
      "Epoch 60/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17122.6680 - rmse: 17122.6680\n",
      "Epoch 60: val_loss did not improve from 16822.82422\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 17188.9395 - rmse: 17231.1699 - val_loss: 21443.7520 - val_rmse: 21270.3652\n",
      "Epoch 61/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18216.2207 - rmse: 18212.6191\n",
      "Epoch 61: val_loss did not improve from 16822.82422\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 18216.2207 - rmse: 18212.6191 - val_loss: 16948.8164 - val_rmse: 16784.3672\n",
      "Epoch 62/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17538.9199 - rmse: 17538.9199\n",
      "Epoch 62: val_loss did not improve from 16822.82422\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 17637.2109 - rmse: 17669.0020 - val_loss: 19556.7656 - val_rmse: 19362.1973\n",
      "Epoch 63/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17097.6504 - rmse: 17097.6504\n",
      "Epoch 63: val_loss did not improve from 16822.82422\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 17108.1113 - rmse: 17112.4688 - val_loss: 19743.1777 - val_rmse: 19539.7031\n",
      "Epoch 64/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17333.0449 - rmse: 17333.0449\n",
      "Epoch 64: val_loss improved from 16822.82422 to 16350.76074, saving model to ./ckpt/reg_lr005/val_rmse_16199.hdf5\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 17331.1191 - rmse: 17350.9941 - val_loss: 16350.7607 - val_rmse: 16199.1982\n",
      "Epoch 65/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17737.4961 - rmse: 17737.4961\n",
      "Epoch 65: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 17813.4395 - rmse: 17858.3516 - val_loss: 19034.4941 - val_rmse: 18938.2676\n",
      "Epoch 66/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16969.2656 - rmse: 16965.6777\n",
      "Epoch 66: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 16969.2656 - rmse: 16965.6777 - val_loss: 17658.2734 - val_rmse: 17543.9004\n",
      "Epoch 67/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18586.6035 - rmse: 18586.6035\n",
      "Epoch 67: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 18591.4375 - rmse: 18625.3984 - val_loss: 18981.9883 - val_rmse: 18808.2891\n",
      "Epoch 68/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17037.6484 - rmse: 17023.0527\n",
      "Epoch 68: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 17037.6484 - rmse: 17023.0527 - val_loss: 16533.0312 - val_rmse: 16398.7754\n",
      "Epoch 69/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16424.4766 - rmse: 16424.4766\n",
      "Epoch 69: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 16396.1211 - rmse: 16379.9160 - val_loss: 17727.8926 - val_rmse: 17550.4414\n",
      "Epoch 70/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17647.1660 - rmse: 17647.1660\n",
      "Epoch 70: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 17739.7949 - rmse: 17787.1191 - val_loss: 17876.5801 - val_rmse: 17680.2500\n",
      "Epoch 71/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15799.8320 - rmse: 15797.1504\n",
      "Epoch 71: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 15799.8320 - rmse: 15797.1504 - val_loss: 16731.5234 - val_rmse: 16533.4961\n",
      "Epoch 72/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16100.2480 - rmse: 16077.5586\n",
      "Epoch 72: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 16100.2480 - rmse: 16077.5586 - val_loss: 17609.4180 - val_rmse: 17426.3496\n",
      "Epoch 73/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18690.2734 - rmse: 18690.2734\n",
      "Epoch 73: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 18716.9785 - rmse: 18726.7500 - val_loss: 26675.2637 - val_rmse: 26480.2090\n",
      "Epoch 74/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16928.0840 - rmse: 16946.2168\n",
      "Epoch 74: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 16928.0840 - rmse: 16946.2168 - val_loss: 17699.4082 - val_rmse: 17521.0859\n",
      "Epoch 75/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18971.5703 - rmse: 18971.5703\n",
      "Epoch 75: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 18897.8770 - rmse: 18913.9727 - val_loss: 16440.9141 - val_rmse: 16286.6523\n",
      "Epoch 76/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15540.9102 - rmse: 15540.9102\n",
      "Epoch 76: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 15544.2432 - rmse: 15546.2148 - val_loss: 19773.4492 - val_rmse: 19596.6367\n",
      "Epoch 77/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16046.2822 - rmse: 16036.5127\n",
      "Epoch 77: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 16046.2822 - rmse: 16036.5127 - val_loss: 16602.4199 - val_rmse: 16453.7656\n",
      "Epoch 78/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16744.2480 - rmse: 16744.2480\n",
      "Epoch 78: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 16708.2559 - rmse: 16686.9688 - val_loss: 24775.4980 - val_rmse: 24604.1719\n",
      "Epoch 79/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15844.5068 - rmse: 15874.0908\n",
      "Epoch 79: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 15844.5068 - rmse: 15874.0908 - val_loss: 17231.6074 - val_rmse: 17033.9785\n",
      "Epoch 80/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15748.9414 - rmse: 15753.9590\n",
      "Epoch 80: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 15748.9414 - rmse: 15753.9590 - val_loss: 17619.4941 - val_rmse: 17467.5273\n",
      "Epoch 81/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15732.4746 - rmse: 15732.4746\n",
      "Epoch 81: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 15825.2607 - rmse: 15860.5928 - val_loss: 17216.3574 - val_rmse: 17063.5098\n",
      "Epoch 82/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16270.7109 - rmse: 16280.1719\n",
      "Epoch 82: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 16270.7109 - rmse: 16280.1719 - val_loss: 24480.6387 - val_rmse: 24322.4746\n",
      "Epoch 83/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15479.3262 - rmse: 15479.3262\n",
      "Epoch 83: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 15515.4668 - rmse: 15547.9482 - val_loss: 22235.7559 - val_rmse: 22050.6660\n",
      "Epoch 84/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16378.5068 - rmse: 16401.8066\n",
      "Epoch 84: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 16378.5068 - rmse: 16401.8066 - val_loss: 24355.4258 - val_rmse: 24195.0938\n",
      "Epoch 85/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16092.2217 - rmse: 16092.2217\n",
      "Epoch 85: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 36ms/step - loss: 16077.5879 - rmse: 16068.9307 - val_loss: 17086.5938 - val_rmse: 16921.2422\n",
      "Epoch 86/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15789.3828 - rmse: 15841.6230\n",
      "Epoch 86: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 15789.3828 - rmse: 15841.6230 - val_loss: 22326.3965 - val_rmse: 22157.2070\n",
      "Epoch 87/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16045.2881 - rmse: 16045.2881\n",
      "Epoch 87: val_loss did not improve from 16350.76074\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 16117.7852 - rmse: 16160.6621 - val_loss: 19329.2539 - val_rmse: 19133.2812\n",
      "Epoch 88/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17009.7129 - rmse: 17009.7129\n",
      "Epoch 88: val_loss improved from 16350.76074 to 15894.95508, saving model to ./ckpt/reg_lr005/val_rmse_15711.hdf5\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 16978.8203 - rmse: 16960.5508 - val_loss: 15894.9551 - val_rmse: 15711.2891\n",
      "Epoch 89/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15567.9629 - rmse: 15567.9629\n",
      "Epoch 89: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 15899.9736 - rmse: 15956.5518 - val_loss: 18042.0566 - val_rmse: 17825.9121\n",
      "Epoch 90/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15764.4658 - rmse: 15779.8320\n",
      "Epoch 90: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 36ms/step - loss: 15764.4658 - rmse: 15779.8320 - val_loss: 16807.4570 - val_rmse: 16668.0859\n",
      "Epoch 91/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16899.6152 - rmse: 16899.6152\n",
      "Epoch 91: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 16901.8438 - rmse: 16923.0293 - val_loss: 16586.4082 - val_rmse: 16464.7207\n",
      "Epoch 92/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16093.8877 - rmse: 16100.3809\n",
      "Epoch 92: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 16093.8877 - rmse: 16100.3809 - val_loss: 16079.3887 - val_rmse: 15920.7344\n",
      "Epoch 93/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15366.8193 - rmse: 15366.8193\n",
      "Epoch 93: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 15425.6885 - rmse: 15460.5059 - val_loss: 17598.0918 - val_rmse: 17466.2520\n",
      "Epoch 94/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15652.3252 - rmse: 15709.8701\n",
      "Epoch 94: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 15652.3252 - rmse: 15709.8701 - val_loss: 23878.8008 - val_rmse: 23745.3594\n",
      "Epoch 95/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16947.1992 - rmse: 16958.1211\n",
      "Epoch 95: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 16947.1992 - rmse: 16958.1211 - val_loss: 22815.5371 - val_rmse: 22666.0098\n",
      "Epoch 96/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15210.5361 - rmse: 15210.5361\n",
      "Epoch 96: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 15195.5586 - rmse: 15186.6982 - val_loss: 21177.9863 - val_rmse: 20990.3652\n",
      "Epoch 97/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15061.4434 - rmse: 15058.5195\n",
      "Epoch 97: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 15061.4434 - rmse: 15058.5195 - val_loss: 26042.5918 - val_rmse: 25874.3926\n",
      "Epoch 98/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16076.2578 - rmse: 16076.2578\n",
      "Epoch 98: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 16161.3145 - rmse: 16211.6182 - val_loss: 16294.4736 - val_rmse: 16120.7656\n",
      "Epoch 99/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15517.6787 - rmse: 15517.6787\n",
      "Epoch 99: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 15489.1104 - rmse: 15487.1729 - val_loss: 16309.2725 - val_rmse: 16145.8145\n",
      "Epoch 100/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 15711.2959 - rmse: 15711.2959\n",
      "Epoch 100: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 15628.0469 - rmse: 15617.6143 - val_loss: 20455.8340 - val_rmse: 20247.6680\n",
      "Epoch 101/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 14973.3291 - rmse: 14973.3291\n",
      "Epoch 101: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 15076.0635 - rmse: 15136.0791 - val_loss: 20195.3457 - val_rmse: 20044.2109\n",
      "Epoch 102/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15885.5840 - rmse: 15895.2041\n",
      "Epoch 102: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 15885.5840 - rmse: 15895.2041 - val_loss: 16183.2490 - val_rmse: 16020.8203\n",
      "Epoch 103/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 14813.5068 - rmse: 14813.5068\n",
      "Epoch 103: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 15275.3955 - rmse: 15351.3643 - val_loss: 22908.0664 - val_rmse: 22723.1328\n",
      "Epoch 104/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14794.9463 - rmse: 14830.7812\n",
      "Epoch 104: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 14794.9463 - rmse: 14830.7812 - val_loss: 16822.5000 - val_rmse: 16657.4961\n",
      "Epoch 105/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15654.8955 - rmse: 15654.8955\n",
      "Epoch 105: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 15639.1367 - rmse: 15624.7354 - val_loss: 19357.9004 - val_rmse: 19180.3594\n",
      "Epoch 106/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 14598.6992 - rmse: 14598.6992\n",
      "Epoch 106: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 4s 57ms/step - loss: 14546.4824 - rmse: 14549.0977 - val_loss: 20145.8262 - val_rmse: 19964.1895\n",
      "Epoch 107/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15460.2715 - rmse: 15439.1660\n",
      "Epoch 107: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 15460.2715 - rmse: 15439.1660 - val_loss: 19938.4375 - val_rmse: 19848.1973\n",
      "Epoch 108/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16025.4150 - rmse: 16025.4150\n",
      "Epoch 108: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 16054.0449 - rmse: 16070.9785 - val_loss: 17003.6289 - val_rmse: 16831.5195\n",
      "Epoch 109/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15153.0312 - rmse: 15148.4141\n",
      "Epoch 109: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 4s 57ms/step - loss: 15153.0312 - rmse: 15148.4141 - val_loss: 16432.2188 - val_rmse: 16258.3281\n",
      "Epoch 110/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 14311.8037 - rmse: 14311.8037\n",
      "Epoch 110: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 5s 66ms/step - loss: 14490.1289 - rmse: 14595.5947 - val_loss: 21752.8848 - val_rmse: 21584.3320\n",
      "Epoch 111/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15788.4600 - rmse: 15819.8086\n",
      "Epoch 111: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 15788.4600 - rmse: 15819.8086 - val_loss: 16965.9004 - val_rmse: 16776.4277\n",
      "Epoch 112/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 14688.6699 - rmse: 14688.6699\n",
      "Epoch 112: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 4s 53ms/step - loss: 14663.3047 - rmse: 14648.3027 - val_loss: 16130.4727 - val_rmse: 16011.4414\n",
      "Epoch 113/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14532.1650 - rmse: 14531.4023\n",
      "Epoch 113: val_loss did not improve from 15894.95508\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 14532.1650 - rmse: 14531.4023 - val_loss: 17403.7109 - val_rmse: 17237.7129\n",
      "Epoch 113: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:58:02.577174: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 110913.1719 - rmse: 110510.2578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:58:06.206991: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 68342.46875, saving model to ./ckpt/reg_lr005/val_rmse_68138.hdf5\n",
      "70/70 [==============================] - 5s 51ms/step - loss: 110913.1719 - rmse: 110510.2578 - val_loss: 68342.4688 - val_rmse: 68138.2344\n",
      "Epoch 2/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 58037.2812 - rmse: 57937.2461\n",
      "Epoch 2: val_loss improved from 68342.46875 to 47010.72656, saving model to ./ckpt/reg_lr005/val_rmse_47053.hdf5\n",
      "70/70 [==============================] - 5s 65ms/step - loss: 58037.2812 - rmse: 57937.2461 - val_loss: 47010.7266 - val_rmse: 47053.0312\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 47949.2383 - rmse: 47949.2383\n",
      "Epoch 3: val_loss improved from 47010.72656 to 45273.05859, saving model to ./ckpt/reg_lr005/val_rmse_45191.hdf5\n",
      "70/70 [==============================] - 5s 78ms/step - loss: 47918.0117 - rmse: 47899.5430 - val_loss: 45273.0586 - val_rmse: 45191.3281\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 44285.6172 - rmse: 44275.3125\n",
      "Epoch 4: val_loss improved from 45273.05859 to 42896.43359, saving model to ./ckpt/reg_lr005/val_rmse_42814.hdf5\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 44285.6172 - rmse: 44275.3125 - val_loss: 42896.4336 - val_rmse: 42814.4180\n",
      "Epoch 5/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 42724.3516 - rmse: 42724.3516\n",
      "Epoch 5: val_loss improved from 42896.43359 to 41323.43359, saving model to ./ckpt/reg_lr005/val_rmse_41226.hdf5\n",
      "70/70 [==============================] - 4s 55ms/step - loss: 42742.7266 - rmse: 42753.5938 - val_loss: 41323.4336 - val_rmse: 41226.2422\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 40988.9609 - rmse: 40949.1250\n",
      "Epoch 6: val_loss improved from 41323.43359 to 37770.06641, saving model to ./ckpt/reg_lr005/val_rmse_37686.hdf5\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 40988.9609 - rmse: 40949.1250 - val_loss: 37770.0664 - val_rmse: 37685.7812\n",
      "Epoch 7/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 39065.3359 - rmse: 39065.3359\n",
      "Epoch 7: val_loss improved from 37770.06641 to 36982.86719, saving model to ./ckpt/reg_lr005/val_rmse_36957.hdf5\n",
      "70/70 [==============================] - 4s 59ms/step - loss: 39358.5703 - rmse: 39386.5586 - val_loss: 36982.8672 - val_rmse: 36957.1016\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 37320.3555 - rmse: 37320.3555\n",
      "Epoch 8: val_loss improved from 36982.86719 to 36186.05859, saving model to ./ckpt/reg_lr005/val_rmse_36152.hdf5\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 37361.2617 - rmse: 37385.4492 - val_loss: 36186.0586 - val_rmse: 36151.5742\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 37988.6211 - rmse: 37985.3438\n",
      "Epoch 9: val_loss improved from 36186.05859 to 35535.24609, saving model to ./ckpt/reg_lr005/val_rmse_35492.hdf5\n",
      "70/70 [==============================] - 3s 35ms/step - loss: 37988.6211 - rmse: 37985.3438 - val_loss: 35535.2461 - val_rmse: 35492.4414\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 33700.7461 - rmse: 33697.3555\n",
      "Epoch 10: val_loss improved from 35535.24609 to 31881.97656, saving model to ./ckpt/reg_lr005/val_rmse_31797.hdf5\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 33700.7461 - rmse: 33697.3555 - val_loss: 31881.9766 - val_rmse: 31796.8242\n",
      "Epoch 11/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 34090.4648 - rmse: 34090.4648\n",
      "Epoch 11: val_loss improved from 31881.97656 to 30840.07617, saving model to ./ckpt/reg_lr005/val_rmse_30755.hdf5\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 33896.8828 - rmse: 33866.5000 - val_loss: 30840.0762 - val_rmse: 30755.2910\n",
      "Epoch 12/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 31574.3223 - rmse: 31574.3223\n",
      "Epoch 12: val_loss did not improve from 30840.07617\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 31544.8555 - rmse: 31527.4297 - val_loss: 32501.7539 - val_rmse: 32383.2031\n",
      "Epoch 13/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 32839.8164 - rmse: 32839.8164\n",
      "Epoch 13: val_loss improved from 30840.07617 to 28863.83008, saving model to ./ckpt/reg_lr005/val_rmse_28768.hdf5\n",
      "70/70 [==============================] - 2s 23ms/step - loss: 32558.5098 - rmse: 32495.8320 - val_loss: 28863.8301 - val_rmse: 28768.2305\n",
      "Epoch 14/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 28604.8711 - rmse: 28604.8711\n",
      "Epoch 14: val_loss did not improve from 28863.83008\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 28684.8086 - rmse: 28732.0840 - val_loss: 33031.0703 - val_rmse: 32989.5391\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 29684.3945 - rmse: 29684.3945\n",
      "Epoch 15: val_loss improved from 28863.83008 to 26523.92188, saving model to ./ckpt/reg_lr005/val_rmse_26456.hdf5\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 29679.2051 - rmse: 29676.1367 - val_loss: 26523.9219 - val_rmse: 26456.0410\n",
      "Epoch 16/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 26308.4102 - rmse: 26308.4102\n",
      "Epoch 16: val_loss did not improve from 26523.92188\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 26293.9902 - rmse: 26326.0039 - val_loss: 30248.0371 - val_rmse: 30174.5859\n",
      "Epoch 17/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 29348.7949 - rmse: 29348.7949\n",
      "Epoch 17: val_loss did not improve from 26523.92188\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 29288.7441 - rmse: 29308.7656 - val_loss: 32408.5469 - val_rmse: 32394.6680\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25960.3184 - rmse: 25978.0137\n",
      "Epoch 18: val_loss improved from 26523.92188 to 23071.14258, saving model to ./ckpt/reg_lr005/val_rmse_23013.hdf5\n",
      "70/70 [==============================] - 2s 22ms/step - loss: 25960.3184 - rmse: 25978.0137 - val_loss: 23071.1426 - val_rmse: 23012.8242\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24732.1484 - rmse: 24720.2051\n",
      "Epoch 19: val_loss did not improve from 23071.14258\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 24732.1484 - rmse: 24720.2051 - val_loss: 26264.0176 - val_rmse: 26189.3457\n",
      "Epoch 20/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 25463.8613 - rmse: 25463.8613\n",
      "Epoch 20: val_loss improved from 23071.14258 to 23002.99414, saving model to ./ckpt/reg_lr005/val_rmse_22903.hdf5\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 25246.3418 - rmse: 25261.2363 - val_loss: 23002.9941 - val_rmse: 22902.7402\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24836.7129 - rmse: 24854.6348\n",
      "Epoch 21: val_loss did not improve from 23002.99414\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 24836.7129 - rmse: 24854.6348 - val_loss: 36256.4805 - val_rmse: 36124.7656\n",
      "Epoch 22/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 23337.8633 - rmse: 23337.8633\n",
      "Epoch 22: val_loss improved from 23002.99414 to 21076.53711, saving model to ./ckpt/reg_lr005/val_rmse_21006.hdf5\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 23371.3359 - rmse: 23436.2207 - val_loss: 21076.5371 - val_rmse: 21006.2344\n",
      "Epoch 23/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22368.2676 - rmse: 22368.2676\n",
      "Epoch 23: val_loss improved from 21076.53711 to 19229.30273, saving model to ./ckpt/reg_lr005/val_rmse_19154.hdf5\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 22367.5156 - rmse: 22367.0723 - val_loss: 19229.3027 - val_rmse: 19153.5918\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23186.0684 - rmse: 23186.0684\n",
      "Epoch 24: val_loss did not improve from 19229.30273\n",
      "70/70 [==============================] - 3s 35ms/step - loss: 23260.6875 - rmse: 23304.8203 - val_loss: 26896.4590 - val_rmse: 26795.0547\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22408.3086 - rmse: 22391.8984\n",
      "Epoch 25: val_loss did not improve from 19229.30273\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 22408.3086 - rmse: 22391.8984 - val_loss: 26702.3184 - val_rmse: 26610.7324\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24473.0098 - rmse: 24481.3027\n",
      "Epoch 26: val_loss did not improve from 19229.30273\n",
      "70/70 [==============================] - 2s 22ms/step - loss: 24473.0098 - rmse: 24481.3027 - val_loss: 20580.9824 - val_rmse: 20476.8750\n",
      "Epoch 27/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21708.6172 - rmse: 21708.6172\n",
      "Epoch 27: val_loss improved from 19229.30273 to 18863.36523, saving model to ./ckpt/reg_lr005/val_rmse_18735.hdf5\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 21747.7539 - rmse: 21770.9004 - val_loss: 18863.3652 - val_rmse: 18735.3164\n",
      "Epoch 28/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21954.2363 - rmse: 21954.2363\n",
      "Epoch 28: val_loss improved from 18863.36523 to 18072.56641, saving model to ./ckpt/reg_lr005/val_rmse_17953.hdf5\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 21979.4844 - rmse: 21958.5527 - val_loss: 18072.5664 - val_rmse: 17952.9609\n",
      "Epoch 29/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 22451.7734 - rmse: 22451.7734\n",
      "Epoch 29: val_loss did not improve from 18072.56641\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 22429.4023 - rmse: 22543.8516 - val_loss: 24228.2559 - val_rmse: 24086.2969\n",
      "Epoch 30/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23554.0117 - rmse: 23554.0117\n",
      "Epoch 30: val_loss did not improve from 18072.56641\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 23553.3516 - rmse: 23544.5508 - val_loss: 21643.9004 - val_rmse: 21543.2188\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21353.1172 - rmse: 21383.5117\n",
      "Epoch 31: val_loss did not improve from 18072.56641\n",
      "70/70 [==============================] - 2s 23ms/step - loss: 21353.1172 - rmse: 21383.5117 - val_loss: 18302.1152 - val_rmse: 18180.1250\n",
      "Epoch 32/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 24116.8789 - rmse: 24116.8789\n",
      "Epoch 32: val_loss did not improve from 18072.56641\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 24071.9082 - rmse: 24055.4141 - val_loss: 21879.8594 - val_rmse: 21752.1758\n",
      "Epoch 33/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21863.6074 - rmse: 21863.6074\n",
      "Epoch 33: val_loss did not improve from 18072.56641\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 21807.0547 - rmse: 21802.6543 - val_loss: 21352.3008 - val_rmse: 21215.2695\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20689.9258 - rmse: 20874.6836\n",
      "Epoch 34: val_loss did not improve from 18072.56641\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 20689.9258 - rmse: 20874.6836 - val_loss: 18509.2402 - val_rmse: 18370.4668\n",
      "Epoch 35/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20598.9473 - rmse: 20572.2324\n",
      "Epoch 35: val_loss did not improve from 18072.56641\n",
      "70/70 [==============================] - 2s 23ms/step - loss: 20598.9473 - rmse: 20572.2324 - val_loss: 18415.4629 - val_rmse: 18249.9102\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20138.5762 - rmse: 20127.1289\n",
      "Epoch 36: val_loss did not improve from 18072.56641\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 20138.5762 - rmse: 20127.1289 - val_loss: 20385.5547 - val_rmse: 20246.8008\n",
      "Epoch 37/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20570.1855 - rmse: 20552.0527\n",
      "Epoch 37: val_loss improved from 18072.56641 to 17468.19922, saving model to ./ckpt/reg_lr005/val_rmse_17300.hdf5\n",
      "70/70 [==============================] - 2s 23ms/step - loss: 20570.1855 - rmse: 20552.0527 - val_loss: 17468.1992 - val_rmse: 17299.9512\n",
      "Epoch 38/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18885.8574 - rmse: 18924.2539\n",
      "Epoch 38: val_loss improved from 17468.19922 to 17429.39258, saving model to ./ckpt/reg_lr005/val_rmse_17272.hdf5\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 18885.8574 - rmse: 18924.2539 - val_loss: 17429.3926 - val_rmse: 17271.8691\n",
      "Epoch 39/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19437.2949 - rmse: 19431.3750\n",
      "Epoch 39: val_loss did not improve from 17429.39258\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 19437.2949 - rmse: 19431.3750 - val_loss: 17491.4258 - val_rmse: 17368.9414\n",
      "Epoch 40/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20034.5156 - rmse: 20015.7539\n",
      "Epoch 40: val_loss did not improve from 17429.39258\n",
      "70/70 [==============================] - 2s 22ms/step - loss: 20034.5156 - rmse: 20015.7539 - val_loss: 18766.5039 - val_rmse: 18611.2207\n",
      "Epoch 41/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19900.7676 - rmse: 19931.5469\n",
      "Epoch 41: val_loss did not improve from 17429.39258\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 19900.7676 - rmse: 19931.5469 - val_loss: 20980.8164 - val_rmse: 20881.8633\n",
      "Epoch 42/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 19617.3516 - rmse: 19617.3516\n",
      "Epoch 42: val_loss improved from 17429.39258 to 17242.08984, saving model to ./ckpt/reg_lr005/val_rmse_17134.hdf5\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 19656.1895 - rmse: 19682.0723 - val_loss: 17242.0898 - val_rmse: 17133.9004\n",
      "Epoch 43/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20016.5430 - rmse: 20016.5430\n",
      "Epoch 43: val_loss did not improve from 17242.08984\n",
      "70/70 [==============================] - 3s 49ms/step - loss: 20041.8125 - rmse: 20037.5332 - val_loss: 20670.9336 - val_rmse: 20511.5879\n",
      "Epoch 44/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19471.9336 - rmse: 19461.1074\n",
      "Epoch 44: val_loss did not improve from 17242.08984\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 19471.9336 - rmse: 19461.1074 - val_loss: 20201.1152 - val_rmse: 20070.7871\n",
      "Epoch 45/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18358.7480 - rmse: 18358.7480\n",
      "Epoch 45: val_loss did not improve from 17242.08984\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 18559.9395 - rmse: 18706.0859 - val_loss: 20603.5723 - val_rmse: 20443.8281\n",
      "Epoch 46/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21147.6875 - rmse: 21147.6875\n",
      "Epoch 46: val_loss did not improve from 17242.08984\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 21230.7324 - rmse: 21265.8730 - val_loss: 17824.0742 - val_rmse: 17638.7969\n",
      "Epoch 47/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20174.2676 - rmse: 20174.2676\n",
      "Epoch 47: val_loss did not improve from 17242.08984\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 20175.8594 - rmse: 20176.8008 - val_loss: 17262.5840 - val_rmse: 17121.3770\n",
      "Epoch 48/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20112.3770 - rmse: 20151.1074\n",
      "Epoch 48: val_loss did not improve from 17242.08984\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 20112.3770 - rmse: 20151.1074 - val_loss: 17505.0137 - val_rmse: 17373.0723\n",
      "Epoch 49/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18335.4844 - rmse: 18357.4062\n",
      "Epoch 49: val_loss improved from 17242.08984 to 16852.55469, saving model to ./ckpt/reg_lr005/val_rmse_16715.hdf5\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 18335.4844 - rmse: 18357.4062 - val_loss: 16852.5547 - val_rmse: 16715.4980\n",
      "Epoch 50/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20064.4941 - rmse: 20229.9941\n",
      "Epoch 50: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 3s 47ms/step - loss: 20064.4941 - rmse: 20229.9941 - val_loss: 16960.5938 - val_rmse: 16760.2559\n",
      "Epoch 51/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18803.1777 - rmse: 18803.1777\n",
      "Epoch 51: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 18752.3672 - rmse: 18722.3145 - val_loss: 18464.1074 - val_rmse: 18310.8828\n",
      "Epoch 52/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20218.8105 - rmse: 20218.8105\n",
      "Epoch 52: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 20234.9570 - rmse: 20244.5078 - val_loss: 17184.3184 - val_rmse: 17014.9395\n",
      "Epoch 53/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21074.7305 - rmse: 21164.0820\n",
      "Epoch 53: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 21074.7305 - rmse: 21164.0820 - val_loss: 18131.3047 - val_rmse: 17988.8711\n",
      "Epoch 54/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18945.9043 - rmse: 18945.9043\n",
      "Epoch 54: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 18830.9473 - rmse: 18828.3320 - val_loss: 18519.8672 - val_rmse: 18388.6133\n",
      "Epoch 55/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18313.7461 - rmse: 18323.4082\n",
      "Epoch 55: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 18313.7461 - rmse: 18323.4082 - val_loss: 17385.6094 - val_rmse: 17215.5273\n",
      "Epoch 56/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18014.4336 - rmse: 18009.8027\n",
      "Epoch 56: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 18014.4336 - rmse: 18009.8027 - val_loss: 20277.4414 - val_rmse: 20122.5156\n",
      "Epoch 57/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19471.0996 - rmse: 19534.2871\n",
      "Epoch 57: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 19471.0996 - rmse: 19534.2871 - val_loss: 20281.6191 - val_rmse: 20153.9473\n",
      "Epoch 58/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18497.9297 - rmse: 18497.9297\n",
      "Epoch 58: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 18574.3809 - rmse: 18588.3340 - val_loss: 21542.4883 - val_rmse: 21398.1406\n",
      "Epoch 59/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19302.1348 - rmse: 19302.1348\n",
      "Epoch 59: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 19255.9082 - rmse: 19228.5703 - val_loss: 17144.4980 - val_rmse: 16969.9531\n",
      "Epoch 60/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17834.7988 - rmse: 17834.7988\n",
      "Epoch 60: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 17758.9199 - rmse: 17769.7637 - val_loss: 19293.2910 - val_rmse: 19173.1934\n",
      "Epoch 61/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20671.7246 - rmse: 20631.1016\n",
      "Epoch 61: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 20671.7246 - rmse: 20631.1016 - val_loss: 18204.9082 - val_rmse: 18065.2852\n",
      "Epoch 62/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17104.3105 - rmse: 17150.8223\n",
      "Epoch 62: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 17104.3105 - rmse: 17150.8223 - val_loss: 17101.1719 - val_rmse: 16952.2578\n",
      "Epoch 63/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17559.8574 - rmse: 17581.4805\n",
      "Epoch 63: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 17559.8574 - rmse: 17581.4805 - val_loss: 18290.3145 - val_rmse: 18156.9277\n",
      "Epoch 64/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17541.2109 - rmse: 17541.5234\n",
      "Epoch 64: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 17541.2109 - rmse: 17541.5234 - val_loss: 17248.4707 - val_rmse: 17082.4824\n",
      "Epoch 65/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17222.7383 - rmse: 17222.7383\n",
      "Epoch 65: val_loss did not improve from 16852.55469\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 17264.4316 - rmse: 17267.1270 - val_loss: 17568.2168 - val_rmse: 17423.0391\n",
      "Epoch 66/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18132.0078 - rmse: 18105.0859\n",
      "Epoch 66: val_loss improved from 16852.55469 to 16744.99219, saving model to ./ckpt/reg_lr005/val_rmse_16593.hdf5\n",
      "70/70 [==============================] - 3s 48ms/step - loss: 18132.0078 - rmse: 18105.0859 - val_loss: 16744.9922 - val_rmse: 16592.9648\n",
      "Epoch 67/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18286.4844 - rmse: 18286.4844\n",
      "Epoch 67: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 18399.6816 - rmse: 18466.6270 - val_loss: 23825.2148 - val_rmse: 23685.1016\n",
      "Epoch 68/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18162.8379 - rmse: 18162.8379\n",
      "Epoch 68: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 4s 55ms/step - loss: 18108.7949 - rmse: 18076.8281 - val_loss: 28037.2676 - val_rmse: 27959.6270\n",
      "Epoch 69/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17061.5938 - rmse: 17075.0547\n",
      "Epoch 69: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 17061.5938 - rmse: 17075.0547 - val_loss: 17032.6211 - val_rmse: 16874.9062\n",
      "Epoch 70/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17714.5156 - rmse: 17714.5156\n",
      "Epoch 70: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 17701.1660 - rmse: 17703.4570 - val_loss: 16856.0098 - val_rmse: 16696.6680\n",
      "Epoch 71/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18063.1914 - rmse: 18063.1914\n",
      "Epoch 71: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 17996.9980 - rmse: 17957.8477 - val_loss: 19324.2227 - val_rmse: 19227.4102\n",
      "Epoch 72/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17397.4668 - rmse: 17397.4668\n",
      "Epoch 72: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 4s 53ms/step - loss: 17458.6582 - rmse: 17494.8477 - val_loss: 18776.3926 - val_rmse: 18640.5703\n",
      "Epoch 73/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16775.8633 - rmse: 16775.8633\n",
      "Epoch 73: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 16735.8945 - rmse: 16728.1367 - val_loss: 18370.7617 - val_rmse: 18228.2012\n",
      "Epoch 74/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16793.4629 - rmse: 16793.4629\n",
      "Epoch 74: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 16753.7949 - rmse: 16723.2598 - val_loss: 16857.3066 - val_rmse: 16712.2227\n",
      "Epoch 75/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17536.5605 - rmse: 17536.5605\n",
      "Epoch 75: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 3s 49ms/step - loss: 17559.5195 - rmse: 17573.0977 - val_loss: 21117.9629 - val_rmse: 21009.2578\n",
      "Epoch 76/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16341.0449 - rmse: 16439.0332\n",
      "Epoch 76: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 16341.0449 - rmse: 16439.0332 - val_loss: 23409.4258 - val_rmse: 23315.0020\n",
      "Epoch 77/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16215.4932 - rmse: 16215.4932\n",
      "Epoch 77: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 16160.9570 - rmse: 16136.0996 - val_loss: 18831.4727 - val_rmse: 18730.2363\n",
      "Epoch 78/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17565.1074 - rmse: 17597.8516\n",
      "Epoch 78: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 17565.1074 - rmse: 17597.8516 - val_loss: 22430.3125 - val_rmse: 22369.3281\n",
      "Epoch 79/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17165.2344 - rmse: 17191.6543\n",
      "Epoch 79: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 6s 89ms/step - loss: 17165.2344 - rmse: 17191.6543 - val_loss: 21217.9648 - val_rmse: 21192.0352\n",
      "Epoch 80/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16692.5176 - rmse: 16692.5176\n",
      "Epoch 80: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 4s 60ms/step - loss: 16753.6953 - rmse: 16789.8770 - val_loss: 16977.2207 - val_rmse: 16787.9316\n",
      "Epoch 81/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17516.5371 - rmse: 17516.5371\n",
      "Epoch 81: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 17698.5566 - rmse: 17806.2090 - val_loss: 16799.6523 - val_rmse: 16637.1738\n",
      "Epoch 82/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16616.0664 - rmse: 16616.0664\n",
      "Epoch 82: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 4s 53ms/step - loss: 16542.1777 - rmse: 16518.8301 - val_loss: 17221.1426 - val_rmse: 17068.0898\n",
      "Epoch 83/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16952.0527 - rmse: 16952.0527\n",
      "Epoch 83: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 4s 57ms/step - loss: 17153.2988 - rmse: 17272.3223 - val_loss: 22127.2207 - val_rmse: 22052.2988\n",
      "Epoch 84/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16575.8340 - rmse: 16539.5391\n",
      "Epoch 84: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 16575.8340 - rmse: 16539.5391 - val_loss: 26019.0000 - val_rmse: 25986.2012\n",
      "Epoch 85/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17015.0625 - rmse: 17015.0625\n",
      "Epoch 85: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 16900.2891 - rmse: 16887.2812 - val_loss: 16993.8594 - val_rmse: 16837.7461\n",
      "Epoch 86/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18366.7832 - rmse: 18366.7832\n",
      "Epoch 86: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 18388.4805 - rmse: 18401.3125 - val_loss: 17228.6152 - val_rmse: 17046.8770\n",
      "Epoch 87/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16793.0176 - rmse: 16793.0176\n",
      "Epoch 87: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 3s 48ms/step - loss: 16856.4980 - rmse: 16889.6543 - val_loss: 20759.4805 - val_rmse: 20533.3340\n",
      "Epoch 88/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17460.7012 - rmse: 17460.7012\n",
      "Epoch 88: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 17465.9004 - rmse: 17515.4199 - val_loss: 20733.7188 - val_rmse: 20681.1348\n",
      "Epoch 89/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15970.2129 - rmse: 15970.2129\n",
      "Epoch 89: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 15942.0557 - rmse: 15942.2910 - val_loss: 17475.5781 - val_rmse: 17364.3184\n",
      "Epoch 90/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16184.2949 - rmse: 16161.3730\n",
      "Epoch 90: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 16184.2949 - rmse: 16161.3730 - val_loss: 18968.9316 - val_rmse: 18894.7246\n",
      "Epoch 91/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16477.5059 - rmse: 16477.5059\n",
      "Epoch 91: val_loss did not improve from 16744.99219\n",
      "70/70 [==============================] - 3s 47ms/step - loss: 16530.6641 - rmse: 16562.1035 - val_loss: 17982.6113 - val_rmse: 17841.1777\n",
      "Epoch 91: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 14:02:16.087788: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 109073.8516 - rmse: 108792.9844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 14:02:21.218481: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 65593.03125, saving model to ./ckpt/reg_lr005/val_rmse_65813.hdf5\n",
      "70/70 [==============================] - 6s 69ms/step - loss: 109073.8516 - rmse: 108792.9844 - val_loss: 65593.0312 - val_rmse: 65812.9531\n",
      "Epoch 2/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 56238.6797 - rmse: 56102.0664\n",
      "Epoch 2: val_loss improved from 65593.03125 to 53694.73828, saving model to ./ckpt/reg_lr005/val_rmse_53860.hdf5\n",
      "70/70 [==============================] - 6s 86ms/step - loss: 56238.6797 - rmse: 56102.0664 - val_loss: 53694.7383 - val_rmse: 53860.3477\n",
      "Epoch 3/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 49847.0938 - rmse: 49847.0938\n",
      "Epoch 3: val_loss improved from 53694.73828 to 43778.79297, saving model to ./ckpt/reg_lr005/val_rmse_43665.hdf5\n",
      "70/70 [==============================] - 5s 69ms/step - loss: 50216.1875 - rmse: 50434.4766 - val_loss: 43778.7930 - val_rmse: 43665.0273\n",
      "Epoch 4/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 45413.5820 - rmse: 45413.5820\n",
      "Epoch 4: val_loss improved from 43778.79297 to 41277.00000, saving model to ./ckpt/reg_lr005/val_rmse_41267.hdf5\n",
      "70/70 [==============================] - 4s 62ms/step - loss: 45293.7148 - rmse: 45222.8164 - val_loss: 41277.0000 - val_rmse: 41266.5938\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 42121.4141 - rmse: 42033.7930\n",
      "Epoch 5: val_loss improved from 41277.00000 to 39504.11328, saving model to ./ckpt/reg_lr005/val_rmse_39460.hdf5\n",
      "70/70 [==============================] - 5s 69ms/step - loss: 42121.4141 - rmse: 42033.7930 - val_loss: 39504.1133 - val_rmse: 39459.7227\n",
      "Epoch 6/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 41058.0312 - rmse: 41058.0312\n",
      "Epoch 6: val_loss did not improve from 39504.11328\n",
      "70/70 [==============================] - 4s 65ms/step - loss: 41096.9062 - rmse: 41119.8984 - val_loss: 40942.4297 - val_rmse: 40642.9844\n",
      "Epoch 7/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 39174.6406 - rmse: 39152.6875\n",
      "Epoch 7: val_loss improved from 39504.11328 to 35874.87891, saving model to ./ckpt/reg_lr005/val_rmse_35663.hdf5\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 39174.6406 - rmse: 39152.6875 - val_loss: 35874.8789 - val_rmse: 35663.4219\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 36888.5352 - rmse: 36888.5352\n",
      "Epoch 8: val_loss improved from 35874.87891 to 34021.59375, saving model to ./ckpt/reg_lr005/val_rmse_33848.hdf5\n",
      "70/70 [==============================] - 3s 49ms/step - loss: 36873.1289 - rmse: 36864.0117 - val_loss: 34021.5938 - val_rmse: 33847.6484\n",
      "Epoch 9/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 39423.0742 - rmse: 39423.0742\n",
      "Epoch 9: val_loss did not improve from 34021.59375\n",
      "70/70 [==============================] - 4s 58ms/step - loss: 39407.3945 - rmse: 39486.1758 - val_loss: 36204.7070 - val_rmse: 36139.0742\n",
      "Epoch 10/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 35052.3594 - rmse: 35165.6602\n",
      "Epoch 10: val_loss did not improve from 34021.59375\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 35052.3594 - rmse: 35165.6602 - val_loss: 44708.7852 - val_rmse: 44379.2461\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 34659.2812 - rmse: 34659.2812\n",
      "Epoch 11: val_loss improved from 34021.59375 to 31526.40234, saving model to ./ckpt/reg_lr005/val_rmse_31234.hdf5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 34697.1562 - rmse: 34719.5586 - val_loss: 31526.4023 - val_rmse: 31234.5000\n",
      "Epoch 12/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 33404.0586 - rmse: 33404.0586\n",
      "Epoch 12: val_loss improved from 31526.40234 to 29223.81641, saving model to ./ckpt/reg_lr005/val_rmse_29012.hdf5\n",
      "70/70 [==============================] - 4s 63ms/step - loss: 33175.8906 - rmse: 33128.1016 - val_loss: 29223.8164 - val_rmse: 29012.4648\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 29639.4102 - rmse: 29784.6641\n",
      "Epoch 13: val_loss improved from 29223.81641 to 28990.57617, saving model to ./ckpt/reg_lr005/val_rmse_28700.hdf5\n",
      "70/70 [==============================] - 6s 78ms/step - loss: 29639.4102 - rmse: 29784.6641 - val_loss: 28990.5762 - val_rmse: 28700.0957\n",
      "Epoch 14/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 27888.0879 - rmse: 27888.0879\n",
      "Epoch 14: val_loss did not improve from 28990.57617\n",
      "70/70 [==============================] - 4s 58ms/step - loss: 27883.2910 - rmse: 27886.9648 - val_loss: 29031.7012 - val_rmse: 28846.0254\n",
      "Epoch 15/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 28993.8418 - rmse: 28993.8418\n",
      "Epoch 15: val_loss improved from 28990.57617 to 25334.55664, saving model to ./ckpt/reg_lr005/val_rmse_25098.hdf5\n",
      "70/70 [==============================] - 4s 53ms/step - loss: 28882.9863 - rmse: 28817.4238 - val_loss: 25334.5566 - val_rmse: 25097.8008\n",
      "Epoch 16/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 28331.9824 - rmse: 28324.6426\n",
      "Epoch 16: val_loss did not improve from 25334.55664\n",
      "70/70 [==============================] - 5s 71ms/step - loss: 28331.9824 - rmse: 28324.6426 - val_loss: 25875.7676 - val_rmse: 25672.9883\n",
      "Epoch 17/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 27329.6992 - rmse: 27329.6992\n",
      "Epoch 17: val_loss did not improve from 25334.55664\n",
      "70/70 [==============================] - 3s 48ms/step - loss: 27226.0977 - rmse: 27164.8242 - val_loss: 27738.6367 - val_rmse: 27555.9023\n",
      "Epoch 18/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24191.9082 - rmse: 24176.4512\n",
      "Epoch 18: val_loss did not improve from 25334.55664\n",
      "70/70 [==============================] - 3s 49ms/step - loss: 24191.9082 - rmse: 24176.4512 - val_loss: 25821.6777 - val_rmse: 25657.2578\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24526.7461 - rmse: 24515.5684\n",
      "Epoch 19: val_loss did not improve from 25334.55664\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 24526.7461 - rmse: 24515.5684 - val_loss: 32221.7793 - val_rmse: 32108.0645\n",
      "Epoch 20/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24919.5957 - rmse: 24897.4434\n",
      "Epoch 20: val_loss improved from 25334.55664 to 22735.45312, saving model to ./ckpt/reg_lr005/val_rmse_22552.hdf5\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 24919.5957 - rmse: 24897.4434 - val_loss: 22735.4531 - val_rmse: 22551.6777\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24035.4863 - rmse: 24089.0488\n",
      "Epoch 21: val_loss did not improve from 22735.45312\n",
      "70/70 [==============================] - 4s 62ms/step - loss: 24035.4863 - rmse: 24089.0488 - val_loss: 24126.6387 - val_rmse: 23933.9062\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25128.5664 - rmse: 25213.9414\n",
      "Epoch 22: val_loss did not improve from 22735.45312\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 25128.5664 - rmse: 25213.9414 - val_loss: 38018.6055 - val_rmse: 37897.1250\n",
      "Epoch 23/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 26988.3652 - rmse: 26988.3652\n",
      "Epoch 23: val_loss did not improve from 22735.45312\n",
      "70/70 [==============================] - 3s 50ms/step - loss: 26766.5430 - rmse: 26738.0488 - val_loss: 22850.4863 - val_rmse: 22684.8262\n",
      "Epoch 24/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 23565.9336 - rmse: 23565.9336\n",
      "Epoch 24: val_loss improved from 22735.45312 to 21990.90039, saving model to ./ckpt/reg_lr005/val_rmse_21867.hdf5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 23547.1680 - rmse: 23536.0684 - val_loss: 21990.9004 - val_rmse: 21866.9609\n",
      "Epoch 25/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22448.7637 - rmse: 22448.7637\n",
      "Epoch 25: val_loss did not improve from 21990.90039\n",
      "70/70 [==============================] - 4s 59ms/step - loss: 22591.1289 - rmse: 22675.3242 - val_loss: 22985.4258 - val_rmse: 22866.9492\n",
      "Epoch 26/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22872.1738 - rmse: 22845.8418\n",
      "Epoch 26: val_loss did not improve from 21990.90039\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 22872.1738 - rmse: 22845.8418 - val_loss: 31122.3281 - val_rmse: 31032.3828\n",
      "Epoch 27/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21999.0430 - rmse: 21999.0430\n",
      "Epoch 27: val_loss did not improve from 21990.90039\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 22038.9688 - rmse: 22062.5781 - val_loss: 22046.1426 - val_rmse: 21907.9004\n",
      "Epoch 28/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20205.0996 - rmse: 20245.4238\n",
      "Epoch 28: val_loss did not improve from 21990.90039\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 20205.0996 - rmse: 20245.4238 - val_loss: 22151.1426 - val_rmse: 22050.3984\n",
      "Epoch 29/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22111.0391 - rmse: 22115.1895\n",
      "Epoch 29: val_loss did not improve from 21990.90039\n",
      "70/70 [==============================] - 4s 61ms/step - loss: 22111.0391 - rmse: 22115.1895 - val_loss: 22256.0840 - val_rmse: 22132.1855\n",
      "Epoch 30/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19651.6934 - rmse: 19651.6934\n",
      "Epoch 30: val_loss did not improve from 21990.90039\n",
      "70/70 [==============================] - 4s 53ms/step - loss: 19700.2402 - rmse: 19728.9492 - val_loss: 24957.1074 - val_rmse: 24892.5234\n",
      "Epoch 31/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20345.3105 - rmse: 20345.3105\n",
      "Epoch 31: val_loss did not improve from 21990.90039\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 20600.4648 - rmse: 20743.5469 - val_loss: 36042.4102 - val_rmse: 36013.6602\n",
      "Epoch 32/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20216.3789 - rmse: 20216.3789\n",
      "Epoch 32: val_loss did not improve from 21990.90039\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 20259.0859 - rmse: 20284.3438 - val_loss: 22863.1270 - val_rmse: 22821.1992\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 22199.9297 - rmse: 22185.8945\n",
      "Epoch 33: val_loss did not improve from 21990.90039\n",
      "70/70 [==============================] - 4s 53ms/step - loss: 22199.9297 - rmse: 22185.8945 - val_loss: 28143.4922 - val_rmse: 28057.2090\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20496.1523 - rmse: 20501.6191\n",
      "Epoch 34: val_loss did not improve from 21990.90039\n",
      "70/70 [==============================] - 3s 47ms/step - loss: 20496.1523 - rmse: 20501.6191 - val_loss: 24250.5918 - val_rmse: 24216.5684\n",
      "Epoch 35/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21392.5410 - rmse: 21363.5332\n",
      "Epoch 35: val_loss improved from 21990.90039 to 21513.39844, saving model to ./ckpt/reg_lr005/val_rmse_21509.hdf5\n",
      "70/70 [==============================] - 4s 60ms/step - loss: 21392.5410 - rmse: 21363.5332 - val_loss: 21513.3984 - val_rmse: 21509.0684\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21501.1758 - rmse: 21474.7852\n",
      "Epoch 36: val_loss improved from 21513.39844 to 21307.48242, saving model to ./ckpt/reg_lr005/val_rmse_21269.hdf5\n",
      "70/70 [==============================] - 4s 58ms/step - loss: 21501.1758 - rmse: 21474.7852 - val_loss: 21307.4824 - val_rmse: 21268.8145\n",
      "Epoch 37/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20034.1719 - rmse: 20050.4219\n",
      "Epoch 37: val_loss did not improve from 21307.48242\n",
      "70/70 [==============================] - 4s 58ms/step - loss: 20034.1719 - rmse: 20050.4219 - val_loss: 23173.4414 - val_rmse: 23145.4102\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19770.5410 - rmse: 19770.5410\n",
      "Epoch 38: val_loss did not improve from 21307.48242\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 19748.1367 - rmse: 19734.8867 - val_loss: 25364.6172 - val_rmse: 25313.5547\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21082.9785 - rmse: 21082.9785\n",
      "Epoch 39: val_loss did not improve from 21307.48242\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 21103.8984 - rmse: 21116.2695 - val_loss: 23563.0098 - val_rmse: 23544.1504\n",
      "Epoch 40/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18004.6309 - rmse: 18004.6309\n",
      "Epoch 40: val_loss did not improve from 21307.48242\n",
      "70/70 [==============================] - 4s 53ms/step - loss: 18131.2676 - rmse: 18118.2637 - val_loss: 21766.3730 - val_rmse: 21756.3438\n",
      "Epoch 41/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18931.3691 - rmse: 18997.0957\n",
      "Epoch 41: val_loss improved from 21307.48242 to 21076.94336, saving model to ./ckpt/reg_lr005/val_rmse_21129.hdf5\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 18931.3691 - rmse: 18997.0957 - val_loss: 21076.9434 - val_rmse: 21128.5918\n",
      "Epoch 42/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19368.6797 - rmse: 19372.5820\n",
      "Epoch 42: val_loss did not improve from 21076.94336\n",
      "70/70 [==============================] - 5s 66ms/step - loss: 19368.6797 - rmse: 19372.5820 - val_loss: 21202.6367 - val_rmse: 21214.3184\n",
      "Epoch 43/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19309.4863 - rmse: 19299.3926\n",
      "Epoch 43: val_loss improved from 21076.94336 to 21064.37500, saving model to ./ckpt/reg_lr005/val_rmse_21080.hdf5\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 19309.4863 - rmse: 19299.3926 - val_loss: 21064.3750 - val_rmse: 21080.0820\n",
      "Epoch 44/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 18696.8516 - rmse: 18696.8516\n",
      "Epoch 44: val_loss did not improve from 21064.37500\n",
      "70/70 [==============================] - 5s 72ms/step - loss: 18770.2188 - rmse: 18783.0293 - val_loss: 21224.9844 - val_rmse: 21238.9375\n",
      "Epoch 45/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18924.3926 - rmse: 18924.3926\n",
      "Epoch 45: val_loss improved from 21064.37500 to 20392.06250, saving model to ./ckpt/reg_lr005/val_rmse_20394.hdf5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 18815.8027 - rmse: 18804.7617 - val_loss: 20392.0625 - val_rmse: 20393.6504\n",
      "Epoch 46/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17635.0527 - rmse: 17635.0527\n",
      "Epoch 46: val_loss did not improve from 20392.06250\n",
      "70/70 [==============================] - 4s 57ms/step - loss: 17728.5449 - rmse: 17805.7617 - val_loss: 28501.3105 - val_rmse: 28465.5449\n",
      "Epoch 47/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19075.7070 - rmse: 19067.8887\n",
      "Epoch 47: val_loss did not improve from 20392.06250\n",
      "70/70 [==============================] - 4s 57ms/step - loss: 19075.7070 - rmse: 19067.8887 - val_loss: 23001.0234 - val_rmse: 23027.4395\n",
      "Epoch 48/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17948.8496 - rmse: 17948.8496\n",
      "Epoch 48: val_loss did not improve from 20392.06250\n",
      "70/70 [==============================] - 3s 35ms/step - loss: 17926.8184 - rmse: 17913.7871 - val_loss: 24925.6426 - val_rmse: 24959.4629\n",
      "Epoch 49/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18263.9824 - rmse: 18263.9824\n",
      "Epoch 49: val_loss did not improve from 20392.06250\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 18223.0371 - rmse: 18217.0664 - val_loss: 27419.0078 - val_rmse: 27417.9688\n",
      "Epoch 50/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17853.1992 - rmse: 17910.6211\n",
      "Epoch 50: val_loss did not improve from 20392.06250\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 17853.1992 - rmse: 17910.6211 - val_loss: 23704.2520 - val_rmse: 23762.7031\n",
      "Epoch 51/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20894.1875 - rmse: 20894.1875\n",
      "Epoch 51: val_loss did not improve from 20392.06250\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 20821.1113 - rmse: 20846.7207 - val_loss: 32763.3281 - val_rmse: 32795.3164\n",
      "Epoch 52/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17735.6523 - rmse: 17727.1836\n",
      "Epoch 52: val_loss did not improve from 20392.06250\n",
      "70/70 [==============================] - 3s 47ms/step - loss: 17735.6523 - rmse: 17727.1836 - val_loss: 21027.3008 - val_rmse: 21030.1309\n",
      "Epoch 53/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18497.9336 - rmse: 18544.8867\n",
      "Epoch 53: val_loss did not improve from 20392.06250\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 18497.9336 - rmse: 18544.8867 - val_loss: 22710.6504 - val_rmse: 22729.6289\n",
      "Epoch 54/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18434.3418 - rmse: 18434.3418\n",
      "Epoch 54: val_loss did not improve from 20392.06250\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 18528.7188 - rmse: 18584.5332 - val_loss: 26177.9863 - val_rmse: 26204.0977\n",
      "Epoch 55/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19530.1211 - rmse: 19619.9746\n",
      "Epoch 55: val_loss did not improve from 20392.06250\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 19530.1211 - rmse: 19619.9746 - val_loss: 20445.3438 - val_rmse: 20459.4414\n",
      "Epoch 56/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18028.7793 - rmse: 18028.7793\n",
      "Epoch 56: val_loss did not improve from 20392.06250\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 17961.4297 - rmse: 17952.3867 - val_loss: 24341.4258 - val_rmse: 24355.1328\n",
      "Epoch 57/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17273.6309 - rmse: 17250.4082\n",
      "Epoch 57: val_loss did not improve from 20392.06250\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 17273.6309 - rmse: 17250.4082 - val_loss: 22128.3906 - val_rmse: 22168.3828\n",
      "Epoch 58/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16578.8203 - rmse: 16578.8203\n",
      "Epoch 58: val_loss did not improve from 20392.06250\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 16578.7188 - rmse: 16575.2090 - val_loss: 23200.9082 - val_rmse: 23241.8711\n",
      "Epoch 59/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16204.5850 - rmse: 16204.5850\n",
      "Epoch 59: val_loss did not improve from 20392.06250\n",
      "70/70 [==============================] - 4s 53ms/step - loss: 16239.5088 - rmse: 16260.1621 - val_loss: 27171.3730 - val_rmse: 27177.5918\n",
      "Epoch 60/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19893.7285 - rmse: 19878.4883\n",
      "Epoch 60: val_loss improved from 20392.06250 to 19926.15234, saving model to ./ckpt/reg_lr005/val_rmse_19927.hdf5\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 19893.7285 - rmse: 19878.4883 - val_loss: 19926.1523 - val_rmse: 19927.4902\n",
      "Epoch 61/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17714.0938 - rmse: 17714.0938\n",
      "Epoch 61: val_loss did not improve from 19926.15234\n",
      "70/70 [==============================] - 5s 65ms/step - loss: 17706.0391 - rmse: 17686.8750 - val_loss: 21095.8926 - val_rmse: 21133.8789\n",
      "Epoch 62/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17302.8320 - rmse: 17302.8320\n",
      "Epoch 62: val_loss improved from 19926.15234 to 19694.64648, saving model to ./ckpt/reg_lr005/val_rmse_19724.hdf5\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 17305.0391 - rmse: 17306.3457 - val_loss: 19694.6465 - val_rmse: 19723.5000\n",
      "Epoch 63/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17276.2129 - rmse: 17276.2129\n",
      "Epoch 63: val_loss did not improve from 19694.64648\n",
      "70/70 [==============================] - 5s 70ms/step - loss: 17494.7949 - rmse: 17624.0703 - val_loss: 23242.2754 - val_rmse: 23231.3359\n",
      "Epoch 64/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16701.8555 - rmse: 16767.8867\n",
      "Epoch 64: val_loss did not improve from 19694.64648\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 16701.8555 - rmse: 16767.8867 - val_loss: 29057.0410 - val_rmse: 29078.9766\n",
      "Epoch 65/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18187.0977 - rmse: 18169.5488\n",
      "Epoch 65: val_loss did not improve from 19694.64648\n",
      "70/70 [==============================] - 3s 49ms/step - loss: 18187.0977 - rmse: 18169.5488 - val_loss: 25572.3906 - val_rmse: 25620.8750\n",
      "Epoch 66/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16370.5566 - rmse: 16370.5566\n",
      "Epoch 66: val_loss did not improve from 19694.64648\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 16325.7529 - rmse: 16342.5898 - val_loss: 23724.9922 - val_rmse: 23728.6484\n",
      "Epoch 67/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17457.3672 - rmse: 17485.4238\n",
      "Epoch 67: val_loss did not improve from 19694.64648\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 17457.3672 - rmse: 17485.4238 - val_loss: 29656.8848 - val_rmse: 29672.0977\n",
      "Epoch 68/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16452.1113 - rmse: 16511.8027\n",
      "Epoch 68: val_loss did not improve from 19694.64648\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 16452.1113 - rmse: 16511.8027 - val_loss: 35852.1172 - val_rmse: 35874.8594\n",
      "Epoch 69/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16187.6016 - rmse: 16187.6016\n",
      "Epoch 69: val_loss did not improve from 19694.64648\n",
      "70/70 [==============================] - 4s 48ms/step - loss: 16380.3994 - rmse: 16494.4258 - val_loss: 22431.5195 - val_rmse: 22429.0430\n",
      "Epoch 70/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15649.0742 - rmse: 15649.0742\n",
      "Epoch 70: val_loss did not improve from 19694.64648\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 15688.7598 - rmse: 15712.2324 - val_loss: 23298.5020 - val_rmse: 23306.4727\n",
      "Epoch 71/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16909.3926 - rmse: 16909.3926\n",
      "Epoch 71: val_loss did not improve from 19694.64648\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 16908.8145 - rmse: 16908.4707 - val_loss: 20788.9746 - val_rmse: 20855.1758\n",
      "Epoch 72/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17194.0254 - rmse: 17194.0254\n",
      "Epoch 72: val_loss did not improve from 19694.64648\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 17238.9941 - rmse: 17265.5898 - val_loss: 33968.8945 - val_rmse: 34014.7656\n",
      "Epoch 73/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19414.7812 - rmse: 19413.0508\n",
      "Epoch 73: val_loss did not improve from 19694.64648\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 19414.7812 - rmse: 19413.0508 - val_loss: 29259.4199 - val_rmse: 29295.1875\n",
      "Epoch 74/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17662.5859 - rmse: 17716.6504\n",
      "Epoch 74: val_loss did not improve from 19694.64648\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 17662.5859 - rmse: 17716.6504 - val_loss: 29207.0273 - val_rmse: 29202.7070\n",
      "Epoch 75/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16546.1172 - rmse: 16546.1172\n",
      "Epoch 75: val_loss did not improve from 19694.64648\n",
      "70/70 [==============================] - 4s 53ms/step - loss: 16511.1836 - rmse: 16518.2969 - val_loss: 20910.7930 - val_rmse: 20918.1660\n",
      "Epoch 76/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16077.3125 - rmse: 16113.2041\n",
      "Epoch 76: val_loss did not improve from 19694.64648\n",
      "70/70 [==============================] - 3s 49ms/step - loss: 16077.3125 - rmse: 16113.2041 - val_loss: 21246.1094 - val_rmse: 21236.1152\n",
      "Epoch 77/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16269.6377 - rmse: 16269.6377\n",
      "Epoch 77: val_loss did not improve from 19694.64648\n",
      "70/70 [==============================] - 4s 56ms/step - loss: 16303.8906 - rmse: 16317.3535 - val_loss: 23628.5703 - val_rmse: 23651.0352\n",
      "Epoch 78/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16855.6973 - rmse: 16859.9746\n",
      "Epoch 78: val_loss improved from 19694.64648 to 19145.99609, saving model to ./ckpt/reg_lr005/val_rmse_19155.hdf5\n",
      "70/70 [==============================] - 4s 58ms/step - loss: 16855.6973 - rmse: 16859.9746 - val_loss: 19145.9961 - val_rmse: 19155.4590\n",
      "Epoch 79/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17193.1309 - rmse: 17197.7578\n",
      "Epoch 79: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 4s 63ms/step - loss: 17193.1309 - rmse: 17197.7578 - val_loss: 27168.5801 - val_rmse: 27154.2949\n",
      "Epoch 80/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16198.3467 - rmse: 16198.3467\n",
      "Epoch 80: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 4s 58ms/step - loss: 16171.6094 - rmse: 16155.7969 - val_loss: 28951.1406 - val_rmse: 28949.9902\n",
      "Epoch 81/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16857.3887 - rmse: 16909.7207\n",
      "Epoch 81: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 3s 50ms/step - loss: 16857.3887 - rmse: 16909.7207 - val_loss: 19744.7090 - val_rmse: 19748.5156\n",
      "Epoch 82/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16132.0078 - rmse: 16132.0078\n",
      "Epoch 82: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 16062.2324 - rmse: 16063.0449 - val_loss: 20509.9160 - val_rmse: 20515.3477\n",
      "Epoch 83/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15294.7900 - rmse: 15294.7900\n",
      "Epoch 83: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 4s 57ms/step - loss: 15382.4512 - rmse: 15434.2949 - val_loss: 20494.7168 - val_rmse: 20472.5391\n",
      "Epoch 84/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15288.4492 - rmse: 15288.4492\n",
      "Epoch 84: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 4s 50ms/step - loss: 15338.5967 - rmse: 15390.4912 - val_loss: 26884.6797 - val_rmse: 26883.1680\n",
      "Epoch 85/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16116.5859 - rmse: 16140.0684\n",
      "Epoch 85: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 16116.5859 - rmse: 16140.0684 - val_loss: 29986.0957 - val_rmse: 29980.0508\n",
      "Epoch 86/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15507.8389 - rmse: 15533.4824\n",
      "Epoch 86: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 3s 49ms/step - loss: 15507.8389 - rmse: 15533.4824 - val_loss: 30561.3848 - val_rmse: 30581.4316\n",
      "Epoch 87/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18702.6211 - rmse: 18687.0254\n",
      "Epoch 87: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 18702.6211 - rmse: 18687.0254 - val_loss: 26277.0254 - val_rmse: 26287.2578\n",
      "Epoch 88/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 15889.7314 - rmse: 15889.7314\n",
      "Epoch 88: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 2s 36ms/step - loss: 15852.3574 - rmse: 15866.3320 - val_loss: 19693.0547 - val_rmse: 19713.1484\n",
      "Epoch 89/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15899.6094 - rmse: 15884.8018\n",
      "Epoch 89: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 15899.6094 - rmse: 15884.8018 - val_loss: 21089.5000 - val_rmse: 21122.4707\n",
      "Epoch 90/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15819.8838 - rmse: 15843.4844\n",
      "Epoch 90: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 15819.8838 - rmse: 15843.4844 - val_loss: 24810.3379 - val_rmse: 24839.9863\n",
      "Epoch 91/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18017.6973 - rmse: 18017.6973\n",
      "Epoch 91: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 18089.9395 - rmse: 18132.6660 - val_loss: 20042.4746 - val_rmse: 20038.1602\n",
      "Epoch 92/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15866.3936 - rmse: 15866.3936\n",
      "Epoch 92: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 15804.0977 - rmse: 15802.4248 - val_loss: 22076.1152 - val_rmse: 22104.6562\n",
      "Epoch 93/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14839.9092 - rmse: 14839.7363\n",
      "Epoch 93: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 3s 50ms/step - loss: 14839.9092 - rmse: 14839.7363 - val_loss: 24511.6641 - val_rmse: 24563.7090\n",
      "Epoch 94/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15408.3271 - rmse: 15408.3271\n",
      "Epoch 94: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 2s 36ms/step - loss: 15528.1064 - rmse: 15582.4463 - val_loss: 20498.4512 - val_rmse: 20550.9805\n",
      "Epoch 95/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15296.7988 - rmse: 15292.1914\n",
      "Epoch 95: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 4s 60ms/step - loss: 15296.7988 - rmse: 15292.1914 - val_loss: 21352.4316 - val_rmse: 21347.3047\n",
      "Epoch 96/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15319.0186 - rmse: 15319.0186\n",
      "Epoch 96: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 15322.7705 - rmse: 15324.9873 - val_loss: 19868.5391 - val_rmse: 19869.3789\n",
      "Epoch 97/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16055.1777 - rmse: 16065.5605\n",
      "Epoch 97: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 4s 50ms/step - loss: 16055.1777 - rmse: 16065.5605 - val_loss: 21411.7383 - val_rmse: 21434.3438\n",
      "Epoch 98/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14737.0879 - rmse: 14776.1758\n",
      "Epoch 98: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 14737.0879 - rmse: 14776.1758 - val_loss: 20301.1523 - val_rmse: 20302.6719\n",
      "Epoch 99/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15558.2822 - rmse: 15572.7354\n",
      "Epoch 99: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 3s 49ms/step - loss: 15558.2822 - rmse: 15572.7354 - val_loss: 21040.7539 - val_rmse: 21076.6270\n",
      "Epoch 100/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16034.4814 - rmse: 16006.1729\n",
      "Epoch 100: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 16034.4814 - rmse: 16006.1729 - val_loss: 30440.6914 - val_rmse: 30436.3301\n",
      "Epoch 101/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 14794.5576 - rmse: 14794.5576\n",
      "Epoch 101: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 3s 49ms/step - loss: 14809.2266 - rmse: 14817.9023 - val_loss: 20452.6953 - val_rmse: 20459.1348\n",
      "Epoch 102/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16616.9453 - rmse: 16616.8027\n",
      "Epoch 102: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 4s 53ms/step - loss: 16616.9453 - rmse: 16616.8027 - val_loss: 28589.2070 - val_rmse: 28604.5840\n",
      "Epoch 103/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15119.4492 - rmse: 15119.4492\n",
      "Epoch 103: val_loss did not improve from 19145.99609\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 15117.3486 - rmse: 15116.8711 - val_loss: 20268.4121 - val_rmse: 20277.7480\n",
      "Epoch 103: early stopping\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 14:08:24.481016: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 107312.1250 - rmse: 106976.3984"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 14:08:28.007575: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 50500.27734, saving model to ./ckpt/reg_lr005/val_rmse_51135.hdf5\n",
      "70/70 [==============================] - 5s 52ms/step - loss: 107312.1250 - rmse: 106976.3984 - val_loss: 50500.2773 - val_rmse: 51134.7305\n",
      "Epoch 2/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 52082.1953 - rmse: 52048.6289\n",
      "Epoch 2: val_loss improved from 50500.27734 to 37282.20312, saving model to ./ckpt/reg_lr005/val_rmse_37378.hdf5\n",
      "70/70 [==============================] - 4s 61ms/step - loss: 52082.1953 - rmse: 52048.6289 - val_loss: 37282.2031 - val_rmse: 37377.5391\n",
      "Epoch 3/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 47712.3672 - rmse: 47638.8984\n",
      "Epoch 3: val_loss improved from 37282.20312 to 35222.20312, saving model to ./ckpt/reg_lr005/val_rmse_35378.hdf5\n",
      "70/70 [==============================] - 4s 60ms/step - loss: 47712.3672 - rmse: 47638.8984 - val_loss: 35222.2031 - val_rmse: 35378.2344\n",
      "Epoch 4/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 44077.6250 - rmse: 44125.8906\n",
      "Epoch 4: val_loss improved from 35222.20312 to 34136.40234, saving model to ./ckpt/reg_lr005/val_rmse_34042.hdf5\n",
      "70/70 [==============================] - 4s 59ms/step - loss: 44077.6250 - rmse: 44125.8906 - val_loss: 34136.4023 - val_rmse: 34042.1797\n",
      "Epoch 5/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 42257.3203 - rmse: 42270.2109\n",
      "Epoch 5: val_loss improved from 34136.40234 to 32113.68359, saving model to ./ckpt/reg_lr005/val_rmse_32044.hdf5\n",
      "70/70 [==============================] - 5s 66ms/step - loss: 42257.3203 - rmse: 42270.2109 - val_loss: 32113.6836 - val_rmse: 32043.9883\n",
      "Epoch 6/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 41713.5000 - rmse: 41616.0508\n",
      "Epoch 6: val_loss improved from 32113.68359 to 30163.30859, saving model to ./ckpt/reg_lr005/val_rmse_30121.hdf5\n",
      "70/70 [==============================] - 5s 69ms/step - loss: 41713.5000 - rmse: 41616.0508 - val_loss: 30163.3086 - val_rmse: 30121.3906\n",
      "Epoch 7/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 40011.3359 - rmse: 40011.3359\n",
      "Epoch 7: val_loss improved from 30163.30859 to 29696.89062, saving model to ./ckpt/reg_lr005/val_rmse_29652.hdf5\n",
      "70/70 [==============================] - 4s 56ms/step - loss: 39825.8633 - rmse: 39749.1406 - val_loss: 29696.8906 - val_rmse: 29651.9199\n",
      "Epoch 8/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 36874.7578 - rmse: 36874.7578\n",
      "Epoch 8: val_loss improved from 29696.89062 to 27094.91406, saving model to ./ckpt/reg_lr005/val_rmse_27105.hdf5\n",
      "70/70 [==============================] - 4s 55ms/step - loss: 36947.8672 - rmse: 36991.1055 - val_loss: 27094.9141 - val_rmse: 27105.0293\n",
      "Epoch 9/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 37508.7500 - rmse: 37504.0586\n",
      "Epoch 9: val_loss improved from 27094.91406 to 25689.13867, saving model to ./ckpt/reg_lr005/val_rmse_25695.hdf5\n",
      "70/70 [==============================] - 4s 57ms/step - loss: 37508.7500 - rmse: 37504.0586 - val_loss: 25689.1387 - val_rmse: 25694.5410\n",
      "Epoch 10/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 35318.9688 - rmse: 35318.9688\n",
      "Epoch 10: val_loss did not improve from 25689.13867\n",
      "70/70 [==============================] - 6s 84ms/step - loss: 35245.4805 - rmse: 35202.0195 - val_loss: 28012.7324 - val_rmse: 28082.1816\n",
      "Epoch 11/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 32205.4453 - rmse: 32205.4453\n",
      "Epoch 11: val_loss did not improve from 25689.13867\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 32198.6035 - rmse: 32194.5566 - val_loss: 31609.8008 - val_rmse: 31681.1289\n",
      "Epoch 12/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 32744.5312 - rmse: 32738.0859\n",
      "Epoch 12: val_loss improved from 25689.13867 to 22597.67383, saving model to ./ckpt/reg_lr005/val_rmse_22647.hdf5\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 32744.5312 - rmse: 32738.0859 - val_loss: 22597.6738 - val_rmse: 22646.8945\n",
      "Epoch 13/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 30884.6211 - rmse: 30837.1758\n",
      "Epoch 13: val_loss improved from 22597.67383 to 21030.73438, saving model to ./ckpt/reg_lr005/val_rmse_21091.hdf5\n",
      "70/70 [==============================] - 6s 86ms/step - loss: 30884.6211 - rmse: 30837.1758 - val_loss: 21030.7344 - val_rmse: 21090.7461\n",
      "Epoch 14/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27745.3066 - rmse: 27744.9258\n",
      "Epoch 14: val_loss improved from 21030.73438 to 20077.05664, saving model to ./ckpt/reg_lr005/val_rmse_20143.hdf5\n",
      "70/70 [==============================] - 6s 83ms/step - loss: 27745.3066 - rmse: 27744.9258 - val_loss: 20077.0566 - val_rmse: 20142.8750\n",
      "Epoch 15/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 27886.8984 - rmse: 27909.6582\n",
      "Epoch 15: val_loss did not improve from 20077.05664\n",
      "70/70 [==============================] - 6s 83ms/step - loss: 27886.8984 - rmse: 27909.6582 - val_loss: 37487.0625 - val_rmse: 37569.1797\n",
      "Epoch 16/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 27225.6191 - rmse: 27225.6191\n",
      "Epoch 16: val_loss did not improve from 20077.05664\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 27250.2520 - rmse: 27264.8203 - val_loss: 21297.0840 - val_rmse: 21325.0293\n",
      "Epoch 17/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 28647.2012 - rmse: 28655.6523\n",
      "Epoch 17: val_loss improved from 20077.05664 to 17990.31055, saving model to ./ckpt/reg_lr005/val_rmse_18050.hdf5\n",
      "70/70 [==============================] - 4s 59ms/step - loss: 28647.2012 - rmse: 28655.6523 - val_loss: 17990.3105 - val_rmse: 18050.3633\n",
      "Epoch 18/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 28889.9941 - rmse: 28889.9941\n",
      "Epoch 18: val_loss did not improve from 17990.31055\n",
      "70/70 [==============================] - 4s 57ms/step - loss: 28783.4648 - rmse: 28720.4590 - val_loss: 18088.7812 - val_rmse: 18162.3262\n",
      "Epoch 19/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25312.2559 - rmse: 25312.9590\n",
      "Epoch 19: val_loss did not improve from 17990.31055\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 25312.2559 - rmse: 25312.9590 - val_loss: 19593.7031 - val_rmse: 19683.1914\n",
      "Epoch 20/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 24084.4688 - rmse: 24084.4688\n",
      "Epoch 20: val_loss did not improve from 17990.31055\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 24082.3750 - rmse: 24081.1387 - val_loss: 23645.7520 - val_rmse: 23676.5547\n",
      "Epoch 21/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 26840.9512 - rmse: 26838.7773\n",
      "Epoch 21: val_loss improved from 17990.31055 to 16638.53125, saving model to ./ckpt/reg_lr005/val_rmse_16669.hdf5\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 26840.9512 - rmse: 26838.7773 - val_loss: 16638.5312 - val_rmse: 16669.4766\n",
      "Epoch 22/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 25398.5605 - rmse: 25366.8340\n",
      "Epoch 22: val_loss did not improve from 16638.53125\n",
      "70/70 [==============================] - 6s 85ms/step - loss: 25398.5605 - rmse: 25366.8340 - val_loss: 21702.1680 - val_rmse: 21828.5273\n",
      "Epoch 23/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21708.4414 - rmse: 21708.4414\n",
      "Epoch 23: val_loss did not improve from 16638.53125\n",
      "70/70 [==============================] - 5s 76ms/step - loss: 22031.0840 - rmse: 22140.1992 - val_loss: 24484.9238 - val_rmse: 24583.1250\n",
      "Epoch 24/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 21530.1523 - rmse: 21530.1523\n",
      "Epoch 24: val_loss improved from 16638.53125 to 16550.21484, saving model to ./ckpt/reg_lr005/val_rmse_16612.hdf5\n",
      "70/70 [==============================] - 5s 64ms/step - loss: 21674.1602 - rmse: 21733.0156 - val_loss: 16550.2148 - val_rmse: 16611.7168\n",
      "Epoch 25/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21775.9883 - rmse: 21794.7012\n",
      "Epoch 25: val_loss did not improve from 16550.21484\n",
      "70/70 [==============================] - 4s 63ms/step - loss: 21775.9883 - rmse: 21794.7012 - val_loss: 16891.1602 - val_rmse: 16924.9316\n",
      "Epoch 26/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 24020.6934 - rmse: 24020.6934\n",
      "Epoch 26: val_loss did not improve from 16550.21484\n",
      "70/70 [==============================] - 3s 49ms/step - loss: 24042.0332 - rmse: 24107.9355 - val_loss: 36676.8281 - val_rmse: 36812.0664\n",
      "Epoch 27/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 22843.4121 - rmse: 22843.4121\n",
      "Epoch 27: val_loss did not improve from 16550.21484\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 22826.8887 - rmse: 22817.1152 - val_loss: 26306.6152 - val_rmse: 26421.6699\n",
      "Epoch 28/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21794.5508 - rmse: 21771.6875\n",
      "Epoch 28: val_loss improved from 16550.21484 to 16329.61035, saving model to ./ckpt/reg_lr005/val_rmse_16449.hdf5\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 21794.5508 - rmse: 21771.6875 - val_loss: 16329.6104 - val_rmse: 16448.9824\n",
      "Epoch 29/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21573.9629 - rmse: 21541.9355\n",
      "Epoch 29: val_loss did not improve from 16329.61035\n",
      "70/70 [==============================] - 4s 62ms/step - loss: 21573.9629 - rmse: 21541.9355 - val_loss: 19966.1289 - val_rmse: 20119.4648\n",
      "Epoch 30/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 20001.8086 - rmse: 20001.8086\n",
      "Epoch 30: val_loss improved from 16329.61035 to 15384.57910, saving model to ./ckpt/reg_lr005/val_rmse_15512.hdf5\n",
      "70/70 [==============================] - 4s 55ms/step - loss: 20031.5078 - rmse: 20069.0176 - val_loss: 15384.5791 - val_rmse: 15511.8018\n",
      "Epoch 31/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20723.4141 - rmse: 20710.6914\n",
      "Epoch 31: val_loss did not improve from 15384.57910\n",
      "70/70 [==============================] - 5s 77ms/step - loss: 20723.4141 - rmse: 20710.6914 - val_loss: 28678.2617 - val_rmse: 28840.2461\n",
      "Epoch 32/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21676.4805 - rmse: 21664.6660\n",
      "Epoch 32: val_loss did not improve from 15384.57910\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 21676.4805 - rmse: 21664.6660 - val_loss: 23141.2598 - val_rmse: 23197.8984\n",
      "Epoch 33/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 24595.1270 - rmse: 24561.3516\n",
      "Epoch 33: val_loss did not improve from 15384.57910\n",
      "70/70 [==============================] - 4s 50ms/step - loss: 24595.1270 - rmse: 24561.3516 - val_loss: 17242.9121 - val_rmse: 17323.1094\n",
      "Epoch 34/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19647.5137 - rmse: 19637.8105\n",
      "Epoch 34: val_loss improved from 15384.57910 to 15270.03223, saving model to ./ckpt/reg_lr005/val_rmse_15363.hdf5\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 19647.5137 - rmse: 19637.8105 - val_loss: 15270.0322 - val_rmse: 15362.5664\n",
      "Epoch 35/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20364.7070 - rmse: 20364.7070\n",
      "Epoch 35: val_loss did not improve from 15270.03223\n",
      "70/70 [==============================] - 6s 79ms/step - loss: 20484.6875 - rmse: 20555.6484 - val_loss: 23511.1270 - val_rmse: 23713.4219\n",
      "Epoch 36/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20222.0410 - rmse: 20210.9551\n",
      "Epoch 36: val_loss did not improve from 15270.03223\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 20222.0410 - rmse: 20210.9551 - val_loss: 19049.0078 - val_rmse: 19128.5488\n",
      "Epoch 37/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20105.1133 - rmse: 20125.1699\n",
      "Epoch 37: val_loss did not improve from 15270.03223\n",
      "70/70 [==============================] - 3s 48ms/step - loss: 20105.1133 - rmse: 20125.1699 - val_loss: 17070.4492 - val_rmse: 17162.7520\n",
      "Epoch 38/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19152.5508 - rmse: 19152.5508\n",
      "Epoch 38: val_loss did not improve from 15270.03223\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 19197.7637 - rmse: 19224.5039 - val_loss: 18052.4395 - val_rmse: 18200.1582\n",
      "Epoch 39/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 20770.4004 - rmse: 20770.4004\n",
      "Epoch 39: val_loss did not improve from 15270.03223\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 20737.3965 - rmse: 20717.8770 - val_loss: 15548.4639 - val_rmse: 15662.6963\n",
      "Epoch 40/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19834.2617 - rmse: 19826.4629\n",
      "Epoch 40: val_loss improved from 15270.03223 to 15072.06641, saving model to ./ckpt/reg_lr005/val_rmse_15208.hdf5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 19834.2617 - rmse: 19826.4629 - val_loss: 15072.0664 - val_rmse: 15207.9209\n",
      "Epoch 41/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19534.7734 - rmse: 19528.6953\n",
      "Epoch 41: val_loss did not improve from 15072.06641\n",
      "70/70 [==============================] - 5s 71ms/step - loss: 19534.7734 - rmse: 19528.6953 - val_loss: 22078.6191 - val_rmse: 22226.0723\n",
      "Epoch 42/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20192.0488 - rmse: 20249.4141\n",
      "Epoch 42: val_loss did not improve from 15072.06641\n",
      "70/70 [==============================] - 5s 78ms/step - loss: 20192.0488 - rmse: 20249.4141 - val_loss: 15853.8105 - val_rmse: 15988.5850\n",
      "Epoch 43/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21304.8359 - rmse: 21304.8359\n",
      "Epoch 43: val_loss did not improve from 15072.06641\n",
      "70/70 [==============================] - 3s 50ms/step - loss: 21504.7520 - rmse: 21622.9902 - val_loss: 30395.9434 - val_rmse: 30510.2695\n",
      "Epoch 44/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 20273.7793 - rmse: 20273.7793\n",
      "Epoch 44: val_loss did not improve from 15072.06641\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 20504.5430 - rmse: 20524.7090 - val_loss: 23443.8516 - val_rmse: 23550.8047\n",
      "Epoch 45/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18977.7969 - rmse: 18977.7969\n",
      "Epoch 45: val_loss did not improve from 15072.06641\n",
      "70/70 [==============================] - 4s 61ms/step - loss: 18900.0859 - rmse: 18912.9316 - val_loss: 19646.0020 - val_rmse: 19756.6152\n",
      "Epoch 46/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 19740.8906 - rmse: 19740.8906\n",
      "Epoch 46: val_loss improved from 15072.06641 to 14289.28613, saving model to ./ckpt/reg_lr005/val_rmse_14396.hdf5\n",
      "70/70 [==============================] - 4s 49ms/step - loss: 19766.5430 - rmse: 19781.7148 - val_loss: 14289.2861 - val_rmse: 14396.0293\n",
      "Epoch 47/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19684.0938 - rmse: 19732.2520\n",
      "Epoch 47: val_loss did not improve from 14289.28613\n",
      "70/70 [==============================] - 5s 68ms/step - loss: 19684.0938 - rmse: 19732.2520 - val_loss: 20498.9395 - val_rmse: 20573.1777\n",
      "Epoch 48/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 21328.4902 - rmse: 21328.4902\n",
      "Epoch 48: val_loss did not improve from 14289.28613\n",
      "70/70 [==============================] - 5s 67ms/step - loss: 21311.0820 - rmse: 21300.7832 - val_loss: 19570.7734 - val_rmse: 19674.9629\n",
      "Epoch 49/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 18243.0059 - rmse: 18243.0059\n",
      "Epoch 49: val_loss improved from 14289.28613 to 14112.10938, saving model to ./ckpt/reg_lr005/val_rmse_14203.hdf5\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 18204.9434 - rmse: 18202.4277 - val_loss: 14112.1094 - val_rmse: 14203.2061\n",
      "Epoch 50/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 20080.9395 - rmse: 20087.8262\n",
      "Epoch 50: val_loss did not improve from 14112.10938\n",
      "70/70 [==============================] - 7s 97ms/step - loss: 20080.9395 - rmse: 20087.8262 - val_loss: 22799.5488 - val_rmse: 22918.5879\n",
      "Epoch 51/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18174.8164 - rmse: 18145.6387\n",
      "Epoch 51: val_loss improved from 14112.10938 to 14048.90137, saving model to ./ckpt/reg_lr005/val_rmse_14142.hdf5\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 18174.8164 - rmse: 18145.6387 - val_loss: 14048.9014 - val_rmse: 14142.0059\n",
      "Epoch 52/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17970.4043 - rmse: 18129.7656\n",
      "Epoch 52: val_loss did not improve from 14048.90137\n",
      "70/70 [==============================] - 5s 70ms/step - loss: 17970.4043 - rmse: 18129.7656 - val_loss: 14233.2090 - val_rmse: 14324.2246\n",
      "Epoch 53/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18567.1152 - rmse: 18631.7129\n",
      "Epoch 53: val_loss did not improve from 14048.90137\n",
      "70/70 [==============================] - 5s 69ms/step - loss: 18567.1152 - rmse: 18631.7129 - val_loss: 14476.4238 - val_rmse: 14549.6035\n",
      "Epoch 54/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18471.6387 - rmse: 18443.2793\n",
      "Epoch 54: val_loss did not improve from 14048.90137\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 18471.6387 - rmse: 18443.2793 - val_loss: 16593.8789 - val_rmse: 16716.4805\n",
      "Epoch 55/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18737.5039 - rmse: 18737.5039\n",
      "Epoch 55: val_loss did not improve from 14048.90137\n",
      "70/70 [==============================] - 4s 49ms/step - loss: 18745.1016 - rmse: 18749.5938 - val_loss: 34429.3594 - val_rmse: 34568.7930\n",
      "Epoch 56/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 21603.0371 - rmse: 21571.7207\n",
      "Epoch 56: val_loss improved from 14048.90137 to 13968.17383, saving model to ./ckpt/reg_lr005/val_rmse_14042.hdf5\n",
      "70/70 [==============================] - 4s 60ms/step - loss: 21603.0371 - rmse: 21571.7207 - val_loss: 13968.1738 - val_rmse: 14042.1074\n",
      "Epoch 57/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17484.4863 - rmse: 17516.3105\n",
      "Epoch 57: val_loss did not improve from 13968.17383\n",
      "70/70 [==============================] - 5s 67ms/step - loss: 17484.4863 - rmse: 17516.3105 - val_loss: 20769.0801 - val_rmse: 20884.0352\n",
      "Epoch 58/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18604.3320 - rmse: 18596.7285\n",
      "Epoch 58: val_loss did not improve from 13968.17383\n",
      "70/70 [==============================] - 3s 47ms/step - loss: 18604.3320 - rmse: 18596.7285 - val_loss: 22321.4785 - val_rmse: 22415.3047\n",
      "Epoch 59/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18213.4766 - rmse: 18213.4766\n",
      "Epoch 59: val_loss did not improve from 13968.17383\n",
      "70/70 [==============================] - 5s 67ms/step - loss: 18367.3555 - rmse: 18458.3652 - val_loss: 18226.8789 - val_rmse: 18287.8164\n",
      "Epoch 60/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 17492.5645 - rmse: 17492.5645\n",
      "Epoch 60: val_loss did not improve from 13968.17383\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 17600.4102 - rmse: 17607.7812 - val_loss: 23176.5918 - val_rmse: 23244.1504\n",
      "Epoch 61/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18249.4727 - rmse: 18261.5957\n",
      "Epoch 61: val_loss did not improve from 13968.17383\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 18249.4727 - rmse: 18261.5957 - val_loss: 21772.5723 - val_rmse: 21859.5957\n",
      "Epoch 62/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18683.4570 - rmse: 18683.4570\n",
      "Epoch 62: val_loss did not improve from 13968.17383\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 18661.4180 - rmse: 18648.3848 - val_loss: 14306.8955 - val_rmse: 14335.7998\n",
      "Epoch 63/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16884.1914 - rmse: 16884.1914\n",
      "Epoch 63: val_loss did not improve from 13968.17383\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 16854.2383 - rmse: 16836.5254 - val_loss: 16197.0439 - val_rmse: 16223.0938\n",
      "Epoch 64/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18054.3535 - rmse: 18054.3535\n",
      "Epoch 64: val_loss did not improve from 13968.17383\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 18080.2969 - rmse: 18095.6367 - val_loss: 20905.2344 - val_rmse: 21004.8730\n",
      "Epoch 65/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17050.5801 - rmse: 17030.7305\n",
      "Epoch 65: val_loss did not improve from 13968.17383\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 17050.5801 - rmse: 17030.7305 - val_loss: 23135.6484 - val_rmse: 23173.0586\n",
      "Epoch 66/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17408.6777 - rmse: 17432.7559\n",
      "Epoch 66: val_loss did not improve from 13968.17383\n",
      "70/70 [==============================] - 4s 55ms/step - loss: 17408.6777 - rmse: 17432.7559 - val_loss: 25781.3594 - val_rmse: 25838.0957\n",
      "Epoch 67/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18111.2012 - rmse: 18096.8711\n",
      "Epoch 67: val_loss improved from 13968.17383 to 13621.57031, saving model to ./ckpt/reg_lr005/val_rmse_13640.hdf5\n",
      "70/70 [==============================] - 4s 58ms/step - loss: 18111.2012 - rmse: 18096.8711 - val_loss: 13621.5703 - val_rmse: 13639.9707\n",
      "Epoch 68/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17387.3457 - rmse: 17387.3457\n",
      "Epoch 68: val_loss did not improve from 13621.57031\n",
      "70/70 [==============================] - 5s 78ms/step - loss: 17501.7305 - rmse: 17569.3809 - val_loss: 16086.4766 - val_rmse: 16092.5791\n",
      "Epoch 69/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18174.3066 - rmse: 18193.7109\n",
      "Epoch 69: val_loss improved from 13621.57031 to 13445.18652, saving model to ./ckpt/reg_lr005/val_rmse_13437.hdf5\n",
      "70/70 [==============================] - 6s 78ms/step - loss: 18174.3066 - rmse: 18193.7109 - val_loss: 13445.1865 - val_rmse: 13437.4707\n",
      "Epoch 70/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17473.4297 - rmse: 17473.4297\n",
      "Epoch 70: val_loss did not improve from 13445.18652\n",
      "70/70 [==============================] - 5s 66ms/step - loss: 17455.6582 - rmse: 17445.1465 - val_loss: 17667.1113 - val_rmse: 17688.5176\n",
      "Epoch 71/120\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17959.3965 - rmse: 17959.3965\n",
      "Epoch 71: val_loss did not improve from 13445.18652\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 17951.6016 - rmse: 17947.1035 - val_loss: 13919.5225 - val_rmse: 13854.3135\n",
      "Epoch 72/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 18693.8867 - rmse: 18693.8867\n",
      "Epoch 72: val_loss did not improve from 13445.18652\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 18713.4941 - rmse: 18725.0918 - val_loss: 16688.5293 - val_rmse: 16682.3750\n",
      "Epoch 73/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17390.7676 - rmse: 17365.0332\n",
      "Epoch 73: val_loss did not improve from 13445.18652\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 17390.7676 - rmse: 17365.0332 - val_loss: 20000.1641 - val_rmse: 20033.7285\n",
      "Epoch 74/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16741.6621 - rmse: 16741.6621\n",
      "Epoch 74: val_loss did not improve from 13445.18652\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 16685.4961 - rmse: 16668.0742 - val_loss: 24110.3301 - val_rmse: 24133.8652\n",
      "Epoch 75/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17356.3770 - rmse: 17356.3770\n",
      "Epoch 75: val_loss did not improve from 13445.18652\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 17355.5820 - rmse: 17355.1113 - val_loss: 24558.4980 - val_rmse: 24588.7090\n",
      "Epoch 76/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16944.1504 - rmse: 16954.5332\n",
      "Epoch 76: val_loss did not improve from 13445.18652\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 16944.1504 - rmse: 16954.5332 - val_loss: 16963.6973 - val_rmse: 17009.2734\n",
      "Epoch 77/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18430.9492 - rmse: 18396.0156\n",
      "Epoch 77: val_loss did not improve from 13445.18652\n",
      "70/70 [==============================] - 3s 47ms/step - loss: 18430.9492 - rmse: 18396.0156 - val_loss: 23673.7910 - val_rmse: 23685.5820\n",
      "Epoch 78/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17252.0391 - rmse: 17232.2715\n",
      "Epoch 78: val_loss did not improve from 13445.18652\n",
      "70/70 [==============================] - 4s 56ms/step - loss: 17252.0391 - rmse: 17232.2715 - val_loss: 21879.9082 - val_rmse: 21937.6621\n",
      "Epoch 79/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16244.9121 - rmse: 16277.9736\n",
      "Epoch 79: val_loss did not improve from 13445.18652\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 16244.9121 - rmse: 16277.9736 - val_loss: 17530.4629 - val_rmse: 17551.9453\n",
      "Epoch 80/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16105.2529 - rmse: 16105.2529\n",
      "Epoch 80: val_loss did not improve from 13445.18652\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 16209.9541 - rmse: 16233.8105 - val_loss: 14105.7900 - val_rmse: 14059.2744\n",
      "Epoch 81/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 16189.9355 - rmse: 16269.5693\n",
      "Epoch 81: val_loss did not improve from 13445.18652\n",
      "70/70 [==============================] - 3s 48ms/step - loss: 16189.9355 - rmse: 16269.5693 - val_loss: 17996.4414 - val_rmse: 18034.7910\n",
      "Epoch 82/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17094.0605 - rmse: 17139.8223\n",
      "Epoch 82: val_loss did not improve from 13445.18652\n",
      "70/70 [==============================] - 5s 68ms/step - loss: 17094.0605 - rmse: 17139.8223 - val_loss: 18964.7773 - val_rmse: 18987.7598\n",
      "Epoch 83/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15370.3643 - rmse: 15370.3643\n",
      "Epoch 83: val_loss improved from 13445.18652 to 12907.06934, saving model to ./ckpt/reg_lr005/val_rmse_12866.hdf5\n",
      "70/70 [==============================] - 4s 59ms/step - loss: 15350.5713 - rmse: 15338.8643 - val_loss: 12907.0693 - val_rmse: 12866.1211\n",
      "Epoch 84/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17273.8262 - rmse: 17269.4199\n",
      "Epoch 84: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 6s 82ms/step - loss: 17273.8262 - rmse: 17269.4199 - val_loss: 28492.0801 - val_rmse: 28571.4062\n",
      "Epoch 85/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 17123.9004 - rmse: 17123.9004\n",
      "Epoch 85: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 6s 91ms/step - loss: 17220.0156 - rmse: 17276.8613 - val_loss: 27587.0996 - val_rmse: 27634.9688\n",
      "Epoch 86/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 18348.3047 - rmse: 18381.4434\n",
      "Epoch 86: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 4s 56ms/step - loss: 18348.3047 - rmse: 18381.4434 - val_loss: 14625.1836 - val_rmse: 14613.5596\n",
      "Epoch 87/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16485.9961 - rmse: 16485.9961\n",
      "Epoch 87: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 4s 55ms/step - loss: 16436.8574 - rmse: 16440.2129 - val_loss: 19978.3848 - val_rmse: 20001.5527\n",
      "Epoch 88/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 16931.8359 - rmse: 16931.8359\n",
      "Epoch 88: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 5s 66ms/step - loss: 17062.0059 - rmse: 17109.8086 - val_loss: 14351.6650 - val_rmse: 14365.6865\n",
      "Epoch 89/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 19799.5898 - rmse: 19855.1953\n",
      "Epoch 89: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 5s 64ms/step - loss: 19799.5898 - rmse: 19855.1953 - val_loss: 14007.1309 - val_rmse: 14014.4346\n",
      "Epoch 90/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16789.6641 - rmse: 16789.6641\n",
      "Epoch 90: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 5s 69ms/step - loss: 16714.2227 - rmse: 16695.1562 - val_loss: 23474.1973 - val_rmse: 23542.4980\n",
      "Epoch 91/120\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 16985.4004 - rmse: 16985.4004\n",
      "Epoch 91: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 4s 58ms/step - loss: 17200.5176 - rmse: 17210.4844 - val_loss: 22005.8184 - val_rmse: 22033.2715\n",
      "Epoch 92/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 17294.9570 - rmse: 17339.1914\n",
      "Epoch 92: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 5s 74ms/step - loss: 17294.9570 - rmse: 17339.1914 - val_loss: 13847.4922 - val_rmse: 13801.9844\n",
      "Epoch 93/120\n",
      "68/70 [============================>.] - ETA: 0s - loss: 15854.9043 - rmse: 15854.9043\n",
      "Epoch 93: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 5s 64ms/step - loss: 15782.2012 - rmse: 15780.0283 - val_loss: 20508.5254 - val_rmse: 20513.7969\n",
      "Epoch 94/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15929.0322 - rmse: 15961.3232\n",
      "Epoch 94: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 5s 67ms/step - loss: 15929.0322 - rmse: 15961.3232 - val_loss: 30422.6094 - val_rmse: 30509.3223\n",
      "Epoch 95/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 16067.6230 - rmse: 16067.6230\n",
      "Epoch 95: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 5s 72ms/step - loss: 16062.8369 - rmse: 16060.0059 - val_loss: 14964.3193 - val_rmse: 14963.5322\n",
      "Epoch 96/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15467.4775 - rmse: 15465.1016\n",
      "Epoch 96: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 8s 117ms/step - loss: 15467.4775 - rmse: 15465.1016 - val_loss: 17381.7715 - val_rmse: 17415.4531\n",
      "Epoch 97/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15675.4512 - rmse: 15671.0234\n",
      "Epoch 97: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 10s 145ms/step - loss: 15675.4512 - rmse: 15671.0234 - val_loss: 17495.2988 - val_rmse: 17504.9570\n",
      "Epoch 98/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15195.8457 - rmse: 15199.0771\n",
      "Epoch 98: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 7s 100ms/step - loss: 15195.8457 - rmse: 15199.0771 - val_loss: 14940.1592 - val_rmse: 14923.7734\n",
      "Epoch 99/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 14635.5625 - rmse: 14635.5625\n",
      "Epoch 99: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 12s 167ms/step - loss: 14723.2129 - rmse: 14775.0518 - val_loss: 20914.5801 - val_rmse: 20915.0352\n",
      "Epoch 100/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 14292.5420 - rmse: 14292.5420\n",
      "Epoch 100: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 9s 132ms/step - loss: 14452.3525 - rmse: 14546.8682 - val_loss: 24605.8340 - val_rmse: 24637.8594\n",
      "Epoch 101/120\n",
      "69/70 [============================>.] - ETA: 0s - loss: 15475.4004 - rmse: 15475.4004\n",
      "Epoch 101: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 6s 85ms/step - loss: 15513.8857 - rmse: 15536.6465 - val_loss: 15262.1416 - val_rmse: 15268.1943\n",
      "Epoch 102/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15376.3418 - rmse: 15366.7842\n",
      "Epoch 102: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 12s 169ms/step - loss: 15376.3418 - rmse: 15366.7842 - val_loss: 17630.5176 - val_rmse: 17669.1094\n",
      "Epoch 103/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15225.3652 - rmse: 15251.5586\n",
      "Epoch 103: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 7s 94ms/step - loss: 15225.3652 - rmse: 15251.5586 - val_loss: 16451.2598 - val_rmse: 16434.8730\n",
      "Epoch 104/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15317.0654 - rmse: 15347.1592\n",
      "Epoch 104: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 9s 125ms/step - loss: 15317.0654 - rmse: 15347.1592 - val_loss: 14953.5908 - val_rmse: 14956.8340\n",
      "Epoch 105/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15855.3486 - rmse: 15854.7559\n",
      "Epoch 105: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 9s 121ms/step - loss: 15855.3486 - rmse: 15854.7559 - val_loss: 15268.5039 - val_rmse: 15227.8232\n",
      "Epoch 106/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15682.4902 - rmse: 15673.0166\n",
      "Epoch 106: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 8s 116ms/step - loss: 15682.4902 - rmse: 15673.0166 - val_loss: 17740.0488 - val_rmse: 17743.0215\n",
      "Epoch 107/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 14997.7539 - rmse: 15028.5322\n",
      "Epoch 107: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 12s 164ms/step - loss: 14997.7539 - rmse: 15028.5322 - val_loss: 24324.4316 - val_rmse: 24328.9707\n",
      "Epoch 108/120\n",
      "70/70 [==============================] - ETA: 0s - loss: 15192.7217 - rmse: 15212.2695\n",
      "Epoch 108: val_loss did not improve from 12907.06934\n",
      "70/70 [==============================] - 9s 134ms/step - loss: 15192.7217 - rmse: 15212.2695 - val_loss: 19512.3105 - val_rmse: 19539.8008\n",
      "Epoch 108: early stopping\n",
      "Finish 10-fold cross validation\n",
      "Best performing model has 50500.27734375 validation loss (RMSE)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# modify to save ckpt for each test\n",
    "ckpt = os.path.join(ckpt_path, \"val_rmse_{val_rmse:.0f}.hdf5\")\n",
    "\n",
    "# training params\n",
    "epochs = epochs\n",
    "lr = lr\n",
    "\n",
    "# the k for k fold CV\n",
    "n_split = 10\n",
    "\n",
    "# for recording best performance\n",
    "min_loss = np.inf\n",
    "best_history = None\n",
    "\n",
    "'''\n",
    "k-fold cross validation\n",
    "Save the best model using validation accuracy as metric\n",
    "Print the global best performace when finished\n",
    "'''\n",
    "for train_index, test_index in KFold(n_split).split(train_examples):\n",
    "\n",
    "    x_train, x_vad = train_examples[train_index], train_examples[test_index]\n",
    "    y_train, y_vad = train_labels[train_index], train_labels[test_index]\n",
    "\n",
    "    model=create_reg_model(lr)\n",
    "  \n",
    "    # callbacks\n",
    "    checkpoint_filepath = ckpt\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='auto',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=25,\n",
    "        verbose=1,\n",
    "        mode='auto',\n",
    "    )\n",
    "\n",
    "    # Model weights are saved at the end of every epoch, if it's the best seen so far.\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_vad, y_vad),\n",
    "                        callbacks=[model_checkpoint_callback, early_stopping_callback])\n",
    "\n",
    "    val_loss = max(history.history['val_loss'])\n",
    "    \n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        best_history = history\n",
    "        # print('Best acc so far. Saving params...\\n')\n",
    "\n",
    "print('Finish {}-fold cross validation'.format(n_split))\n",
    "print('Best performing model has {} validation loss (RMSE)'.format(min_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 14:57:38.706077: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/evantilu/miniforge3/envs/6998_DL_tf/lib/python3.8/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x288560ee0>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = best_history\n",
    "\n",
    "train_rmse = history.history['rmse']\n",
    "val_rmse = history.history['val_rmse']\n",
    "\n",
    "epochs_range = range(len(history.history['loss']))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_rmse, label='Training RMSE')\n",
    "plt.plot(epochs_range, val_rmse, label='Validation RMSE')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "\n",
    "'''How far are predictions from real values?'''\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def format_tick_labels(x, pos):\n",
    "    return '{:.0f}k'.format(x/1000)\n",
    "\n",
    "predictions = model.predict(test_examples)\n",
    "\n",
    "xlims = [0, max(test_labels)*1.1]\n",
    "ylims = [0, max(predictions)*1.1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(test_labels, predictions)\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(format_tick_labels))\n",
    "ax.set_xlim(xlims)\n",
    "ax.set_ylim(ylims)\n",
    "ax.set_xlabel('Actual Price')\n",
    "ax.set_ylabel('Predicted Price')\n",
    "\n",
    "ax.plot(xlims, ylims, 'r-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 11ms/step - loss: 13880.7266 - rmse: 13770.8945\n",
      "\n",
      "evaluation on test set:\n",
      "loss (RMSE) = 13880.72656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 15:02:24.797010: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "model = create_reg_model()\n",
    "model.load_weights(os.path.join(ckpt_path, \"val_rmse_12866.hdf5\"))\n",
    "\n",
    "loss, acc = model.evaluate(test_examples, test_labels)\n",
    "\n",
    "print('\\nevaluation on test set:\\nloss (RMSE) = {:.5f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29cc21816e506614f017a9125cfdb1f5dc655865e499c52ef5f5406a40d25695"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
